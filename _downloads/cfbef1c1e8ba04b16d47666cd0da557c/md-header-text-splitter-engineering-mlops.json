[
    {
        "chunkId": "chunk0_0",
        "chunkContent": "Engineering Fundamentals Checklist  \nThis checklist helps to ensure that our projects meet our Engineering Fundamentals.  \nSource Control  \n[ ] The default target branch is locked.  \n[ ] Merges are done through PRs.  \n[ ] PRs reference related work items.  \n[ ] Commit history is consistent and commit messages are informative (what, why).  \n[ ] Consistent branch naming conventions.  \n[ ] Clear documentation of repository structure.  \n[ ] Secrets are not part of the commit history or made public. (see Credential scanning)  \n[ ] Public repositories follow the OSS guidelines, see Required files in default branch for public repositories.  \nMore details on source control  \nWork Item Tracking  \n[ ] All items are tracked in AzDevOps (or similar).  \n[ ] The board is organized (swim lanes, feature tags, technology tags).  \nMore details on backlog management  \nTesting  \n[ ] Unit tests cover the majority of all components (>90% if possible).  \n[ ] Integration tests run to test the solution e2e.  \nMore details on automated testing  \nCI/CD  \n[ ] Project runs CI with automated build and test on each PR.  \n[ ] Project uses CD to manage deployments to a replica environment before PRs are merged.  \n[ ] Main branch is always shippable.  \nMore details on continuous integration and continuous delivery  \nSecurity  \n[ ] Access is only granted on an as-needed basis  \n[ ] Secrets are stored in secured locations and not checked in to code  \n[ ] Data is encrypted in transit (and if necessary at rest) and passwords are hashed  \n[ ] Is the system split into logical segments with separation of concerns? This helps limiting security vulnerabilities.  \nMore details on security  \nObservability  \n[ ] Significant business and functional events are tracked and related metrics collected.  \n[ ] Application faults and errors are logged.  \n[ ] Health of the system is monitored.  \n[ ] The client and server side observability data can be differentiated.  \n[ ] Logging configuration can be modified without code changes (eg: verbose mode).  \n[ ] Incoming tracing context is propagated to allow for production issue debugging purposes.  \n[ ] GDPR compliance is ensured regarding PII (Personally Identifiable Information).  \nMore details on observability  \nAgile/Scrum  \n[ ] Process Lead (fixed/rotating) runs the daily standup  \n[ ] The agile process is clearly defined within team.  \n[ ] The Dev Lead (+ PO/Others) are responsible for backlog management and refinement.  \n[ ] A working agreement is established between team members and customer.  \nMore details on agile development  \nDesign Reviews  \n[ ] Process for conducting design reviews is included in the Working Agreement.  \n[ ] Design reviews for each major component of the solution are carried out and documented, including alternatives.  \n[ ] Stories and/or PRs link to the design document.  \n[ ] Each user story includes a task for design review by default, which is assigned or removed during sprint planning.  \n[ ] Project advisors are invited to design reviews or asked to give feedback to the design decisions captured in documentation.  \n[ ] Discover all the reviews that the customer's processes require and plan for them.  \n[ ] Clear non-functional requirements captured (see Non-Functional Requirements Guidance)  \n[ ] Risks and opportunities captured (see Risk/Opportunity Management)  \nMore details on design reviews  \nCode Reviews  \n[ ] There is a clear agreement in the team as to function of code reviews.  \n[ ] The team has a code review checklist or established process.  \n[ ] A minimum number of reviewers (usually 2) for a PR merge is enforced by policy.  \n[ ] Linters/Code Analyzers, unit tests and successful builds for PR merges are set up.  \n[ ] There is a process to enforce a quick review turnaround.  \nMore details on code reviews  \nRetrospectives  \n[ ] Retrospectives are conducted each week/at the end of each sprint.  \n[ ] The team identifies 1-3 proposed experiments to try each week/sprint to improve the process.  \n[ ] Experiments have owners and are added to project backlog.  \n[ ] The team conducts longer retrospective for Milestones and project completion.  \nMore details on retrospectives  \nEngineering Feedback  \n[ ] The team submits feedback on business and technical blockers that prevent project success  \n[ ] Suggestions for improvements are incorporated in the solution  \n[ ] Feedback is detailed and repeatable  \nMore details on engineering feedback  \nDeveloper Experience (DevEx)  \nDevelopers on the team can:  \n[ ] Build/Compile source to verify it is free of syntax errors and compiles.  \n[ ] Execute all automated tests (unit, e2e, etc).  \n[ ] Start/Launch end-to-end to simulate execution in a deployed environment.  \n[ ] Attach a debugger to started solution or running automated tests, set breakpoints, step through code, and inspect variables.  \n[ ] Automatically install dependencies by pressing F5 (or equivalent) in their IDE.  \n[ ] Use local dev configuration values (i.e. .env, appsettings.development.json).  \nMore details on developer experience",
        "source": "..\\data\\docs\\code-with-engineering\\ENG-FUNDAMENTALS-CHECKLIST.md"
    },
    {
        "chunkId": "chunk1_0",
        "chunkContent": "ISE Code-With Engineering Playbook  \nAn engineer working for a ISE project...  \nHas responsibilities to their team \u2013 mentor, coach, and lead.  \nKnows their playbook. Follows their playbook. Fixes their playbook if it is broken. If they find a better playbook, they copy it. If somebody could use their playbook, they share it.  \nLeads by example. Models the behaviors we desire both interpersonally and technically.  \nStrives to understand how their work fits into a broader context and ensures the outcome.  \nThis is our playbook. All contributions are welcome! Please feel free to submit a pull request to get involved.  \nWhy Have A Playbook  \nTo increase overall efficiency for team members and the whole team in general.  \nTo reduce the number of mistakes and avoid common pitfalls.  \nTo strive to be better engineers and learn from other people's shared experience.  \n\"The\" Checklist  \nIf you do nothing else follow the Engineering Fundamentals Checklist!  \nStructure of a Sprint  \nThe structure of a sprint is a breakdown of the sections of the playbook according to the structure of an Agile sprint.  \nGeneral Guidance  \nKeep the code quality bar high.  \nValue quality and precision over \u2018getting things done\u2019.  \nWork diligently on the one important thing.  \nAs a distributed team take time to share context via wiki, teams and backlog items.  \nMake the simple thing work now. Build fewer features today, but ensure they work amazingly. Then add more features tomorrow.  \nAvoid adding scope to a backlog item, instead add a new backlog item.  \nOur goal is to ship incremental customer value.  \nKeep backlog item details up to date to communicate the state of things with the rest of your team.  \nReport product issues found and provide clear and repeatable engineering feedback!  \nWe all own our code and each one of us has an obligation to make all parts of the solution great.  \nQuickLinks  \nEngineering Fundamentals Checklist  \nStructure of a Sprint  \nEngineering Fundamentals  \nAccessibility  \nAgile Development  \nAutomated Testing  \nCode Reviews  \nContinuous Delivery (CD)  \nContinuous Integration (CI)  \nDesign  \nDeveloper Experience  \nDocumentation  \nEngineering Feedback  \nObservability  \nSecurity  \nPrivacy  \nSource Control  \nReliability  \nFundamentals for Specific Technology Areas  \nMachine Learning Fundamentals  \nUser-Interface Engineering  \nContributing  \nSee CONTRIBUTING.md for contribution guidelines.",
        "source": "..\\data\\docs\\code-with-engineering\\index.md"
    },
    {
        "chunkId": "chunk2_0",
        "chunkContent": "Who We Are  \nOur team, ISE (Industry Solutions Engineering), works side by side with customers to help them tackle their toughest technical problems both in the cloud and on the edge. We meet customers where they are, work in the languages they use, with the open source frameworks they use, on the operating systems they use. We work with enterprises and start-ups across many industries from financial services to manufacturing. Our work covers a broad spectrum of domains including IoT, machine learning, and high scale compute. Our \"superpower\" is that we work closely with both our customers\u2019 engineering teams and Microsoft\u2019s product engineering teams, developing real-world expertise that we can use to help our customers grow their business and help Microsoft improve our products and services.  \nWe are very community focused in our work, with one foot in Microsoft and one foot in the open source communities that we help. We make pull requests on open source projects to add support for Microsoft platforms and/or improve existing implementations. We build frameworks and other tools to make it easier for developers to use Microsoft platforms. We source all the ideas for this work by maintaining very deep connections with these communities and the customers and partners that use them.  \nIf you like variety, coding in many languages, using any available tech across our industry, digging in with our customers, hack fests, occasional travel, and telling the story of what you\u2019ve done in blog posts and at conferences, then come talk to us.  \nYou can check out some of our work on our Developer Blog",
        "source": "..\\data\\docs\\code-with-engineering\\ISE.md"
    },
    {
        "chunkId": "chunk3_0",
        "chunkContent": "Structure of a Sprint  \nThe purpose of this document is to:  \nOrganize content in the playbook for quick reference and discoverability  \nProvide content in a logical structure which reflects the engineering process  \nExtensible hierarchy to allow teams to share deep subject-matter expertise  \nThe first week of an ISE Project  \nBefore starting the project  \n[ ] Discuss and start writing the Team Agreements. Update these documents with any process decisions made throughout the project  \nWorking Agreement  \nDefinition of Ready  \nDefinition of Done  \nEstimation  \n[ ] Set up the repository/repositories  \nDecide on repository structure/s  \nAdd README.md, LICENSE, CONTRIBUTING.md, .gitignore, etc  \n[ ] Build a Product Backlog  \nSet up a project in your chosen project management tool (ex. Azure DevOps)  \nINVEST in good User Stories and Acceptance Criteria  \nNon-Functional Requirements Guidance  \nDay 1  \n[ ] Plan the first sprint  \nAgree on a sprint goal, and how to measure the sprint progress  \nDetermine team capacity  \nAssign user stories to the sprint and split user stories into tasks  \nSet up Work in Progress (WIP) limits  \n[ ] Decide on test frameworks and discuss test strategies  \nDiscuss the purpose and goals of tests and how to measure test coverage  \nAgree on how to separate unit tests from integration, load and smoke tests  \nDesign the first test cases  \n[ ] Decide on branch naming  \n[ ] Discuss security needs and verify that secrets are kept out of source control  \nDay 2  \n[ ] Set up Source Control  \nAgree on best practices for commits  \n[ ] Set up basic Continuous Integration with linters and automated tests  \n[ ] Set up meetings for Daily Stand-ups and decide on a Process Lead  \nDiscuss purpose, goals, participants and facilitation guidance  \nDiscuss timing, and how to run an efficient stand-up  \n[ ] If the project has sub-teams, set up a Scrum of Scrums  \nDay 3  \n[ ] Agree on code style and on how to assign Pull Requests  \n[ ] Set up Build Validation for Pull Requests (2 reviewers, linters, automated tests) and agree on Definition of Done  \n[ ] Agree on a Code Merging strategy and update the CONTRIBUTING.md  \n[ ] Agree on logging and observability frameworks and strategies  \nDay 4  \n[ ] Set up Continuous Deployment  \nDetermine what environments are appropriate for this solution  \nFor each environment discuss purpose, when deployment should trigger, pre-deployment approvers, sing-off for promotion.  \n[ ] Decide on a versioning strategy  \n[ ] Agree on how to Design a feature and conduct a Design Review  \nDay 5  \n[ ] Conduct a Sprint Demo  \n[ ] Conduct a Retrospective  \nDetermine required participants, how to capture input (tools) and outcome  \nSet a timeline, and discuss facilitation, meeting structure etc.  \n[ ] Refine the Backlog  \nDetermine required participants  \nUpdate the Definition of Ready  \nUpdate estimates, and the Estimation document  \n[ ] Submit Engineering Feedback for issues encountered",
        "source": "..\\data\\docs\\code-with-engineering\\SPRINT-STRUCTURE.md"
    },
    {
        "chunkId": "chunk4_0",
        "chunkContent": "Accessibility  \nAccessibility is a critical component of any successful project and ensures the solutions we build are usable and enjoyed by as many people as possible. While meeting accessibility compliance standards is required, accessibility is much broader than compliance alone. Accessibility is about using techniques like inclusive design to infuse different perspectives and the full range of human diversity into the products we build. By incorporating accessibility into your project from the initial envisioning through MVP and beyond, you are promoting a more inclusive environment for your team and helping close the \"Disability Divide\" that exists for many people living with disabilities.  \nGetting Started  \nIf you are new to accessibility or are looking for an overview of accessibility fundamentals, Microsoft Learn offers a great training course that covers a broad range of topics from creating accessible content in Office to designing accessibility features in your own apps. You can learn more about the course or get started at Microsoft Learn: Accessibility Fundamentals.  \nInclusive Design  \nInclusive design is a methodology that embraces the full range of human diversity as a resource to help build better products and services. Inclusive design compliments accessibility going beyond accessibility compliance standards to ensure products are usable and enjoyed by all people. By leveraging the inclusive design methodology early in a project, you can expect a more inclusive and better solution for everyone. The Microsoft Inclusive Design website offers a variety of resources for incorporating inclusive design in your projects including inclusive design activities that can be used in envisioning and architecture design sessions.  \nThe Microsoft Inclusive Design methodology includes the following principles:  \nRecognize exclusion  \nDesigning for inclusivity not only opens up our products and services to more people, it also reflects how people really are. All humans grow and adapt to the world around them and we want our designs to reflect that.  \nSolve for one, extend to many  \nEveryone has abilities, and limits to those abilities. Designing for people with permanent disabilities actually results in designs that benefit people universally. Constraints are a beautiful thing.  \nLearn from diversity  \nHuman beings are the real experts in adapting to diversity. Inclusive design puts people in the center from the very start of the process, and those fresh, diverse perspectives are the key to true insight.  \nTools  \nAccessibility Insights  \nAccessibility Insights is a free, open-source solution for identifying accessibility issues in Windows, Android, and web applications. Accessibility Insights can identify a broad range of accessibility issues including problems with missing image alt tags, heading organization, tab order, color contrast, and many more. In addition, you can use Accessibility Insights to simulate color blindness to ensure your user interface is accessible to those that experience some form of color blindness. You can download Accessibility Insights here: https://accessibilityinsights.io/downloads/  \nAccessibility Linter  \nDeque Systems are web accessibility experts that provide accessibility training and tools to many organizations including Microsoft. One of the many tools offered by Deque is the axe Accessibility Linter for VS Code. This VS Code extension use the axe-core rules engine to identify accessibility issues in HTML, Angular, React, Markdown, and Vue. Using an accessibility linter can help ensure accessibility issues get addressed early in the development lifecycle.  \nPractices  \nAccessibility Testing  \nAccessibility testing is a specialized subset of software testing and includes automated tools and manual testing processes that vary from project to project. In addition to tools like Accessibility Insights discussed earlier, there are many other solutions for accessibility testing. The W3C provides a comprehensive list of evaluation and testing tools on their website at https://www.w3.org/WAI/ER/tools/.  \nIf you are looking to add automated testing to your Azure Pipelines, you may want to consider the Accessibility Testing extension built by Drew Lewis, a former Microsoft employee.  \nIt's important to keep in mind that automated tooling alone is not enough - make sure to augment your automated tests with manual ones. Accessibility Insights (linked above) can guide users through some manual testing steps.  \nCode and Documentation Basics  \nBefore you get to testing, you can make some small changes in how you write code and documentation.  \nDocument! Beyond text documentation, this also means code comments, clear variable and file naming, and pipeline or script outputs that clearly report success or failure and give details.  \nAvoid small case for variable and file names, hashtags, neologisms, etc. Use camelCase, snake_case, or other methods of creating separation between words.  \nIntroduce abbreviations by spelling the full term out, then the abbreviation in parentheses.  \nUse headers effectively to break up content by topic. Don't use more than one h1 per page, and don't skip levels (e.g. use an h3 directly under an h1). Avoid using formatting to make something look like a header when it's not.  \nUse descriptive link text. Avoid attaching a link to phrases like \"Read more\" and ensure that the text directly states what it links to. Link text should be able to stand on its own.  \nWhen including images or diagrams, add alt text. This should never just be \"Image\" or \"Diagram\" (or similar). In your description, highlight the purpose of the image or diagram in the page and what it is intended to convey.  \nPrefer tabs to spaces when possible. This allows users to default to their preferred tab width, so users with a range of vision can all take in code easily.  \nAdditional Resources  \nMicrosoft Accessibility Technology & Tools  \nWeb Content Accessibility Guidelines (WCAG)  \nAccessibility Guidelines and Requirements | Microsoft Style Guide  \nGoogle Developer Style Guide: Write Accessible Documentation",
        "source": "..\\data\\docs\\code-with-engineering\\accessibility\\README.md"
    },
    {
        "chunkId": "chunk5_0",
        "chunkContent": "Agile documentation  \nAgile Basics: Learn or refresh your basic agile knowledge.  \nAgile Core Expectations: What are our core expectations from an Agile team.  \nAgile Advanced Topics: Go beyond the basics.",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\README.md"
    },
    {
        "chunkId": "chunk6_0",
        "chunkContent": "Agile Development advanced topics  \nDocumentation that help you going beyond the basics and core expectations.  \nBacklog Management  \nCollaboration  \nEffective Organization  \nTeam Agreements",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\README.md"
    },
    {
        "chunkId": "chunk7_0",
        "chunkContent": "External Feedback  \nVarious stakeholders can provide feedback to the working product during a project, beyond any formal\nreview and feedback sessions required by the organization. The frequency and method of collecting\nfeedback through reviews varies depending on the case, but a couple of good practices are:  \nCapture each review in the backlog as a separate user story.  \nStandardize the tasks that implement this user story.  \nPlan for a review user story per Epic / Feature in your backlog proactively.",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\backlog-management\\external-feedback.md"
    },
    {
        "chunkId": "chunk8_0",
        "chunkContent": "Minimalism Slices  \nAlways deliver your work using minimal valuable slices  \nSplit your work item into small chunks that are contributed in incremental commits.  \nContribute your chunks frequently. Follow an iterative approach by regularly providing updates and changes to the team. This allows for instant feedback and early issue discovery and ensures you are developing in the right direction, both technically and functionally.  \nDo NOT work independently on your task without providing any updates to your team.  \nExample  \nImagine you are working on adding UWP (Universal Windows Platform) application building functionality for existing continuous integration service which already has Android/iOS support.  \nBad approach  \nAfter six weeks of work you created PR with all required functionality, including portal UI (build settings), backend REST API (UWP build functionality), telemetry, unit and integration tests, etc.  \nGood approach  \nYou divided your feature into smaller user stories (which in turn were divided into multiple tasks) and started working on them one by one:  \nAs a user I can successfully build UWP apps using current service  \nAs a user I can see telemetry when building the apps  \nAs a user I have the ability to select build configuration (debug, release)  \nAs a user I have the ability to select target platform (arm, x86, x64)  \n...  \nYou also divided your stories into smaller tasks and sent PRs based on those tasks.\nE.g. you have the following tasks for the first user story above:  \nEnable UWP platform on backend  \nAdd build button to the UI (build first solution file found)  \nAdd select solution file dropdown to the UI  \nImplement unit tests  \nImplement integration tests to verify build succeeded  \nUpdate documentation  \n...  \nResources  \nMinimalism Rules",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\backlog-management\\minimal-slices.md"
    },
    {
        "chunkId": "chunk9_0",
        "chunkContent": "Advanced recommendations for Backlog Management  \nExternal Feedback  \nMinimal slices  \nRisk Management",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\backlog-management\\README.md"
    },
    {
        "chunkId": "chunk10_0",
        "chunkContent": "Risk Management  \nAgile methodologies are conceived to be driven by risk management principles, but no methodology can eliminate all risks.  \nGoal  \nAnticipation is a key aspect of software project management, involving the proactive identification and assessment of potential risks and challenges to enable effective planning and mitigation strategies.  \nThe following guidance aims to provide decision-makers with the information needed to make informed choices, understanding trade-offs, costs, and project timelines throughout the project.  \nGeneral Guidance  \nIdentify risks in every activity such as a planning meetings, design and code reviews, or daily standups. All team members are responsible for identifying relevant risks.  \nAssess risks in terms of their likelihood and potential impact on the project. Use the issues to report and track risks. Issues represent unplanned activities.  \nPrioritize them based on their severity and likelihood, focusing on addressing the most critical ones first.  \nMitigate or reduce the impact and likelihood of the risks.  \nMonitor continuously to ensure the effectiveness of the mitigation strategies.  \nPrepare contingency plans for high-impact risks that may still materialize.  \nCommunicate and report risks to keep all stakeholders informed.  \nOpportunity Management  \nThe same process can be applied to opportunities, but while risk management involves applying mitigation actions to decrease the likelihood of a risk, in opportunity management, you enhance actions to increase the likelihood of a positive outcome.",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\backlog-management\\risk-management.md"
    },
    {
        "chunkId": "chunk11_0",
        "chunkContent": "How to add a Pairing Custom Field in Azure DevOps User Stories  \nThis document outlines the benefits of adding a custom field of type Identity in Azure DevOps user stories, prerequisites, and a step-by-step guide.  \nBenefits of adding a custom field  \nHaving the names of both individuals pairing on a story visible on the Azure DevOps cards can be helpful during sprint ceremonies and lead to greater accountability by the pairing assignee. For example, it is easier to keep track of the individuals assigned stories as part of a pair during sprint planning by using the \"pairing names\" field. During stand-up it can also help the Process Lead filter stories assigned to the individual (both as an owner or as a pairing assignee) and show these on the board. Furthermore, the pairing field can provide an additional data point for reports and burndown rates.  \nPrerequisites  \nPrior to customizing Azure DevOps, review Configure and customize Azure Boards.  \nIn order to add a custom field to user stories in Azure DevOps changes must be made as an Organizational setting. This document therefore assumes use of an existing Organization in Azure DevOps and that the user account used to make these changes is a member of the Project Collection Administrators Group.  \nChange the organization settings  \nDuplicate the process currently in use.\nNavigate to the Organization Settings, within the Boards / Process tab.  \nSelect the Process type, click on the icon with three dots ... and click Create inherited process.  \nClick on the newly created inherited process.\nAs you can see in the example below, we called it 'Pairing'.  \nClick on the work item type User Story.  \nClick New Field.  \nGive it a Name and select Identity in Type. Click on Add Field.  \nThis completes the change in Organization settings. The rest of the instructions must be completed under Project Settings.  \nChange the project settings  \nGo to the Project that is to be modified, select Project Settings.  \nSelect Project configuration.  \nClick on process customization page.  \nClick on Projects then click on Change process.  \nChange the target process to Pairing then click Save.  \nGo to Boards.  \nClick on the Gear icon to open Settings.  \nAdd field to card.\nClick on the green + icon to add select the Pairing field. Check the box to display fields, even when they are empty. Save and close.  \nView the modified the card.\nNotice the new Pairing field. The Story can now be assigned an Owner and a Pairing assignee!",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\collaboration\\add-pairing-field-azure-devops-cards.md"
    },
    {
        "chunkId": "chunk12_0",
        "chunkContent": "Effortless Pair Programming with GitHub Codespaces and VSCode  \nPair programming used to be a software development technique in which two programmers work together on a single computer, sharing one keyboard and mouse, to jointly design, code, test, and debug software. It is one of the patterns explored in the section why collaboration? of this playbook, however with teams that work mostly remotely, sharing a physical computer became a challenge, but opened the door to a more efficient approach of pair programming.  \nThrough the effective utilization of a range of tools and techniques, we have successfully implemented both pair and swarm programming methodologies. As such, we are eager to share some of the valuable insights and knowledge gained from this experience.  \nHow to make pair programming a painless experience?  \nWorking Sessions  \nIn order to enhance pair programming capabilities, you can create regular working sessions that are open to all team members. This facilitates smooth and efficient collaboration as everyone can simply join in and work together before branching off into smaller groups. This approach has proven particularly beneficial for new team members who may otherwise feel overwhelmed by a large codebase. It emulates the concept of the \"humble water cooler,\" which fosters a sense of connectedness among team members through their shared work.  \nAdditionally, scheduling these working sessions in advance ensures intentional collaboration and provides clarity on user story responsibilities. To this end, assign a single person to each user story to ensure clear ownership and eliminate ambiguity. By doing so, this could eliminate the common problem of engineers being hesitant to modify code outside of their assigned tasks due to the sentiment of lack of ownership. These working sessions are instrumental in promoting a cohesive team dynamic, allowing for effective knowledge sharing and collective problem-solving.  \nGitHub Codespaces  \nGitHub Codespaces is a vital component in an efficient development environment, particularly in the context of pair programming. Prioritize setting up a Codespace as the initial step of the project, preceding tasks such as local machine project compilation or VSCode plugin installation. To this end, make sure to update the Codespace documentation before incorporating any quick start instructions for local environments. Additionally, consistently demonstrate demos in codespaces environment to ensure its prominent integration into our workflow.  \nWith its cloud-based infrastructure, GitHub Codespaces presents a highly efficient and simplified approach to real-time collaborative coding. As a result, new team members can easily access the GitHub project and begin coding within seconds, without requiring installation on their local machines. This seamless, integrated solution for pair programming offers a streamlined workflow, allowing you to direct your attention towards producing exemplary code, free from the distractions of cumbersome setup processes.  \nVSCode Live Share  \nVSCode Live Share is specifically designed for pair programming and enables you to work on the same codebase, in real-time, with your team members. The arduous process of configuring complex setups, grappling with confusing configurations, straining one's eyes to work on small screens, or physically switching keyboards is not a problem with LiveShare. This innovative solution enables seamless sharing of your development environment with your team members, facilitating smooth collaborative coding experiences.  \nFully integrated into Visual Studio Code and Visual Studio, LiveShare offers the added benefit of terminal sharing, debug session collaboration, and host machine control. When paired with GitHub Codespaces, it presents a potent tool set for effective pair programming.  \nTip: Share VSCode extensions (including Live Share) using a base devcontainer.json. This ensure all team members have available the same set of extensions, and allow them to focus in solving the business needs from day one.  \nResources  \nGitHub Codespaces.  \nVSCode Live Share.  \nCreate a Dev Container.  \nHow companies have optimized the humble office water cooler.",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\collaboration\\pair-programming-tools.md"
    },
    {
        "chunkId": "chunk13_0",
        "chunkContent": "Advanced recommendations for collaboration  \nWhy Collaboration  \nHow to use the \"Social Question of the Day\"  \nEngagement Team Development  \nPair and Swarm programming  \nVirtual Collaboration and Pair Programming  \nHow to add a Pairing Custom Field in Azure DevOps User Stories  \nEffortless Pair Programming with GitHub Codespaces and VSCode",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\collaboration\\README.md"
    },
    {
        "chunkId": "chunk14_0",
        "chunkContent": "Social Question of the Day  \nThe social question of the day is an optional short question to follow the three project questions in the daily stand-up. It develops team cohesion and interpersonal trust over the course of an engagement by facilitating the sharing of personal preferences, lifestyle, or other context.  \nThe social question should be chosen before the stand-up. The facilitator should select the question either independently or from the team's asynchronous suggestions. This minimizes delays at the start of the stand-up.  \nTip: having the stand-up facilitator role rotate each sprint lets the facilitator choose the social question independently without burdening any one team member.  \nProperties of a good question  \nA good question has a brief answer with small optional elaboration. A yes or no answer doesn't tell you very much about someone, while knowing that their favorite fruit is a durian is informative.  \nGood questions are low in consequence but allow controversy. Watching someone strongly exclaim that salmon and lox on cinnamon-raisin is the best bagel order is endearing. As a corollary, a good question is one someone is likely to be passionate about. You know a little more about a team member's personality if their eyes light up when describing their favorite karaoke song.  \nStarter list of questions  \nPotentially good questions include:  \nWhat's your Starbucks order?  \nWhat's your favorite operating system?  \nWhat's your favorite version of Windows?  \nWhat's your favorite plant, houseplant or otherwise?  \nWhat's your favorite fruit?  \nWhat's your favorite fast food?  \nWhat's your favorite noodle?  \nWhat's your favorite text editor?  \nMountains or beach?  \nDC or Marvel?  \nCoffee with one person from history: who?  \nWhat's your silliest online purchase?  \nWhat's your alternate career?  \nWhat's the best bagel topping?  \nWhat's your guilty TV pleasure?  \nWhat's your go-to karaoke song?  \nWould you rather see the past or the future?  \nWould you rather be able to teleport or to fly?  \nWould you rather live underwater or in space for a year?  \nWhat's your favorite phone app?  \nWhat's your favorite fish, to eat or otherwise?  \nWhat was your best costume?  \nWho is someone you admire (from history, from your personal life, etc.)? Give one reason why.  \nWhat's the best compliment you've ever received?  \nWhat's your favorite or most used emoji right now?  \nWhat was your biggest DIY project?  \nWhat's a spice that you use on everything?  \nWhat's your top Spotify (or just your favorite) genre/artist for this year?  \nWhat was your first computer?  \nWhat's your favorite kind of taco?  \nWhat's your favorite decade?  \nWhat's the best way to eat potatoes?  \nWhat was your best vacation (stay-cations acceptable)?  \nFavorite cartoon?  \nPick someone in your family and tell us something awesome about them.  \nWhat was your longest road trip?  \nWhat thing do you remember learning when you were young that is taught differently now?  \nWhat was your favorite toy as a child?",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\collaboration\\social-question.md"
    },
    {
        "chunkId": "chunk15_0",
        "chunkContent": "Engagement Team Development  \nIn every ISE engagement, dynamics are different so are the team requirements. Based on transfer learning among teams, we aim to build right \"code-with\" environments in every team.  \nThis documentation gives a high-level template with some suggestions by aiming to accelerate team swarming phase to achieve a high speed agility however it has no intention to provide a list of \"must-do\" items.  \nIdentification  \nAs it's stated in Tuckman's team phases, traditional team development has several stages.\nHowever those phases can be extremely fast or sometimes mismatched in teams due to external factors, what applies to ISE engagements.  \nIn order to minimize the risk and set the expectations on the right way for all parties, an identification phase is important to understand each other.\nSome potential steps in this phase may be as following (not limited):  \nWorking agreement  \nIdentification of styles/preferences in communication, sharing, learning, decision making of each team member  \nTalking about necessity of pair programming  \nDecisions on backlog management & refinement meetings, weekly design sessions, social time sessions...etc.  \nSync/Async communication methods, work hours/flexible times  \nDecisions and identifications of charts that will be helpful to provide transparent and true information to everyone  \nIdentification of \"Software Craftspersonship\" areas which means the tools and methods will be widely used during the engagement and taking the required actions on team upskilling side if necessary.  \nGitHub, VSCode LiveShare, AzDevOps, necessary development tools & libraries ... more.  \nIf upskilling on certain topic(s) is needed, identifying the areas and arranging code spikes for increasing the team knowledge on the regarding topic(s).  \nIdentification of communication channels, feedback loops and recurrent team call slots out of regular sprint meetings  \nIntroduction to Technical Agility Team Manifesto and planning the technical delivery by aiming to keep\ntechnical debt risk minimum.  \nFollowing the Plan and Agile Debugging  \nIdentification phase accelerates the process of building a safe environment for every individual in the team, later on team has the required assets to follow the plan.\nAnd it is team's itself responsibility (engineers,PO,Process Lead) to debug their Agility level.  \nIn every team stabilization takes time and pro-active agile debugging is the best accelerator to decrease the distraction away from sprint/engagement goal.\nTeam is also responsible to keep the plan up-to-date based on team changes/needs and debugging results.  \nJust as an example, agility debugging activities may include:  \nDashboards related with \"Goal\" such as burndown/burnout, Item/PR Aging, Mood Chart ..etc. are accessible to the team and team is always up-to-date  \nBacklog Refinement meetings  \nSize of stories (Too big? Too small?)  \nAre \"User Stories\" and \"Tasks\" clear ?  \nAre Acceptance Criteria enough and right?  \nIs everyone ready-to-go after taking the User Story/Task?  \nRunning Efficient Retrospectives  \nIs the Sprint Goal clear in every iteration ?  \nIs the estimation process in the team improving over time or does it meet the delivery/workload prediction?  \nKindly check Scrum Values to have a better understanding to improve team commitment.  \nFollowing that, above suggestions aim to remove agile/team disfunctionalities and provide a broader team understanding, potential time savings and full transparency.  \nResources  \nTuckman's Stages of Group Development  \nScrum Values",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\collaboration\\teaming-up.md"
    },
    {
        "chunkId": "chunk16_0",
        "chunkContent": "Virtual Collaboration and Pair Programming  \nPair programming is the de facto work method that most large engineering organizations use for \u201chands on keyboard\u201d coding. Two developers, working synchronously, looking at the same screen and attempting to code and design together, which often results in better and clearer code than either could produce individually.  \nPair programming works well under the correct circumstances, but it loses some of its charm when executed in a completely virtual setting. The virtual setup still involves two developers looking at the same screen and talking out their designs, but there are often logistical issues to deal with, including lag, microphone set up issues, workspace and personal considerations, and many other small, individually trivial problems that worsen the experience.  \nVirtual work patterns are different from the in-person patterns we are accustomed to. Pair programming at its core is based on the following principles:  \nGenerating clarity through communication  \nProducing higher quality through collaboration  \nCreating ownership through equal contribution  \nPair programming is one way to achieve these results. Red Team Testing (RTT) is an alternate programming method that uses the same principles but with some of the advantages that virtual work methods provide.  \nRed Team Testing  \nRed Team Testing borrows its name from the \u201cRed Team\u201d and \u201cBlue Team\u201d paradigm of penetration testing, and is a collaborative, parallel way of working virtually. In Red Team Testing, two developers jointly decide on the interface, architecture, and design of the program, and then separate for the implementation phase. One developer writes tests using the public interface, attempting to perform edge case testing, input validation, and otherwise stress testing the interface. The second developer is simultaneously writing the implementation which will eventually be tested.  \nRed Team Testing has the same philosophy as any other Test-Driven Development lifecycle: All implementation is separated from the interface, and the interface can be tested with no knowledge of the implementation.  \nSteps  \nDesign Phase: Both developers design the interface together. This includes:  \nMethod signatures and names\nWriting documentation or docstrings for what the methods are intended to do.\nArchitecture decisions that would influence testing (Factory patterns, etc.)  \nImplementation Phase: The developers separate and parallelize work, while continuing to communicate.  \nDeveloper A will design the implementation of the methods, adhering to the previously decided design.\nDeveloper B will concurrently write tests for the same method signatures, without knowing details of the implementation.  \nIntegration & Testing Phase: Both developers commit their code and run the tests.  \nUtopian Scenario: All tests run and pass correctly.\nRealistic Scenario: The tests have either broken or failed due to flaws in testing. This leads to further clarification of the design and a discussion of why the tests failed.  \nThe developers will repeat the three phases until the code is functional and tested.  \nWhen to follow the RTT strategy  \nRTT works well under specific circumstances. If collaboration needs to happen virtually, and all communication is virtual, RTT reduces the need for constant communication while maintaining the benefits of a joint design session. This considers the human element: Virtual communication is more exhausting than in person communication.  \nRTT also works well when there is complete consensus, or no consensus at all, on what purpose the code serves. Since creating the design jointly and agreeing to implement and test against it are part of the RTT method, RTT forcibly creates clarity through iteration and communication.  \nBenefits  \nRTT has many of the same benefits as Pair Programming and Test-Driven development but tries to update them for a virtual setting.  \nCode implementation and testing can be done in parallel, over long distances or across time zones, which reduces the overall time taken to finish writing the code.  \nRTT maintains the pair programming paradigm, while reducing the need for video communication or constant communication between developers.  \nRTT allows detailed focus on design and engineering alignment before implementing any code, leading to cleaner and simpler interfaces.  \nRTT encourages testing to be prioritized alongside implementation, instead of having testing follow or be influenced by the implementation of the code.  \nDocumentation is inherently a part of RTT, since both the implementer and the tester need correct, up to date documentation, in the implementation phase.  \nWhat you need for RTT to work well  \nDemand for constant communication and good teamwork may pose a challenge; daily updates amongst team members are essential to maintain alignment on varying code requirements.  \nClarity of the code design and testing strategy must be established beforehand and documented as reference. Lack of an established design will cause misalignment between the two major pieces of work and a need for time-consuming refactoring.  \nRTT does not work well if only one developer has knowledge of the overall design. Team communication is critical to ensuring that every developer involved in RTT is on the same page.",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\collaboration\\virtual-collaboration.md"
    },
    {
        "chunkId": "chunk17_0",
        "chunkContent": "Why Collaboration  \nWhy collaboration is important  \nIn engagements, we aim to be highly collaborative because when we code together, we perform better, have a higher sprint velocity, and have a greater degree of knowledge sharing across the team.  \nThere are two common patterns we use for collaboration: Pairing and swarming.  \nPair programming (\u201cpairing\u201d) - two software engineers assigned to, and working on, one shared story at a time during the sprint. The Dev Lead assigns a user story to two engineers -- one primary engineer (story owner) and one secondary engineer (pairing assignee).  \nSwarm programming (\u201cswarming\u201d) - three or more software engineers collaborating on a high-priority item to bring it to completion.  \nHow to pair program  \nAs mentioned, every story is intentionally assigned to a pair. The pairing assignee may be in the process of upskilling, nevertheless, they are equal partners in the development effort.\nBelow are some general guidelines for pairing:  \nUpon assignment of the story/product backlog item (PBI), the pair needs to be deliberate about defining how to work together and have a firm definition of the work to be completed. This information should be expressed clearly in the story\u2019s description and acceptance criteria. The expectations about this need to be communicated and agreed upon by both engineers and should be done prior to any actual working sessions.  \nThe story owner and pairing assignee do not merely split the work up and sync regularly \u2013 they actively work together on the same tasks, and might share their screens via a Teams online session. Collaborative tools like VS Live Share can be preferable to sharing screens. Not all collaboration needs to be screen-share based.  \nDuring the collaborative sessions, one engineer provides the development environment while the other actively views and comments verbally.  \nEngineers trade places often from one session to the next so that everyone has time in control of the keyboard.  \nEngineers leverage feature branches for the collaboration during the development of each story to have small Pull Requests (PRs) (as opposed to a single giant PR) at the end of the sprint.  \nCode is committed to the repository by both members of the assigned pair where and when it makes sense as tasks were completed.  \nThe pairing assignee is the voice representing the pair during the daily standup while being supported by the story owner.  \nHaving the names of both individuals (owner and pair assignee) visible on the PBI can be helpful during sprint ceremonies and lead to greater accountability by the pairing assignee. An example of this using Azure DevOps cards can be found here.  \nWhy pair programming helps collaboration  \nPair programming helps collaboration because both engineers share equal responsibility for bringing the story to completion. This is a mutually beneficial exercise because, while the story owner often has more experience to lean on, the pairing assignee brings a fresh view that is unclouded by repetition.  \nSome other benefits include:  \nFewer defects and increased accountability. Having two sets of eyes allows the engineers more opportunity to catch errors and to remember often-overlooked tasks such as writing unit and integration tests.  \nPairing allows engineers with different experience and expertise to learn from one another by collaborating and receiving feedback in real-time. Instead of having an engineer work alone on a task for long hours and hit an isolation breaking point, pairing allows the pair to check in with one another.  \nEven something as simple as describing the problem out loud can help uncover issues or bugs in the code.  \nPairing can help brainstorming as well as validating details such as making the variable names consistent.  \nWhen to swarm program  \nIt is important to know that not every PBI needs to use swarming. Some sprints may not even warrant swarming at all.\nSwarm when:  \nThe work is complex enough to have collective minds collaborating (not because the quantity of work is more than what would be completed in one sprint).  \nThe task that the swarm works on has become (or is in imminent danger of becoming) a blocker to other stories.  \nAn unknown is discovered that needs a collaborative effort to form a decision on how to move forward. The collective knowledge and expertise help move the story forward more quickly and ultimately produced better quality code.  \nA conflict or unresolved difference of opinion arises during a pairing session. Promote the work to become a swarming session to help resolve the conflict.  \nHow to swarm program  \nAs soon the pair finds out that the PBI will warrant swarming, the pair brings it up to the rest of the team (via parking lot during stand-up or asynchronously). Members of the team agree or volunteer to assist.  \nThe story owner (or pairing assignee) sends Teams call invite to the interested parties. This allows the swarm to have dedicated focus time by blocking time in calendars.  \nDuring a swarming session, an engineer can branch out if there is something that needs to be handled while the swarm tackles the main problem at hand, then reconnects and reports back. This allows the swarm to focus on a core aspect and to be all on the same page.  \nThe Teams call is repeated until resolution is found or alternative path forward is formulated.  \nWhy swarm programming helps collaboration  \nSwarming allows the collective knowledge and expertise of the team to come together in a focused and unified way.  \nNot only does swarming help close out the item faster, but it also helps the team understand each other\u2019s strengths and weaknesses.  \nAllows the team to build a higher level of trust and work as a cohesive unit.  \nWhen to decide to swarm, pair, and/or split  \nWhile a lot of time can be spent on pair programming, it does make sense to split the work when folks understand how the work will be carried out, and the work to be done is largely prescriptive.  \nOnce the story has been jointly tasked out by both engineers, the engineers may choose to tackle some tasks separately and then combine the work together at the end.  \nPair programming is more helpful when the engineers do not have perfect clarity about what is needed to be done or how it can be done.  \nSwarming is done when the two engineers assigned to the story need an additional sounding board or need expertise that other team members could provide.  \nBenefits of increased collaboration  \nKnowledge sharing and bringing ISE and customer engineers together in a \u2018code-with\u2019 manner is an important aspect of ISE engagements. This grows both our customers\u2019 and our ISE team\u2019s capability to build on Azure. We are responsible for demonstrating engineering fundamentals and leaving the customer in a better place after we disengage. This can only happen if we collaborate and engage together as a team. In addition to improved software quality, this also adds a beneficial social aspect to the engagements.  \nResources  \nHow to add a pairing custom field in Azure DevOps User Stories - adding a custom field of type Identity in Azure DevOps for pairing  \nOn Pair Programming - Martin Fowler  \nPair Programming hands-on lessons - these can be used (and adapted) to support bringing pair programming into your team (MS internal or including customers)  \nEffortless Pair Programming with GitHub Codespaces and VSCode",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\collaboration\\why-collaboration.md"
    },
    {
        "chunkId": "chunk18_0",
        "chunkContent": "Delivery Plan  \nGoals  \nWhile Scrum does not require and discourages planning more than one sprint at a time. Most of us work in enterprises where we are dependent outside teams (for example: marketing, sales, support).  \nA rough assessment of the planned project scope is achievable within a reasonable time frame and resources. The goal is to have a rough plan and estimate as a starting point, not to implement \"Agilefall.\"  \nNote that this is just a starting point to enable planning discussions. We expect the actual schedule to evolve and shift over time and that you will update the scope and timeline as you progress.  \nDelivery Plans ensure your teams are aligning with your organizational goals.  \nBenefits  \nAs you complete the assessment, you can push back on the scope, time frame or ask for more resources.  \nAs you progress in your project/product delivery, you can highlight risks to the scope, time frame, and resources.  \nApproach  \nOne approach you can take to accomplish is with stickies and a spreadsheet.  \nStep 1: Stack rank the features for everything in your backlog  \nFunctional Features  \n[Non-functional Features] (docs/TECH-LEADS-CHECKLIST.md)  \nUser Research and Design  \nTesting  \nDocumentation  \nKnowledge Transfer/Support Processes  \nStep 2: T-Shirt Features in terms of working weeks per person. In some scenarios, you have no idea how complex the work. In this situation, you can ask for time to conduct a spike (timebox the effort so you can get back on time).  \nStep 3: Calculate the capacity for the team based on the number of weeks person with his/her start and end date and minus holidays, vacation, conferences, training, and onboarding days. Also, minus time if the person is also working on defects and support.  \nStep 4: Based on your capacity, you know have the options  \nAsk for more resources. Caution: onboarding new resources take time.  \nReduce the scope to the most MVP.  Caution: as you trim more of the scope, it might not be valuable anymore to the customer. Consider a cupcake which is everything you need. You don't want to skim off the frosting.  \nAsk for more time. Usually, this is the most flexible, but if there is a marketing date that you need to hit, this might be as flexible.  \nTools  \nYou can also leverage one of these tools by creating your epics and features and add the weeks estimates.  \nThe Plans (Preview) feature on Azure DevOps will help you make a plan. Delivery Plans provide a schedule of stories or features your team plan to deliver. Delivery Plans show the scheduled work items by a sprint (iteration path) of selected teams against a calendar view.  \nConfluence JIRA, Trello, Rally, Asana, Basecamp, and GitHub Issues are other similar tools in the market (some are free, others you pay a monthly fee, or you can install on-prem) that you can leverage.",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\effective-organization\\delivery-plan.md"
    },
    {
        "chunkId": "chunk19_0",
        "chunkContent": "Advanced recommendations for a more effective organization  \nDelivery/Release plan  \nScrum of Scrum",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\effective-organization\\README.md"
    },
    {
        "chunkId": "chunk20_0",
        "chunkContent": "Scrum of Scrums  \nScrum of scrums is a technique used to scale Scrum to a larger group working towards the same project goal. In Scrum, we consider a team being too big when going over 10-12 individuals. This should be decided on a case by case basis. If the project is set up in multiple work streams that contain a fixed group of people and a common stand-up meeting is slowing down productivity: scrum of scrums should be considered. The team would identify the different subgroups that would act as a separate scrum teams with their own backlog, board and stand-up.  \nGoals  \nThe goal of the scrum of scrums ceremony is to give sub-teams the agility they need while not loosing visibility and coordination. It also helps to ensure that the sub-teams are achieving their sprint goals, and they are going in the right direction to achieve the overall project goal.  \nThe scrum of scrums ceremony happens every day and can be seen as a regular stand-up:  \nWhat was done the day before by the sub-team.  \nWhat will be done today by the sub-team.  \nWhat are blockers or other issues for the sub-team.  \nWhat are the blockers or issues that may impact other sub-teams.  \nThe outcome of the meeting will result in a list of impediments related to coordination of the whole project. Solutions could be: agreeing on interfaces between teams, discussing architecture changes, evolving responsibility boundaries, etc.  \nThis list of impediments is usually managed in a separate backlog but does not have to.  \nParticipation  \nThe common guideline is to have on average one person per sub-team to participate in the scrum of scrums. Ideally, the Process Lead of each sub-team would represent them in this ceremony. In some instances, the representative for the day is selected at the end of each sub-team daily stand-up and could change every day. In practice, having a fixed representative tends to be more efficient in the long term.  \nImpact  \nThis practice is helpful in cases of longer projects and with a larger scope, requiring more people. When having more people, it is usually easier to divide the project in sub-teams. Having a daily scrum of scrums improves communication, lowers the risk of integration issues and increases the project chances of success.  \nWhen choosing to implement Scrum of Scrums, you need to keep in mind that some team members will have additional meetings to coordinate and participate in. Also: all team members for each sub-team need to be updated on the decisions at a later point to ensure a good flow of information.  \nMeasures  \nThe easiest way to measure the impact is by tracking the time to resolve issues in the scrum of scrums backlog. You can also track issues reported during the retrospective related to global coordination (is it well done? can it be improved?).  \nFacilitation Guidance  \nThis should be facilitated like a regular stand-up.",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\effective-organization\\scrum-of-scrums.md"
    },
    {
        "chunkId": "chunk21_0",
        "chunkContent": "Definition of Done  \nTo close a user story, a sprint, or a milestone it is important to verify that the tasks are complete.  \nThe development team should decide together what their Definition of Done is and document this in the project. Below are some examples of checks to verify that the user story, sprint, task is completed.  \nFeature/User Story  \n[ ] Acceptance criteria are met  \n[ ] Refactoring is complete  \n[ ] Code builds with no error  \n[ ] Unit tests are written and pass  \n[ ] Existing Unit Tests pass  \n[ ] Sufficient diagnostics/telemetry are logged  \n[ ] Code review is complete  \n[ ] UX review is complete (if applicable)  \n[ ] Documentation is updated  \n[ ] The feature is merged into the develop branch  \n[ ] The feature is signed off by the product owner  \nSprint Goal  \n[ ] Definition of Done for all user stories included in the sprint are met  \n[ ] Product backlog is updated  \n[ ] Functional and Integration tests pass  \n[ ] Performance tests pass  \n[ ] End 2 End tests pass  \n[ ] All bugs are fixed  \n[ ] The sprint is signed off from developers, software architects, project manager, product owner etc.  \nRelease/Milestone  \n[ ] Code Complete (goals of sprints are met)  \n[ ] Release is marked as ready for production deployment by product owner",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\team-agreements\\definition-of-done.md"
    },
    {
        "chunkId": "chunk22_0",
        "chunkContent": "Definition of Ready  \nWhen the development team picks a user story from the top of the backlog, the user story needs to have enough detail to estimate the work needed to complete the story within the sprint. If it has enough detail to estimate, it is Ready to be developed.  \nIf a user story is not Ready in the beginning of the Sprint it increases the chance that the story will not be done at the end of this sprint.  \nWhat it is  \nDefinition of Ready is the agreement made by the scrum team around how complete a user story should be in order to be selected as candidate for estimation in the sprint planning. These can be codified as a checklist in user stories using GitHub Issue Templates or Azure DevOps Work Item Templates.  \nIt can be understood as a checklist that helps the Product Owner to ensure that the user story they wrote contains all the necessary details for the scrum team to understand the work to be done.  \nExamples of ready checklist items  \n[ ] Does the description have the details including any input values required to implement the user story?  \n[ ] Does the user story have clear and complete acceptance criteria?  \n[ ] Does the user story address the business need?  \n[ ] Can we measure the acceptance criteria?  \n[ ] Is the user story small enough to be implemented in a short amount of time, but large enough to provide value to the customer?  \n[ ] Is the user story blocked? For example, does it depend on any of the following:  \nThe completion of unfinished work  \nA deliverable provided by another team (code artifact, data, etc...)  \nWho writes it  \nThe ready checklist can be written by a Product Owner in agreement with the development team and the Process Lead.  \nWhen should a Definition of Ready be updated  \nUpdate or change the definition of ready anytime the scrum team observes that there are missing information in the user stories that recurrently impacts the planning.  \nWhat should be avoided  \nThe ready checklist should contain items that apply broadly. Don't include items or details that only apply to one or two user stories. This may become an overhead when writing the user stories.  \nHow to get stories ready  \nIn the case that the highest priority work is not yet ready, it still may be possible to make forward progress. Here are some strategies that may help:  \nBacklog Refinement sessions are a good time to validate that high priority user stories are verified to have a clear description, acceptance criteria and demonstrable business value. It is also a good time to breakdown large stories will likely not be completable in a single sprint.  \nPrioritization sessions are a good time to prioritize user stories that unblock other blocked high priority work.  \nBlocked user stories can often be broken down in a way that unblocks a portion of the original stories scope. This is a good way to make forward progress even when some work is blocked.",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\team-agreements\\definition-of-ready.md"
    },
    {
        "chunkId": "chunk23_0",
        "chunkContent": "Team Agreements  \nDefinition of Done  \nDefinition of Ready  \nWorking Agreements  \nTeam Manifesto  \nGoals  \nTeam agreements help clarify expectations for all team members, whether they are expectations around how the team works together (Working Agreements) or how to judge if a story is complete (Definition of Done).",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\team-agreements\\README.md"
    },
    {
        "chunkId": "chunk24_0",
        "chunkContent": "Team Manifesto  \nIntroduction  \nISE teams work with a new development team in each customer engagement which requires a phase of introduction & knowledge transfer before starting an engagement.  \nCompletion of this phase of ice-breakers and discussions about the standards takes time, but is required to start increasing the learning curve of the new team.  \nA team manifesto is a light-weight one page agile document among team members which summarizes the basic principles and values of the team and aiming to provide a consensus about technical expectations from each team member in order to deliver high quality output at the end of each engagement.  \nIt aims to reduce the time on setting the right expectations without arranging longer \"team document reading\" meetings and provide a consensus among team members to answer the question - \"How does the new team develop the software?\" - by covering all engineering fundamentals and excellence topics such as release process, clean coding, testing.  \nAnother main goal of writing the manifesto is to start a conversation during the \"manifesto building session\" to detect any differences of opinion around how the team should work.  \nIt also serves in the same way when a new team member joins to the team. New joiners can quickly get up to speed on the agreed standards.  \nHow to Build a Team Manifesto  \nIt can be said that the best time to start building it is at the very early phase of the engagement when teams meet with each other for swarming or during the preparation phase.  \nIt is recommended to keep team manifesto as simple as possible, so preferably, one-page simple document which doesn't include any references or links is a nice format for it.\nIf there is a need for providing knowledge on certain topics, the way to do is delivering brown-bag sessions, technical katas, team practices, documentations and others later on.  \nA few important points about the team manifesto  \nThe team manifesto is built by the development team itself  \nIt should cover all required technical engineering points for the excellence as well as behavioral agility mindset items that the team finds relevant  \nIt aims to give a common understanding about the desired expertise, practices and/or mindset within the team  \nBased on the needs of the team and retrospective results, it can be modified during the engagement.  \nIn ISE, we aim for quality over quantity, and well-crafted software as well as to a comfortable/transparent environment where each team member can reach their highest potential.  \nThe difference between the team manifesto and other team documents is that it is used to give a short summary of expectations around the technical way of working and supported mindset in the team, before code-with sprints starts.  \nBelow, you can find some including, but not limited, topics many teams touch during engagements,  \nTopic What is it about ? Collective Ownership Does team own the code rather than individuals? What is the expectation? Respect Any preferred statement about it's a \"must-have\" team value Collaboration Any preferred statement about how does team want to collaborate ? Transparency A simple statement about it's a \"must-have\" team value and if preferred, how does this being provided by the team ? meetings, retrospective, feedback mechanisms etc. Craftspersonship Which tools such as Git, VS Code LiveShare, etc. are being used ? What is the definition of expected best usage of them? PR sizing What does team prefer in PRs ? Branching Team's branching strategy and standards Commit standards Preferred format in commit messages, rules and more Clean Code Does team follow clean code principles ? Pair/Mob Programming Will team apply pair/mob programming ? If yes, what programming styles are suitable for the team ? Release Process Principles around release process such as quality gates, reviewing process ...etc. Code Review Any rule for code reviewing such as min number of reviewers, team rules ...etc. Action Readiness How the backlog will be refined? How do we ensure clear Definition of Done and Acceptance Criteria ? TDD Will the team follow TDD ? Test Coverage Is there any expected number, percentage or measurement ? Dimensions in Testing Required tests for high quality software, eg : unit, integration, functional, performance, regression, acceptance Build process build for all? or not; The clear statement of where code and under what conditions code should work ? eg : OS, DevOps, tool dependency Bug fix The rules of bug fixing in the team ? eg: contact people, attaching PR to the issue etc. Technical debt How does team manage/follow it? Refactoring How does team manage/follow it? Agile Documentation Does team want to use diagrams and tables more rather than detailed KB articles ? Efficient Documentation When is it necessary ? Is it a prerequisite to complete tasks/PRs etc.? Definition of Fun How will we have fun for relaxing/enjoying the team spirit during the engagement?  \nTools  \nGenerally team sessions are enough for building a manifesto and having a consensus around it, and if there is a need for improving it in a structured way, there are many blogs and tools online, any retrospective tool can be used.  \nResources  \nTechnical Agility*",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\team-agreements\\team-manifesto.md"
    },
    {
        "chunkId": "chunk25_0",
        "chunkContent": "Sections of a Working Agreement  \nA working agreement is a document, or a set of documents that describe how we work together as a team and what our\nexpectations and principles are.  \nThe working agreement created by the team at the beginning of the project, and is stored in the repository so that it is\nreadily available for everyone working on the project.  \nThe following are examples of sections and points that can be part of a working agreement but each team should compose\ntheir own, and adjust times, communication channels, branch naming policies etc. to fit their team needs.  \nGeneral  \nWe work as one team towards a common goal and clear scope  \nWe make sure everyone's voice is heard, listened to  \nWe show all team members equal respect  \nWe work as a team to have common expectations for technical delivery that are documented in a Team Manifesto.  \nWe make sure to spread our expertise and skills in the team, so no single person is relied on for one skill  \nAll times below are listed in CET  \nCommunication  \nWe communicate all information relevant to the team through the Project Teams channel  \nWe add all technical spikes, trade studies, and other technical documentation to the project repository through async design reviews in PRs  \nWork-life Balance  \nOur office hours, when we can expect to collaborate via Microsoft Teams, phone or face-to-face are Monday to Friday 10AM - 5PM  \nWe are not expected to answer emails past 6PM, on weekends or when we are on holidays or vacation.  \nWe work in different time zones and respect this, especially when setting up recurring meetings.  \nWe record meetings when possible, so that team members who could not attend live can listen later.  \nQuality and not Quantity  \nWe agree on a Definition of Done for our user story's and sprints and live by it.  \nWe follow engineering best practices like the Code With Engineering Playbook  \nScrum Rhythm  \nActivity When Duration Who Accountable Goal Project Standup Tue-Fri 9AM 15 min Everyone Process Lead What has been accomplished, next steps, blockers Sprint Demo Monday 9AM 1 hour Everyone Dev Lead Present work done and sign off on user story completion Sprint Retro Monday 10AM 1 hour Everyone Process Lead Dev Teams shares learnings and what can be improved Sprint Planning Monday 11AM 1 hour Everyone PO Size and plan user stories for the sprint Task Creation After Sprint Planning - Dev Team Dev Lead Create tasks to clarify and determine velocity Backlog refinement Wednesday 2PM 1 hour Dev Lead, PO PO Prepare for next sprint and ensure that stories are ready for next sprint.  \nProcess Lead  \nThe Process Lead is responsible for leading any scrum or agile practices to enable the project to move forward.  \nFacilitate standup meetings and hold team accountable for attendance and participation.  \nKeep the meeting moving as described in the Project Standup page.  \nMake sure all action items are documented and ensure each has an owner and a due date and tracks the open issues.  \nNotes as needed after planning / stand-ups.  \nMake sure that items are moved to the parking lot and ensure follow-up afterwards.  \nMaintain a location showing team\u2019s work and status and removing impediments that are blocking the team.  \nHold the team accountable for results in a supportive fashion.  \nMake sure that project and program documentation are up-to-date.  \nGuarantee the tracking/following up on action items from retrospectives (iteration and release planning) and from daily standup meetings.  \nFacilitate the sprint retrospective.  \nCoach Product Owner and the team in the process, as needed.  \nBacklog Management  \nWe work together on a Definition of Ready and all user stories assigned to a sprint need to follow this  \nWe communicate what we are working on through the board  \nWe assign ourselves a task when we are ready to work on it (not before) and move it to active  \nWe capture any work we do related to the project in a user story/task  \nWe close our tasks/user stories only when they are done (as described in the Definition of Done)  \nWe work with the PM if we want to add a new user story to the sprint  \nIf we add new tasks to the board, we make sure it matches the acceptance criteria of the user story (to avoid scope creep).\nIf it doesn't match the acceptance criteria we should discuss with the PM to see if we need a new user story for the task or if we should adjust the acceptance criteria.  \nCode Management  \nWe follow the git flow branch naming convention for branches and identify the task number e.g. feature/123-add-working-agreement  \nWe merge all code into main branches through PRs  \nAll PRs are reviewed by one person from [Customer/Partner Name] and one from Microsoft (for knowledge transfer and to ensure code and security standards are met)  \nWe always review existing PRs before starting work on a new task  \nWe look through open PRs at the end of stand-up to make sure all PRs have reviewers.  \nWe treat documentation as code and apply the same standards to Markdown as code",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\team-agreements\\working-agreements.md"
    },
    {
        "chunkId": "chunk26_0",
        "chunkContent": "Agile Development Basics  \nIf you are new to Agile development or if you are looking for a refresher, this section will provides links to information that provide best pracices for Backlog Management, Agile Ceremonies, Roles within Agile and Agile Sprints.  \nWhat is Agile  \nWhat is Agile Development  \nBacklog Management  \nCeremonies  \nRoles  \nSprints",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\basics\\README.md"
    },
    {
        "chunkId": "chunk27_0",
        "chunkContent": "Backlog Management basics for the Product and Sprint backlog  \nThis section has links directing you to best practices for managing Product and Sprint backlogs.  After reading through the best practices you should have a basic understanding for managing both product and sprint backlogs, how to create acceptance criteria for user stories, creating a definition of done and definition of ready for user stories and the basics around estimating user stories.  \nProduct Backlog  \nSprint Backlog  \nAcceptance Criteria  \nDefinition of Done  \nDefinition of Ready  \nEstimation Basics in Agile",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\basics\\Backlog Management\\README.md"
    },
    {
        "chunkId": "chunk28_0",
        "chunkContent": "Agile Ceremonies basics  \nThis section has links directing you to best practices for conducting the Agile ceremonies.  After reading through the best practices you should have a basic understanding of the key Agile ceremonies in terms of purpose, value and best practices around conducting and facilitating these ceremonies.  \nPlanning  \nRefinement  \nRetrospective  \nSprint Review/Demo  \nStand-Up/Daily Scrum",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\basics\\Ceremonies\\README.md"
    },
    {
        "chunkId": "chunk29_0",
        "chunkContent": "Agile/Scrum Roles  \nThis section has links directing you to definitions for the traditional roles within Agile/Scrum.  After reading through the best practices you should have a basic understanding of the key Agile roles in terms of what they are and the expectations for the role.  \nProduct Owner  \nScrum Master  \nDevelopment Team",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\basics\\Roles\\README.md"
    },
    {
        "chunkId": "chunk30_0",
        "chunkContent": "The Sprint  \nThis section has links directing you to best practices in regards to what a sprint is within agile and the practices around the sprint.  After reading through the best practices you should have a basic understanding of Sprint Planning and the Sprint Backlog, Sprint Execution and the Daily Standup, Sprint Review and Sprint Retrospective and the key output of the sprint which is called the Increment.  \nSprint Planning and the Sprint Backlog  \nSprint Execution and the Daily Standup  \nSprint Review and Sprint Retrospective  \nIncrement",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\basics\\Sprints\\README.md"
    },
    {
        "chunkId": "chunk31_0",
        "chunkContent": "Agile core expectations  \nThis section contains core expectations for agile practices in ISE:  \nIt should stay concise and link to external documentation for details.  \nEach section contains a list of core expectations and suggestions:  \nCore expectations are what each dev team is expected to aim for.  \nSuggestions are not expectations. They are our learned experience for meeting those expectations, and can be adopted and modified to suit the project.  \nNotes:  \nWe prefer the usage of \"process lead\" over \"scrum master\". It describes the same role.  \n\"Crew\", in this document, refers to the entire team working on an project (dev team, dev lead, PM, etc.).  \nWe follow Agile principles and usually Scrum  \nOverall expectations for a project  \nThe crew is predictable in their delivery.  \nThe crew makes relevant adjustments and shares these transparently.  \nRoles and Responsibilities are clarified and agreed before the project starts.  \nThe crew is driving continuous improvement of their own process to meet the core expectations and increase project success.  \nCore expectations and suggestions  \nSprints  \nExpectations:  \nSprint structure gives frequent opportunities for feedback and adjustment in the context of relatively small projects.  \nSprint ceremonies should be planned to accommodate working schedules of the team and take into consideration hard and soft time constraints.  \nSuggestions:  \nSprinting starts day 1: Game plan creation, game plan review and sharing are included in sprints and should be reflected in the backlog.  \nDefine a sprint goal that will be used to determine the success of the sprint.  \nNote: Sprints are usually 1 week long to increase the number of opportunities for adjustments. And minimize the risk of missing the sprint goal.  \nEstimation  \nExpectations:  \nEstimation supports the predictability of the team work and delivery.  \nEstimation re-enforces the value of accountability to the team.  \nThe estimation process is improved over time and discussed on a regular basis.  \nEstimation is inclusive of the different individuals in the team.  \nSuggestions:\nRough estimation is usually done for a generic SE 2 dev.  \nExample 1  \nThe team use t-shirt sizes (S, M, L, XL) and agrees in advance which size fits a sprint.  \nIn this example: S, M fits a sprint, L, XL too big for a sprint and need to be split / refined  \nThe dev lead with support of the team roughly estimates how much S and M stories can be done in the first sprints  \nThis rough estimation is refined over time and used to as an input for future sprint planning and to adjust project end date forecasting  \nExample 2  \nThe team uses a single indicator: \"does this story fits in one sprint?\", if not, the story needs to be split  \nThe dev lead with support of the team roughly estimates how many stories can be done in the first sprints  \nHow many stories are done in each sprint on average is used as an input for future sprint planning and as an indicator to adjust project end date forecasting  \nExample 3  \nThe team does planning poker and estimates in story points  \nStory points are roughly used to estimate how much can be done in next sprint  \nThe dev lead and the TPM uses the past sprints and observed velocity to adjust project end date forecasting  \nOther considerations  \nEstimating stories using story points in smaller project does not always provide the value it would in bigger ones.  \nAvoid converting story points or t-shirt sizes to days.  \nMeasure estimation accuracy:\nCollect data to monitor estimation accuracy and sprint completion over time to drive improvements.\nUse the sprint goal to understand if the estimation was correct. If the sprint goal is met: does anything else matter?  \nScrum Practices: While Scrum does not prescribe how to size work, Professional Scrum is biased away from absolute estimation (hours, function points, ideal-days, etc.) and towards relative sizing.\nPlanning Poker: is a collaborative technique to assign relative size. Developers may choose whatever units they want - story points and t-shirt sizes are examples of units.\n'Same-Size' PBIs is a relative estimation approach that involves breaking items down small enough that they are roughly the same size. Velocity can be understood as a count of PBIs; this is sometimes used by teams doing continuously delivery.\n'Right-Size' PBIs is a relative estimation approach that involves breaking things down small enough to deliver value in a certain time period (i.e. get to Done by the end of a Sprint). This is sometimes associated with teams utilizing flow for forecasting. Teams use historical data to determine if they think they can get the PBI done within the confidence level that their historical data says they typically get a PBI done.  \nLinks:  \nThe Most Important Thing You Are Missing about Estimation  \nSprint planning  \nExpectations:  \nThe planning supports Diversity and Inclusion principles and provides equal opportunities.  \nThe Planning defines how the work is going to be completed in the sprint.  \nStories fit in a sprint and are designed and ready before the planning.  \nSuggestions:  \nSprint goal:  \nConsider defining a sprint goal, or list of goals for each sprint. Effective sprint goals are a concise bullet point list of items. A Sprint goal can be created first and used as an input to choose the Stories for the sprint. A sprint goal could also be created from the list of stories that were picked for the Sprint.  \nThe sprint goal can be used :  \nAt the end of each stand up meeting, to remember the north star for the Sprint and help everyone taking a step back  \n*During the sprint review (\"was the goal achieved?\", \"If not, why?\")  \nNote: A simple way to define a sprint goal, is to create a User Story in each sprint backlog and name it \"Sprint XX goal\". You can add the bullet points in the description.  \nStories:  \nExample 1 - Preparing in advance:  \nThe dev lead and product owner plan time to prepare the sprint backlog ahead of sprint planning.  \nThe dev lead uses their experience (past and on the current project) and the estimation made for these stories to gauge how many should be in the sprint.  \nThe dev lead asks the entire team to look at the tentative sprint backlog in advance of the sprint planning.  \nThe dev lead assigns stories to specific developers after confirming with them that it makes sense  \nDuring the sprint planning meeting, the team reviews the sprint goal and the stories. Everyone confirm they understand the plan and feel it's reasonable.  \nExample 2 - Building during the planning meeting:  \nThe product owner ensures that the highest priority items of the product backlog is refined and estimated following the team estimation process.  \nDuring the Sprint planning meeting, the product owner describe each stories, one by one, starting by highest priority.  \nFor each story, the dev lead and the team confirm they understand what needs to be done and add the story to the sprint backlog.  \nThe team keeps considering more stories up to a point where they agree the sprint backlog is full. This should be informed by the estimation, past developer experience and past experience in this specific project.  \nStories are assigned during the planning meeting:\nOption 1: The dev lead makes suggestion on who could work on each stories. Each engineer agrees or discuss if required.\nOption 2: The team review each story and engineer volunteer select the one they want to be assigned to. (Note: this option might cause issues with the first core expectations. Who gets to work on what? Ultimately, it is the dev lead responsibility to ensure each engineer gets the opportunity to work on what makes sense for their growth.)  \nTasks:  \nExamples of approaches for task creation and assignment:  \nStories are split into tasks ahead of time by dev lead and assigned before/during sprint planning to engineers.  \nStories are assigned to more senior engineers who are responsible for splitting into tasks.  \nStories are split into tasks during the Sprint planning meeting by the entire team.  \nNote: Depending on the seniority of the team, consider splitting into tasks before sprint planning. This can help getting out of sprint planning with all work assigned. It also increase clarity for junior engineers.  \nLinks:  \nDefinition of Ready  \nSprint Goal Template  \nNotes: Self assignment by team members can give a feeling of fairness in how work is split in the team. Sometime, this ends up not being the case as it can give an advantage to the loudest or more experienced voices in the team. Individuals also tend to stay in their comfort zone, which might not be the right approach for their own growth.  \nBacklog  \nExpectations:  \nUser stories have a clear acceptance criteria and definition of done.  \nDesign activities are planned as part of the backlog (a design for a story that needs it should be done before it is added in a Sprint).  \nSuggestions:  \nConsider the backlog refinement as an ongoing activity, that expands outside of the typical \"Refinement meeting\".  \nTechnical debt is mostly due to shortcuts made in the implementation as well as the future maintenance cost as the natural result of continuous improvement. Shortcuts should generally be avoided. In some rare instances where they happen, prioritizing and planning improvement activities to reduce this debt at a later time is the recommended approach.  \nRetrospectives  \nExpectations:  \nRetrospectives lead to actionable items that help grow the team's engineering practices. These items are in the backlog, assigned, and prioritized to be fixed by a date agreed upon (default being next retrospective).  \nIs used to ask the hard questions (\"we usually don't finish what we plan, let's talk about this\") when necessary.  \nSuggestions:  \nConsider other retro formats available outside of Mad Sad Glad.  \nGather Data: Triple Nickels, Timeline, Mad Sad Glad, Team Radar  \nGenerate Insights: 5 Whys, Fishbone, Patterns and Shifts  \nConsider setting a retro focus area.  \nSchedule enough time to ensure that you can have the conversation you need to get the correct plan an action and improve how you work.  \nBring in a neutral facilitator for project retros or retros that introspect after a difficult period.  \nUse the following retrospectives techniques to address specific trends that might be  emerging on an engagement:  \n5 whys:  \nIf a team is confronting a problem and is unsure of the exact root cause, the 5 whys exercise taken from the business analysis sector can help get to the bottom of it.\u00a0For example, if a team cannot get to Done each Sprint, that would go at the top of the whiteboard. The team then asks why that problem exists, writing that answer in the box below.\u00a0 Next, the team asks why again, but this time in response to the why they just identified. Continue this process until the team identifies an actual root cause, which usually becomes apparent within five steps.  \nProcesses, tools, individuals, interactions and the Definition of Done:  \nThis approach encourages team members to think more broadly.\u00a0 Ask team members to identify what is going well and ideas for improvement within the categories of processes, tools, individuals/interactions, and the Definition of Done.\u00a0 Then, ask team members to vote on which improvement ideas to focus on during the upcoming Sprint.  \nFocus:  \nThis retrospective technique incorporates the concept of visioning. Using this technique, you ask team members where they would like to go?\u00a0 Decide what the team should look like in 4 weeks, and then ask what is holding them back from that and how they can resolve the impediment.\u00a0 If you are focusing on specific improvements, you can use this technique for one or two Retrospectives in a row so that the team can see progress over time.  \nSprint Demo  \nExpectations:  \nEach sprint has demos that illustrate the sprint goal and how it fits in the engagement goal.  \nSuggestions:  \nConsider not pre-recording sprint demos in advance. You can record the demo meeting and archive them.  \nA demo does not have to be about running code. It can be showing documentation that was written.  \nStand-up  \nExpectations:  \nThe stand-up is run efficiently.  \nThe stand-up helps the team understand what was done, what will be done and what are the blockers.  \nThe stand-up helps the team understand if they will meet the sprint goal or not.  \nSuggestions:  \nKeep stand up short and efficient. Table the longer conversations for a parking lot section, or for a conversation that will be planned later.  \nRun daily stand ups: 15 minutes of stand up and 15 minutes of parking lot.  \nIf someone cannot make the stand-up exceptionally: Ask them to do a written stand up in advance.  \nStand ups should include everyone involved in the project, including the customer.  \nProjects with widely divergent time zones should be avoided if possible, but if you are on one, you should adapt the standups to meet the needs and time constraints of all team members.  \nDocumentation  \nWhat Is Scrum?  \nAgile Retrospective: Making Good Teams Great  \nUser Stories Applied: For Software Development  \nEssential Scrum: A Practical Guide to The Most Popular Agile Process",
        "source": "..\\data\\docs\\code-with-engineering\\agile-development\\core-expectations\\README.md"
    },
    {
        "chunkId": "chunk32_0",
        "chunkContent": "Testing  \nMap of Outcomes to Testing Techniques  \nThe table below maps outcomes -- the results that you may want to achieve in your validation efforts -- to one or more techniques that can be used to accomplish that outcome.  \nWhen I am working on... I want to get this outcome... ...so I should consider Development Prove backward compatibility with existing callers and clients Shadow testing Development Ensure program logic is correct for a variety of expected, mainline, edge and unexpected inputs Unit testing ; Functional tests; Consumer-driven Contract Testing ; Integration testing Development Prevent regressions in logical correctness; earlier is better Unit testing ; Functional tests; Consumer-driven Contract Testing ; Integration testing ; Rings (each of these are expanding scopes of coverage) Development Validate interactions between components in isolation, ensuring that consumer and provider components are compatible and conform to a shared understanding documented in a contract Consumer-driven Contract Testing Development; Integration testing Validate that multiple components function together across multiple interfaces in a call chain, incl network hops Integration testing ; End-to-end ( End-to-End testing ) tests; Segmented end-to-end ( End-to-End testing ) Development Prove disaster recoverability \u2013 recover from corruption of data DR drills Development Find vulnerabilities in service Authentication or Authorization Scenario (security) Development Prove implementation correctness in advance of a dependency or absent a dependency Unit testing (with mocks); Unit testing (with emulators); Consumer-driven Contract Testing Development Ensure that the user interface is accessible Accessibility Development Ensure that users can operate the interface UI testing (automated) (human usability observation) Development Prevent regression in user experience UI automation; End-to-End testing Development Detect and prevent 'noisy neighbor' phenomena Load testing Development Detect availability drops Synthetic Transaction testing ; Outside-in probes Development Prevent regression in 'composite' scenario use cases / workflows (e.g. an e-commerce system might have many APIs that used together in a sequence perform a \"shop-and-buy\" scenario) End-to-End testing ; Scenario Development; Operations Prevent regressions in runtime performance metrics e.g. latency / cost / resource consumption; earlier is better Rings; Synthetic Transaction testing / Transaction; Rollback Watchdogs Development; Optimization Compare any given metric between 2 candidate implementations or variations in functionality Flighting; A/B testing Development; Staging Prove production system of provisioned capacity meets goals for reliability, availability, resource consumption, performance Load testing (stress) ; Spike; Soak; Performance testing Development; Staging Understand key user experience performance characteristics \u2013 latency, chattiness, resiliency to network errors Load; Performance testing ; Scenario (network partitioning) Development; Staging; Operation Discover melt points (the loads at which failure or maximum tolerable resource consumption occurs) for each individual component in the stack Squeeze; Load testing (stress) Development; Staging; Operation Discover overall system melt point (the loads at which the end-to-end system fails) and which component is the weakest link in the whole stack Squeeze; Load testing (stress) Development; Staging; Operation Measure capacity limits for given provisioning to predict or satisfy future provisioning needs Squeeze; Load testing (stress) Development; Staging; Operation Create / exercise failover runbook Failover drills Development; Staging; Operation Prove disaster recoverability \u2013 loss of data center (the meteor scenario); measure MTTR DR drills Development; Staging; Operation Understand whether observability dashboards are correct, and telemetry is complete; flowing Trace Validation; Load testing (stress) ; Scenario; End-to-End testing Development; Staging; Operation Measure impact of seasonality of traffic Load testing Development; Staging; Operation Prove Transaction and alerts correctly notify / take action Synthetic Transaction testing (negative cases); Load testing Development; Staging; Operation; Optimizing Understand scalability curve, i.e. how the system consumes resources with load Load testing (stress) ; Performance testing Operation; Optimizing Discover system behavior over long-haul time Soak Optimizing Find cost savings opportunities Squeeze Staging; Operation Measure impact of failover / scale-out (repartitioning, increasing provisioning) / scale-down Failover drills; Scale drills Staging; Operation Create/Exercise runbook for increasing/reducing provisioning Scale drills Staging; Operation Measure behavior under rapid changes in traffic Spike Staging; Optimizing Discover cost metrics per unit load volume (what factors influence cost at what load points, e.g. cost per million concurrent users) Load (stress) Development; Operation Discover points where a system is not resilient to unpredictable yet inevitable failures (network outage, hardware failure, VM host servicing, rack/switch failures, random acts of the Malevolent Divine, solar flares, sharks that eat undersea cable relays, cosmic radiation, power outages, renegade backhoe operators, wolves chewing on junction boxes, \u2026) Chaos Development Perform unit testing on Power platform custom connectors Custom Connector Testing  \nSections within Testing  \nConsumer-driven contract (CDC) testing  \nEnd-to-End testing  \nFault Injection testing  \nIntegration testing  \nPerformance testing  \nShadow testing  \nSmoke testing  \nSynthetic Transaction testing  \nUI testing  \nUnit testing  \nTechnology Specific Testing  \nUsing DevTest Pattern for building containers with AzDO  \nUsing Azurite to run blob storage tests in pipeline  \nBuild for Testing  \nTesting is a critical part of the development process.  It is important to build your application with testing in mind.  Here are some tips to help you build for testing:  \nParameterize everything. Rather than hard-code any variables, consider making everything a configurable parameter with a reasonable default. This will allow you to easily change the behavior of your application during testing. Particularly during performance testing, it is common to test different values to see what impact that has on performance. If a range of defaults need to change together, consider one or more parameters which set \"modes\", changing the defaults of a group of parameters together.  \nDocument at startup. When your application starts up, it should log all parameters. This ensures the person reviewing the logs and application behavior know exactly how the application is configured.  \nLog to console. Logging to external systems like Azure Monitor is desirable for traceability across services. This requires logs to be dispatched from the local system to the external system and that is a dependency that can fail. It is important that someone be able to console logs directly on the local system.  \nLog to external system. In addition to console logs, logging to an external system like Azure Monitor is desirable for traceability across services and durability of logs.  \nLog all activity. If the system is performing some activity (reading data from a database, calling an external service, etc.), it should log that activity. Ideally, there should be a log message saying the activity is starting and another log message saying the activity is complete. This allows someone reviewing the logs to understand what the application is doing and how long it is taking. Depending on how noisy this is, different messages can be associated with different log levels, but it is important to have the information available when it comes to debugging a deployed system.  \nCorrelate distributed activities. If the system is performing some activity that is distributed across multiple systems, it is important to correlate the activity across those systems. This can be done using a Correlation ID that is passed from system to system. This allows someone reviewing the logs to understand the entire flow of activity. For more information, please see Observability in Microservices.  \nLog metadata. When logging, it is important to include metadata that is relevant to the activity. For example, a Tenant ID, Customer ID, or Order ID. This allows someone reviewing the logs to understand the context of the activity and filter to a manageable set of logs.  \nLog performance metrics. Even if you are using App Insights to capture how long dependency calls are taking, it is often useful to know long certain functions of your application took. It then becomes possible to evaluate the performance characteristics of your application as it is deployed on different compute platforms with different limitations on CPU, memory, and network bandwidth. For more information, please see Metrics.",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\README.md"
    },
    {
        "chunkId": "chunk33_0",
        "chunkContent": "Consumer-driven Contract Testing (CDC)  \nConsumer-driven Contract Testing (or CDC for short) is a software testing methodology used to test components of a system in isolation while ensuring that provider components are compatible with the expectations that consumer components have of them.  \nWhy Consumer-driven Contract Testing  \nCDC tries to overcome the several painful drawbacks of automated E2E tests with components interacting together:  \nE2E tests are slow  \nE2E tests break easily  \nE2E tests are expensive and hard to maintain  \nE2E tests of larger systems may be hard or impossible to run outside a dedicated testing environment  \nAlthough testing best practices suggest to write just a few E2E tests compared to the cheaper, faster and more stable integration and unit tests as pictured in the testing pyramid below, experience shows many teams end up writing too many E2E tests. A reason for this is that E2E tests give developers the highest confidence to release as they are testing the \"real\" system.  \nCDC addresses these issues by testing interactions between components in isolation using mocks that conform to a shared understanding documented in a \"contract\". Contracts are agreed between consumer and provider, and are regularly verified against a real instance of the provider component. This effectively partitions a larger system into smaller pieces that can be tested individually in isolation of each other, leading to simpler, fast and stable tests that also give confidence to release.  \nSome E2E tests are still required to verify the system as a whole when deployed in the real environment, but most functional interactions between components can be covered with CDC tests.  \nCDC testing was initially developed for testing RESTful API's, but the pattern scales to all consumer-provider systems and tooling for other messaging protocols besides HTTP does exist.  \nConsumer-driven Contract Testing Design Blocks  \nIn a consumer-driven approach the consumer drives changes to contracts between a consumer (the client) and a provider (the server). This may sound counterintuitive, but it helps providers create APIs that fit the real requirements of the consumers rather than trying to guess these in advance. Next we describe the CDC building blocks ordered by their occurrence in the development cycle.  \nConsumer Tests with Provider Mock  \nThe consumers start by creating integration tests against a provider mock and running them as part of their CI pipeline. Expected responses are defined in the provider mock for requests fired from the tests. Through this, the consumer essentially defines the contract they expect the provider to fulfill.  \nContract  \nContracts are generated from the expectations defined in the provider mock as a result of a successful test run. CDC frameworks like Pact provide a specification for contracts in json format consisting of the list of request/responses generated from the consumer tests plus some additional metadata.  \nContracts are not a replacement for a discussion between the consumer and provider team. This is the moment where this discussion should take place (if not already done before). The consumer tests and generated contract are refined with the feedback and cooperation of the provider team. Lastly the finalized contract is versioned and stored in a central place accessible by both consumer and provider.  \nContracts are complementary to API specification documents like OpenAPI. API specifications describe the structure and the format of the API. A contract instead specifies that for a given request, a given response is expected. An API specifications document is helpful in writing an API contract and can be used to validate that the contract conforms to the API specification.  \nProvider Contract Verification  \nOn the provider side tests are also executed as part of a separate pipeline which verifies contracts against real responses of the provider. Contract verification fails if real responses differ from the expected responses as specified in the contract. The cause of this can be:  \nInvalid expectations on the consumer side leading to incompatibility with the current provider implementation  \nBroken provider implementation due to some missing functionality or a regression  \nEither way, thanks to CDC it is easy to pinpoint integration issues down to the consumer/provider of the affected interaction. This is a big advantage compared to the debugging pain this could have been with an E2E test approach.  \nCDC Testing Frameworks and Tools  \nPact is an implementation of CDC testing that allows mocking of responses in the consumer codebase, and verification of the interactions in the provider codebase, while defining a specification for contracts. It was originally written in Ruby but has available wrappers for multiple languages. Pact is the de-facto standard to use when working with CDC.  \nSpring Cloud Contract is an implementation of CDC testing from Spring, and offers easy integration in the Spring ecosystem. Support for non-Spring and non-JVM providers and consumers also exists.  \nConclusion  \nCDC has several benefits that make it an approach worth considering when dealing with systems composed of multiple components interacting together.  \nMaintenance efforts can be reduced by testing consumer-provider interactions in isolation without the need of a complex integrated environment, specially as the interactions between components grow in number and become more complex.  \nAdditionally, a close collaboration between consumer and provider teams is strongly encouraged through the CDC development process, which can bring many other benefits. Contracts offer a formal way to document the shared understanding how components interact with each other, and serve as a base for the communication between teams. In a way, the contract repository serves as a live documentation of all consumer-provider interactions of a system.  \nCDC has some drawbacks as well. An extra layer of testing is added requiring a proper investment in education for team members to understand and use CDC correctly.  \nAdditionally, the CDC test scope should be considered carefully to prevent blurring CDC with other higher level functional testing layers. Contract tests are not the place to verify internal business logic and correctness of the consumer.  \nResources  \nTesting pyramid from Kent C. Dodd's blog  \nPact, a code-first consumer-driven contract testing tool with support for several different programming languages  \nConsumer-driven contracts from Ian Robinson  \nContract test from Martin Fowler  \nA simple example of using Pact consumer-driven contract testing in a Java client-server application  \nPact dotnet workshop",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\cdc-testing\\README.md"
    },
    {
        "chunkId": "chunk34_0",
        "chunkContent": "E2E Testing  \nEnd-to-end (E2E) testing is a Software testing methodology to test a functional and data application flow consisting of several sub-systems working together from  start to end.  \nAt times, these systems are developed in different technologies by different teams or organizations. Finally, they come together to form a functional business application.  Hence, testing a single system would not suffice. Therefore, end-to-end testing verifies the application from start to end putting all its components together.  \nWhy E2E Testing [The Why]  \nIn many commercial software application scenarios, a modern software system consists of its interconnection with multiple sub-systems. These sub-systems can be within the same organization or can be components of different organizations. Also, these sub-systems can have somewhat similar or different lifetime release cycle from the current system. As a result, if there is any failure or fault in any sub-system, it can adversely affect the whole software system leading to its collapse.  \nThe above illustration is a testing pyramid from Kent C. Dodd's blog which is a combination of the pyramids from Martin Fowler\u2019s blog and the Google Testing Blog.  \nThe majority of your tests are at the bottom of the pyramid. As you move up the pyramid, the number of tests gets smaller. Also, going up the pyramid, tests get slower and more expensive to write, run, and maintain. Each type of testing vary for its purpose, application and the areas it's supposed to cover. For more information on comparison analysis of different testing types, please see this ## Unit vs Integration vs System vs E2E Testing document.  \nE2E Testing Design Blocks [The What]  \nWe will look into all the 3 categories one by one:  \nUser Functions  \nFollowing actions should be performed as a part of building user functions:  \nList user initiated functions of the software systems, and their interconnected sub-systems.  \nFor any function, keep track of the actions performed as well as Input and Output data.  \nFind the relations, if any between different Users functions.  \nFind out the nature of different user functions i.e. if they are independent or are reusable.  \nConditions  \nFollowing activities should be performed as a part of building conditions based on user functions:  \nFor each and every user functions, a set of conditions should be prepared.  \nTiming, data conditions and other factors that affect user functions can be considered as parameters.  \nTest Cases  \nFollowing factors should be considered for building test cases:  \nFor every scenario, one or more test cases should be created to test each and every functionality of the user functions. If possible, these test cases should be automated through the standard CI/CD build pipeline processes with the track of each successful and failed build in AzDO.  \nEvery single condition should be enlisted as a separate test case.  \nApplying the E2E testing [The How]  \nLike any other testing, E2E testing also goes through formal planning, test execution, and closure phases.  \nE2E testing is done with the following steps:  \nPlanning  \nBusiness and Functional Requirement analysis  \nTest plan development  \nTest case development  \nProduction like Environment setup for the testing  \nTest data setup  \nDecide exit criteria  \nChoose the testing methods that most applicable to your system. For the definition of the various testing methods, please see Testing Methods document.  \nPre-requisite  \nSystem Testing should be complete for all the participating systems.  \nAll subsystems should be combined to work as a complete application.  \nProduction like test environment should be ready.  \nTest Execution  \nExecute the test cases  \nRegister the test results and decide on pass and failure  \nReport the Bugs in the bug reporting tool  \nRe-verify the bug fixes  \nTest closure  \nTest report preparation  \nEvaluation of exit criteria  \nTest phase closure  \nTest Metrics  \nThe tracing the quality metrics gives insight about the current status of testing. Some common metrics of E2E testing are:  \nTest case preparation status: Number of test cases ready versus the total number of test cases.  \nFrequent Test progress: Number of test cases executed in the consistent frequent manner, e.g. weekly, versus a target number of the test cases in the same time period.  \nDefects Status: This metric represents the status of the defects found during testing. Defects should be logged into defect tracking tool (e.g. AzDO backlog) and resolved as per their severity and priority. Therefore, the percentage of open and closed defects as per their severity and priority should be calculated to track this metric. The AzDO Dashboard Query can be used to track this metric.  \nTest environment availability: This metric tracks the duration of the test environment used for end-to-end testing versus its scheduled allocation duration.  \nE2E Testing Frameworks and Tools  \n1. Gauge Framework  \nGauge is a free and open source framework for writing and running E2E tests. Some key features of Gauge that makes it unique include:  \nSimple, flexible and rich syntax based on Markdown.  \nConsistent cross-platform/language support for writing test code.  \nA modular architecture with plugins support.  \nSupports data driven execution and external data sources.  \nHelps you create maintainable test suites.  \nSupports Visual Studio Code, Intellij IDEA, IDE Support.  \nSupports html, json and XML reporting.  \nGauge Framework Website  \n2. Robot Framework  \nRobot Framework is a generic open source automation framework. The framework has easy syntax, utilizing human-readable keywords. Its capabilities can be extended by libraries implemented with Python or Java.  \nRobot shares a lot of the same \"pros\" as Gauge, except the developer tooling and the syntax. In our usage, we found the VS Code Intellisense offered with Gauge to be much more stable than the offerings for Robot. We also found the syntax to be less readable than what Gauge offered. While both frameworks allow for markup based test case definitions, the Gauge syntax reads much more like an English sentence than Robot. Finally, Intellisense is baked into the markup files for Gauge test cases, which will create a function stub for the actual test definition if the developer allows it. The same cannot be said of the Robot Framework.  \nRobot Framework Website  \n3. TestCraft  \nTestCraft is a codeless Selenium test automation platform. Its revolutionary AI technology and unique visual modeling allow for faster test creation and execution while eliminating test maintenance overhead.  \nThe testers create fully automated test scenarios without coding. Customers find bugs faster, release more frequently, integrate with the CI/CD approach and improve the overall quality of their digital products. This all creates a complete end-to-end testing experience.  \nPerfecto (TestCraft) Website or get it  from the Visual Studio Marketplace  \n4. Ranorex Studio  \nRanorex Studio is a complete end-to-end test automation tool for desktop, web, and mobile applications. Create reliable tests fast without any coding at all, or using the full IDE. Use external CSV or Excel files, or a SQL database as inputs to your tests.  \nRun tests in parallel or on a Selenium Grid with built-in Selenium WebDriver. Ranorex Studio integrates with your CI/CD process to shorten your release cycles without sacrificing quality.  \nRanorex Studio tests also integrate with Azure DevOps (AzDO), which can be run as part of a build pipeline in AzDO.  \nRanorex Studio Website  \n5. Katalon Studio  \nKatalon Studio is an excellent end-to-end automation solution for web, API, mobile, and desktop testing with DevOps support.  \nWith Katalon Studio, automated testing can be easily integrated into any CI/CD pipeline to release products faster while guaranteeing high quality. Katalon Studio customizes for users from beginners to experts. Robust functions such as Spying, Recording, Dual-editor interface and Custom Keywords make setting up, creating and maintaining tests possible for users.  \nBuilt on top of Selenium and Appium, Katalon Studio helps standardize your end-to-end tests standardized. It also complies with the most popular frameworks to work seamlessly with other tools in the automated testing ecosystem.  \nKatalon is endorsed by Gartner, IT professionals, and a large testing community.  \nNote: At the time of this writing, Katalon Studio extension for AzDO was NOT available for Linux.  \nKatalon Studio Website or read about its integration with AzDO  \n6. BugBug.io  \nBugBug is an easy way to automate tests for web applications. The tool focuses on simplicity, yet allows you to cover all essential test cases without coding. It's an all-in-one solution - you can easily create tests and use the built-in cloud to run them on schedule or from your CI/CD, without changes to your own infrastructure.  \nBugBug is an interesting alternative to Selenium because it's actually a completely different technology. It is based on a Chrome extension that allows BugBug to record and run tests faster than old-school frameworks.  \nThe biggest advantage of BugBug is its user-friendliness. Most tests created with BugBug simply work out of the box. This makes it easier for non-technical people to maintain tests - with BugBug you can save money on hiring a QA engineer.  \nBugBug Website  \nConclusion  \nHope you learned various aspects of E2E testing like its processes, metrics, the difference between Unit, Integration  and E2E testing, and the various recommended E2E test frameworks and tools.  \nFor any commercial release of the software, E2E test verification plays an important role as it tests the entire application in an environment that exactly imitates real-world users like network communication, middleware and backend services interaction, etc.  \nFinally, the E2E test is often performed manually as the cost of automating such test cases is too high to be afforded by any organization. Having said that, the ultimate goal of each organization is to make the e2e testing as streamlined as possible adding full and semi-automation testing components into the process. Hence, the various E2E testing frameworks and tools listed in this article come to the rescue.  \nResources  \nWikipedia: Software testing  \nWikipedia: Unit testing  \nWikipedia: Integration testing  \nWikipedia: System testing",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\e2e-testing\\README.md"
    },
    {
        "chunkId": "chunk35_0",
        "chunkContent": "Unit vs Integration vs System vs E2E Testing  \nThe table below illustrates the most critical characteristics and differences among Unit, Integration, System, and End-to-End Testing, and when to apply each methodology in a project.  \nUnit Test Integration Test System Testing E2E Test Scope Modules, APIs Modules, interfaces Application, system All sub-systems, network dependencies, services and databases Size Tiny Small to medium Large X-Large Environment Development Integration test QA test Production like Data Mock data Test data Test data Copy of real production data System Under Test Isolated unit test Interfaces and flow data between the modules Particular system as a whole Application flow from start to end Scenarios Developer perspectives Developers and IT Pro tester perspectives Developer and QA tester perspectives End-user perspectives When After each build After Unit testing Before E2E testing and after Unit and Integration testing After System testing Automated or Manual Automated Manual or automated Manual or automated Manual",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\e2e-testing\\testing-comparison.md"
    },
    {
        "chunkId": "chunk36_0",
        "chunkContent": "E2E Testing Methods  \nHorizontal Test  \nThis method is used very commonly. It occurs horizontally across the context of multiple applications. Take an example of a data ingest management system.  \nThe inbound data may be  injected from various sources, but it then \"flatten\" into a horizontal processing pipeline that may include various components, such as a gateway API, data transformation, data validation, storage, etc... Throughout the entire Extract-Transform-Load (ETL) processing, the data flow can be tracked and monitored under the horizontal spectrum with little sprinkles of optional, and thus not important for the overall E2E test case, services, like logging, auditing, authentication.  \nVertical Test  \nIn this method, all most critical transactions of any application are verified and evaluated right from the start to finish. Each individual layer of the application is tested starting from top to bottom. Take an example of a web-based application that uses middleware services for reaching back-end resources.  \nIn such case, each layer (tier) is required to be fully tested in conjunction with the \"connected\" layers above and beneath, in which services \"talk\" to each other during the end to end data flow. All these complex testing scenarios will require proper validation and dedicated automated testing. Thus, this method is much more difficult.  \nE2E Test Cases Design Guidelines  \nBelow enlisted are few guidelines that should be kept in mind while designing the test cases for performing E2E testing:  \nTest cases should be designed from the end user\u2019s perspective.  \nShould focus on testing some existing features of the system.  \nMultiple scenarios should be considered for creating multiple test cases.  \nDifferent sets of test cases should be created to focus on multiple scenarios of the system.",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\e2e-testing\\testing-methods.md"
    },
    {
        "chunkId": "chunk37_0",
        "chunkContent": "Gauge Framework  \nGauge is a free and open source framework for writing and running E2E tests. Some key features of Gauge that makes it unique include:  \nSimple, flexible and rich syntax based on Markdown.  \nConsistent cross-platform/language support for writing test code.  \nA modular architecture with plugins support  \nExtensible through plugins and hackable.  \nSupports data driven execution and external data sources  \nHelps you create maintainable test suites  \nSupports Visual Studio Code, Intellij IDEA, IDE Support  \nWhat is a Specification  \nGauge specifications are written using a Markdown syntax. For example  \n{% raw %}  \n```bash\n\nSearch for the data blob\n\nLook for file\n\nGoto Azure blob\n```  \n{% endraw %}  \nIn this specification Search for the data blob is the specification heading, Look for file is a scenario with a step Goto Azure blob  \nWhat is an Implementation  \nYou can implement the steps in a specification using a programming language, for example:  \n{% raw %}  \nbash\nfrom getgauge.python import step\nimport os\nfrom step_impl.utils.driver import Driver\n@step(\"Goto Azure blob\")\ndef gotoAzureStorage():\nURL = os.getenv('STORAGE_ENDPOINT')\nDriver.driver.get(URL)  \n{% endraw %}  \nThe Gauge runner reads and runs steps and its implementation for every scenario in the specification and generates a report of passing or failing scenarios.  \n{% raw %}  \n```bash\n\nSearch for the data blob\n\nLook for file  \u2714\n\nSuccessfully generated html-report to => reports/html-report/index.html\nSpecifications:       1 executed      1 passed        0 failed        0 skipped\nScenarios:    1 executed      1 passed        0 failed        0 skipped\n```  \n{% endraw %}  \nRe-using Steps  \nGauge helps you focus on testing the flow of an application. Gauge does this by making steps as re-usable as possible. With Gauge, you don\u2019t need to build custom frameworks using a programming language.  \nFor example, Gauge steps can pass parameters to an implementation by using a text with quotes.  \n{% raw %}  \n```bash\n\nSearch for the data blob\n\nLook for file\n\nGoto Azure blob\n\nSearch for \"store_data.csv\"\n```  \n{% endraw %}  \nThe implementation can now use \u201cstore_data.csv\u201d as follows  \n{% raw %}  \n```bash\nfrom getgauge.python import step\nimport os\n@step(\"Search for \")\ndef searchForQuery(query):\nwrite(query)\npress(\"Enter\")\n\nstep(\"Search for \", (query) => {\nwrite(query);\npress(\"Enter\");\n```  \n{% endraw %}  \nYou can then re-use this step within or across scenarios with different parameters:  \n{% raw %}  \n```bash\n\nSearch for the data blob\n\nLook for Store data #1\n\nGoto Azure blob\n\nSearch for \"store_1.csv\"\n\nLook for Store data #2\n\nGoto Azure blob\n\nSearch for \"store_2.csv\"\n```  \n{% endraw %}  \nOr combine more than one step into concepts  \n{% raw %}  \n```bash\n\nSearch Azure Storage for\n\nGoto Azure blob\n\nSearch for \"store_1.csv\"\n```  \n{% endraw %}  \nThe concept, Search Azure Storage for <query> can be used like a step in a specification  \n{% raw %}  \n```bash\n\nSearch for the data blob\n\nLook for Store data #1\n\nSearch Azure Storage for \"store_1.csv\"\n\nLook for Store data #2\n\nSearch Azure Storage for \"store_2.csv\"\n```  \n{% endraw %}  \nData-Driven Testing  \nGauge also supports data driven testing using Markdown tables as well as external csv files for example  \n{% raw %}  \n```bash\n\nSearch for the data blob\n\nquery store_1 store_2 store_3\n\nLook for stores data\n\nSearch Azure Storage for\n```  \n{% endraw %}  \nThis will execute the scenario for all rows in the table.  \nIn the examples above, we refactored a specification to be concise and flexible without changing the implementation.  \nOther Features  \nThis is brief introduction to a few Gauge features. Please refer to the Gauge documentation for additional features such as:  \nReports  \nTags  \nParallel execution  \nEnvironments  \nScreenshots  \nPlugins  \nAnd much more  \nInstalling Gauge  \nThis getting started guide takes you through the core features of Gauge. By the end of this guide, you\u2019ll be able to install Gauge and learn how to create your first Gauge test automation project.  \nInstallation Instructions for Windows OS  \nStep 1: Installing Gauge on Windows  \nThis section gives specific instructions on setting up Gauge in a Microsoft Windows environment.\nDownload the following installation bundle to get the latest stable release of Gauge.  \nStep 2: Installing Gauge extension for Visual Studio Code  \nFollow the steps to add the Gauge Visual Studio Code plugin from the IDE  \nInstall the following Gauge extension for Visual Studio Code.  \nTroubleshooting Installation  \nIf, when you run your first gauge spec you receive the error of missing python packages, open the command line terminal window and run this command:  \n{% raw %}  \nbash\npython.exe -m pip install getgauge==0.3.7 --user  \n{% endraw %}  \nInstallation Instructions for macOS  \nStep 1: Installing Gauge on macOS  \nThis section gives specific instructions on setting up Gauge in a macOS environment.  \nInstall brew if you haven\u2019t already: Go to the brew website, and follow the directions there.  \nRun the brew command to install Gauge  \n{% raw %}  \n```bash\n\nbrew install gauge\n```  \n{% endraw %}  \nif HomeBrew is working properly, you should see something similar to the following:  \n{% raw %}  \n```bash\n==> Fetching gauge\n==> Downloading https://ghcr.io/v2/homebrew/core/gauge/manifests/1.4.3\n\n################################################################## 100.0%\n\n==> Downloading https://ghcr.io/v2/homebrew/core/gauge/blobs/sha256:05117bb3c0b2efeafe41e817cd3ad86307c1d2ea7e0e835655c4b51ab2472893\n==> Downloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sha256:05117bb3c0b2efeafe41e817cd3ad86307c1d2ea7e0e835655c4b51ab2472893?se=2022-12-13T12%3A35%3A00Z&sig=I78SuuwNgSMFoBTT\n\n################################################################## 100.0%\n\n==> Pouring gauge--1.4.3.ventura.bottle.tar.gz\n/usr/local/Cellar/gauge/1.4.3: 6 files, 18.9MB\n```  \n{% endraw %}  \nStep 2 : Installing Gauge extension for Visual Studio Code  \nFollow the steps to add the Gauge Visual Studio Code plugin from the IDE  \nInstall the following Gauge extension for Visual Studio Code.  \nPost-Installation Troubleshooting  \nIf, when you run your first gauge spec you receive the error of missing python packages, open the command line terminal window and run this command:  \n{% raw %}  \nbash\npython.exe -m pip install getgauge==0.3.7 --user  \n{% endraw %}",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\e2e-testing\\recipes\\gauge-framework.md"
    },
    {
        "chunkId": "chunk38_0",
        "chunkContent": "Postman Testing  \nThis purpose of this document is to provide guidance on how to use Newman in your CI/CD pipeline to run End-to-end (E2E) tests defined in Postman Collections while following security best practices.  \nFirst, we'll introduce Postman and Newman and then outline several Postman testing use cases that answer why you may want to go beyond local testing with Postman Collections.  \nIn the final use case, we are looking to use a shell script that references the Postman Collection file path and Environment file path as inputs to Newman. Below is a flow diagram representing the outcome of the final use case:  \nPostman and Newman  \nPostman is a free API platform for testing APIs. Key features highlighted in this guidance include:  \nPostman Collections  \nPostman Environment Files  \nPostman Scripts  \nNewman is a command-line Collection Runner for Postman. It enables you to run and test a Postman Collection directly from the command line. Key features highlighted in this guidance include:  \nNewman Run Command  \nWhat is a Collection  \nA Postman Collection is a group of executable saved requests. A collection can be exported as a json file.  \nWhat is an Environment File  \nA Postman Environment file holds environment variables that can be referenced by a valid Postman Collection.  \nWhat is a Postman Script  \nA Postman Script is Javascript hosted within a Postman Collection that can be written to execute against your Postman Collection and Environment File.  \nWhat is the Newman Run Command  \nA Newman CLI command that allows you to specify a Postman Collection to be run.  \nInstalling Postman and Newman  \nFor specific instruction on installing Postman, visit the Downloads Postman page.  \nFor specific instruction on installing Newman, visit the NPMJS Newman package page.  \nImplementing Automated End-to-end (E2E) Tests With Postman Collections  \nIn order to provide guidance on implementing automated E2E tests with Postman, the section below begins with a use case that explains the trade-offs a dev or QA analyst might face when intending to use Postman for early testing. Each use case represents scenarios that facilitate the end goal of automated E2E tests.  \nUse Case - Hands-on Functional Testing Of Endpoints  \nA developer or QA analyst would like to locally test input data against API services all sharing a common oauth2 token. As a result, they use Postman to craft an API test suite of Postman Collections that can be locally executed against individual endpoints across environments. After validating that their Postman Collection works, they share it with their team.  \nSteps may look like the following:  \nFor each of your existing API services, use the Postman IDE's import feature to import its OpenAPI Spec (Swagger) as a Postman Collection.\nIf a service is not already using Swagger, look for language specific guidance on how to use Swagger to generate an OpenAPI Spec for your service. Finally, if your service only has a few endpoints, read Postman docs for guidance on how to manually build a Postman Collection.  \nProvide extra clarity about a request in a Postman Collection by using Postman's Example feature to save its responses as examples. You can also simply add an example manually. Please read Postman docs for guidance on how to specify examples.  \nCombine each Postman Collection into a centralized Postman Collection.  \nBuild Postman Environment files (local, Dev and/or QA) and parameterize all saved requests of the Postman Collection in a way that references the Postman Environment files.  \nUse the Postman Script feature to create a shared prefetch script that automatically refreshes expired auth tokens per saved request. This would require referencing secrets from a Postman Environment file.\n{% raw %}\n```javascript\n// Please treat this as pseudocode, and adjust as necessary.\n/ The request to an oauth2 authorization endpoint that will issue a token\nbased on provided credentials./\nconst oauth2Request = POST {...};\nvar getToken = true;\nif (pm.environment.get('ACCESS_TOKEN_EXPIRY') <= (new Date()).getTime()) {\nconsole.log('Token is expired')\n} else {\ngetToken = false;\nconsole.log('Token and expiry date are all good');\n}\nif (getToken === true) {\npm.sendRequest(oauth2Request, function (_, res) {\nconsole.log('Save the token')\nvar responseJson = res.json();\npm.environment.set('token', responseJson.access_token)\nconsole.log('Save the expiry date')\nvar expiryDate = new Date();\nexpiryDate.setSeconds(expiryDate.getSeconds() + responseJson.expires_in);\npm.environment.set('ACCESS_TOKEN_EXPIRY', expiryDate.getTime());\n});\n}\n```\n{% endraw %}  \nUse Postman IDE to exercise endpoints.  \nExport collection and environment files then remove any secrets before committing to your repo.  \nStarting with this approach has the following upsides:  \nYou've set yourself up for the beginning stages of an E2E postman collection by aggregating the collections into a single file and using environment files to make it easier to switch environments.  \nToken is refreshed automatically on every call in the collection. This saves you time normally lost from manually having to request a token that expired.  \nGrants QA/Dev granular control of submitting combinations of input data per endpoint.  \nGrants developers a common experience via Postman IDE features.  \nEnding with this approach has the following downsides:  \nPromotes unsafe sharing of secrets. Credentials needed to request JWT token in the prefetch script are being manually shared.  \nSecrets may happen to get exposed in the git commit history for various reasons (ex. Sharing the exported Postman Environment files).  \nCollections can only be used locally to hit APIs (local or deployed). Not CI based.  \nEach developer has to keep both their Postman Collection and Postman environment file(s) updated in order to keep up with latest changes to deployed services.  \nUse Case - Hands-on Functional Testing Of Endpoints with Azure Key Vault and Azure App Config  \nA developer or QA analyst may have an existing API test suite of Postman Collections, however, they now want to discourage unsafe sharing of secrets. As a result, they build a script that connects to both Key Vault and Azure App Config in order to automatically generate Postman Environment files instead of checking them into a shared repository.  \nSteps may look like the following:  \nCreate an Azure Key Vault and store authentication secrets per environment:\n\"Key:value\" (ex. \"dev-auth-password:12345\")\n\"Key:value\" (ex. \"qa-auth-password:12345\")  \nCreate a shared Azure App Configuration instance and save all your Postman environment variables. This instance will be dedicated to holding all your Postman environment variables:\n> NOTE: Use the Label feature to delineate between environments.\n\"Key:value\" -> \"apiRoute:url\" (ex. \"servicename:https://servicename.net\" & Label = \"QA\")\n\"Key:value\" -> \"Header:value\"(ex. \"token: \" & Label = \"QA\")\n\"Key:value\" -> \"KeyVaultKey:KeyVaultSecret\" (ex. \"authpassword:qa-auth-password\" & Label = \"QA\")  \nInstall Powershell or Bash. Powershell works for both Azure Powershell and Azure CLI.  \nDownload Azure CLI, login to the appropriate subscription and ensure you have access to the appropriate resources. Some helpful commands are below:\n{% raw %}\n```powershell\nlogin to the appropriate subscription\naz login\nvalidate login\naz account show\nvalidate access to Key Vault\naz keyvault secret list --vault-name \"$KeyvaultName\"\nvalidate access to App Configuration\naz appconfig kv list --name \"$AppConfigName\"\n```\n{% endraw %}  \nBuild a script that automatically generates your environment files.\n> NOTE: App Configuration references Key Vault, however, your script is responsible for authenticating properly to both App Configuration and Key Vault. The two services don't communicate directly.\n{% raw %}\n```powershell (CreatePostmanEnvironmentFiles.ps1)\nPlease treat this as pseudocode, and adjust as necessary.\n\nenv = $arg1\n1. list app config vars for an environment\nenvVars = az appconfig kv list --name PostmanAppConfig --label $env | ConvertFrom-Json\n2. step through envVars array to get Key Vault uris\nkeyvaultURI = \"\"\n$envVars | % {if($.key -eq 'password'){keyvaultURI = $.value}}\n3. parse uris for Key Vault name and secret names\n4. get secret from Key Vault\nkvsecret = az keyvault secret show --name $secretName --vault-name $keyvaultName --query \"value\"\n5. set password value to returned Key Vault secret\n$envVars | % {if($.key -eq 'password'){$.value=$kvsecret}}\n6. create environment file\nenvFile = @{ \"_postman_variable_scope\" = \"environment\", \"name\" = $env, values = @() }\nforeach($var in $envVars){\n$envFile.values += @{ key = $var.key; value = $var.value; }\n}\n$envFile | ConvertTo-Json -depth 50 | Out-File -encoding ASCII -FilePath .\\$env.postman_environment.json\n```\n{% endraw %}  \nUse Postman IDE to import the Postman Environment files to be referenced by your collection.  \nThis approach has the following upsides:  \nInherits all the upsides of the previous case.  \nDiscourages unsafe sharing of secrets. Secrets are now pulled from Key Vault via Azure CLI. Key Vault Uri also no longer needs to be shared for access to auth tokens.  \nSingle source of truth for Postman Environment files. There's no longer a need to share them via repo.  \nDeveloper only has to manage a single Postman Collection.  \nEnding with this approach has the following downsides:  \nSecrets may happen to get exposed in the git commit history if .gitIgnore is not updated to ignore Postman Environment files.  \nCollections can only be used locally to hit APIs (local or deployed). Not CI based.  \nUse Case - E2E testing With Continuous Integration and Newman  \nA developer or QA analyst may have an existing API test suite of local Postman Collections that follow security best practices for development, however, they now want E2E tests to run as part of automated CI pipeline. With the advent of Newman, you can now more readily use Postman to craft an API test suite executable in your CI.  \nSteps may look like the following:  \nUpdate your Postman Collection to use the Postman Test feature in order to craft test assertions that will cover all saved requests E2E. Read Postman docs for guidance on how to use the Postman Test feature.  \nLocally use Newman to validate tests are working as intended\n{% raw %}\npowershell\nnewman run tests\\e2e_Postman_collection.json -e qa.postman_environment.json\n{% endraw %}  \nBuild a script that automatically executes Postman Test assertions via Newman and Azure CLI.\n> NOTE: An Azure Service Principal must be setup to continue using azure cli in this CI pipeline example.\n{% raw %}\n```powershell (RunPostmanE2eTests.ps1)\nPlease treat this as pseudocode, and adjust as necessary.\n\n1. login to Azure using a Service Principal\naz login --service-principal -u $APP_ID -p $AZURE_SECRET --tenant $AZURE_TENANT\n2. list app config vars for an environment\nenvVars = az appconfig kv list --name PostmanAppConfig --label $env | ConvertFrom-Json\n3. step through envVars array to get Key Vault uris\nkeyvaultURI = \"\"\n@envVars | % {if($.key -eq 'password'){keyvaultURI = $.value}}\n4. parse uris for Key Vault name and secret names\n5. get secret from Key Vault\nkvsecret = az keyvault secret show --name $secretName --vault-name $keyvaultName --query \"value\"\n6. set password value to returned Key Vault secret\n$envVars | % {if($.key -eq 'password'){$.value=$kvsecret}}\n7. create environment file\nenvFile = @{ \"_postman_variable_scope\" = \"environment\", \"name\" = $env, values = @() }\nforeach($var in $envVars){\n$envFile.values += @{ key = $var.key; value = $var.value; }\n}\n$envFile | ConvertTo-Json -depth 50 | Out-File -encoding ASCII $env.postman_environment.json\n8. install Newman\nnpm install --save-dev newman\n9. run automated E2E tests via Newman\nnode_modules.bin\\newman run tests\\e2e_Postman_collection.json -e $env.postman_environment.json\n```\n{% endraw %}  \nCreate a yaml file and define a step that will run your test script. (ex. A yaml file targeting Azure Devops that runs a Powershell script.)\n{% raw %}\n```yaml\nPlease treat this as pseudocode, and adjust as necessary.\n\ndisplayName: 'Run Postman E2E tests'\ninputs:\ntargetType: 'filePath'\nfilePath: RunPostmanE2eTests.ps1\nenv:\nAPP_ID: $(environment.appId) # credentials for az cli\nAZURE_SECRET: $(environment.secret)\nAZURE_TENANT: $(environment.tenant)\n```\n{% endraw %}  \nThis approach has the following upside:  \nE2E tests can now be run automatically as part of a CI pipeline.  \nEnding with this approach has the following downside:  \nPostman Environment files are no longer being output to a local environment for hands-on manual testing. However, this can be solved by managing 2 scripts.",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\e2e-testing\\recipes\\postman-testing.md"
    },
    {
        "chunkId": "chunk39_0",
        "chunkContent": "Templates  \nGauge Framework  \nPostman",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\e2e-testing\\recipes\\README.md"
    },
    {
        "chunkId": "chunk40_0",
        "chunkContent": "Fault Injection Testing  \nFault injection testing is the deliberate introduction of errors and faults to a system to validate and harden its stability and reliability. The goal is to improve the system's design for resiliency and performance under intermittent failure conditions over time.  \nWhen To Use  \nProblem Addressed  \nSystems need to be resilient to the conditions that caused inevitable production disruptions. Modern applications are built with an increasing number of dependencies; on infrastructure, platform, network, 3rd party software or APIs, etc. Such systems increase the risk of impact from dependency disruptions. Each dependent component may fail. Furthermore, its interactions with other components may propagate the failure.  \nFault injection methods are a way to increase coverage and validate software robustness and error handling, either at build-time or at run-time, with the intention of \"embracing failure\" as part of the development lifecycle. These methods assist engineering teams in designing and continuously validating for failure, accounting for known and unknown failure conditions, architect for redundancy, employ retry and back-off mechanisms, etc.  \nApplicable to  \nSoftware - Error handling code paths, in-process memory management.  \nExample tests: Edge-case unit/integration tests and/or load tests (i.e. stress and soak).  \nProtocol - Vulnerabilities in communication interfaces such as command line parameters or APIs.  \nExample tests: Fuzzing provides invalid, unexpected, or random data as input we can assess the level of protocol stability of a component.  \nInfrastructure - Outages, networking issues, hardware failures.  \nExample tests: Using different methods to cause fault in the underlying infrastructure such as Shut down virtual machine (VM) instances, crash processes, expire certificates, introduce network latency, etc. This level of testing relies on statistical metrics observations over time and measuring the deviations of its observed behavior during fault, or its recovery time.  \nHow to Use  \nArchitecture  \nTerminology  \nFault - The adjudged or hypothesized cause of an error.  \nError - That part of the system state that may cause a subsequent failure.  \nFailure - An event that occurs when the delivered service deviates from correct state.  \nFault-Error-Failure cycle - A key mechanism in dependability: A fault may cause an error. An error may cause further errors within the system boundary; therefore each new error acts as a fault. When error states are observed at the system boundary, they are termed failures.  \nFault Injection Testing Basics  \nFault injection is an advanced form of testing where the system is subjected to different failure modes, and where the testing engineer may know in advance what is the expected outcome, as in the case of release validation tests, or in an exploration to find potential issues in the product, which should be mitigated.  \nFault Injection and Chaos Engineering  \nFault injection testing is a specific approach to testing one condition. It introduces a failure into a system to validate its robustness. Chaos engineering, coined by Netflix, is a practice for generating new information. There is an overlap in concerns and often in tooling between the terms, and many times chaos engineering uses fault injection to introduce the required effects to the system.  \nHigh-level Step-by-step  \nFault injection testing in the development cycle  \nFault injection is an effective way to find security bugs in software, so much so that the Microsoft Security Development Lifecycle requires fuzzing at every untrusted interface of every product and penetration testing which includes introducing faults to the system, to uncover potential vulnerabilities resulting from coding errors, system configuration faults, or other operational deployment weaknesses.  \nAutomated fault injection coverage in a CI pipeline promotes a Shift-Left approach of testing earlier in the lifecycle for potential issues.\nExamples of performing fault injection during the development lifecycle:  \nUsing fuzzing tools in CI.  \nExecute existing end-to-end scenario tests (such as integration or stress tests), which are augmented with fault injection.  \nWrite regression and acceptance tests based on issues that were found and fixed or based on resolved service incidents.  \nAd-hoc (manual) validations of fault in the dev environment for new features.  \nFault injection testing in the release cycle  \nMuch like Synthetic Monitoring Tests, fault injection testing in the release cycle is a part of Shift-Right testing approach, which uses safe methods to perform tests in a production or pre-production environment. Given the nature of distributed, cloud-based applications, it is very difficult to simulate the real behavior of services outside their production environment. Testers are encouraged to run tests where it really matters, on a live system with customer traffic.  \nFault injection tests rely on metrics observability and are usually statistical; The following high-level steps provide a sample of practicing fault injection and chaos engineering:  \nMeasure and define a steady (healthy) state for the system's interoperability.  \nCreate hypotheses based on predicted behavior when a fault is introduced.  \nIntroduce real-world fault-events to the system.  \nMeasure the state and compare it to the baseline state.  \nDocument the process and the observations.  \nIdentify and act on the result.  \nFault injection testing in kubernetes  \nWith the advancement of kubernetes (k8s) as the infrastructure platform, fault injection testing in kubernetes has become inevitable to ensure that system behaves in a reliable manner in the event of a fault or failure. There could be different type of workloads running within a k8s cluster which are written in different languages. For eg. within a K8s cluster, you can run a micro service, a web app and/or a scheduled job. Hence you need to have mechanism to inject fault into any kind of workloads running within the cluster. In addition, kubernetes clusters are managed differently from traditional infrastructure. The tools used for fault injection testing within kubernetes should have compatibility with k8s infrastructure. These are the main characteristics which are required:  \nEase of injecting fault into kubernetes pods.  \nSupport for faster tool installation within the cluster.  \nSupport for YAML based configurations which works well with kubernetes.  \nEase of customization to add custom resources.  \nSupport for workflows to deploy various workloads and faults.  \nEase of maintainability of the tool  \nEase of integration with telemetry  \nBest Practices and Advice  \nExperimenting in production has the benefit of running tests against a live system with real user traffic, ensuring its health, or building confidence in its ability to handle errors gracefully. However, it has the potential to cause unnecessary customer pain.\nA test can either succeed or fail. In the event of failure, there will likely be some impact on the production environment. Thinking about the Blast Radius of the effect, should the test fail, is a crucial step to conduct beforehand. The following practices may help minimize such risk:  \nRun tests in a non-production environment first. Understand how the system behaves in a safe environment, using synthetic workload, before introducing potential risk to customer traffic.  \nUse fault injection as gates in different stages through the CD pipeline.  \nDeploy and test on Blue/Green and Canary deployments. Use methods such as traffic shadowing (a.k.a. Dark Traffic) to get customer traffic to the staging slot.  \nStrive to achieve a balance between collecting actual result data while affecting as few production users as possible.  \nUse defensive design principles such as circuit breaking and the bulkhead patterns.  \nAgreed on a budget (in terms of Service Level Objective (SLO)) as an investment in chaos and fault injection.  \nGrow the risk incrementally - Start with hardening the core and expand out in layers. At each point, progress should be locked in with automated regression tests.  \nFault Injection Testing Frameworks and Tools  \nFuzzing  \nOneFuzz - is a Microsoft open-source self-hosted fuzzing-as-a-service platform which is easy to integrate into CI pipelines.  \nAFL and WinAFL - Popular fuzz tools by Google's project zero team which is used locally to target binaries on Linux or Windows.  \nWebScarab - A web-focused fuzzer owned by OWASP which can be found in Kali linux distributions.  \nChaos  \nAzure Chaos Studio - An in-preview tool for orchestrating controlled fault injection experiments on Azure resources.  \nChaos toolkit - A declarative, modular chaos platform with many extensions, including the Azure actions and probes kit.  \nKraken - An Openshift-specific chaos tool, maintained by Redhat.  \nChaos Monkey - The Netflix platform which popularized chaos engineering (doesn't support Azure OOTB).  \nSimmy - A .NET library for chaos testing and fault injection integrated with the Polly library for resilience engineering.  \nLitmus - A CNCF open source tool for chaos testing and fault injection for kubernetes cluster.  \nThis ISE dev blog post provides code snippets as an example of how to use Polly and Simmy to implement a hypothesis-driven approach to resilience and chaos testing.  \nConclusion  \nFrom the principals of chaos: \"The harder it is to disrupt the steady-state, the more confidence we have in the behavior of the system. If a weakness is uncovered, we now have a target for improvement before that behavior manifests in the system at large\".  \nFault injection techniques increase resilience and confidence in the products we ship. They are used across the industry to validate applications and platforms before and while they are delivered to customers.\nFault injection is a powerful tool and should be used with caution. Cases such as the Cloudflare 30 minute global outage, which was caused due to a deployment of code that was meant to be \u201cdark launched\u201d, entail the importance of curtailing the blast radius in the system during experiments.  \nResources  \nMark Russinovich's fault injection and chaos engineering blog post  \nCindy Sridharan's Testing in production blog post  \nCindy Sridharan's Testing in production blog post cont.  \nFault injection in Azure Search  \nAzure Architecture Framework - Chaos engineering  \nAzure Architecture Framework - Testing resilience  \nLandscape of Software Failure Cause Models",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\fault-injection-testing\\README.md"
    },
    {
        "chunkId": "chunk41_0",
        "chunkContent": "Integration Testing  \nIntegration testing is a software testing methodology used to determine how well individually developed components, or modules of a system communicate with each other. This method of testing confirms that an aggregate of a system, or sub-system, works together correctly or otherwise exposes erroneous behavior between two or more units of code.  \nWhy Integration Testing  \nBecause one component of a system may be developed independently or in isolation of another it is important to verify the interaction of some or all components. A complex system may be composed of databases, APIs, interfaces, and more, that all interact with each other or additional external systems. Integration tests expose system-level issues such as broken database schemas or faulty third-party API integration. It ensures higher test coverage and serves as an important feedback loop throughout development.  \nIntegration Testing Design Blocks  \nConsider a banking application with three modules: login, transfers, and current balance, all developed independently. An integration test may verify when a user logs in they are re-directed to their current balance with the correct amount for the specific mock user. Another integration test may perform a transfer of a specified amount of money. The test may confirm there are sufficient funds in the account to perform the transfer, and after the transfer the current balance is updated appropriately for the mock user. The login page may be mocked with a test user and mock credentials if this module is not completed when testing the transfers module.  \nIntegration testing is done by the developer or QA tester. In the past, integration testing always happened after unit and before system and E2E testing. Compared to unit-tests, integration tests are fewer in quantity, usually run slower, and are more expensive to set up and develop. Now, if a team is following agile principles, integration tests can be performed before or after unit tests, early and often, as there is no need to wait for sequential processes. Additionally, integration tests can utilize mock data in order to simulate a complete system. There is an abundance of language-specific testing frameworks that can be used throughout the entire development lifecycle.  \n** It is important to note the difference between integration and acceptance testing. Integration testing confirms a group of components work together as intended from a technical perspective, while acceptance testing confirms a group of components work together as intended from a business scenario.  \nApplying Integration Testing  \nPrior to writing integration tests, the engineers must identify the different components of the system, and their intended behaviors and inputs and outputs. The architecture of the project must be fully documented or specified somewhere that can be readily referenced (e.g., the architecture diagram).  \nThere are two main techniques for integration testing.  \nBig Bang  \nBig Bang integration testing is when all components are tested as a single unit. This is best for small system as a system too large may be difficult to localize for potential errors from failed tests. This approach also requires all components in the system under test to be completed which may delay when testing begins.  \nIncremental Testing  \nIncremental testing is when two or more components that are logically related are tested as a unit. After testing the unit, additional components are combined and tested all together. This process repeats until all necessary components are tested.  \nTop Down  \nTop down testing is when higher level components are tested following the control flow of a software system. In the scenario, what is commonly referred to as stubs are used to emulate the behavior of lower level modules not yet complete or merged in the integration test.  \nBottom Up  \nBottom up testing is when lower level modules are tested together. In the scenario, what is commonly referred to as drivers are used to emulate the behavior of higher level modules not yet complete or included in the integration test.  \nA third approach known as the sandwich or hybrid model combines the bottom up and town down approaches to test lower and higher level components at the same time.  \nThings to Avoid  \nThere is a tradeoff a developer must make between integration test code coverage and engineering cycles. With mock dependencies, test data, and multiple environments at test, too many integration tests are infeasible to maintain and become increasingly less meaningful. Too much mocking will slow down the test suite, make scaling difficult, and may be a sign the developer should consider other tests for the scenario such as acceptance or E2E.  \nIntegration tests of complex systems require high maintenance. Avoid testing business logic in integration tests by keeping test suites separate. Do not test beyond the acceptance criteria of the task and be sure to clean up any resources created for a given test. Additionally, avoid writing tests in a production environment. Instead, write them in a scaled-down copy environment.  \nIntegration Testing Frameworks and Tools  \nMany tools and frameworks can be used to write both unit and integration tests. The following tools are for automating integration tests.  \nJUnit  \nRobot Framework  \nmoq  \nCucumber  \nSelenium  \nBehave (Python)  \nConclusion  \nIntegration testing demonstrates how one module of a system, or external system, interfaces with another. This can be a test of two components, a sub-system, a whole system, or a collection of systems. Tests should be written frequently and throughout the entire development lifecycle using an appropriate amount of mocked dependencies and test data. Because integration tests prove that independently developed modules interface as technically designed, it increases confidence in the development cycle providing a path for a system that deploys and scales.  \nResources  \nIntegration testing approaches  \nIntegration testing pros and cons  \nIntegration tests mocks and stubs  \nSoftware Testing: Principles and Practices  \nIntegration testing Behave test quick start",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\integration-testing\\README.md"
    },
    {
        "chunkId": "chunk42_0",
        "chunkContent": "Performance Test Iteration Template  \nThis document provides template for capturing results of performance tests. Performance tests are done in iterations and each iteration should have a clear goal. The results of any iteration is immutable regardless whether the goal was achieved or not. If the iteration failed or the goal is not achieved then a new iteration of testing is carried out with appropriate fixes. It is recommended to keep track of the recorded iterations to maintain a timeline of how system evolved and which changes affected the performance in what way. Feel free to modify this template as needed.  \nIteration Template  \nGoal  \nMention in bullet points the goal for this iteration of test. The goal should be small and measurable within this iteration.  \nTest Details  \nDate: Date and time when this iteration started and ended  \nDuration: Time it took to complete this iteration.  \nApplication Code: Commit id and link to the commit for the code(s) which are being tested in this iteration  \nBenchmarking Configuration:  \nApplication Configuration: In bullet points mention the configuration for application that should be recorded  \nSystem Configuration: In bullet points mention the configuration of the infrastructure  \nRecord different types of configurations. Usually application specific configuration changes between iterations whereas system or infrastructure configurations rarely change  \nWork Items  \nList of links to relevant work items (task, story, bug) being tested in this iteration.  \nResults  \n{% raw %}  \nmd\nIn bullet points document the results from the test.\n- Attach any documents supporting the test results.\n- Add links to the dashboard for metrics and logs such as Application Insights.\n- Capture screenshots for metrics and include it in the results. Good candidate for this is CPU/Memory/Disk usage.  \n{% endraw %}  \nObservations  \nObservations are insights derived from test results. Keep the observations brief and as bullet points. Mention outcomes supporting the goal of the iteration. If any of the observation results in a work item (task, story, bug) then add the link to the work item together with the observation.",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\performance-testing\\iterative-perf-test-template.md"
    },
    {
        "chunkId": "chunk43_0",
        "chunkContent": "Load Testing  \n\"Load testing is performed to determine a system's behavior under both normal and anticipated peak load conditions.\" - Load testing - Wikipedia  \nA load test is designed to determine how a system behaves under expected normal and peak workloads. Specifically its main purpose is to confirm if a system can handle the expected load level. Depending on the target system this could be concurrent users, requests per second or data size.  \nWhy Load Testing  \nThe main objective is to prove the system can behave normally under the expected normal load before releasing it to production. The criteria that define \"behave normally\" will depend on your target, this may be as simple as \"the system remains available\", but it could also include meeting a response time SLA or error rate.  \nAdditionally, the results of a load test can also be used as data to help with capacity planning and calculating scalability.  \nLoad Testing Design Blocks  \nThere are a number of basic components that are required to carry out a load test.  \nIn order to have meaningful results the system needs to be tested in a production-like environment with a network and hardware which closely resembles the expected deployment environment.  \nThe load test will consist of a module which simulates user activity. Of course the composition of this \"user activity\" will vary based on the type of application being tested. For example, an e-commerce website might simulate user browsing and purchasing items, but an IoT data ingestion pipeline would simulate a stream of device readings. Please ensure the simulation is as close to real activity as possible, and consider not just volume but also patterns and variability. For example, if the simulator data is too uniform or predictable, then cache/hit ratios may impact your results.  \nThe load test will be initiated from a component external to the target system which can control the amount of load applied. This can be a single agent, but may need to scaled to multiple agents in order to achieve higher levels of activity.  \nAlthough not required to run a load test, it is advisable to have monitoring and/or logging in place to be able to measure the impact of the test and discover potential bottlenecks.  \nApplying the Load Testing  \nPlanning  \nIdentify key scenarios to measure - Gather these scenarios from Product Owner, they should provide a representative sample of real world traffic. The key activity of this phase is to agree on and define the load test cases.  \nDetermine expected normal and peak load for the scenarios - Determine a load level such as concurrent users or requests per second to find the size of the load test you will run.  \nIdentify success criteria metrics - These may be on testing side such as response time and error rate, or they may be on the system side such as CPU and memory usage.  \nAgree on test matrix - Which load test cases should be run for which combinations of input parameters.  \nSelect the right tool - Many frameworks exist for load testing so consider if features and limitations are suitable for your needs (Some popular tools are listed below). This may also include development of a custom load test client, see Preparation phase below.  \nObservability - Determine which metrics need to gathered to gain insight into throughput, latency, resource utilization, etc.  \nScalability - Determine the amount of scale needed by load generator, workload application, CPU, Memory, and network components needed to achieve testing goals. The use of kubernetes on the cloud can be used to make testing infinitely scalable.  \nPreparation  \nThe key activity is to replace the end user client with a test bench that simulates one or more instances of the original client. For standard 3rd party tools it may suffice to configure the existing test UI before initiating the load tests.  \nIf a custom client is used, code development will be required:  \nCustom development - Design for minimal impact/overhead. Be sure to capture only those features of the production client that are relevant from a load perspective. Does it matter if the same test is duplicated, or must the workload be unique for each test? Can all tests be run under the same user context?  \nTest environment - Create test environment that resembles production environment. This includes the platform as well as external systems, e.g., data sources.  \nSecurity contexts - Be sure to have all requisite security contexts for the test environment. Automation like pipelines may require special setup, e.g., OAuth2 client credential flow instead of auth code flow, because interactive login is replaced by non-interactive. Allow planning leeway in case admin approval is required for new security contexts.  \nTest data strategy - Make sure that output data format (ascii/binary/...) is compatible with whatever analysis tool is used in the analysis phase. This also includes storage areas (local/cloud/...), which may trigger new security contexts. Bear in mind that it may be necessary to collect data from sources external to the application to correlate potential performance issues with the application behavior. This includes platform and network metrics. Make sure to collect data that covers analysis needs (statistical measures, distributions, graphs, etc.).  \nAutomation - Repeatability is critical. It must be possible to re-run a given test multiple times to verify consistency and resilience of the application itself and the underlying platform.  Pipelines are recommended whenever possible.\nEvaluate whether load tests should be run as part of the PR strategy.  \nTest client debugging - All test modules should be carefully debugged to ensure that the execution phase progresses smoothly.  \nTest client validation - All test modules should be validated for extreme values of the input parameters. This reduces the risk of running into unexpected difficulties when stepping through the full test matrix during the execution phase.  \nExecution  \nIt is recommended to use an existing testing framework (see below). These tools will provide a method of both specifying the user activity scenarios and how to execute those at load. Depending on the situation, it may be advisable to coordinate testing activities with the platform operations team.  \nIt is common to slowly ramp up to your desired load to better replicate real world behavior. Once you have reached your defined workload, maintain this level long enough to see if your system stabilizes. To finish up the test you should also ramp to see record how the system slows down as well.  \nYou should also consider the origin of your load test traffic. Depending on the scope of the target system you may want to initiate from a different location to better replicate real world traffic such as from a different region.  \nNote: Before starting please be aware of any restrictions on your network such as DDOS protection where you may need to notify a network administrator or apply for an exemption.  \nNote: In general, the preferred approach to load testing would be the usage of a standard test framework such as the ones discussed below.  There are cases, however, where a custom test client may be advantageous. Examples include batch oriented workloads that can be run under a single security context and the same test data can be re-used for multiple load tests.  In such a scenario it may be beneficial to develop a custom script that can be used interactively as well as non-interactively.  \nAnalysis  \nThe analysis phase represents the work that brings all previous activities together:  \nSet aside time to allow for collection of new test data based on the analysis of the load tests.  \nCorrelate application metrics and platform metrics to identify potential pitfalls and bottlenecks.  \nInclude business stakeholders early in the analysis phase to validate application findings. Include platform operations to validate platform findings.  \nReport writing  \nSummarize your findings from the analysis phase. Be sure to include application and platform enhancement suggestions, if any.  \nFurther Testing  \nAfter completing your load test you should be set up to continue on to additional related testing such as;  \nSoak Testing - Also known as Endurance Testing. Performing a load test over an extended period of time to ensure long term stability.  \nStress Testing - Gradually increasing the load to find the limits of the system and identify the maximum capacity.  \nSpike Testing - Introduce a sharp short-term increase into the load scenarios.  \nScalability Testing - Re-testing of a system as your expand horizontally or vertically to measure how it scales.  \nDistributed Testing - Distributed testing allows you to leverage the power of multiple machines to perform larger or more in-depth tests faster. Is necessary when a fully optimized node cannot produce the load required by your extremely large test.  \nLoad Generation Testing Frameworks and Tools  \nHere are a few popular load testing frameworks you may consider, and the languages used to define your scenarios.  \nAzure Load Testing (https://learn.microsoft.com/en-us/azure/load-testing/) - Managed platform for running load tests on Azure. It allows to run and monitor tests automatically, source secrets from the KeyVault, generate traffic at scale, and load test Azure private endpoints. In the simple case, it executes load tests with HTTP GET traffic to a given endpoint. For the more complex cases, you can upload your own JMeter scenarios.  \nJMeter (https://github.com/apache/jmeter) - Has built in patterns to test without coding, but can be extended with Java.  \nArtillery (https://artillery.io/) - Write your scenarios in Javascript, executes a node application.  \nGatling (https://gatling.io/) -  Write your scenarios in Scala with their DSL.  \nLocust (https://locust.io/) - Write your scenarios in Python using the concept of concurrent user activity.  \nK6 (https://k6.io/) - Write your test scenarios in Javascript, available as open source kubernetes operator, open source Docker image, or as SaaS. Particularly useful for distributed load testing. Integrates easily with prometheus.  \nNBomber (https://nbomber.com/) - Write your test scenarios in C# or F#, available integration with test runners (NUnit/xUnit).  \nWebValidate (https://github.com/microsoft/webvalidate) - Web request validation tool used to run end-to-end tests and long-running performance and availability tests.  \nSample Workload Applications  \nIn the case where a specific workload application is not being provided and the focus is instead on the system, here are a few popular sample workload applications you may consider.  \nHttpBin (Python, GoLang) - Supports variety of endpoint types and language implementations. Can echo data used in request.  \nNGSA (Java, C#) - Intended for Kubernetes Platform and Monitoring Testing. Built on top of IMDB data store with many CRUD endpoints available. Does not need to have a live database connection.  \nMockBin (https://github.com/Kong/mockbin) - Allows you to generate custom endpoints to test, mock, and track HTTP requests & responses between libraries, sockets and APIs.  \nConclusion  \nA load test is critical step to understand if a target system will be reliable under the expected real world traffic.  \nOf course, it's only as good as your ability to predict the expected load, so it's important to follow up with other further testing to truly understand how your system behaves in different situations.  \nResources  \nList additional readings about this test type for those that would like to dive deeper.  \nMicrosoft Azure Well-Architected Framework > Load Testing",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\performance-testing\\load-testing.md"
    },
    {
        "chunkId": "chunk44_0",
        "chunkContent": "Performance Testing  \nPerformance Testing is an overloaded term that is used to refer to several\nsubcategories of performance related testing, each of which has different purpose.  \nA good description of overall performance testing is as follows:  \nPerformance testing is a type of testing intended to determine the\nresponsiveness, throughput, reliability, and/or scalability of a system under a\ngiven workload. Performance Testing Guidance for Web\nApplications.  \nBefore getting into the different subcategories of performance tests let us\nunderstand why performance testing is typically done.  \nWhy Performance Testing  \nPerformance testing is commonly conducted to accomplish one or more the\nfollowing:  \nTune the system's performance  \nIdentifying bottlenecks and issues with the system at different load\nlevels.  \nComparing performance characteristics of the system for different system\nconfigurations.  \nCome up with a scaling strategy for the system.  \nAssist in capacity planning  \nCapacity planning is the process of determining what type of hardware and\nsoftware resources are required to run an application to support pre-defined performance goals.  \nCapacity planning involves identifying business\nexpectations, the periodic fluctuations of application usage, considering\nthe cost of running the hardware and software infrastructure.  \nAssess the system's readiness for release:  \nEvaluating the system's performance characteristics (response time, throughput)\nin a production-like environment.\nThe goal is to ensure that performance goals can be achieved upon release.  \nEvaluate the performance impact of application changes  \nComparing the performance characteristics of an application after a change\nto the values of performance characteristics during previous runs (or\nbaseline values), can provide an indication of performance issues (performance regression) or\nenhancements introduced due to a change  \nKey Performance Testing categories  \nPerformance testing is a broad topic. There are many areas where you can perform\ntests. In broad strokes you can perform tests on the backend and on the front\nend. You can test the performance of individual components as well as testing\nthe end-to-end functionality.  \nThere are several categories of tests as well:  \nLoad Testing  \nThis is the subcategory of performance testing that focuses on validating the\nperformance characteristics of a system, when the system faces the load volumes\nwhich are expected during production operation. An Endurance Test or a Soak Test\nis a load test carried over a long duration ranging from several hours to\ndays.  \nStress Testing  \nThis is the subcategory of performance testing that focuses on validating the\nperformance characteristics of a system when the system faces extreme load. The\ngoal is to evaluate how does the system handles being pressured to its limits,\ndoes it recover (i.e., scale-out) or does it just break and fail?  \nEndurance Testing  \nThe goal of endurance testing is to make sure that the system can maintain\ngood performance under extended periods of load.  \nSpike testing  \nThe goal of Spike testing is to validate that a software system can respond well\nto large and sudden spikes.  \nChaos testing  \nChaos testing or Chaos engineering is the practice of experimenting on a system\nto build confidence that the system can withstand turbulent conditions in\nproduction. Its goal is to identify weaknesses before they manifest system wide.\nDevelopers often implement fallback procedures for service failure. Chaos\ntesting arbitrarily shuts down different parts of the system to validate that\nfallback procedures function correctly.  \nBest practices  \nConsider the following best practices for performance testing:  \nMake one change at a time. Don't make multiple changes to the system\nbetween tests. If you do, you won't know which change caused the performance\nto improve or degrade.  \nAutomate testing. Strive to automate the setup and teardown of resources\nfor a performance run as much as possible. Manual execution can lead to\nmisconfigurations.  \nUse different IP addresses. Some systems will throttle requests from a\nsingle IP address. If you are testing a system that has this type of\nrestriction, you can use different IP addresses to simulate multiple users.  \nPerformance monitor metrics  \nWhen executing the various types of testing approaches, whether it is stress,\nendurance, spike, or chaos testing, it is important to capture various\nmetrics to see how the system performs.  \nAt the basic hardware level, there are four areas to consider.  \nPhysical disk  \nMemory  \nProcessor  \nNetwork  \nThese four areas are inextricably linked, meaning that poor performance in one\narea will lead to poor performance in another area. Engineers concerned with\nunderstanding application performance, should focus on these four core areas.  \nThe classic example of how performance in one area can affect performance in\nanother area is memory pressure.  \nIf an application's available memory is running low, the operating system will\ntry to compensate for shortages in memory by transferring pages of data from\nmemory to disk, thus freeing up memory. But this work requires help from the CPU\nand the physical disk.  \nThis means that when you look at performance when there are low amounts of\nmemory, you will also notice spikes in disk activity as well as CPU.  \nPhysical Disk  \nAlmost all software systems are dependent on the performance of the physical\ndisk. This is especially true for the performance of databases. More modern\napproaches to using SSDs for physical disk storage can dramatically improve the\nperformance of applications. Here are some of the metrics that you can capture\nand analyze:  \nCounter Description Avg. Disk Queue Length This value is derived using the (Disk Transfers/sec)*(Disk sec/Transfer) counters. This metric describes the disk queue over time, smoothing out any quick spikes. Having any physical disk with an average queue length over 2 for prolonged periods of time can be an indication that your disk is a bottleneck. % Idle Time This is a measure of the percentage of time that the disk was idle. ie. there are no pending disk requests from the operating system waiting to be completed. A low number here is a positive sign that disk has excess capacity to service or write requests from the operating system. Avg. Disk sec/Read and Avg. Disk sec/Write These both measure the latency of your disks. Latency is defined as the average time it takes for a disk transfer to complete. You obviously want is low numbers as possible but need to be careful to account for inherent speed differences between SSD and traditional spinning disks. For this counter is important to define a baseline after the hardware is installed. Then use this value going forward to determine if you are experiencing any latency issues related to the hardware. Disk Reads/sec and Disk Writes/sec These counters each measure the total number of IO requests completed per second. Similar to the latency counters, good and bad values for these counters depend on your disk hardware but values higher than your initial baseline don't normally point to a hardware issue in this case. This counter can be useful to identify spikes in disk I/O.  \nProcessor  \nIt is important to understand the amount of time spent in kernel or privileged\nmode. In general, if code is spending too much time executing operating system\ncalls, that could be an area of concern because it will not allow you to run\nyour user mode applications, such as your databases, Web servers/services, etc.  \nThe guideline is that the CPU should only spend about 20% of the total processor\ntime running in kernel mode.  \nCounter Description % Processor time This is the percentage of total elapsed time that the processor was busy executing. This counter can either be too high or too low. If your processor time is consistently below 40%, then there is a question as to whether you have over provisioned your CPU. 70% is generally considered a good target number and if you start going higher than 70%, you may want to explore why there is high CPU pressure. % Privileged (Kernel Mode) time This measures the percentage of elapsed time the processor spent executing in kernel mode. Since this counter takes into account only kernel operations a high percentage of privileged time (greater than 25%) may indicate driver or hardware issue that should be investigated. % User time The percentage of elapsed time the processor spent executing in user mode (your application code). A good guideline is to be consistently below 65% as you want to have some buffer for both the kernel operations mentioned above as well as any other bursts of CPU required by other applications. Queue Length This is the number of threads that are ready to execute but waiting for a core to become available. On single core machines a sustained value greater than 2-3 can mean that you have some CPU pressure. Similarly, for a multicore machine divide the queue length by the number of cores and if that is continuously greater than 2-3 there might be CPU pressure.  \nNetwork Adapter  \nNetwork speed is often a hidden culprit of poor performance. Finding the root\ncause to poor network performance is often difficult. The source of issues can\noriginate from bandwidth hogs such as videoconferencing, transaction data,\nnetwork backups, recreational videos.  \nIn fact, the three most common reasons for a network slow down are:  \nCongestion  \nData corruption  \nCollisions  \nSome of the tools that can help include:  \nifconfig  \nnetstat  \niperf  \ntcpretrans  \ntcpdump  \nWireShark  \nTroubleshooting network performance usually begins with checking the hardware.\nTypical things to explore is whether there are any loose wires or checking that\nall routers are powered up. It is not always possible to do so, but sometimes a\nsimple case of power recycling of the modem or router can solve many problems.  \nNetwork specialists often perform the following sequence of troubleshooting steps:  \nCheck the hardware  \nUse IP config  \nUse ping and tracert  \nPerform DNS Check  \nMore advanced approaches often involve looking at some of the networking\nperformance counters, as explained below.  \nNetwork Counters  \nThe table above gives you some reference points to better understand what you\ncan expect out of your network. Here are some counters that can help you\nunderstand where the bottlenecks might exist:  \nCounter Description Bytes Received/sec The rate at which bytes are received over each network adapter. Bytes Sent/sec The rate at which bytes are sent over each network adapter. Bytes Total/sec The number of bytes sent and received over the network. Segments Received/sec The rate at which segments are received for the protocol Segments Sent/sec The rate at which segments are sent. % Interrupt Time The percentage of time the processor spends receiving and servicing hardware interrupts. This value is an indirect indicator of the activity of devices that generate interrupts, such as network adapters.  \nThere is an important distinction between latency and throughput.\nLatency measures the time it takes for a packet to be transferred across the\nnetwork, either in terms of a one-way transmission or a round-trip\ntransmission. Throughput is different and attempts to measure the quantity\nof data being sent and received within a unit of time.  \nMemory  \nCounter Description Available MBs This counter represents the amount of memory that is available to applications that are executing. Low memory can trigger Page Faults, whereby additional pressure is put on the CPU to swap memory to and from the disk. if the amount of available memory dips below 10%, more memory should be obtained. Pages/sec This is actually the sum of \"Pages Input/sec\" and \"Pages Output/sec\" counters which is the rate at which pages are being read and written as a result of pages faults. Small spikes with this value do not mean there is an issue but sustained values of greater than 50 can mean that system memory is a bottleneck. Paging File(_Total)\\% Usage The percentage of the system page file that is currently in use. This is not directly related to performance, but you can run into serious application issues if the page file does become completely full and additional memory is still being requested by applications.  \nKey Performance testing activities  \nPerformance testing activities vary depending on the subcategory of performance\ntesting and the system's requirements and constraints. For specific guidance you can\nfollow the link to the subcategory of performance tests listed above.\nThe following activities might be included depending on the performance test subcategory:  \nIdentify the Acceptance criteria for the tests  \nThis will generally include identifying the goals and constraints\nfor the performance characteristics of the system  \nPlan and design the tests  \nIn general we need to consider the following points:  \nDefining the load the application should be tested with  \nEstablishing the metrics to be collected  \nEstablish what tools will be used for the tests  \nEstablish the performance test frequency: whether the performance tests be\ndone as a part of the feature development sprints, or only prior to release to\na major environment?  \nImplementation  \nImplement the performance tests according to the designed approach.  \nInstrument the system and ensure that is emitting the needed performance metrics.  \nTest Execution  \nExecute the tests and collect performance metrics.  \nResult analysis and re-testing  \nAnalyze the results/performance metrics from the tests.  \nIdentify needed changes to tweak the system (i.e., code, infrastructure) to better accommodate the test objectives.  \nThen test again. This cycle continues until the test objective is achieved.  \nThe Iterative Performance Test Template can be used to capture details about the test result for every iterations.  \nResources  \nPatters and Practices: Performance Testing Guidance for Web\nApplications",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\performance-testing\\README.md"
    },
    {
        "chunkId": "chunk45_0",
        "chunkContent": "Shadow Testing  \nShadow testing is one approach to reduce risks before going to production. Shadow testing is also known as \"Shadow Deployment\" or \"Shadowing Traffic\" and similarities with \"Dark launching\".  \nWhen to use  \nShadow Testing reduces risks when you consider replacing the current environment (V-Current) with candidate environment with new feature (V-Next). This approach is monitoring and capturing differences between two environments then compare and reduces all risks before you introduce a new feature/release.  \nIn our test cases, code coverage is very important however sometimes providing code coverage can be tricky to replicate real-life combinations and possibilities. In this approach, to test V-Next environment we have side by side deployment, we're replicating the same traffic with V-Current environment and directing same traffic to V-Next environment, the only difference is we don't return any response from V-Next environment to users, but we collect those responses to compare with V-Current responses.  \nReferencing back to one of the Principles of Chaos Engineering, mentions importance of sampling real traffic like below:  \nSystems behave differently depending on environment and traffic patterns. Since the behavior of utilization can change at any time, sampling real traffic is the only way to reliably capture the request path. To guarantee both authenticity of the way in which the system is exercised and relevance to the current deployed system, Chaos strongly prefers to experiment directly on production traffic.  \nWith this Shadow Testing approach we're leveraging real customer behavior in V-Next environment with sampling real traffic and mitigating the risks which users may face on production. At the same time we're testing V-Next environment infrastructure for scaling with real sampled traffic. V-Next should scale with the same way V-Current does. We're testing actual behavior of the product and this cause zero impact to production to test new features since traffic is replicated to V-next environment.  \nThere are some similarities with Dark Launching, Dark Launching proposes to integrate new feature into production code, but users can't use the feature. On the backend you can test your feature and improve the performance until it's acceptable. It is also similar to Feature Toggles which provides you with an ability to enable/disable your new feature in production on a UI level. With this approach your new feature will be visible to users, and you can collect feedback. Using Dark Launching with Feature Toggles can be very useful for introducing a new feature.  \nApplicable to  \nProduction deployments: V-Next in Shadow testing always working separately and not effecting production. Users are not effected with this test.  \nInfrastructure: Shadow testing replicating the same traffic, in test environment you can have the same traffic on the production. It helps to produce real life test scenarios  \nHandling Scale: All traffic is replicated, and you have a chance to see how your system scaling.  \nShadow Testing Frameworks and Tools  \nThere are some tools to implement shadow testing. The main purpose of these tools is to compare responses of V-Current and V-Next then find the differences.  \nDiffy  \nEnvoy  \nMcRouter  \nScientist  \nOne of the most popular tools is Diffy. It was created and used at Twitter. Now the original author and a former Twitter employee maintains their own version of this project, called Opendiffy. Twitter announced this tool on their engineering blog as \"Testing services without writing tests\".  \nAs of today Diffy is used in production by Twitter, Airbnb, Baidu and Bytedance companies. Diffy explains the shadow testing feature like this:  \nDiffy finds potential bugs in your service using running instances of your new code, and your old code side by side. Diffy behaves as a proxy and multicasts whatever requests it receives to each of the running instances. It then compares the responses, and reports any regressions that may surface from those comparisons. The premise for Diffy is that if two implementations of the service return \u201csimilar\u201d responses for a sufficiently large and diverse set of requests, then the two implementations can be treated as equivalent, and the newer implementation is regression-free.  \nDiffy architecture  \nConclusion  \nShadow Testing is a useful approach to reduce risks when you consider replacing the current environment with candidate environment using new feature(s). Shadow testing replicates traffic of the production to candidate environment for testing, so you get same production use case scenarios in the test environment. You can compare differences on both environments and validate your candidate environment to be ready for releasing.  \nSome advantages of shadow testing are:  \nZero impact to production environment  \nNo need to generate test scenarios and test data  \nWe can test real-life scenarios with real-life data.  \nWe can simulate scale with replicated production traffic.  \nReferences  \nMartin Fowler - Dark Launching  \nMartin Fowler - Feature Toggle  \nTraffic Shadowing/Mirroring",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\shadow-testing\\README.md"
    },
    {
        "chunkId": "chunk46_0",
        "chunkContent": "Smoke Testing  \nSmoke tests, sometimes named Sanity, Acceptance, or Build/Release Verification tests, are a sub-type of system/functional tests that are usually used as gates that verify the application's readiness as a preliminary step. If an application passes the smoke tests, it is acceptable, or in a stable-enough state, for the next stages of testing or deployment.  \nWhen To Use  \nProblem Addressed  \nSmoke tests are meant to find, as early as possible, if an application is working or not. The goal of smoke tests is to save time; if the current version of the application does not pass smoke tests, then the rest of the integration or deployment chain for it can be abandoned. Smoke tests do not aim to provide full functionality coverage but instead focus on a few quick acceptance invocations for which the application should, at all times, respond correctly to.  \nROI Tipping Point  \nSmoke tests cover only the most critical application path, and should not be used to actually test the application's behavior, keeping execution time and complexity to minimum. The tests can be formed of a subset of the application's integration or e2e tests, and they cover as much of the functionality with as little depth as required.  \nThe golden rule of a good smoke test is that it saves time on validating that the application is acceptable to a stage where better, more thorough testing will begin.  \nApplicable to  \n[x] Local dev desktop - Example: Applying manual smoke testing to verify that the application is OK.  \n[x] Build pipelines - Example: Running a small set of the integration test suite before running the full coverage of tests, which may take a long time.  \n[x] Non-production and Production deployments - Example: Running a curl command to the product's API and asserting the response is 200 before running load test which consume resources.  \n[x] PR Validation - Example: - Deploying the application chart to a test namespace and validating the release is successful and no immediate regressions are merged.  \nConclusion  \nSmoke testing is a low-effort, high-impact step to ship more reliable software. It should be considered amongst the first stages to implement when planning continuously integrated and delivered systems.  \nResources  \nWikipedia - Smoke Testing  \nGoogle SRE Book - System Tests",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\smoke-testing\\README.md"
    },
    {
        "chunkId": "chunk47_0",
        "chunkContent": "Synthetic Monitoring Tests  \nSynthetic Monitoring Tests are a set of functional tests that target a live system in production. The focus of these tests, which are sometimes named \"watchdog\", \"active monitoring\" or \"synthetic transactions\", is to verify the product's health and resilience continuously.  \nWhy Synthetic Monitoring tests  \nTraditionally, software providers rely on testing through CI/CD stages in the well known testing pyramid (unit, integration, e2e) to validate that the product is healthy and without regressions. Such tests will run on the build agent or in the test/stage environment before being deployed to production and released to live user traffic. During the services' lifetime in the production environment, they are safeguarded by monitoring and alerting tools that rely on Real User Metrics/Monitoring (RUM).  \nHowever, as more organizations today provide highly-available (99.9+ SLA) products, they find that the nature of long-lived distributed applications, which typically rely on several hardware and software components, is to fail. Frequent releases (sometimes multiple times per day) of various components of the system can create further instability. This rapid rate of change to the production environment tends to make testing during CI/CD stages not hermetic and actually not representative of the end user experience and how the production system actually behaves.  \nFor such systems, the ambition of service engineering teams is to reduce to a minimum the time it takes to fix errors, or the MTTR - Mean Time To Repair. It is a continuous effort, performed on the live/production system. Synthetic Monitors can be used to detect the following issues:  \nAvailability - Is the system or specific region available.  \nTransactions and customer journeys - Known good requests should work, while known bad requests should error.  \nPerformance - How fast are actions and is that performance maintained through high loads and through version releases.  \n3rd Party components - Cloud or software components used by the system may fail.  \nShift-Right Testing  \nSynthetic Monitoring tests are a subset of tests that run in production, sometimes named Test-in-Production or Shift-Right tests.\nWith Shift-Left paradigms that are so popular, the approach is to perform testing as early as possible in the application development lifecycle (i.e., moved left on the project timeline).\nShift right compliments and adds on top of Shift-Left. It refers to running tests late in the cycle, during deployment, release, and post-release when the product is serving production traffic. They provide modern engineering teams a broader set of tools to assure high SLAs over time.  \nSynthetic Monitoring tests Design Blocks  \nA synthetic monitoring test is a test that uses synthetic data and real testing accounts to inject user behaviors to the system and validates their effect, usually by passively relying on existing monitoring and alerting capabilities.\nComponents of synthetic monitoring tests include Probes, test code/ accounts which generates data, and Monitoring tools placed to validate both the system's behavior under test and the health of the probes themselves.  \nProbes  \nProbes are the source of synthetic user actions that drive testing. They target the product's front-end or publicly-facing APIs and are running on their own production environment.\nA Synthetic Monitoring test is, in fact, very related to black-box tests and would usually focus on end-to-end scenarios from a user's perspective. It is not uncommon for the same code for e2e or integration tests to be used to implement the probe.  \nMonitoring  \nGiven that Synthetic Monitoring tests are continuously running, at intervals, in a production environment, the assertion of system behavior through analysis relies on existing monitoring pillars used in live system (Logging, Metrics, Distributed Tracing).\nThere would usually be a finite set of tests, and key metrics that are used to build monitors and alerts to assert against the known SLO, and verify that the OKR for that system are maintained. The monitoring tools are effectively capturing both RUMs and synthetic data generated by the probes.  \nApplying Synthetic Monitoring Tests  \nAsserting the system under tests  \nSynthetic monitoring tests are usually statistical. Test metrics are compared against some historical or running average with a time dimension (Example: Over the last 30 days, for this time of day, the mean average response time is 250ms for AddToCart operation with a standard deviation from the mean of +/- 32ms). So if an observed measurement is within a deviation of the norm at any time, the services are probably healthy.  \nBuilding a Synthetic Monitoring Solution  \nAt a high level, building synthetic monitors usually consists of the following steps:  \nDetermine the metric to be validated (functional result, latency, etc.)  \nBuild a piece of automation that measures that metric against the system, and gathers telemetry into the system's existing monitoring infrastructure.  \nSet up monitoring alarms/actions/responses that detect the failure of the system to meet the desired goal of the metric.  \nRun the test case automation continuously at an appropriate interval.  \nMonitoring the health of tests  \nProbes runtime is a production environment on its own, and the health of tests is critical. Many providers offer cloud-based systems that host such runtimes, while some organizations use existing production environments to run these tests on. In either way, a monitor-the-monitor strategy should be a first-class citizen of the production environment's alerting systems.  \nSynthetic Monitoring and Real User Monitoring  \nSynthetic monitoring does not replace the need for RUM. Probes are predictable code that verifies specific scenarios, and they do not 100% completely and truly represent how a user session is handled. On the other hand, prefer not to use RUMs to test for site reliability because:  \nAs the name implies, RUM requires user traffic. The site may be down, but since no user visited the monitored path, no alerts were triggered yet.  \nInconsistent Traffic and usage patterns make it hard to gauge for benchmarks.  \nRisks  \nTesting in production, in general, has a risk factor attached to it, which does not exist tests executed during CI/CD stages. Specifically, in synthetic monitoring tests, the following may affect the production environment:  \nCorrupted or invalid data - Tests inject test data which may be in some ways corrupt. Consider using a testing schema.  \nProtected data leakage - Tests run in a production environment and emit logs or trace that may contain protected data.  \nOverloaded systems - Synthetic tests may cause errors or overload the system.  \nUnintended side effects or impacts on other production systems.  \nSkewed analytics (traffic funnels, A/B test results, etc.)  \nAuth/AuthZ - Tests are required to run in production where access to tokens and secrets may be restricted or more challenging to retrieve.  \nSynthetic Monitoring tests Frameworks and Tools  \nMost key monitoring/APM players have an enterprise product that supports synthetic monitoring built into their systems (see list below). Such offerings make some of the risks raised above irrelevant as the integration and runtime aspects of the solution are OOTB. However, such solutions are typically pricey.  \nSome organizations prefer running probes on existing infrastructure using known tools such as Postman, Wrk, JMeter, Selenium or even custom code to generate the synthetic data. Such solutions must account for isolating and decoupling the probe's production environment from the core product's as well as provide monitoring, geo-distribution, and maintaining test health.  \nApplication Insights availability - Simple availability tests that allow some customization using Multi-step web test  \nDataDog Synthetics  \nDynatrace Synthetic Monitoring  \nNew Relic Synthetics  \nCheckly  \nConclusion  \nThe value of production tests, in general, and specifically Synthetic monitoring, is only there for particular engagement types, and there is associated risk and cost to them. However, when applicable, they provide continuous assurance that there are no system failures from a user's perspective.\nWhen developing a PaaS/SaaS solution, Synthetic monitoring is key to the success of service reliability teams, and they are becoming an integral part of the quality assurance stack of highly available products.  \nResources  \nGoogle SRE book - Testing Reliability  \nMicrosoft DevOps Architectures - Shift Right to Test in Production  \nMartin Fowler - Synthetic Monitoring",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\synthetic-monitoring-tests\\README.md"
    },
    {
        "chunkId": "chunk48_0",
        "chunkContent": "Tech specific samples  \nazdo-container-dev-test-release  \nblobstorage-unit-tests",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\tech-specific-samples\\README.md"
    },
    {
        "chunkId": "chunk49_0",
        "chunkContent": "Building Containers with Azure DevOps using DevTest Pattern  \nIn this documents, we highlight learnings from applying the DevTest pattern to container development in Azure DevOps through pipelines.  \nThe pattern enabled as to build container for development, testing and releasing the container for further reuse (production ready).  \nWe will dive into tools needed to build, test and push a container, our environment and go through each step separately.  \nFollow this link to dive deeper or revisit the DevTest pattern.  \nTable of Contents  \nBuild the Container\nTest the Container\nPush Container\nReferences  \nBuild the Container  \nThe first step in container development, after creating the necessary Dockerfiles and source code, is building the container. Even the Dockerfile itself can include some basic testing. Code tests are performed when pushing the code to the repository origin, where it is then used to build the container.  \nThe first step in our pipeline is to run the docker build command with a temporary tag and the required build arguments:  \n{% raw %}  \nyaml\n- task: Bash@3\nname: BuildImage\ndisplayName: 'Build the image via docker'\ninputs:\nworkingDirectory: \"$(System.DefaultWorkingDirectory)${{ parameters.buildDirectory }}\"\ntargetType: 'inline'\nscript: |\ndocker build -t ${{ parameters.imageName }} --build-arg YOUR_BUILD_ARG -f ${{ parameters.dockerfileName }} .\nenv:\nPredefinedPassword: $(Password)\nNewVariable: \"newVariableValue\"  \n{% endraw %}  \nThis task includes the parameters buildDirectory, imageName and dockerfileName, which have to be set beforehand.\nThis task can for example be used in a template for multiple containers to improve code reuse.  \nIt is also possible to pass environment variables directly to the Dockerfile through the env section of the task.  \nIf this task succeeds, the Dockerfile was build without errors and we can continue to testing the container itself.  \nTest the Container  \nTo test the container, we are using the tox environment.\nFor more details on tox please visit the tox section of this repository or visit the official tox documentation page.  \nBefore we test the container, we are checking for exposed credentials in the docker image history.\nIf known passwords, used to access our internal resources, are exposed here, the build step will fail:  \n{% raw %}  \nyml\n- task: Bash@3\nname: CheckIfPasswordInDockerHistory\ndisplayName: 'Check for password in docker history'\ninputs:\nworkingDirectory: \"$(System.DefaultWorkingDirectory)\"\ntargetType: 'inline'\nfailOnStdErr: true\nscript: |\nif docker image history --no-trunc ${{ parameters.imageName }} | grep -qF $PredefinedPassword; then\nexit 1;\nfi\nexit 0;\nenv:\nPredefinedPassword: $(Password)  \n{% endraw %}  \nAfter the credential test, the container is tested through the pytest extension testinfra.\nTestinfra is a Python-based tool which can be used to start a container, gather prerequisites, test the container and shut it down again, without any effort besides writing the tests. These tests can for example include:  \nif files exist  \nif environment variables are set correctly  \nif certain processes are running  \nif the correct host environment is used  \nFor a complete collection of capabilities and requirements, please visit the testinfra project on GitHub.  \nA few methods of a Linux-based container test can look like this:  \n{% raw %}  \n```python\ndef test_dependencies(host):\n'''\nCheck all files needed to run the container properly.\n'''\nenv_file = \"/app/environment.sh.env\"\nassert host.file(env_file).exists\n\ndef test_container_running(host):\nprocess = host.process.get(comm=\"start.sh\")\nassert process.user == \"root\"\n\ndef test_host_system(host):\nsystem_type = 'linux'\ndistribution = 'ubuntu'\nrelease = '18.04'\n\ndef extract_env_var(file_content):\nimport re\n\ndef test_ports_exposed(host):\nport1 = \"9010\"\nst1 = f\"grep -q {port1} /app/Dockerfile && echo 'true' || echo 'false'\"\ncmd1 = host.run(st1)\nassert cmd1.stdout\n\ndef test_listening_simserver_sockets(host):\nassert host.socket(\"tcp://0.0.0.0:32512\").is_listening\nassert host.socket(\"tcp://0.0.0.0:32513\").is_listening\n```  \n{% endraw %}  \nTo start the test, a pytest command is executed through tox.  \nA task containing the tox command can look like this:  \n{% raw %}  \nyaml\n- task: Bash@3\nname: RunTestCommands\ndisplayName: \"Test - Run test commands\"\ninputs:\nworkingDirectory: \"$(System.DefaultWorkingDirectory)\"\ntargetType: 'inline'\nscript: |\ntox -e testinfra-${{ parameters.makeTarget }} -- ${{ parameters.imageName }}\nfailOnStderr: true  \n{% endraw %}  \nWhich could trigger the following pytest code, which is contained in the tox.ini file:  \n{% raw %}  \nbash\npytest -vv tests/{env:CONTEXT} --container-image={posargs:{env:IMAGE_TAG}} --volume={env:VOLUME}  \n{% endraw %}  \nAs a last task of this pipeline to build and test the container, we set a variable called testsPassed which is only true, if the previous tasks succeeded:  \n{% raw %}  \nyml\n- task: Bash@3\nname: UpdateTestResultVariable\ncondition: succeeded()\ninputs:\ntargetType: 'inline'\nscript: |\necho '##vso[task.setvariable variable=testsPassed]true'  \n{% endraw %}  \nPush container  \nAfter building and testing, if our container runs as expected, we want to release it to our Azure Container Registry (ACR) to be used by our larger application. Before that, we want to automate the push behavior and define a meaningful tag.  \nAs a developer it is often helpful to have containers pushed to ACR, even if they are failing.\nThis can be done by checking for the testsPassed variable we introduced at the end of our testing.  \nIf the test failed, we want to add a failed suffix at the end of the tag:  \n{% raw %}  \nyml\n- task: Bash@3\nname: SetFailedSuffixTag\ndisplayName: \"Set failed suffix, if the tests failed.\"\ncondition: and(eq(variables['testsPassed'], false), ne(variables['Build.SourceBranchName'], 'main'))",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\tech-specific-samples\\azdo-container-dev-test-release\\README.md"
    },
    {
        "chunkId": "chunk49_1",
        "chunkContent": "# if this is not a release and failed -> retag the image to add failedSuffix\ninputs:\ntargetType: inline\nscript: |\ndocker tag ${{ parameters.containerRegistry }}/${{ parameters.imageRepository }}:${{ parameters.imageTag }} ${{ parameters.containerRegistry }}/${{ parameters.imageRepository }}:${{ parameters.imageTag }}$(failedSuffix)  \n{% endraw %}  \nThe condition checks, if the value of testsPassed is false and also if we\nare not on the main branch, as we don't want to push failed containers from main.\nThis helps us to keep our production environment clean.  \nThe value for imageRepository was defined in another template, along with\nthe failedSuffix and testsPassed:  \n{% raw %}  \n```yml\nparameters:\n- name: component\n\nvariables:\ntestsPassed: false\nfailedSuffix: \"-failed\"\n# the imageRepo will changed based on dev or release\n${{ if eq( variables['Build.SourceBranchName'], 'main' ) }}:\nimageRepository: 'stable/${{ parameters.component }}'\n${{ if ne( variables['Build.SourceBranchName'], 'main' ) }}:\nimageRepository: 'dev/${{ parameters.component }}'\n```  \n{% endraw %}  \nThe imageTag is open to discussion, as it depends highly on how your team wants\nto use the container. We went for Build.SourceVersion which is the commit ID\nof the branch the container was developed in.\nThis allows you to easily track the origin of the container and aids debugging.  \nA link to Azure DevOps predefined variables can be found in the\nAzure Docs on Azure DevOps  \nAfter a tag was added to the container, the image must be pushed.\nThis can be done with the following task:  \n{% raw %}  \nyml\n- task: Docker@1\nname: pushFailedDockerImage\ndisplayName: 'Pushes failed image via Docker'\ncondition: and(eq(variables['testsPassed'], false), ne(variables['Build.SourceBranchName'], 'main'))",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\tech-specific-samples\\azdo-container-dev-test-release\\README.md"
    },
    {
        "chunkId": "chunk49_2",
        "chunkContent": "# if this is not a release and failed -> push the image with the failed tag\ninputs:\ncontainerregistrytype: 'Azure Container Registry'\nazureSubscriptionEndpoint: ${{ parameters.serviceConnection }}\nazureContainerRegistry: ${{ parameters.containerRegistry }}\ncommand: 'Push an image'\nimageName: '${{ parameters.imageRepository }}:${{ parameters.imageTag }}$(failedSuffix)'  \n{% endraw %}  \nSimilarly, these are the steps to publish the container to the ACR,\nif the tests succeeded:  \n{% raw %}  \nyml\n- task: Bash@3\nname: SetLatestSuffixTag\ndisplayName: \"Set latest suffix, if the tests succeed.\"\ncondition:  eq(variables['testsPassed'], true)\ninputs:\ntargetType: inline\nscript: |\ndocker tag ${{ parameters.containerRegistry }}/${{ parameters.imageRepository }}:${{ parameters.imageTag }} ${{ parameters.containerRegistry }}/${{ parameters.imageRepository }}:latest\n- task: Docker@1\nname: pushSuccessfulDockerImageSha\ndisplayName: 'Pushes successful image via Docker'\ncondition: eq(variables['testsPassed'], true)\ninputs:\ncontainerregistrytype: 'Azure Container Registry'\nazureSubscriptionEndpoint: ${{ parameters.serviceConnection }}\nazureContainerRegistry: ${{ parameters.containerRegistry }}\ncommand: 'Push an image'\nimageName: '${{ parameters.imageRepository }}:${{ parameters.imageTag }}'\n- task: Docker@1\nname: pushSuccessfulDockerImageLatest\ndisplayName: 'Pushes successful image as latest'\ncondition: eq(variables['testsPassed'], true)\ninputs:\ncontainerregistrytype: 'Azure Container Registry'\nazureSubscriptionEndpoint: ${{ parameters.serviceConnection }}\nazureContainerRegistry: ${{ parameters.containerRegistry }}\ncommand: 'Push an image'\nimageName: '${{ parameters.imageRepository }}:latest'  \n{% endraw %}  \nIf you don't want to include the latest tag, you can also remove the steps\ninvolving latest (SetLatestSuffixTag & pushSuccessfulDockerImageLatest).  \nReferences  \nDevTest pattern  \nAzure Docs on Azure DevOps  \nofficial tox documentation page  \nTestinfra  \nTestinfra project on GitHub  \npytest",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\tech-specific-samples\\azdo-container-dev-test-release\\README.md"
    },
    {
        "chunkId": "chunk50_0",
        "chunkContent": "Using Azurite to Run Blob Storage Tests in a Pipeline  \nThis document determines the approach for writing automated tests with a short feedback loop (i.e. unit tests) against security considerations (private endpoints) for the Azure Blob Storage functionality.  \nOnce private endpoints are enabled for the Azure Storage accounts, the current tests will fail when executed locally or as part of a pipeline because this connection will be blocked.  \nUtilize an Azure Storage emulator - Azurite  \nTo emulate a local Azure Blob Storage, we can use Azure Storage Emulator. The Storage Emulator currently runs only on Windows. If you need a Storage Emulator for Linux, one option is the community maintained, open-source Storage Emulator Azurite.  \nThe Azure Storage Emulator is no longer being actively developed. Azurite is the Storage Emulator platform going forward. Azurite supersedes the Azure Storage Emulator. Azurite will continue to be updated to support the latest versions of Azure Storage APIs. For more information, see Use the Azurite emulator for local Azure Storage development.  \nSome differences in functionality exist between the Storage Emulator and Azure storage services. For more information about these differences, see the Differences between the Storage Emulator and Azure Storage.  \nThere are several ways to install and run Azurite on your local system as listed here. In this document we will cover Install and run Azurite using NPM and Install and run the Azurite Docker image.  \n1. Install and run Azurite  \na. Using NPM  \nIn order to run Azurite V3 you need Node.js >= 8.0 installed on your system. Azurite works cross-platform on Windows, Linux, and OS X.  \nAfter the Node.js installation, you can install Azurite simply with npm which is the Node.js package management tool included with every Node.js installation.  \n{% raw %}  \n```bash\n\nInstall Azurite\n\nnpm install -g azurite\n\nCreate azurite directory\n\nmkdir c:/azurite\n\nLaunch Azurite for Windows\n\nazurite --silent --location c:\\azurite --debug c:\\azurite\\debug.log\n```  \n{% endraw %}  \nIf you want to avoid any disk persistence and destroy the test data when the Azurite process terminates, you can pass the --inMemoryPersistence option, as of Azurite 3.28.0.  \nThe output will be:  \n{% raw %}  \nshell\nAzurite Blob service is starting at http://127.0.0.1:10000\nAzurite Blob service is successfully listening at http://127.0.0.1:10000\nAzurite Queue service is starting at http://127.0.0.1:10001\nAzurite Queue service is successfully listening at http://127.0.0.1:10001  \n{% endraw %}  \nb. Using a docker image  \nAnother way to run Azurite is using docker, using default HTTP endpoint  \n{% raw %}  \nbash\ndocker run -p 10000:10000 mcr.microsoft.com/azure-storage/azurite azurite-blob --blobHost 0.0.0.0  \n{% endraw %}  \nDocker Compose is another option and can run the same docker image using the docker-compose.yml file below.  \n{% raw %}  \nyaml\nversion: '3.4'\nservices:\nazurite:\nimage: mcr.microsoft.com/azure-storage/azurite\nhostname: azurite\nvolumes:\n- ./cert/azurite:/data\ncommand: \"azurite-blob --blobHost 0.0.0.0 -l /data --cert /data/127.0.0.1.pem --key /data/127.0.0.1-key.pem --oauth basic\"\nports:\n- \"10000:10000\"\n- \"10001:10001\"  \n{% endraw %}  \n2. Run tests on your local machine  \nPython 3.8.7 is used for this, but it should be fine on other 3.x versions as well.  \nInstall and run Azurite for local tests:  \nOption 1: using npm:  \n{% raw %}  \nbash",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\tech-specific-samples\\blobstorage-unit-tests\\README.md"
    },
    {
        "chunkId": "chunk50_1",
        "chunkContent": "# Install Azurite\nnpm install -g azurite",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\tech-specific-samples\\blobstorage-unit-tests\\README.md"
    },
    {
        "chunkId": "chunk50_2",
        "chunkContent": "# Create azurite directory\nmkdir c:/azurite",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\tech-specific-samples\\blobstorage-unit-tests\\README.md"
    },
    {
        "chunkId": "chunk50_3",
        "chunkContent": "# Launch Azurite for Windows\nazurite --silent --location c:\\azurite --debug c:\\azurite\\debug.log  \n{% endraw %}  \nOption 2: using docker  \n{% raw %}  \nbash\ndocker run -p 10000:10000 mcr.microsoft.com/azure-storage/azurite azurite-blob --blobHost 0.0.0.0  \n{% endraw %}  \nIn Azure Storage Explorer, select Attach to a local emulator  \nProvide a Display name and port number, then your connection will be ready, and you can use Storage Explorer to manage your local blob storage.  \nTo test and see how these endpoints are running you can attach your local blob storage to the Azure Storage Explorer.  \nCreate a virtual python environment\npython -m venv .venv  \nContainer name and initialize env variables: Use conftest.py for test integration.  \n{% raw %}  \n```python\nfrom azure.storage.blob import BlobServiceClient\nimport os\n\ndef pytest_generate_tests(metafunc):\nos.environ['STORAGE_CONNECTION_STRING'] = 'DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://127.0.0.1:10000/devstoreaccount1;'\nos.environ['STORAGE_CONTAINER'] = 'test-container'\n\n```  \n{% endraw %}  \nNote: value for STORAGE_CONNECTION_STRING is default value for Azurite, it's not a private key  \nInstall the dependencies\npip install -r requirements_tests.txt  \nRun tests:  \n{% raw %}  \nbash\npython -m pytest ./tests  \n{% endraw %}  \nAfter running tests, you can see the files in your local blob storage  \n3. Run tests on Azure Pipelines  \nAfter running tests locally we need to make sure these tests pass on Azure Pipelines too. We have 2 options here, we can use docker image as hosted agent on Azure or install an npm package in the Pipeline steps.  \n{% raw %}  \n```bash\ntrigger:\n- master\n\nsteps:\n- task: UsePythonVersion@0\ndisplayName: 'Use Python 3.7'\ninputs:\nversionSpec: 3.7\n\nbash: |\npip install -r requirements_tests.txt\ndisplayName: 'Setup requirements for tests'\n\nbash: |\nsudo npm install -g azurite\nsudo mkdir azurite\nsudo azurite --silent --location azurite --debug azurite\\debug.log &\ndisplayName: 'Install and Run Azurite'\n\nbash: |\npython -m pytest --junit-xml=unit_tests_report.xml --cov=tests --cov-report=html --cov-report=xml ./tests\ndisplayName: 'Run Tests'\n\ntask: PublishCodeCoverageResults@1\ninputs:\ncodeCoverageTool: Cobertura\nsummaryFileLocation: '$(System.DefaultWorkingDirectory)//coverage.xml'\nreportDirectory: '$(System.DefaultWorkingDirectory)//htmlcov'\n\ntask: PublishTestResults@2\ninputs:\ntestResultsFormat: 'JUnit'\ntestResultsFiles: '*/_tests_report.xml'\nfailTaskOnFailedTests: true\n```  \n{% endraw %}  \nOnce we set up our pipeline in Azure Pipelines, result will be like below",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\tech-specific-samples\\blobstorage-unit-tests\\README.md"
    },
    {
        "chunkId": "chunk51_0",
        "chunkContent": "~Customer Project~ Case Study  \nBackground  \nDescribe the customer and business requirements with the explicit problem statement.  \nSystem Under Test (SUT)  \nInclude the system's conceptual architecture and highlight the architecture components that were included in the E2E testing.  \nProblems and Limitations  \nDescribe about the problems of the overall SUT solution that prevented from testing specific (or any) part of the solution.\nDescribe limitation of the testing tools and framework(s) used in this implementation  \nE2E Testing Framework and Tools  \nDescribe what testing framework and/or tools were used to implement E2E testing in the SUT.  \nTest Cases  \nDescribe the E2E test cases were created to E2E test the SUT  \nTest Metrics  \nDescribe any architecture solution were used to monitor, observe and track the various service states that were used as the E2E testing metrics. Also, include the list of test cases were build to measure the progress of E2E testing.  \nE2E Testing Architecture  \nDescribe any testing architecture were built to run E2E testing.  \nE2E Testing Implementation (Code samples)  \nInclude sample test cases and their implementation in the programming language of choice.\nInclude any common reusable code implementation blocks that could be leveraged in the future project's E2E testing implementation.  \nE2E Testing Reporting and Results  \nInclude sample of E2E testing reports and results obtained from the E2E testing runs in this project.",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\templates\\case-study-template.md"
    },
    {
        "chunkId": "chunk52_0",
        "chunkContent": "Templates  \ncase-study-template  \ntest-type-template",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\templates\\README.md"
    },
    {
        "chunkId": "chunk53_0",
        "chunkContent": "Insert Test Technique Name Here  \nPut a 2-3 sentence overview about the test technique here.  \nWhen To Use  \nProblem Addressed  \nDescribing the problem that this test type addresses, this should focus on the motivation behind the test type/technique to help the reader correlate this technique to their problem.  \nWhen to Avoid  \nDescribe when NOT to use, if applicable.  \nROI Tipping Point  \nHow much is enough?  For example, some opine that unit test ROI drops significantly at 80% block coverage and when the codebase is well-exercised by real traffic in production.  \nApplicable to  \n[ ] Local dev 'desktop'  \n[ ] Build pipelines  \n[ ] Non-production deployments  \n[ ] Production deployments  \nNOTE: If there is great (clear, succinct) documentation for the technique on the web, supply a pointer and skip the rest of this template.  No need to re-type content  \nHow to Use  \nArchitecture  \nDescribe the components of the technique and how they interact with each other and the subject of the test technique.  Add a simple diagram of how the technique's parts are organized, if helpful to illustrate.  \nPre-requisites  \nAnything required in advance?  \nHigh-level Step-by-step  \n1.\n1.\n1.  \nBest Practices and Advice  \nDescribe what good testing looks like for this technique, best practices, pitfalls.  \nAnti patterns  \ne.g. unit tests should never require off-box or even out-of-process dependencies.  Are there similar things to avoid when applying this technique?  \nFrameworks, Tools, Templates  \nDescribe known good (i.e. actually used and known to provide good results) frameworks, tools, templates, their pros and cons, with links.  \nResources  \nProvide links to further readings about this technique to dive deeper.",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\templates\\test-type-template.md"
    },
    {
        "chunkId": "chunk54_0",
        "chunkContent": "User Interface Testing  \nThis section is primarily geared towards web-based UIs, but the guidance is similar for mobile and OS based applications.  \nApplicability  \nUI Testing is not always going to be applicable, for example applications without a UI or parts of an application that require no human interaction.  In those cases unit, functional and integration/e2e testing would be the primary means.  UI Testing is going to be mainly applicable when dealing with a public facing UI that is used in a diverse environment or in a mission critical UI that requires higher fidelity.  With something like an admin UI that is used by just a handful of people, UI Testing is still valuable but not as high priority.  \nGoals  \nUI testing provides the ability to ensure that users have a consistent visual user experience across a variety of means of access and that the user interaction is consistent with the function requirements.  \nEnsure the UI appearance and interaction satisfy the functional and non-functional requirements  \nDetect changes in the UI both across devices and delivery platforms and between code changes  \nProvide confidence to designers and developers the user experience is consistent  \nSupport fast code evolution and refactoring while reducing the risk of regressions  \nEvidence and Measures  \nIntegrating UI Tests in to your CI/CD is necessary but more challenging than unit tests.  The increased challenge is that UI tests either need to run in headless mode with something like Puppeteer or there needs to be more extensive orchestration with Azure DevOps or GitHub that would handle the full testing integration for you like BrowserStack  \nIntegrations like BrowserStack are nice since they provide Azure DevOps reports as part of the test run.  \nThat said, Azure DevOps supports a variety of test adapters, so you can use any UI Testing framework that supports outputting the test results to one of the output formats listed at Publish Test Results task.  \nIf you're using an Azure DevOps pipeline to run UI tests, consider using a self hosted agent in order to manage framework versions and avoid unexpected updates.  \nGeneral Guidance  \nThe scope of UI testing should be strategic. UI tests can take a significant amount of time to both implement and run, and it's challenging to test every type of user interaction in a production application due to the large number of possible interactions.  \nDesigning the UI tests around the functional tests makes sense.  For example, given an input form, a UI test would ensure that the visual representation is consistent across devices, is accessible and easy to interact with, and is consistent across code changes.  \nUI Tests will catch 'runtime' bugs that unit and functional tests won't.  For example if the submit button for an input form is rendered but not clickable due to a positioning bug in the UI, then this could be considered a runtime bug that would not have been caught by unit or functional tests.  \nUI Tests can run on mock data or snapshots of production data, like in QA or staging.  \nWriting Tests  \nGood UI tests follow a few general principles:  \nChoose a UI testing framework that enables quick feedback and is easy to use  \nDesign the UI to be easily testable.  For example, add CSS selectors or set the id on elements in a web page to allow easier selecting.  \nTest on all primary devices that the user uses, don't just test on a single device or OS.  \nWhen a test mutates data ensure that data is created on demand and cleaned up after.  The consequence of not doing this would be inconsistent testing.  \nCommon Issues  \nUI Testing can get very challenging at the lower level, especially with a testing framework like Selenium.  If you choose to go this route, then you'll likely encounter timeouts, missing elements, and you'll have significant friction with the testing framework itself.  Due to many issues with UI testing there have been a number of free and paid solutions that help alleviate certain issues with frameworks like Selenium.  This is why you'll find Cypress in the recommended frameworks as it solves many of the known issues with Selenium.  \nThis is an important point though.  Depending on the UI testing framework you choose will result in either a smoother test creation experience, or a very frustrating and time-consuming one.  If you were to choose just Selenium the development costs and time costs would likely be very high.  It's better to use either a framework built on top of Selenium or one that attempts to solve many of the problems with something like Selenium.  \nNote there that there are further considerations as when running in headless mode the UI can render differently than what you may see on your development machine, particularly with web applications.  Furthermore, note that when rendering in different page dimensions elements may disappear on the page due to CSS rules, therefore not be selectable by certain frameworks with default options out of the box.  All of these issues can be resolved and worked around, but the rendering demonstrates another particular challenge of UI testing.  \nSpecific Guidance  \nRecommended testing frameworks:  \nWeb  \nBrowserStack  \nCypress  \nJest  \nSelenium  \nOS/Mobile Applications  \nCoded UI tests (CUITs)  \nXamarin.UITest  \nNote that the framework listed above that is paid is BrowserStack, it's listed as it's an industry standard, the rest are open source and free.",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\ui-testing\\README.md"
    },
    {
        "chunkId": "chunk55_0",
        "chunkContent": "Example: Authoring a unit test  \nTo illustrate some unit testing techniques for an object-oriented language, let's start with an example of some\ncode we wish to add unit tests for. In this example, we have a configuration class that contains all the startup options\nfor an app we are writing. Normally it reads from a .config file, but we are having three problems with the current\nimplementation:  \nThere is a bug in the Configuration class, and we have no unit tests since it relies on reading a config file  \nWe can't unit test any of the code that relies on the Configuration class reading a config file  \nIn the future, we want to allow for configuration to be saved in the cloud and accessed via REST api.  \nThe bug we are trying to fix is that if there are multiple empty lines in the configuration file, an\nIndexOutOfRangeException is being thrown. Our class currently looks like this:  \n{% raw %}  \n```csharp\nusing System.IO;\nusing System.Linq;\n\npublic class Configuration\n{\n// Public getter properties from configuration object\npublic string MyProperty { get; private set; }\n\n}\n```  \n{% endraw %}  \nAbstraction  \nIn our example, we have a single dependency: the file system. Rather than just abstracting the file system entirely, let\nus think about why we need the file system and abstract the concept rather than the implementation. In this case, we\nare using the File class to read from the config file, and the config contents. The abstraction concept here is some\nform or configuration reader that returns each line of the configuration in a string array. We could call it\nConfigurationReader, and it has a single method, Read, which returns the contents.  \nWhen creating abstractions, it can be good practice creating an interface for that abstraction, in languages that\nsupport it. In the example with C#, we can create an IConfigurationReader interface, and instead of just having a\nConfigurationReader class we can be more specific and name if FileConfigurationReader to indicate that it reads from\nthe file system:  \n{% raw %}  \n```csharp\n// IConfigurationReader.cs\npublic interface IConfigurationReader\n{\nstring[] Read();\n}\n\n// FileConfigurationReader.cs\npublic class FileConfigurationReader : IConfigurationReader\n{\npublic string[] Read()\n{\nreturn File.ReadAllLines(\".config\");\n}\n}\n```  \n{% endraw %}  \nNow that the file dependency has been abstracted away, we need to update our Configuration class's Initialize method to\nuse the new abstraction instead of calling File.ReadAllLines directly:  \n{% raw %}  \n```csharp\npublic void Initialize()\n{\nvar configContents = new FileConfigurationReader().Read();\n\n}\n```  \n{% endraw %}  \nAs you can see, we still have a dependency on the file system, but that dependency has been abstracted out. We will need\nto use other techniques to break the dependency completely.  \nDependency Injection  \nIn the previous section, we abstracted the file access into a FileConfigurationReader but we still had a dependency on\nthe file system in our function. We can use dependency injection to inject the right reader into our Configuration\nclass:  \n{% raw %}  \n```csharp\nusing System.IO;\nusing System.Linq;\n\npublic class Configuration\n{\nprivate readonly IConfigurationReader configReader;\n\n}\n```  \n{% endraw %}  \nAbove, a technique was used called Constructor Injection.\nThis uses the object's constructor to set what our dependencies will be, which means whichever object creates the\nConfiguration object will control which reader needs to get passed in. This is an example of \"inversion of control\",\npreviously the Configuration object controlled the dependency, but instead we pushed up the control to whatever\ncomponent creates this object.  \nNote that we injected the interface IConfigurationReader and not the concrete class. This is what allows us to break\nthe dependency; whereas originally we had a hard-coded dependency on the File class, now we only depend on an object\nthat implements IConfigurationReader.  \nWriting our first unit tests  \nWe started down this venture because we have a bug in the Configuration class that was not caught because we do not\nhave unit tests. Let us write some unit tests that gives us full coverage of the Configuration class, including a test\nthat tests the scenario described by the bug (if there are multiple empty lines in the configuration file, an\nIndexOutOfRangeException is being thrown).  \nHowever, we still have one problem, we only have a single implementation of IConfigurationReader, and it uses the file\nsystem, meaning any unit tests we write will still have a dependency on the file system! Luckily since we used\ndependency injection, all we need to do is create an implementation of IConfigurationReader that does not depend on\nthe file system. We could create a mock here, but instead let's create a concrete implementation of the interface which\nsimply returns the passed in string[] - we can call it PassThroughConfigurationReader (for more details on why this\napproach may be better than mocking, see the page on mocking)  \n{% raw %}  \n```csharp\npublic class PassThroughConfigurationReader : IConfigurationReader\n{\nprivate readonly string[] contents;\n\n}\n```  \n{% endraw %}  \nThis simple class will be used in our unit tests, so we can create different states without requiring lots of file\naccess. Now that we have this in place, we can go ahead and write our unit tests, starting with the tests that describe\nthe current behavior:  \n{% raw %}  \n```csharp\npublic class ConfigurationTests\n{\n[Fact]\npublic void Initialize_EmptyConfig_Throws()\n{\nvar reader = new PassThroughConfigurationReader(Array.Empty());\nvar config = new Configuration(reader);\n\n}\n```  \n{% endraw %}  \nFixing the bug  \nAll our current tests pass, and give us 100% coverage, however as evidenced by the bug, we must not be covering all\npossible inputs and outputs. In the case of the bug, multiple empty lines would cause an issue. Additionally,\nKeyNotFoundException is not a very friendly exception and is an implementation detail, not something that makes sense\nwhen designing the Configuration API. Let's add some more tests and align the tests with how we think the\nConfiguration class should behave:  \n{% raw %}  \n```csharp\npublic class ConfigurationTests\n{\n[Fact]\npublic void Initialize_EmptyConfig_Throws()\n{\nvar reader = new PassThroughConfigurationReader(Array.Empty());\nvar config = new Configuration(reader);\n\n}\n```  \n{% endraw %}  \nNow we have 4 failing tests and 1 passing test, but we have firmly established through the use of these tests how we\nexpect callers to user the Configuration class and what is and isn't allowed as inputs. Now we just need to fix the\nConfiguration class so that our tests pass:  \n{% raw %}  \n```csharp\npublic void Initialize()\n{\nvar configContents = configReader.Read();\n\n}\n```  \n{% endraw %}  \nNow all our tests pass! We have fixed our bug, added unit tests to the Configuration class, and have much higher\nconfidence in future changes.  \nUntestable Code  \nAs described in the abstraction section, not all code can be properly unit tested. In our case\nwe have a single class that has 0% test coverage: FileConfigurationReader. This is expected; in this case we kept\nFileConfigurationReader as light as possible with no additional logic other than calling into the third-party\ndependency. FileConfigurationReader is an example of the facade design pattern.  \nTestable Design and Future Improvements  \nOne of our original problems described in this example is that in the future we expect to load the configuration from a\nweb API. By doing all the work of abstracting the way we load the configuration text and breaking the dependency on the\nfile system, we have already done all the hard work to enable this future scenario! All that needs to be done next is to\ncreate a WebApiConfigurationReader implementation and use that the construct the Configuration object, and it should\njust work.  \nThat is one of the benefits of testable design, in the process of writing our tests in a safe way, a side effect of that\nis that we already have our dependencies that might change abstracted, and will require minimal changes to implement.  \nAnother added benefit is we have multiple possibilities opened by this testable design. For example, we can have a\ncascading configuration set up now using all 3 IConfigurationReader implementations, including the one we wrote only\nfor our tests! We can first check if internet access is available and if so use WebApiConfigurationReader. If no\ninternet is available, we can fall back to the local config file on the current system using FileConfigurationReader.\nIf for some reason the config file does not exist, we can use the PassThroughConfigurationReader as a hard-coded\ndefault configuration somewhere in the code. We have full flexibility to do whatever we may need to do in the future!",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\unit-testing\\authoring_example.md"
    },
    {
        "chunkId": "chunk56_0",
        "chunkContent": "Custom Connector Testing  \nWhen developing Custom Connectors to put data into the Power Platform there are some strategies you can follow:  \nUnit Testing  \nThere are several verifications one can do while developing custom connectors in order to be sure the code is working properly.  \nThere are two main ones:  \nValidating the OpenAPI schema which the connector is defined.  \nValidating if the schema also have all the information necessary for the certified connector process.  \n(the later one is optional, but necessary in case you want to publish it as a certified connector).  \nThere are several tool to help validate the OpenAPI schema, a list of them are available in this link. A suggested tool would be swagger-cli.  \nOn the other hand, to validate if the custom connector you are building is correct to become a certified connector, use the paconn-cli, since it has a validate command that shows missing information from the custom connector definition.",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\unit-testing\\custom-connector.md"
    },
    {
        "chunkId": "chunk57_0",
        "chunkContent": "Mocking in Unit Tests  \nOne of the key components of writing unit tests is to remove the dependencies your system has and replacing it with an\nimplementation you control. The most common method people use as the replacement for the dependency is a mock, and\nmocking frameworks exist to help make this process easier.  \nMany frameworks and articles use different meanings for the differences between test doubles. A test double is a generic\nterm for any \"pretend\" object used in place of a real one. This term, as well as others used in this page are the\ndefinitions provided by Martin Fowler.\nThe most commonly used form of test double is Mocks, but there are many cases where Mocks perhaps are not the best\nchoice and Fakes should be considered instead.  \nStubs  \nStub allows you to have predetermined behavior that substitutes real behavior.\nThe dependency (abstract class or interface) is implemented as a stub with a logic as expected by the client.\nStubs can be useful when the clients of the stubs all expect the same set of responses, e.g. you use a third party service.\nThe key concept here is that stubs should never fail a unit or integration test where a mock can.\nStubs do not require any sort of framework to run, but are usually supported by mocking frameworks to quickly build the stubs.\nStubs are commonly used in combination with a dependency injection frameworks or libraries, where the real object is replaced by a stub implementation.  \nStubs can be useful especially during early development of a system, but since nearly every test requires its own stubs\n(to test the different states), this quickly becomes repetitive and involves a lot of boilerplate code. Rarely will you\nfind a codebase that uses only stubs for mocking, they are usually paired with other test doubles.  \nStubs do not require any sort of framework to run, but are usually supported by mocking frameworks to quickly build the\nstubs.  \n{% raw %}  \n```python\n\nPython test example, that creates an application\n\nwith a dependency injection framework an overrides\n\na service with a stub\n\nclass StubTestCase(TestBase):\ndef setUp(self) -> None:\nsuper(StubTestCase, self).setUp()\nself.app.container.service_a.override(StubService())\n\n```  \n{% endraw %}  \nUpsides  \nDo not require any framework, easy to set up.  \nDownsides  \nCan involve rewriting the same code many times, lots of boilerplate.  \nMocks  \nFowler describes mocks as pre-programmed objects with expectations which form a specification of the calls they are\nexpected to receive. In other words, mocks are a replacement object for the dependency that has certain expectations\nthat are placed on it; those expectations might be things like validating a sub-method has been called a certain number\nof times or that arguments are passed down in a certain way.  \nMocking frameworks are abundant for every language, with some languages having mocks built into the unit test packages.\nThey make writing unit tests easy and still encourage good unit testing practices.  \nThe main difference between a mock and most of the other test doubles is that mocks do behavioral verification,\nwhereas other test doubles do state verification. With behavioral verification, you end up testing that the\nimplementation of the system under test is as you expect, whereas with state verification the implementation is not\ntested, rather the inputs and the outputs to the system are validated.  \nThe major downside to behavioral verification is that it is tied to the implementation. One of the biggest advantages of\nwriting unit tests is that when you make code changes you have confidence that if your unit tests continue to pass, that\nyou are making a relatively safe change. If tests need to be updated every time because the behavior of the method has\nchanged, then you lose that confidence because bugs could also be introduced into the test code. This also increases the\ndevelopment time and can be a source of frustration.  \nFor example, let's assume you have a method that you are testing that makes 5 web service calls. With mocks, one of your\ntests could be to check that those 5 web service calls were made. Sometime later the API is updated and only a single\nweb service call needs to be made. Once the system code is changed, the unit test will fail because it expects 5 calls\nand not 1. The test needs to be updated, which results in lowered confidence in the change, as well as potentially\nintroduces more areas for bugs to sneak in.  \nSome would argue that in the example above, the unit test is not a good test anyway because it depends on the\nimplementation, and that may be true; but one of the biggest problems with using mocks (and specifically mocking\nframeworks that allow these verifications), is that it encourages these types of tests to be written. By not using a\nmock framework that allows this, you never run the risk of writing tests that are validating the implementation.  \nUpsides to Mocking  \nEasy to write.  \nEncourages testable design.  \nDownsides to Mocking  \nBehavioral testing can present problems with maintainability in unit test code.  \nUsually requires a framework to be installed (or if no framework, lots of boilerplate code)  \nFakes  \nFake objects actually have working implementations, but usually take some shortcut which may make them not suitable\nfor production. One of the common examples of using a Fake is an in-memory database - typically you want your database\nto be able to save data somewhere between application runs, but when writing unit tests if you have a fake implementation of\nyour database APIs that are store all data in memory, you can use these for unit tests and not break abstraction as well\nas still keep your tests fast.  \nWriting a fake does take more time than other test doubles, because they are full implementations, and can have\ntheir own suite of unit tests. In this sense though, they increase confidence in your code even more because your test\ndouble has been thoroughly tested for bugs before you even use it as a downstream dependency.  \nSimilarly to mocks, fakes also promote testable design, but unlike mocks they do not require any frameworks to write.\nWriting a fake is as easy as writing any other implementation class. Fakes can be included in the test code only, but\nmany times they end up being \"promoted\" to the product code, and in some cases can even start off in the product code\nsince it is held to the same standard with full unit tests. Especially if writing a library or an API that other\ndevelopers can use, providing a fake in the product code means those developers no longer need to write their own mock\nimplementations, further increasing re-usability of code.  \nUpsides to Fakes  \nNo framework needed, is just like any other implementation.  \nEncourages testable design.  \nCode can be \"promoted\" to product code, so it is not wasted effort.  \nDownsides to Fakes  \nTakes more time to implement.  \nBest Practices  \nTo keep your mocking efficient, consider these best practices to make your code testable, save time and make your\ntest assertions more meaningful.  \nDependency Injection  \nIf you don\u2019t keep testability in mind from the beginning, once you start writing your tests, you might realize you have\nto do a time-intensive refactor to make the code unit testable. A common problem that can lead to non-testable code in certain\nlanguages such as C# is not using dependency injection. Consider using dependency injection so that a mock can easily be injected\ninto your Subject Under Test (SUT) during a unit test.  \nMore information on using dependency injection can be found here.  \nAssertions  \nWhen it comes to assertions in unit tests you want to make sure that you assert the right things, not necessarily lots\nof things. Some assertions can be inefficient and not give you the confidence you need in the test result. When you are\nmocking a client or configuration and your method passes the mock result directly as a return value without significant\nchanges, consider not asserting on the return value. Because if you do, you are mainly asserting whether you set up the\nmock correctly. For a very simple example, look at this class:  \n{% raw %}  \n```csharp\n\npublic class SearchController : ControllerBase {\n\npublic ISearchClient SearchClient { get; }\n\npublic SearchController(ISearchClient searchClient)\n{\nSearchClient = searchClient;\n}\n\npublic String GetName(string id)\n{\nreturn this.SearchClient.GetName(id);\n}\n}\n```  \n{% endraw %}  \nWhen testing the GetName method, you can set up a mock search client to return a certain value. Then, it\u2019s easy to\nassert that the return value is, in fact, this value from the mock.  \n{% raw %}  \ncsharp\nmockSearchClient.Setup(x => x.GetName(id))\n.ReturnsAsync(\"myResult\");\nvar result = searchController.GetName(id);\nAssert.Equal(\"myResult\",result.Value);  \n{% endraw %}  \nBut now, your method could look like this, and the test would still pass:  \n{% raw %}  \ncsharp\npublic String GetName(string id)\n{\nreturn \"myResult\";\n}  \n{% endraw %}  \nSimilarly, if you set up your mock wrong, the test would fail even though the logic inside the method is sound. For efficient\nassertions that will give you confidence in your SUT, make assertions on your logic, not mock return values.\nThe simple example above doesn\u2019t have a lot of logic, but you want to make sure that it calls the search client to retrieve\nthe result. For this, you can use the verify method to make sure the search client was called using the right parameters even\nthough you don\u2019t care about the result.  \n{% raw %}  \ncsharp\nmockSearchClient.Verify(mock => mock.GetName(id), Times.Once());  \n{% endraw %}  \nThis example is kept simple to visualize the principle of making meaningful assertions. In a real world application, your SUT\nwill probably have more logic inside. Pieces of glue code that have as little logic as this example don't always have to be\nunit tested and might instead be covered by integration tests. If there is more logic and a unit test with mocking is required,\nyou should apply this principle by verifying mock calls and making assertions on the part of the mock result that was modified\nby your SUT.  \nCallbacks  \nIt can be time-consuming to set up mocks if you want to make sure they are being called with the right parameters, especially\nif the parameters are complex. To make your testing more efficient, consider using callbacks to make assertions on the\nparameters after a method was called. Often you don\u2019t care about all the parameters but only a few, or even only parts of\nthem if the parameters are also objects. It\u2019s easy to make a small mistake in the creation of the parameter, like missing\nan attribute that the actual method sets, and then your mock won\u2019t be called, even though you might not care about this\nattribute at all. To avoid this, you can define only the most relevant parameters to differentiate between method calls and\nuse an any-statement for the others. In this example, the method has a complex search options parameter which would take a\nlot of time to set up manually. Since you only care about 2 attributes in the search options, you use an any-statement and\nstore the options in a callback for later assertions.  \n{% raw %}  \n```csharp\nvar actualOptions = new SearchOptions();\n\nmockSearchClient\n.Setup(x =>\nx.Search(\n\"[This parameter is most relevant]\",\nIt.IsAny()\n)\n)\n.Returns(mockResults)\n.Callback((query, searchOptions) =>\n{\nactualOptions = searchOptions;\n}\n);\n```  \n{% endraw %}  \nSince you want to test your method logic, you should care only about the parts of the parameter which are influenced by your SUT,\nin this example, let's say the search mode and the search query type. So, with the variable you stored in the callback, you can\nmake assertions on only these two attributes.  \n{% raw %}  \ncsharp\nAssert.Equal(SearchMode.All, actualOptions.SearchMode);\nAssert.Equal(SearchQueryType.Full, actualOptions.QueryType);  \n{% endraw %}  \nThis makes the test more explicit since it shows which parts of the logic you care about. It\u2019s also more efficient since you don\u2019t\nhave to spend a lot of time setting up the parameters for the mock.  \nConclusion  \nUsing test doubles in unit tests is an essential part of having a healthy test suite. When looking at mocking frameworks\nand using test doubles, it is important to consider the future implications of integrating with a mocking framework from\nthe start. Sometimes certain features of mocking frameworks seem essential, but usually that is a sign that the code\nitself is not abstracted enough if it requires a framework.  \nIf possible, starting without a mocking framework and attempting to create fake implementations will lead to a more\nhealthy code base, but when that is not possible the onus is on the technical leaders of the team to find cases where\nmocks may be overused, rely too much on implementation details, or end up not testing the right things.",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\unit-testing\\mocking.md"
    },
    {
        "chunkId": "chunk58_0",
        "chunkContent": "Unit Testing  \nUnit testing is a fundamental tool in every developer's toolbox. Unit tests not only help us test our code, they\nencourage good design practices, reduce the chances of bugs reaching production, and can even serve as examples or\ndocumentation on how code functions. Properly written unit tests can also improve developer efficiency.  \nUnit testing also is one of the most commonly misunderstood forms of testing. Unit testing refers to a very specific\ntype of testing; a unit test should be:  \nProvably reliable - should be 100% reliable so failures indicate a bug in the code  \nFast - should run in milliseconds, a whole unit testing suite shouldn't take longer than a couple seconds  \nIsolated - removing all external dependencies ensures reliability and speed  \nWhy Unit Testing  \nIt is no secret that writing unit tests is hard, and even harder to write well. Writing unit tests also increases the\ndevelopment time for every feature. So why should we write them?  \nUnit tests  \nreduce costs by catching bugs earlier and preventing regressions  \nincrease developer confidence in changes  \nspeed up the developer inner loop  \nact as documentation as code  \nFor more details, see all the detailed descriptions of the points above.  \nUnit Testing Design Blocks  \nUnit testing is the lowest level of testing and as such generally has few components and dependencies.  \nThe system under test (abbreviated SUT) is the \"unit\" we are testing. Generally these are methods or functions, but\ndepending on the language these could be different. In general, you want the unit to be as small as possible though.  \nMost languages also have a wide suite of unit testing frameworks and test runners. These test frameworks have\na wide range of functionality, but the base functionality should be a way to organize your tests and run them quickly.  \nFinally, there is your unit test code; unit test code is generally short and simple, preferring repetition to adding\nlayers and complexity to the code.  \nApplying the Unit Testing  \nGetting started with writing a unit test is much easier than some other test types since it should require next to no\nsetup and is just code. Each test framework is different in how you organize and write your tests,\nbut the general techniques and best practices of writing a unit test are universal.  \nTechniques  \nThese are some commonly used techniques that will help when authoring unit tests. For some examples, see the pages on\nusing abstraction and dependency injection to author a unit test, or how to do test-driven development.  \nNote that some of these techniques are more specific to strongly typed, object-oriented languages. Functional languages\nand scripting languages have similar techniques that may look different, but these terms are commonly used in all unit\ntesting examples.  \nAbstraction  \nAbstraction is when we take an exact implementation detail, and we generalize it into a concept instead. This technique\ncan be used in creating testable design and is used often especially in object-oriented languages. For unit tests,\nabstraction is commonly used to break a hard dependency and replace it with an abstraction. That abstraction then allows\nfor greater flexibility in the code and allows for the a mock or simulator to be used in its place.  \nOne of the side effects of abstracting dependencies is that you may have an abstraction that has no test coverage. This\nis case where unit testing is not well-suited, you can not expect to unit test everything, things like dependencies will\nalways be an uncovered case. This is why even if you have a robust unit testing suite, integration or functional testing\nshould still be used - without that, a change in the way the dependency functions would never be caught.  \nWhen building wrappers around third-party dependencies, it is best to keep the implementations with as little logic as\npossible, using a very simple facade that calls the dependency.  \nAn example of using abstraction can be found here.  \nDependency Injection  \nDependency injection is a technique which allows us to extract\ndependencies from our code. In a normal use-case of a dependant class, the dependency is constructed and used within the\nsystem under test. This creates a hard dependency between the two classes, which can make it particularly hard to test\nin isolation. Dependencies could be things like classes wrapping a REST API, or even something as simple as file access.\nBy injecting the dependencies into our system rather than constructing them, we have \"inverted control\" of the\ndependency. You may see \"Inversion of Control\" and \"Dependency Injection\" used as separate terms, but it is very hard to\nhave one and not the other, with some arguing that Dependency Injection is a more specific way of saying inversion of\ncontrol. In certain languages such as C#, not using\ndependency injection can lead to code that is not unit testable since there is no way to inject mocked objects.\nKeeping testability in mind from the beginning and evaluating using dependency injection can save you from a time-intensive\nrefactor later.  \nOne of the downsides of dependency injection is that\nit can easily go overboard. While there are no longer hard dependencies, there is still coupling between the interfaces,\nand passing around every interface implementation into every class presents just as many downsides as not using\nDependency Injection. Being intentional with what dependencies get injected to what classes, is key to developing a maintainable\nsystem.  \nMany languages include special Dependency Injection frameworks that take care of the boilerplate code and construction\nof the objects. Examples of this are Spring in Java or built into ASP.NET Core  \nAn example of using dependency injection can be found here.  \nTest-Driven Development  \nTest-Driven Development (TDD) is less a technique in how your code is designed, but a technique for writing your\ncode that will lead you to a testable design from the start. The basic premise of test-driven development is that you\nwrite your test code first and then write the system under test to match the test you just wrote. This way all the test\ndesign is done up front and by the time you finish writing your system code, you are already at 100% test pass rate and\ntest coverage. It also guarantees testable design is built into the system since the test was written first!  \nFor more information on TDD and an example, see the page on Test-Driven Development  \nBest Practices  \nArrange/Act/Assert  \nOne common form of organizing your unit test code is called Arrange/Act/Assert. This divides up your unit test into 3\ndifferent discrete sections:  \nArrange - Set up all the variables, mocks, interfaces, and state you will need to run the test  \nAct - Run the system under test, passing in any of the above objects that were created  \nAssert - Check that with the given state that the system acted appropriately.  \nUsing this pattern to write tests makes them very readable and also familiar to future developers who would need to read\nyour unit tests.  \nExample  \nLet's assume we have a class MyObject with a method TrySomething that interacts with an array of strings, but if the\narray has no elements, it will return false. We want to write a test that checks the case where array has no elements:  \n{% raw %}  \n```csharp\n[Fact]\npublic void TrySomething_NoElements_ReturnsFalse()\n{\n// Arrange\nvar elements = Array.Empty();\nvar myObject = new MyObject();\n\n}\n```  \n{% endraw %}  \nKeep tests small and test only one thing  \nUnit tests should be short and test only one thing. This makes it easy to diagnose when there was a failure without\nneeding something like which line number the test failed at. When using Arrange/Act/Assert, think\nof it like testing just one thing in the \"Act\" phase.  \nThere is some disagreement on whether testing one thing means \"assert one thing\" or \"test one state, with\nmultiple asserts if needed\". Both have their advantages and disadvantages, but as with most technical disagreements\nthere is no \"right\" answer. Consistency when writing your tests one way or the other is more important!  \nUsing a standard naming convention for all unit tests  \nWithout having a set standard convention for unit test names, unit test names end up being either not descriptive\nenough, or duplicated across multiple different test classes. Establishing a standard is not only important for keeping\nyour code consistent, but a good standard also improves the readability and debug-ability of a test. In this article,\nthe convention used for all unit tests has been UnitName_StateUnderTest_ExpectedResult, but there are lots of other\npossible conventions as well, the important thing is to be consistent and descriptive. Having descriptive names such as\nthe one above makes it trivial to find the test when there is a failure, and also already explains what the expectation\nof the test was and what state caused it to fail. This can be especially helpful when looking at failures in a CI/CD\nsystem where all you know is the name of the test that failed - instead now you know the name of the test and exactly\nwhy it failed (especially coupled with a test framework that logs helpful output on failures).  \nThings to Avoid  \nSome common pitfalls when writing a unit test that are important to avoid:  \nSleeps - A sleep can be an indicator that perhaps something is making a request to a dependency that it should not be.\nIn general, if your code is flaky without the sleep, consider why it is failing and if you can remove the flakiness by\nintroducing a more reliable way to communicate potential state changes. Adding sleeps to your unit tests also breaks\none of our original tenets of unit testing: tests should be fast, as in order of milliseconds. If tests are taking on\nthe order of seconds, they become more cumbersome to run.  \nReading from disk - It can be really tempting to the expected value of a function return in a file and read that file\nto compare the results. This creates a dependency with the system drive, and it breaks our tenet of keeping our unit\ntests isolated and 100% reliable. Any outside dependency such as file system access could potentially cause\nintermittent failures. Additionally, this could be a sign that perhaps the test or unit under test is too complex and\nshould be simplified.  \nCalling third-party APIs - When you do not control a third-party library that you are calling into, it's impossible to\nknow for sure what that is doing, and it is best to abstract it out. Otherwise, you may be making REST calls or other\npotential areas of failure without directly writing the code for it. This is also generally a sign that the design of\nthe system is not entirely testable. It is best to wrap third party API calls in interfaces or other structures so\nthat they do not get invoked in unit tests. For more information see the page on mocking.  \nUnit Testing Frameworks and Tools  \nTest Frameworks  \nUnit test frameworks are constantly changing. For a full list of every unit testing framework see the page on\nWikipedia. Frameworks have many features and\nshould be picked based on which feature-set fits best for the particular project.  \nMock Frameworks  \nMany projects start with both a unit test framework, and also add a mock framework. While mocking frameworks have their\nuses and sometimes can be a requirement, it should not be something that is added without considering the broader\nimplications and risks associated with heavy usage of mocks.  \nTo see if mocking is right for your project, or if a mock-free approach is more appropriate, see the page on mocking.  \nTools  \nThese tools allow for constant running of your unit tests with in-line code coverage, making the dev inner loop\nextremely fast and allows for easy TDD:  \nVisual Studio Live Unit Testing  \nWallaby.js  \nInfinitest for Java  \nPyCrunch for Python  \nThings to consider  \nTransferring responsibility to integration tests  \nIn some situations it is worth considering to include the integration tests in the inner development loop to provide a sufficient code coverage to ensure the system is working properly. The prerequisite for this approach to be successful is to have integration tests being able to execute at a speed comparable to that of unit tests both locally and in a CI environment. Modern application frameworks like .NET or Spring Boot combined with the right mocking or stubbing approach for external dependencies offer excellent capabilities to enable such scenarios for testing.  \nUsually, integration tests only prove that independently developed modules connect together as designed. The test coverage of integration tests can be extended to verify the correct behavior of the system as well. The responsibility of providing a sufficient branch and line code coverage can be transferred from unit tests to integration tests.\nInstead of several unit tests needed to test a specific case of functionality of the system, one integration scenario is created that covers the entire flow. For example in case of an API, the received HTTP responses and their content are verified for each request in test. This covers both the integration between components of the API and the correctness of its business logic.  \nWith this approach efficient integration tests can be treated as an extension of unit testing, taking over the responsibility of validating happy/failure path scenarios. It has the advantage of testing the system as a black box without any knowledge of its internals. Code refactoring has no impact on tests. Common testing techniques as TDD can be applied at a higher level which results in a development process that is driven by acceptance tests. Depending on the project specifics unit tests still play an important role. They can be used to help dictate a testable design at a lower level or to test complex business logic and corner cases if necessary.  \nConclusion  \nUnit testing is extremely important, but it is also not the silver bullet; having proper unit tests is just a part of a\nwell-tested system. However, writing proper unit tests will help with the design of your system as well as help catch\nregressions, bugs, and increase developer velocity.  \nResources  \nUnit Testing Best Practices",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\unit-testing\\README.md"
    },
    {
        "chunkId": "chunk59_0",
        "chunkContent": "Test-Driven Development Example  \nWith this method, rather than writing all your tests up front, you write one test at a time and then switch to write the\nsystem code that would make that test pass. It's important to write the bare minimum of code necessary even if it is not\nactually \"correct\". Once the test passes you can refactor the code to make it maybe make more sense, but again the logic\nshould be simple. As you write more tests, the logic gets more and more complex, but you can continue to make the\nminimal changes to the system code with confidence because all code that was written is covered.  \nAs an example, let's assume we are trying to write a new function that validates a string is a valid password format.\nThe password format should be a string larger than 8 characters containing at least one number. We start with the\nsimplest possible test; one of the easiest ways to do this is to first write tests that validate inputs into the\nfunction:  \n{% raw %}  \n```csharp\n// Tests.cs\npublic class Tests\n{\n[Fact]\npublic void ValidatePassword_NullInput_Throws()\n{\nvar s = new MyClass();\nAssert.Throws(() => s.ValidatePassword(null));\n}\n}\n\n// MyClass.cs\npublic class MyClass\n{\npublic bool ValidatePassword(string input)\n{\nreturn false;\n}\n}\n```  \n{% endraw %}  \nIf we run this code, the test will fail as no exception was thrown since our code in ValidateString is just a stub.\nThis is ok! This is the \"Red\" part of Red-Green-Refactor. Now we want to move onto the \"Green\" part - making the minimal\nchange required to make this test pass:  \n{% raw %}  \ncsharp\n// MyClass.cs\npublic class MyClass\n{\npublic bool ValidatePassword(string input)\n{\nthrow new ArgumentNullException(nameof(input));\n}\n}  \n{% endraw %}  \nOur tests pass, but this function doesn't really work, it will always throw the exception. That's ok! As we\ncontinue to write tests we will slowly add the logic for this function, and it will build on itself, all while\nguaranteeing our tests continue to pass.  \nWe will skip the \"Refactor\" stage at this point because there isn't anything to refactor. Next let's add a test that\nchecks that the function returns false if the password is less than size 8:  \n{% raw %}  \ncsharp\n[Fact]\npublic void ValidatePassword_SmallSize_ReturnsFalse()\n{\nvar s = new MyClass();\nAssert.False(s.ValidatePassword(\"abc\"));\n}  \n{% endraw %}  \nThis test will pass as it still only throws an ArgumentNullException, but again, that is an expected failure. Fixing\nour function should see it pass:  \n{% raw %}  \n```csharp\npublic bool ValidatePassword(string input)\n{\nif (input == null)\n{\nthrow new ArgumentNullException(nameof(input));\n}\n\n}\n```  \n{% endraw %}  \nFinally, some code that looks real! Note how it wasn't the test that checked for null that had us add the if statement\nfor the null-check, but rather the subsequent test which unlocked a whole new branch. By adding that if statement, we\nmade the bare minimum change necessary in order to get both tests to pass, but we still have work to do.  \nIn general, working in the order of adding a negative test first before adding a positive test will ensure that both\ncases get covered by the code in a way that can get tests. Red-Green-Refactor makes that process super easy by requiring\nthe bare minimum change - since we only want to make the bare minimum changes, we just simply return false here, knowing\nfull well that we will be adding logic later that will expand on this.  \nSpeaking of which, let's add the positive test now:  \n{% raw %}  \ncsharp\n[Fact]\npublic void ValidatePassword_RightSize_ReturnsTrue()\n{\nvar s = new MyClass();\nAssert.True(s.ValidatePassword(\"abcdefgh1\"));\n}  \n{% endraw %}  \nAgain, this test will fail at the start. One thing to note here if that its important that we try and make our tests\nresilient to future changes. When we write the code under test, we act very naively, only trying to make the current\ntests we have pass; when you write tests though, you want to ensure that everything you are doing is a valid case in the\nfuture. In this case, we could have written the input string as abcdefgh and when we eventually write the function it\nwould pass, but later when we add tests that validate the function has the rest of the proper inputs it would fail\nincorrectly.  \nAnyways, the next code change is:  \n{% raw %}  \n```csharp\npublic bool ValidatePassword(string input)\n{\nif (input == null)\n{\nthrow new ArgumentNullException(nameof(input));\n}\n\n}\n```  \n{% endraw %}  \nHere we now have a passing test! However, the logic doesn't actually make much sense. We did the bare minimum\nchange which was adding a new condition that passed for longer strings, but thinking forward we know this\nwon't work as soon as we add additional validations. So let's use our first \"Refactor\" step in the Red-Green-Refactor flow!  \n{% raw %}  \n```csharp\npublic bool ValidatePassword(string input)\n{\nif (input == null)\n{\nthrow new ArgumentNullException(nameof(input));\n}\n\n}\n```  \n{% endraw %}  \nThat looks better. Note how from a functional perspective, inverting the if-statement does not change what the function returns.\nThis is an important part of the refactor flow, maintaining the logic by doing provably safe refactors, usually through the use of tooling and automated refactors from\nyour IDE.  \nFinally, we have one last requirement for our ValidatePassword method and that is that it needs to check that there is\na number in the password. Let's again start with the negative test and validate that with a string with the valid length\nthat the function returns false if we do not pass in a number:  \n{% raw %}  \ncsharp\n[Fact]\npublic void ValidatePassword_ValidLength_ReturnsFalse()\n{\nvar s = new MyClass();\nAssert.False(s.ValidatePassword(\"abcdefghij\"));\n}  \n{% endraw %}  \nOf course the test fails as it is only checking length requirements. Let's fix the method to check for numbers:  \n{% raw %}  \n```csharp\npublic bool ValidatePassword(string input)\n{\nif (input == null)\n{\nthrow new ArgumentNullException(nameof(input));\n}\n\n}\n```  \n{% endraw %}  \nHere we use a handy LINQ method to check if any of the chars in the string are a digit, and if not, return false.\nTests now pass, and we can refactor. For readability, why not combine the if statements:  \n{% raw %}  \n```csharp\npublic bool ValidatePassword(string input)\n{\nif (input == null)\n{\nthrow new ArgumentNullException(nameof(input));\n}\n\n}\n```  \n{% endraw %}  \nAs we refactor this code, we feel 100% confident in the changes we made as we have 100% test coverage which tests both\npositive and negative scenarios. In this case we actually already have a method that tests the positive case, so our function is done!  \nNow that our code is completely tested we can make all sorts of changes and still have confidence that it works. For\nexample, if we wanted to change the implementation of the method to use regex, all of our tests would still pass and\nstill be valid.  \nThat is it! We finished writing our function, we have 100% test coverage, and if we had done something a little more\ncomplex, we are guaranteed that whatever we designed is already testable since the tests were written first!",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\unit-testing\\tdd_example.md"
    },
    {
        "chunkId": "chunk60_0",
        "chunkContent": "Why Unit Tests  \nIt is no secret that writing unit tests is hard, and even harder to write well. Writing unit tests also increases the\ndevelopment time for every feature. So why should we bother writing them?  \nReduce costs  \nThere is no question that the later a bug is found, the more expensive it is to fix; especially so if the bug makes it\ninto production. A 2008 research study by IBM\nestimates that a bug caught in production could cost 6 times as much as if it was caught during implementation.  \nIncrease Developer Confidence  \nMany changes that developers make are not big features or something that requires an entire testing suite. A strong unit\ntest suite helps increase the confidence of the developer that their change is not going to cause any downstream bugs.\nHaving unit tests also helps with making safe, mechanical refactors that are provably safe; using things like\nrefactoring tools to do mechanical refactoring and running unit tests that cover the refactored code should be enough to\nincrease confidence in the commit.  \nSpeed up development  \nUnit tests take time to write, but they also speed up development? While this may seem like an oxymoron, it is one of\nthe strengths of a unit testing suite - over time it continues to grow and evolve until the tests become an essential\npart of the developer workflow.  \nIf the only testing available to a developer is a long-running system test, integration tests that require a deployment,\nor manual testing, it will increase the amount of time taken to write a feature. These types of tests should be a part of\nthe \"Outer loop\"; tests that may take some time to run and validate more than just the code you are writing. Usually\nthese types of outer loop tests get run at the PR stage or even later during merges into branches.  \nThe Developer Inner Loop is the process that developers go through as they are authoring code. This varies from\ndeveloper to developer and language to language but typically is something like code -> build -> run -> repeat. When\nunit tests are inserted into the inner loop, developers can get early feedback and results from the code they are\nwriting. Since unit tests execute really quickly, running tests shouldn't be seen as a barrier to entry for this loop.\nTooling such as Visual Studio Live Unit Testing\nalso help to shorten the inner loop even more.  \nDocumentation as code  \nWriting unit tests is a great way to show how the units of code you are writing are supposed to be used. In some ways,\nunit tests are better than any documentation or samples because they are (or at least should be) executed with every\nbuild so there is confidence that they are not out of date. Unit tests also should be so simple that they are easy to follow.",
        "source": "..\\data\\docs\\code-with-engineering\\automated-testing\\unit-testing\\why-unit-tests.md"
    },
    {
        "chunkId": "chunk61_0",
        "chunkContent": "FAQ  \nThis is a list of questions / frequently occurring issues when working with code reviews and answers how you can possibly tackle them.  \nWhat makes a code review different from a PR?  \nA pull request (PR) is a way to notify a task is finished and ready to be merged into the main working branch (source of truth). A code review is having someone go over the code in a PR and validate it before it is merged, but, in general, code reviews can take place outside PRs too.  \nCode Review Pull Request Source code focused Intended to enhance and enable code reviews. Includes both source code but can have a broader scope (e.g., docs, integration tests, compiles) Intended for early feedback before submitting a PR Not intended for early feedback . Created when author is ready to merge Usually a synchronous review with faster feedback cycles (draft PRs as an exception). Examples: scheduled meetings, over-the-shoulder review, pair programming Usually a tool assisted asynchronous review but can be elevated to a synchronous meeting when needed  \nWhy do we need code reviews?  \nOur peer code reviews are structured around best practices, to find specific kinds of errors. Much like you would still run a linter over mobbed code, you would still ask someone to make the last pass to make sure the code conforms to expected standards and avoids common pitfalls.  \nPRs are too large, how can we fix this?  \nMake sure you size the work items into small clear chunks, so the reviewer will be able to understand the code on their own. The team is instructed to commit early, before the full product backlog item / user story is complete, but rather when an individual item is done. If the work would result in an incomplete feature, make sure it can be turned off, until the full feature is delivered.\nMore information can be found in Pull Requests - Size Guidance.  \nHow can we expedite code reviews?  \nSlow code reviews might cause delays in delivering features and cause frustration amongst team members.  \nPossible actions you can take  \nAdd a rule for PR turnaround time to your work agreement.  \nSet up a slot after the standup to go through pending PRs and assign the ones that are inactive.  \nDedicate a PR review manager who will be responsible to keep things flowing by assigning or notifying people when PR got stale.  \nUse tools to better indicate stale reviews - Customize ADO - Task Boards.  \nWhich tools can I use to review a complex PR?  \nCheckout the Tools for help on how to perform reviews out of Visual Studio or Visual Studio Code.  \nHow can we enforce code review policies?  \nBy configuring Branch Policies , you can easily enforce code reviews rules.  \nWe pair or mob. How should this reflect in our code reviews?  \nThere are two ways to perform a code review:  \nPair - Someone outside the pair should perform the code review. One of the other major benefits of code reviews is spreading knowledge about the code base to other members of the team that don't usually work in the part of the codebase under review.  \nMob - A member of the mob who spent less (or no) time at the keyboard should perform the code review.",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\faq.md"
    },
    {
        "chunkId": "chunk62_0",
        "chunkContent": "Inclusion in Code Review  \nBelow are some points which emphasize why inclusivity in code reviews is important:  \nCode reviews are an important part of our job as software professionals.  \nIn ISE we work with cross cultural teams from across the globe.  \nHow we communicate affects team morale.  \nInclusive code reviews welcome new developers and make them comfortable with the team.  \nRude or personal attacks doing code reviews alienate - people can unknowingly make rude comments when reviewing pull requests (PRs).  \nTypes and Examples of Non-Inclusive Code Review Behavior  \nInequitable review assignments.  \nExample: Assigning most reviews to few people and dismissing some members of the team altogether.  \nNegative interpersonal interactions.  \nExample: Long arguments over subjective topics such as code style.  \nBiased decision making.  \nExample: Comments about the developer and not the code. Assuming code from developer X will always be good and hence not reviewing it properly and vice versa.  \nExamples of Inclusive Code Reviews  \nAnyone and everyone in the team should be assigned PRs to review.  \nReviewer should be clear about what is an opinion, their personal preference, best practice or a fact. Arguments over personal preferences and opinions are mostly avoidable.  \nUsing inclusive language and tone in the code review comments. For example, being suggestive rather being prescriptive in the review comments is a good way to get the point across the table.  \nIt's a good practice for the author of a PR to thank the reviewer for the review, when they have contributed in improving the code or you have learnt something new.  \nUsing the sandwich method for recommending a code change to a new developer or a new customer: Sandwich the suggestion between 2 compliments. For example: \"Great work so far, but I would recommend a few changes here. Btw, I loved the use of XYZ here, nice job!\"  \nGuidelines for the Author  \nAim to write a code that is easy to read, review and maintain.  \nIt\u2019s important to ensure that whoever is looking at the code, whether that be the reviewer or a future engineer, can understand the motivations and how your code achieves its goals.  \nProactively asking for targeted help or feedback.  \nRespond clearly to questions asked by the reviewers.  \nAvoid huge commits by submitting incremental changes. Commits which are large and contain changes to multiple files will lead to unfair review of the code. Biased behavior of reviewers may kick in while reviewing such PRs. For e.g. a huge commit from a senior developer may get approved without thorough review whereas a huge commit from a junior developer may never get reviewed and approved.  \nGuidelines for the Reviewer  \nAssume positive intent from the author.  \nWrite clear and elaborate comments.  \nIdentify subjectivity, choice of coding and best practice. It is good to discuss coding style and subjective coding choices in some other forum and not in the PR. A PR should not become a ground to discuss subjective coding choices and having long arguments over it.  \nIf you do not understand the code properly, refrain from commenting e.g., \"This code is incomprehensible\". It is better to have a call with the author and get a basic understanding of their work.  \nBe suggestive and not prescriptive. A reviewer should suggest changes and not prescribe changes, let the author decide if they really want to accept the changes proposed.  \nCulture and Code Reviews  \nWe in ISE, may come across situations in which code reviews are not ideal and often we are observing non inclusive code review behaviors. Its important to be aware of the fact that culture and communication style of a particular geography also influences how people interact over pull requests.\nIn such cases, assuming positive intent of the author and reviewer is a good start to start analyzing quality of code reviews.  \nDealing with the Impostor Phenomenon  \nImpostor phenomenon is a psychological pattern in which an individual doubts their skills, talents, or accomplishments and has a persistent internalized fear of being exposed as a \"fraud\" - Wikipedia.  \nSomeone experiencing impostor phenomenon may find submitting code for a review particularly stressful. It is important to realize that everybody can have meaningful contributions and not to let the perceived weaknesses prevent contributions.  \nSome tips for overcoming the impostor phenomenon for authors:  \nReview the guidelines highlighted above and make sure your code change adhere to them.  \nAsk for help from a colleague - pair program with an experienced colleague that you can learn from.  \nSome tips for overcoming the impostor phenomenon for reviewers:  \nAnyone can have valuable insights.  \nA fresh new pair of eyes are always welcome.  \nStudy the review until you have clearly understood it, check the corner cases and look for ways to improve it.  \nIf something is not clear, a simple specific question should be asked.  \nIf you have learnt something, you can always compliment the author.  \nIf possible, pair with someone to review the code so that you can establish a personal connection and have a more profound discussion about the code.  \nTools  \nBelow are some tools which may help in establishing inclusive code review culture within our teams.  \nAnonymous GitHub  \nBlind Code Reviews  \nGitmask  \ninclusivelint",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\inclusion-in-code-review.md"
    },
    {
        "chunkId": "chunk63_0",
        "chunkContent": "Work Item ID  \nFor more information about how to contribute to this repo, visit this page  \nDescription  \nShould include a concise description of the changes (bug or feature), it's impact, along with a summary of the solution  \nSteps to Reproduce Bug and Validate Solution  \nOnly applicable if the work is to address a bug. Please remove this section if the work is for a feature or story\nProvide details on the environment the bug is found, and detailed steps to recreate the bug.\nThis should be detailed enough for a team member to confirm that the bug no longer occurs  \nPR Checklist  \nUse the check-list below to ensure your branch is ready for PR.  If the item is not applicable, leave it blank.  \n[ ] I have updated the documentation accordingly.  \n[ ] I have added tests to cover my changes.  \n[ ] All new and existing tests passed.  \n[ ] My code follows the code style of this project.  \n[ ] I ran the lint checks which produced no new errors nor warnings for my changes.  \n[ ] I have checked to ensure there aren't other open Pull Requests for the same update/change.  \nDoes this introduce a breaking change?  \n[ ] Yes  \n[ ] No  \nIf this introduces a breaking change, please describe the impact and migration path for existing applications below.  \nTesting  \nInstructions for testing and validation of your code:  \nWhat OS was used for testing.  \nWhich test sets were used.  \nDescription of test scenarios that you have tried.  \nAny relevant logs or outputs  \nUse this section to attach pictures that demonstrates your changes working / healthy  \nIf you are printing something show a screenshot  \nWhen you want to share long logs upload to:\n(StorageAccount)/pr-support/attachments/(PR Number)/(yourFiles) using [Azure Storage Explorer](https://azure.microsoft.com/en-us/features/storage-explorer/) or portal.azure.com and insert the link here.  \nOther information or known dependencies  \nAny other information or known dependencies that is important to this PR.  \nTODO that are to be done after this PR.",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\pull-request-template.md"
    },
    {
        "chunkId": "chunk64_0",
        "chunkContent": "Pull Requests  \nChanges to any main codebase - main branch in Git repository, for example - must be done using pull requests (PR).  \nPull requests enable:  \nCode inspection - see Code Reviews  \nRunning automated qualification of the code  \nLinters  \nCompilation  \nUnit tests  \nIntegration tests etc.  \nThe requirements of pull requests can and should be enforced by policies, which can be set in the most modern version control and work item tracking systems. See Evidence and Measures section for more information.  \nGeneral Process  \nImplement changes based on the well-defined description and acceptance criteria of the task at hand  \nThen, before creating a new pull request:\nMake sure the code conforms with the agreed coding conventions\nThis can be partially automated using linters  \nEnsure the code compiles and runs without errors or warnings\nWrite and/or update tests to cover the changes and make sure all new and existing tests pass\nWrite and/or update the documentation to match the changes  \nOnce convinced the criteria above are met, create and submit a new pull request adhering to the pull request template  \nFollow the code review process to merge the changes to the main codebase  \nThe following diagram illustrates this approach.  \n{% raw %}  \nmermaid\nsequenceDiagram\nNew branch->>+Pull request: New PR creation\nPull request->>+Code review: Review process\nCode review->>+Pull request: Code updates\nPull request->>+New branch: Merge Pull Request\nPull request-->>-New branch: Delete branch\nPull request ->>+ Main branch: Merge after completion\nNew branch->>+Main branch: Goal of the Pull request  \n{% endraw %}  \nSize Guidance  \nWe should always aim to keep pull requests small. Small PRs have multiple advantages:  \nThey are easier to review; a clear benefit for the reviewers.  \nThey are easier to deploy; this is aligned with the strategy of release fast and release often.  \nMinimizes possible conflicts and stale PRs.  \nHowever, we should keep PRs focused - for example around a functional feature, optimization or code readability and avoid having PRs that include code that is without context or loosely coupled. There is no right size, but keep in mind that a code review is a collaborative process, a big PRs could be difficult and therefore slower to review. We should always strive to have as small PRs as possible that still add value.  \nBest Practices  \nBeyond the size, remember that every PR should:  \nbe consistent,  \nnot break the build, and  \ninclude related tests as part of the PR.  \nBe consistent means that all the changes included on the PR should aim to solve one goal (ex. one user story) and be intrinsically related. Think of this as the Single-responsibility principle in terms of the whole project, the PR should have only one reason to change the project.  \nStart small, it is easier to create a small PR from the start than to break up a bigger one.  \nThese are some strategies to keep PRs small depending on the \"cause\" of the inevitability, you could break the PR into self-container changes which still add value, release features that are hidden (see feature flag, feature toggling or canary releases) or break the PR into different layers (for example using design patterns like MVC or Observer/Subject). No matter the strategy.  \nPull Request Description  \nWell written PR descriptions helps maintain a clean, well-structured change history. While every team need not conform to the same specification, it is important that the convention is agreed upon at the start of the project.  \nOne popular specification for open-source projects and others is the Conventional Commits specification, which is structured as:  \n{% raw %}  \n```txt\n[optional scope]:\n\n[optional body]\n\n[optional footer]\n```  \n{% endraw %}  \nlist of commit types from the Angular open-source project. It should be clear that  \nSee also Pull Request Template  \nResources  \nWriting a great pull request description  \nReview code with pull requests (Azure DevOps)  \nCollaborating with issues and pull requests (GitHub)  \nGoogle approach to PR size  \nFeature Flags  \nFacebook approach to hidden features  \nConventional Commits specification  \nAngular Commit types",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\pull-requests.md"
    },
    {
        "chunkId": "chunk65_0",
        "chunkContent": "Code Reviews  \nDevelopers working on projects should conduct peer code reviews on every pull request (or check-in to a shared branch).  \nGoals  \nCode review is a way to have a conversation about the code where participants will:  \nImprove code quality by identifying and removing defects before they can be introduced into shared code branches.  \nLearn and grow by having others review the code, we get exposed to unfamiliar design patterns or languages among other topics, and even break some bad habits.  \nShared understanding between the developers over the project's code.  \nResources  \nCode review tools  \nGoogle's Engineering Practices documentation: How to do a code review  \nBest Kept Secrets of Peer Code Review",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\README.md"
    },
    {
        "chunkId": "chunk66_0",
        "chunkContent": "Code Review Tools  \nCustomize ADO  \nTask boards  \nAzDO: Customize cards  \nAzDO: Add columns on task board  \nReviewer policies  \nSetting required reviewer group in AzDO - Automatically include code reviewers  \nConfiguring Branch Policies  \nAzDO: Configure branch policies  \nAzDO: Configuring branch policies with the CLI tool:  \nCreate a policy configuration file  \nApproval count policy  \nGitHub: Configuring protected branches  \nVisual Studio Code  \nGitHub: GitHub Pull Requests  \nSupports processing GitHub pull requests inside VS Code.  \nOpen the plugin from the Activity Bar  \nSelect Assigned To Me  \nSelect a PR  \nUnder Description you can choose to Check Out the branch and get into Review Mode and get a more integrated experience  \nAzure DevOps: Azure DevOps Pull Requests  \nSupports processing Azure DevOps pull requests inside VS Code.  \nOpen the plugin from the Activity Bar  \nSelect Assigned To Me  \nSelect a PR  \nUnder Description you can choose to Check Out the branch and get into Review Mode and get a more integrated experience  \nVisual Studio  \nThe following extensions can be used to create an integrated code review experience in Visual Studio working with either GitHub or Azure DevOps.  \nGitHub: GitHub Extension for Visual Studio  \nProvides extended functionality for working with pull requests on GitHub directly out of Visual Studio.  \nView -> Other Windows -> GitHub  \nClick on the Pull Requests icon in the task bar  \nDouble click on a pending pull request  \nAzure DevOps: Pull Requests for Visual Studio  \nWork with pull requests on Azure DevOps directly out of Visual Studio.  \nOpen Team Explorer  \nClick on Pull Requests  \nDouble-click a pull request - the Pull Request Details open  \nClick on Checkout if you want to have the full change locally and have a more integrated experience  \nGo through the changes and make comments  \nWeb  \nReviewable: Seamless multi-round GitHub reviews  \nSupports multi-round GitHub code reviews, with keyboard shortcuts and more. VS Code extension is in-progress.  \nVisit the Review Dashboard to see reviews awaiting your action, that have new comments for you, and more.  \nSelect a Pull Request from that list.  \nOpen any file in your browser, in Visual Studio Code, or any editor you've configured by clicking on your profile photo in the top-right  \nSelect an editor under \"External editor link template\". VS Code is an option, but so is any editor that supports URI's.  \nReview the diff on an overall or per-file basis, leaving comments, code suggestions, and more",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\tools.md"
    },
    {
        "chunkId": "chunk67_0",
        "chunkContent": "Evidence and Measures  \nEvidence  \nMany of the code quality assurance items can be automated or enforced by policies in modern version control and work item tracking systems. Verification of the policies on the main branch in Azure DevOps (AzDO) or GitHub, for example, may be sufficient evidence that a project team is conducting code reviews.  \n[ ] The main branches in all repositories have branch policies. - Configure branch policies  \n[ ] All builds produced out of project repositories include appropriate linters, run unit tests.  \n[ ] Every bug work item should include a link to the pull request that introduced it, once the error has been diagnosed. This helps with learning.  \n[ ] Each bug work item should include a note on how the bug might (or might not have) been caught in a code review.  \n[ ] The project team regularly updates their code review checklists to reflect common issues they have encountered.  \n[ ] Dev Leads should review a sample of pull requests and/or be co-reviewers with other developers to help everyone improve their skills as code reviewers.  \nMeasures  \nThe team can collect metrics of code reviews to measure their efficiency. Some useful metrics include:  \nDefect Removal Efficiency (DRE) - a measure of the development team's ability to remove defects prior to release  \nTime metrics:  \nTime used preparing for code inspection sessions  \nTime used in review sessions  \nLines of code (LOC) inspected per time unit/meeting  \nIt is a perfectly reasonable solution to track these metrics manually e.g. in an Excel sheet. It is also possible to utilize the features of project management platforms - for example, AzDO enables dashboards for metrics including tracking bugs. You may find ready-made plugins for various platforms - see GitHub Marketplace for instance - or you can choose to implement these features yourself.  \nRemember that since defects removed thanks to reviews is far less costly compared to finding them in production, the cost of doing code reviews is actually negative!  \nFor more information, see links under resources.  \nResources  \nA Guide to Code Inspections",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\evidence-and-measures\\README.md"
    },
    {
        "chunkId": "chunk68_0",
        "chunkContent": "Author Guidance  \nProperly describe your pull request (PR)  \nGive the PR a descriptive title, so that other members can easily (in one short sentence) understand what a PR is about.  \nEvery PR should have a proper description, that shows the reviewer what has been changed and why.  \nAdd relevant reviewers  \nAdd one or more reviewers (depending on your project's guidelines) to the PR. Ideally, you would add at least someone who has expertise and is familiar with the project, or the language used  \nAdding someone less familiar with the project or the language can aid in verifying the changes are understandable, easy to read, and increases the expertise within the team  \nIn ISE code-with projects with a customer team, it is important to include reviewers from both organizations for knowledge transfer - Customize Reviewers Policy  \nBe open to receive feedback  \nDiscuss design/code logic and address all comments as follows:  \nResolve a comment, if the requested change has been made.  \nMark the comment as \"won't fix\", if you are not going to make the requested changes and provide a clear reasoning  \nIf the requested change is within the scope of the task, \"I'll do it later\" is not an acceptable reason!  \nIf the requested change is out of scope, create a new work item (task or bug) for it  \nIf you don't understand a comment, ask questions in the review itself as opposed to a private chat  \nIf a thread gets bloated without a conclusion, have a meeting with the reviewer (call them or knock on door)  \nUse checklists  \nWhen creating a PR, it is a good idea to add a checklist of objectives of the PR in the description. This helps the reviewers to focus on the key areas of the code changes.  \nLink a task to your PR  \nLink the corresponding work items/tasks to the PR. There is no need to duplicate information between the work item and the PR, but if some details are missing in either one, together they provide more context to the reviewer.  \nCode should have annotations before the review  \nIf you can't avoid large PRs, include explanations of the changes in order to make it easier for the reviewer to review the code, with clear comments the reviewer can identify the goal of every code block.",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\process-guidance\\author-guidance.md"
    },
    {
        "chunkId": "chunk69_0",
        "chunkContent": "Process Guidance  \nGeneral Guidance  \nCode reviews should be part of the software engineering team process regardless of the development model. Furthermore, the team should learn to execute reviews in a timely manner. Pull requests (PRs) left hanging can cause additional merge problems and go stale resulting in lost work. Qualified PRs are expected to reflect well-defined, concise tasks, and thus be compact in content. Reviewing a single task should then take relatively little time to complete.  \nTo ensure that the code review process is healthy, inclusive and meets the goals stated above, consider following these guidelines:  \nEstablish a service-level agreement (SLA) for code reviews and add it to your teams working agreement.  \nAlthough modern DevOps environments incorporate tools for managing PRs, it can be useful to label tasks pending for review or to have a dedicated place for them on the task board - Customize AzDO task boards  \nIn the daily standup meeting check tasks pending for review and make sure they have reviewers assigned.  \nJunior teams and teams new to the process can consider creating separate tasks for reviews together with the tasks themselves.  \nUtilize tools to streamline the review process - Code review tools  \nFoster inclusive code reviews - Inclusion in Code Review  \nMeasuring code review process  \nIf the team is finding that code reviews are taking a significant time to merge, and it is becoming a blocker, consider the following additional recommendations:  \nMeasure the average time it takes to merge a PR per sprint cycle.  \nReview during retrospective how the time to merge can be improved and prioritized.  \nAssess the time to merge across sprints to see if the process is improving.  \nPing required approvers directly as a reminder.  \nCode reviews shouldn't include too many lines of code  \nIt's easy to say a developer can review few hundred lines of code, but when the code surpasses certain amount of lines, the effectiveness of defects discovery will decrease and there is a lesser chance of doing a good review. It's not a matter of setting a code line limit, but rather using common sense. More code there is to review, the higher chances there are letting a bug sneak through. See PR size guidance.  \nAutomate whenever reasonable  \nUse automation (linting, code analysis etc.) to avoid the need for \"nits\" and allow the reviewer to focus more on the functional aspects of the PR. By configuring automated builds, tests and checks (something achievable in the CI process), teams can save human reviewers some time and let them focus in areas like design and functionality for proper evaluation. This will ensure higher chances of success as the team is focusing on the things that matter.  \nRole specific guidance  \nAuthor Guidance  \nReviewer Guidance",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\process-guidance\\README.md"
    },
    {
        "chunkId": "chunk70_0",
        "chunkContent": "Reviewer Guidance  \nSince parts of reviews can be automated via linters and such, human reviewers can focus on architectural and functional correctness. Human reviewers should focus on:  \nThe correctness of the business logic embodied in the code.  \nThe correctness of any new or changed tests.  \nThe \"readability\" and maintainability of the overall design decisions reflected in the code.  \nThe checklist of common errors that the team maintains for each programming language.  \nCode reviews should use the below guidance and checklists to ensure positive and effective code reviews.  \nGeneral guidance  \nUnderstand the code you are reviewing  \nRead every line changed.  \nIf we have a stakeholder review, it\u2019s not necessary to run the PR unless it aids your understanding of the code.  \nAzDO orders the files for you, but you should read the code in some logical sequence to aid understanding.  \nIf you don\u2019t fully understand a change in a file because you don\u2019t have context, click to view the whole file and read through the surrounding code or checkout the changes and view them in IDE.  \nAsk the author to clarify.  \nTake your time and keep focus on scope  \nYou shouldn't review code hastily but neither take too long in one sitting. If you have many pull requests (PRs) to review or if the complexity of code is demanding, the recommendation is to take a break between the reviews to recover and focus on the ones you are most experienced with.  \nAlways remember that a goal of a code review is to verify that the goals of the corresponding task have been achieved. If you have concerns about the related, adjacent code that isn't in the scope of the PR, address those as separate tasks (e.g., bugs, technical debt). Don't block the current PR due to issues that are out of scope.  \nFoster a positive code review culture  \nCode reviews play a critical role in product quality and it should not represent an arena for long discussions or even worse a battle of egos. What matters is a bug caught, not who made it, not who found it, not who fixed it. The only thing that matters is having the best possible product.  \nBe considerate  \nBe positive \u2013 encouraging, appreciation for good practices.  \nPrefix a \u201cpoint of polish\u201d with \u201cNit:\u201d.  \nAvoid language that points fingers like \u201cyou\u201d but rather use \u201cwe\u201d or \u201cthis line\u201d -- code reviews are not personal and language matters.  \nPrefer asking questions above making statements. There might be a good reason for the author to do something.  \nIf you make a direct comment, explain why the code needs to be changed, preferably with an example.  \nTalking about changes, you can suggest changes to a PR by using the suggestion feature (available in GitHub and Azure DevOps) or by creating a PR to the author branch.  \nIf a few back-and-forth comments don't resolve a disagreement, have a quick talk with each other (in-person or call) or create a group discussion this can lead to an array of improvements for upcoming PRs. Don't forget to update the PR with what you agreed on and why.  \nFirst Design Pass  \nPull Request Overview  \nDoes the PR description make sense?  \nDo all the changes logically fit in this PR, or are there unrelated changes?  \nIf necessary, are the changes made reflected in updates to the README or other docs? Especially if the changes affect how the user builds code.  \nUser Facing Changes  \nIf the code involves a user-facing change, is there a GIF/photo that explains the functionality? If not, it might be key to validate the PR to ensure the change does what is expected.  \nEnsure UI changes look good without unexpected behavior.  \nDesign  \nDo the interactions of the various pieces of code in the PR make sense?  \nDoes the code recognize and incorporate architectures and coding patterns?  \nCode Quality Pass  \nComplexity  \nAre functions too complex?  \nIs the single responsibility principle followed? Function or class should do one \u2018thing\u2019.  \nShould a function be broken into multiple functions?  \nIf a method has greater than 3 arguments, it is potentially overly complex.  \nDoes the code add functionality that isn\u2019t needed?  \nCan the code be understood easily by code readers?  \nNaming/readability  \nDid the developer pick good names for functions, variables, etc?  \nError Handling  \nAre errors handled gracefully and explicitly where necessary?  \nFunctionality  \nIs there parallel programming in this PR that could cause race conditions? Carefully read through this logic.  \nCould the code be optimized? For example: are there more calls to the database than need be?  \nHow does the functionality fit in the bigger picture? Can it have negative effects to the overall system?  \nAre there security flaws?  \nDoes a variable name reveal any customer specific information?  \nIs PII and EUII treated correctly? Are we logging any PII information?  \nStyle  \nAre there extraneous comments? If the code isn\u2019t clear enough to explain itself, then the code should be made simpler. Comments may be there to explain why some code exists.  \nDoes the code adhere to the style guide/conventions that we have agreed upon? We use automated styling like black and prettier.  \nTests  \nTests should always be committed in the same PR as the code itself (\u2018I\u2019ll add tests next\u2019 is not acceptable).  \nMake sure tests are sensible and valid assumptions are made.  \nMake sure edge cases are handled as well.  \nTests can be a great source to understand the changes. It can be a strategy to look at tests first to help you understand the changes better.",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\process-guidance\\reviewer-guidance.md"
    },
    {
        "chunkId": "chunk71_0",
        "chunkContent": "YAML(Azure Pipelines) Code Reviews  \nStyle Guide  \nDevelopers should follow the YAML schema reference.  \nCode Analysis / Linting  \nThe most popular YAML linter is YAML extension. This extension provides YAML validation, document outlining, auto-completion, hover support and formatter features.  \nVS Code Extensions  \nThere is an Azure Pipelines for VS Code extension to add syntax highlighting and autocompletion for Azure Pipelines YAML to VS Code. It also helps you set up continuous build and deployment for Azure WebApps without leaving VS Code.  \nYAML in Azure Pipelines Overview  \nWhen the pipeline is triggered, before running the pipeline, there are a few phases such as Queue Time, Compile Time and Runtime where variables are interpreted by their runtime expression syntax.  \nWhen the pipeline is triggered, all nested YAML files are expanded to run in Azure Pipelines. This checklist contains some tips and tricks for reviewing all nested YAML files.  \nThese documents may be useful when reviewing YAML files:  \nAzure Pipelines YAML documentation.  \nPipeline run sequence  \nKey concepts for new Azure Pipelines  \nKey concepts overview  \nA trigger tells a Pipeline to run.  \nA pipeline is made up of one or more stages. A pipeline can deploy to one or more environments.  \nA stage is a way of organizing jobs in a pipeline and each stage can have one or more jobs.  \nEach job runs on one agent. A job can also be agentless.  \nEach agent runs a job that contains one or more steps.  \nA step can be a task or script and is the smallest building block of a pipeline.  \nA task is a pre-packaged script that performs an action, such as invoking a REST API or publishing a build artifact.  \nAn artifact is a collection of files or packages published by a run.  \nCode Review Checklist  \nIn addition to the Code Review Checklist you should also look for these Azure Pipelines YAML specific code review items.  \nPipeline Structure  \n[ ] The steps are well understood and components are easily identifiable. Ensure that there is a proper description displayName: for every step in the pipeline.  \n[ ] Steps/stages of the pipeline are checked in Azure Pipelines to have more understanding of components.  \n[ ] In case you have complex nested YAML files, The pipeline in Azure Pipelines is edited to find trigger root file.  \n[ ] All the template file references are visited to ensure a small change does not cause breaking changes, changing one file may affect multiple pipelines  \n[ ] Long inline scripts in YAML file are moved into script files  \nYAML Structure  \n[ ] Re-usable components are split into separate YAML templates.  \n[ ] Variables are separated per environment stored in templates or variable groups.  \n[ ] Variable value changes in Queue Time, Compile Time and Runtime are considered.  \n[ ] Variable syntax values used with Macro Syntax, Template Expression Syntax and Runtime Expression Syntax are considered.  \n[ ] Variables can change during the pipeline, Parameters cannot.  \n[ ] Unused variables/parameters are removed in pipeline.  \n[ ] Does the pipeline meet with stage/job Conditions criteria?  \nPermission Check & Security  \n[ ] Secret values shouldn't be printed in pipeline, issecret is used for printing secrets for debugging  \n[ ] If pipeline is using variable groups in Library, ensure pipeline has access to the variable groups created.  \n[ ] If pipeline has a remote task in other repo/organization, does it have access?  \n[ ] If pipeline is trying to access a secure file, does it have the permission?  \n[ ] If pipeline requires approval for environment deployments, Who is the approver?  \n[ ] Does it need to keep secrets and manage them, did you consider using Azure KeyVault?  \nTroubleshooting Tips  \nConsider Variable Syntax with Runtime Expressions in the pipeline. Here is a nice sample to understand Expansion of variables.  \nWhen we assign variable like below it won't set during initialize time, it'll assign during runtime, then we can retrieve some errors based on when template runs.  \n{% raw %}  \nyaml\n- task: AzureWebApp@1\ndisplayName: 'Deploy Azure Web App : $(webAppName)'\ninputs:\nazureSubscription: '$(azureServiceConnectionId)'\nappName: '$(webAppName)'\npackage: $(Pipeline.Workspace)/drop/Application$(Build.BuildId).zip\nstartUpCommand: 'gunicorn --bind=0.0.0.0 --workers=4 app:app'  \n{% endraw %}  \nError:  \nAfter passing these variables as parameter, it loads values properly.  \n{% raw %}  \nyaml\n- template: steps-deployment.yaml\nparameters:\nazureServiceConnectionId: ${{ variables.azureServiceConnectionId  }}\nwebAppName: ${{ variables.webAppName  }}  \n{% endraw %}  \n{% raw %}  \nyaml\n- task: AzureWebApp@1\ndisplayName: 'Deploy Azure Web App :${{ parameters.webAppName }}'\ninputs:\nazureSubscription: '${{ parameters.azureServiceConnectionId }}'\nappName: '${{ parameters.webAppName }}'\npackage: $(Pipeline.Workspace)/drop/Application$(Build.BuildId).zip\nstartUpCommand: 'gunicorn --bind=0.0.0.0 --workers=4 app:app'  \n{% endraw %}  \nUse issecret for printing secrets for debugging  \n{% raw %}  \nbash\necho \"##vso[task.setvariable variable=token;issecret=true]${token}\"  \n{% endraw %}",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\recipes\\azure-pipelines-yaml.md"
    },
    {
        "chunkId": "chunk72_0",
        "chunkContent": "Bash Code Reviews  \nStyle Guide  \nDevelopers should follow Google's Bash Style Guide.  \nCode Analysis / Linting  \nProjects must check bash code with shellcheck as part of the CI process.\nApart from linting, shfmt can be used to automatically format shell scripts. There are few vscode code extensions which are based on shfmt like shell-format which can be used to automatically format shell scripts.  \nProject Setup  \nvscode-shellcheck  \nShellcheck extension should be used in VS Code, it provides static code analysis capabilities and auto fixing linting issues. To use vscode-shellcheck in vscode do the following:  \nInstall shellcheck on your machine  \nFor macOS  \n{% raw %}  \nbash\nbrew install shellcheck  \n{% endraw %}  \nFor Ubuntu:  \n{% raw %}  \nbash\napt-get install shellcheck  \n{% endraw %}  \nInstall shellcheck on vscode  \nFind the vscode-shellcheck extension in vscode and install it.  \nAutomatic Code Formatting  \nshell-format  \nshell-format extension does automatic formatting of your bash scripts, docker files and several configuration files. It is dependent on shfmt which can enforce google style guide checks for bash.\nTo use shell-format in vscode do the following:  \nInstall shfmt(Requires Go 1.13 or later) on your machine  \n{% raw %}  \nbash\nGO111MODULE=on go get mvdan.cc/sh/v3/cmd/shfmt  \n{% endraw %}  \nInstall shell-format on vscode  \nFind the shell-format extension in vscode and install it.  \nBuild Validation  \nTo automate this process in Azure DevOps you can add the following snippet to you azure-pipelines.yaml file. This will lint any scripts in the ./scripts/ folder.  \n{% raw %}  \nyaml\n- bash: |\necho \"This checks for formatting and common bash errors. See wiki for error details and ignore options: https://github.com/koalaman/shellcheck/wiki/SC1000\"\nexport scversion=\"stable\"\nwget -qO- \"https://github.com/koalaman/shellcheck/releases/download/${scversion?}/shellcheck-${scversion?}.linux.x86_64.tar.xz\" | tar -xJv\nsudo mv \"shellcheck-${scversion}/shellcheck\" /usr/bin/\nrm -r \"shellcheck-${scversion}\"\nshellcheck ./scripts/*.sh\ndisplayName: \"Validate Scripts: Shellcheck\"  \n{% endraw %}  \nAlso, your shell scripts can be formatted in your build pipeline by using the shfmt tool. To integrate shfmt in your build pipeline do the following:  \n{% raw %}  \nyaml\n- bash: |\necho \"This step does auto formatting of shell scripts\"\nshfmt -l -w ./scripts/*.sh\ndisplayName: \"Format Scripts: shfmt\"  \n{% endraw %}  \nUnit testing using shunit2 can also be added to the build pipeline, using the following block:  \n{% raw %}  \nyaml\n- bash: |\necho \"This step unit tests shell scripts by using shunit2\"\n./shunit2\ndisplayName: \"Format Scripts: shfmt\"  \n{% endraw %}  \nPre-Commit Hooks  \nAll developers should run shellcheck and shfmt as pre-commit hooks.  \nStep 1- Install pre-commit  \nRun pip install pre-commit to install pre-commit.\nAlternatively you can run brew install pre-commit if you are using homebrew.  \nStep 2- Add shellcheck and shfmt  \nAdd .pre-commit-config.yaml file to root of the go project. Run shfmt on pre-commit by adding it to .pre-commit-config.yaml file like below.  \n{% raw %}  \nyaml\n-   repo: git://github.com/pecigonzalo/pre-commit-fmt\nsha: master\nhooks:\n-   id: shell-fmt\nargs:\n- --indent=4  \n{% endraw %}  \n{% raw %}  \nyaml\n-   repo: https://github.com/shellcheck-py/shellcheck-py\nrev: v0.7.1.1\nhooks:\n-   id: shellcheck  \n{% endraw %}  \nStep 3  \nRun $ pre-commit install to set up the git hook scripts  \nDependencies  \nBash scripts are often used to 'glue together' other systems and tools. As such, Bash scripts can often have numerous and/or complicated dependencies. Consider using Docker containers to ensure that scripts are executed in a portable and reproducible environment that is guaranteed to contain all the correct dependencies. To ensure that dockerized scripts are nevertheless easy to execute, consider making the use of Docker transparent to the script's caller by wrapping the script in a 'bootstrap' which checks whether the script is running in Docker and re-executes itself in Docker if it's not the case. This provides the best of both worlds: easy script execution and consistent environments.  \n{% raw %}  \n```bash\nif [[ \"${DOCKER}\" != \"true\" ]]; then\ndocker build -t my_script -f my_script.Dockerfile . > /dev/null\ndocker run -e DOCKER=true my_script \"$@\"\nexit $?\nfi\n\n... implementation of my_script here can assume that all of its dependencies exist since it's always running in Docker ...\n\n```  \n{% endraw %}  \nCode Review Checklist  \nIn addition to the Code Review Checklist you should also look for these bash specific code review items  \n[ ] Does this code use Built-in Shell Options like set -o, set -e, set -u for execution control of shell scripts ?  \n[ ] Is the code modularized? Shell scripts can be modularized like python modules. Portions of bash scripts should be sourced in complex bash projects.  \n[ ] Are all exceptions handled correctly? Exceptions should be handled correctly using exit codes or trapping signals.  \n[ ] Does the code pass all linting checks as per shellcheck and unit tests as per shunit2 ?  \n[ ] Does the code uses relative paths or absolute paths? Relative paths should be avoided as they are prone to environment attacks. If relative path is needed, check that the PATH variable is set.  \n[ ] Does the code take credentials as user input? Are the credentials masked or encrypted in the script?",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\recipes\\bash.md"
    },
    {
        "chunkId": "chunk73_0",
        "chunkContent": "C# Code Reviews  \nStyle Guide  \nDevelopers should follow Microsoft's C# Coding Conventions and, where applicable, Microsoft's Secure Coding Guidelines.  \nCode Analysis / Linting  \nWe strongly believe that consistent style increases readability and maintainability of a code base. Hence, we are recommending analyzers / linters to enforce consistency and style rules.  \nProject Setup  \nWe recommend using a common setup for your solution that you can refer to in all the projects that are part of the solution. Create a common.props file that contains the defaults for all of your projects:  \n{% raw %}  \nxml\n<Project>\n...\n<ItemGroup>\n<PackageReference Include=\"Microsoft.CodeAnalysis.NetAnalyzers\" Version=\"5.0.3\">\n<PrivateAssets>all</PrivateAssets>\n<IncludeAssets>runtime; build; native; contentfiles; analyzers; buildtransitive</IncludeAssets>\n</PackageReference>\n<PackageReference Include=\"StyleCop.Analyzers\" Version=\"1.1.118\">\n<PrivateAssets>all</PrivateAssets>\n<IncludeAssets>runtime; build; native; contentfiles; analyzers; buildtransitive</IncludeAssets>\n</PackageReference>\n</ItemGroup>\n<PropertyGroup>\n<TreatWarningsAsErrors>true</TreatWarningsAsErrors>\n</PropertyGroup>\n<ItemGroup Condition=\"Exists('$(MSBuildThisFileDirectory)../.editorconfig')\" >\n<AdditionalFiles Include=\"$(MSBuildThisFileDirectory)../.editorconfig\" />\n</ItemGroup>\n...\n</Project>  \n{% endraw %}  \nYou can then reference the common.props in your other project files to ensure a consistent setup.  \n{% raw %}  \nxml\n<Project Sdk=\"Microsoft.NET.Sdk.Web\">\n<Import Project=\"..\\common.props\" />\n</Project>  \n{% endraw %}  \nThe .editorconfig allows for configuration and overrides of rules. You can have an .editorconfig file at project level to customize rules for different projects (test projects for example).  \nDetails about the configuration of different rules.  \n.NET analyzers  \nMicrosoft's .NET analyzers has code quality rules and .NET API usage rules implemented as analyzers using the .NET Compiler Platform (Roslyn). This is the replacement for Microsoft's legacy FxCop analyzers.  \nEnable or install first-party .NET analyzers.  \nIf you are currently using the legacy FxCop analyzers, migrate from FxCop analyzers to .NET analyzers.  \nStyleCop analyzer  \nThe StyleCop analyzer is a nuget package (StyleCop.Analyzers) that can be installed in any of your projects. It's mainly around code style rules and makes sure the team is following the same rules without having subjective discussions about braces and spaces. Detailed information can be found here: StyleCop Analyzers for the .NET Compiler Platform.  \nThe minimum rules set teams should adopt is the Managed Recommended Rules rule set.  \nAutomatic Code Formatting  \nUse .editorconfig to configure code formatting rules in your project.  \nBuild validation  \nIt's important that you enforce your code style and rules in the CI to avoid any team member merging code that does not comply with your standards into your git repo.  \nIf you are using FxCop analyzers and StyleCop analyzer, it's very simple to enable those in the CI. You have to make sure you are setting up the project using nuget and .editorconfig (see Project setup). Once you have this setup, you will have to configure the pipeline to build your code. That's pretty much it. The FxCop analyzers will run and report the result in your build pipeline. If there are rules that are violated, your build will be red.  \n{% raw %}  \nyaml\n- task: DotNetCoreCLI@2\ndisplayName: 'Style Check & Build'\ninputs:\ncommand: 'build'\nprojects: '**/*.csproj'  \n{% endraw %}  \nEnable Roslyn Support in Visual Studio Code  \nThe above steps also work in VS Code provided you enable Roslyn support for Omnisharp. The setting is omnisharp.enableRoslynAnalyzers and must be set to true. After enabling this setting you must \"Restart Omnisharp\" (this can be done from the Command Palette in VS Code or by restarting VS Code).  \nCode Review Checklist  \nIn addition to the Code Review Checklist you should also look for these C# specific code review items  \n[ ] Does this code make correct use of asynchronous programming constructs, including proper use of await and Task.WhenAll including CancellationTokens?  \n[ ] Is the code subject to concurrency issues? Are shared objects properly protected?  \n[ ] Is dependency injection (DI) used? Is it setup correctly?  \n[ ] Are middleware included in this project configured correctly?  \n[ ] Are resources released deterministically using the IDispose pattern? Are all disposable objects properly disposed (using pattern)?  \n[ ] Is the code creating a lot of short-lived objects. Could we optimize GC pressure?  \n[ ] Is the code written in a way that causes boxing operations to happen?  \n[ ] Does the code handle exceptions correctly?  \n[ ] Is package management being used (NuGet) instead of committing DLLs?  \n[ ] Does this code use LINQ appropriately? Pulling LINQ into a project to replace a single short loop or in ways that do not perform well are usually not appropriate.  \n[ ] Does this code properly validate arguments sanity (i.e. CA1062)? Consider leveraging extensions such as Ensure.That  \n[ ] Does this code include telemetry (metrics, tracing and logging) instrumentation?  \n[ ] Does this code leverage the options design pattern by using classes to provide strongly typed access to groups of related settings?  \n[ ] Instead of using raw strings, are constants used in the main class? Or if these strings are used across files/classes, is there a static class for the constants?  \n[ ] Are magic numbers explained? There should be no number in the code without at least a comment of why this is here. If the number is repetitive, is there a constant/enum or equivalent?  \n[ ] Is proper exception handling set up? Catching the exception base class (catch (Exception)) is generally not the right pattern. Instead, catch the specific exceptions that can happen e.g., IOException.  \n[ ] Is the use of #pragma fair?  \n[ ] Are tests arranged correctly with the Arrange/Act/Assert pattern and properly documented in this way?  \n[ ] If there is an asynchronous method, does the name of the method end with the Async suffix?  \n[ ] If a method is asynchronous, is Task.Delay used instead of Thread.Sleep? Task.Delay is not blocking the current thread and creates a task that will complete without blocking the thread, so in a multi-threaded, multi-task environment, this is the one to prefer.  \n[ ] Is a cancellation token for asynchronous tasks needed rather than bool patterns?  \n[ ] Is a minimum level of logging in place? Are the logging levels used sensible?  \n[ ] Are internal vs private vs public classes and methods used the right way?  \n[ ] Are auto property set and get used the right way? In a model without constructor and for deserialization, it is ok to have all accessible. For other classes usually a private set or internal set is better.  \n[ ] Is the using pattern for streams and other disposable classes used? If not, better to have the Dispose method called explicitly.  \n[ ] Are the classes that maintain collections in memory, thread safe? When used under concurrency, use lock pattern.",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\recipes\\csharp.md"
    },
    {
        "chunkId": "chunk74_0",
        "chunkContent": "Go Code Reviews  \nStyle Guide  \nDevelopers should follow the Effective Go Style Guide.  \nCode Analysis / Linting  \nProject Setup  \nBelow is the project setup that you would like to have in your VS Code.  \nvscode-go extension  \nUsing the Go extension for Visual Studio Code, you get language features like IntelliSense, code navigation, symbol search, bracket matching, snippets, etc. This extension includes rich language support for go in VS Code.  \ngo vet  \ngo vet is a static analysis tool that checks for common go errors, such as incorrect use of range loop variables or misaligned printf arguments. Go code should be able to build with no go vet errors. This will be part of vscode-go extension.  \ngolint  \n:exclamation: NOTICE: The golint library is deprecated and archived.  \nThe linter revive (below) might be a suitable replacement.  \ngolint can be an effective tool for finding many issues, but it errors on the side of false positives. It is best used by developers when working on code, not as part of an automated build process. This is the default linter which is set up as part of the vscode-go extension.  \nrevive  \nRevive is a linter for go, it provides a framework for development of custom rules, and lets you define a strict preset for enhancing your development & code review processes.  \nAutomatic Code Formatting  \ngofmt  \ngofmt is the automated code format style guide for Go. This is part of the vs-code extension, and it is enabled by default to run on save of every file.  \nAggregator  \ngolangci-lint  \ngolangci-lint is the replacement for the now deprecated gometalinter. It is 2-7x faster than gometalinter along with a host of other benefits.  \ngolangci-lint is a powerful, customizable aggregator of linters. By default, several are enabled but not all. A full list of linters and their usages can be found here.  \nIt will allow you to configure each linter and choose which ones you would like to enable in your project.  \nOne awesome feature of golangci-lint is that is can be easily introduced to an existing large codebase using the --new-from-rev COMMITID. With this setting only newly introduced issues are flagged, allowing a team to improve new code without having to fix all historic issues in a large codebase. This provides a great path to improving code-reviews on existing solutions. golangci-lint can also be setup as the default linter in VS Code.  \nInstallation options for golangci-lint are present at golangci-lint.  \nTo use golangci-lint with VS Code, use the below recommended settings:  \n{% raw %}  \njson\n\"go.lintTool\":\"golangci-lint\",\n\"go.lintFlags\": [\n\"--fast\"\n]  \n{% endraw %}  \nPre-Commit Hooks  \nAll developers should run gofmt in a pre-commit hook to ensure standard formatting.  \nStep 1- Install pre-commit  \nRun pip install pre-commit to install pre-commit.\nAlternatively you can run brew install pre-commit if you are using homebrew.  \nStep 2- Add go-fmt in pre-commit  \nAdd .pre-commit-config.yaml file to root of the go project. Run go-fmt on pre-commit by adding it to .pre-commit-config.yaml file like below.  \n{% raw %}  \nyaml\n- repo: git://github.com/dnephin/pre-commit-golang\nrev: master\nhooks:\n- id: go-fmt  \n{% endraw %}  \nStep 3  \nRun $ pre-commit install to set up the git hook scripts  \nBuild Validation  \ngofmt should be run as a part of every build to enforce the common standard.  \nTo automate this process in Azure DevOps you can add the following snippet to your azure-pipelines.yaml file. This will format any scripts in the ./scripts/ folder.  \n{% raw %}  \nyaml\n- script: go fmt\nworkingDirectory: $(System.DefaultWorkingDirectory)/scripts\ndisplayName: \"Run code formatting\"  \n{% endraw %}  \ngovet should be run as a part of every build to check code linting.  \nTo automate this process in Azure DevOps you can add the following snippet to your azure-pipelines.yaml file. This will check linting of any scripts in the ./scripts/ folder.  \n{% raw %}  \nyaml\n- script: go vet\nworkingDirectory: $(System.DefaultWorkingDirectory)/scripts\ndisplayName: \"Run code linting\"  \n{% endraw %}  \nAlternatively you can use golangci-lint as a step in the pipeline to do multiple enabled validations(including go vet and go fmt) of golangci-lint.  \n{% raw %}  \nyaml\n- script: golangci-lint run --enable gofmt --fix\nworkingDirectory: $(System.DefaultWorkingDirectory)/scripts\ndisplayName: \"Run code linting\"  \n{% endraw %}  \nSample Build Validation Pipeline in Azure DevOps  \n{% raw %}  \n```yaml\ntrigger: master\n\npool:\nvmImage: 'ubuntu-latest'\n\nsteps:\n\ntask: GoTool@0\ninputs:\nversion: '1.13.5'\n\ntask: Go@0\ninputs:\ncommand: 'get'\narguments: '-d'\nworkingDirectory: '$(System.DefaultWorkingDirectory)/scripts'\n\nscript: go fmt\nworkingDirectory: $(System.DefaultWorkingDirectory)/scripts\ndisplayName: \"Run code formatting\"\n\nscript: go vet\nworkingDirectory: $(System.DefaultWorkingDirectory)/scripts\ndisplayName: 'Run go vet'\n\ntask: Go@0\ninputs:\ncommand: 'build'\nworkingDirectory: '$(System.DefaultWorkingDirectory)'\n\ntask: CopyFiles@2\ninputs:\nTargetFolder: '$(Build.ArtifactStagingDirectory)'\n\ntask: PublishBuildArtifacts@1\ninputs:\nartifactName: drop\n```  \n{% endraw %}  \nCode Review Checklist  \nThe Go language team maintains a list of common Code Review Comments for go that form the basis for a solid checklist for a team working in Go that should be followed in addition to the ISE Code Review Checklist  \n[ ] Does this code handle errors correctly? This includes not throwing away errors with _ assignments and returning errors, instead of in-band error values?  \n[ ] Does this code follow Go standards for method receiver types?  \n[ ] Does this code pass values when it should?  \n[ ] Are interfaces in this code defined in the correct packages?  \n[ ] Do go-routines in this code have clear lifetimes?  \n[ ] Is parallelism in this code handled via go-routines and channels with synchronous methods?  \n[ ] Does this code have meaningful Doc Comments?  \n[ ] Does this code have meaningful Package Comments?  \n[ ] Does this code use Contexts correctly?  \n[ ] Do unit tests fail with meaningful messages?",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\recipes\\go.md"
    },
    {
        "chunkId": "chunk75_0",
        "chunkContent": "Java Code Reviews  \nJava Style Guide  \nDevelopers should follow the Google Java Style Guide.  \nCode Analysis / Linting  \nWe strongly believe that consistent style increases readability and maintainability of a code base. Hence, we are recommending analyzers to enforce consistency and style rules.  \nWe make use of Checkstyle using the same configuration used in the Azure Java SDK.  \nFindBugs and PMD are also commonly used.  \nAutomatic Code Formatting  \nEclipse, and other Java IDEs, support automatic code formatting.  If using Maven, some developers also make use of the formatter-maven-plugin.  \nBuild Validation  \nIt's important to enforce your code style and rules in the CI to avoid any team members merging code that does not comply with standards into your git repo.  If building using Azure DevOps, Azure DevOps support Maven and Gradle build tasks using PMD, Checkstyle, and FindBugs code analysis tools as part of every build.  \nHere is an example yaml for a Maven build task with all three analysis tools enabled:  \n{% raw %}  \nyaml\n- task: Maven@3\ndisplayName: 'Maven pom.xml'\ninputs:\nmavenPomFile: '$(Parameters.mavenPOMFile)'\ncheckStyleRunAnalysis: true\npmdRunAnalysis: true\nfindBugsRunAnalysis: true  \n{% endraw %}  \nHere is an example yaml for a Gradle build task with all three analysis tools enabled:  \n{% raw %}  \nyaml\n- task: Gradle@2\ndisplayName: 'gradlew build'\ninputs:\ncheckStyleRunAnalysis: true\nfindBugsRunAnalysis: true\npmdRunAnalysis: true  \n{% endraw %}  \nCode Review Checklist  \nIn addition to the Code Review Checklist you should also look for these Java specific code review items  \n[ ] Does the project use Lambda to make code cleaner?  \n[ ] Is dependency injection (DI) used?  Is it setup correctly?  \n[ ] If the code uses Spring Boot, are you using @Inject instead of @Autowire?  \n[ ] Does the code handle exceptions correctly?  \n[ ] Is the Azul Zulu OpenJDK being used?  \n[ ] Is a build automation and package management tool (Gradle or Maven) being used?",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\recipes\\java.md"
    },
    {
        "chunkId": "chunk76_0",
        "chunkContent": "JavaScript/TypeScript Code Reviews  \nStyle Guide  \nDevelopers should use prettier to do code formatting for JavaScript.  \nUsing an automated code formatting tool like Prettier enforces a well accepted style guide that was collaboratively built by a wide range of companies including Microsoft, Facebook, and AirBnB.  \nFor higher level style guidance not covered by prettier, we follow the AirBnB Style Guide.  \nCode Analysis / Linting  \neslint  \nPer guidance outlined in Palantir's 2019 TSLint road map,\nTypeScript code should be linted with ESLint. See the typescript-eslint documentation for more information around linting TypeScript code with ESLint.  \nTo install and configure linting with ESLint,\ninstall the following packages as dev-dependencies:  \n{% raw %}  \nbash\nnpm install -D eslint @typescript-eslint/parser @typescript-eslint/eslint-plugin  \n{% endraw %}  \nAdd a .eslintrc.js to the root of your project:  \n{% raw %}  \njavascript\nmodule.exports = {\nroot: true,\nparser: '@typescript-eslint/parser',\nplugins: [\n'@typescript-eslint',\n],\nextends: [\n'eslint:recommended',\n'plugin:@typescript-eslint/eslint-recommended',\n'plugin:@typescript-eslint/recommended',\n],\n};  \n{% endraw %}  \nAdd the following to the scripts of your package.json:  \n{% raw %}  \njson\n\"scripts\": {\n\"lint\": \"eslint . --ext .js,.jsx,.ts,.tsx --ignore-path .gitignore\"\n}  \n{% endraw %}  \nThis will lint all .js, .jsx, .ts, .tsx files in your project and omit any files or\ndirectories specified in your .gitignore.  \nYou can run linting with:  \n{% raw %}  \nbash\nnpm run lint  \n{% endraw %}  \nSetting up Prettier  \nPrettier is an opinionated code formatter.  \nGetting started guide.  \nInstall with npm as a dev-dependency:  \n{% raw %}  \nbash\nnpm install -D prettier eslint-config-prettier eslint-plugin-prettier  \n{% endraw %}  \nAdd prettier to your .eslintrc.js:  \n{% raw %}  \njavascript\nmodule.exports = {\nroot: true,\nparser: '@typescript-eslint/parser',\nplugins: [\n'@typescript-eslint',\n],\nextends: [\n'eslint:recommended',\n'plugin:@typescript-eslint/eslint-recommended',\n'plugin:@typescript-eslint/recommended',\n'prettier/@typescript-eslint',\n'plugin:prettier/recommended',\n],\n};  \n{% endraw %}  \nThis will apply the prettier rule set when linting with ESLint.  \nAuto formatting with VS Code  \nVS Code can be configured to automatically perform eslint --fix on save.  \nCreate a .vscode folder in the root of your project and add the following to your\n.vscode/settings.json:  \n{% raw %}  \njson\n{\n\"editor.codeActionsOnSave\": {\n\"source.fixAll.eslint\": true\n},\n}  \n{% endraw %}  \nBy default, we use the following overrides should be added to the VS Code configuration to standardize on single quotes, a four space drop, and to do ESLinting:  \n{% raw %}  \njson\n{\n\"prettier.singleQuote\": true,\n\"prettier.eslintIntegration\": true,\n\"prettier.tabWidth\": 4\n}  \n{% endraw %}  \nSetting Up Testing  \nPlaywright is highly recommended to be set up within a project. its an open source testing suite created by Microsoft.  \nTo install it use this command:  \n{% raw %}  \nbash\nnpm install playwright  \n{% endraw %}  \nSince playwright shows the tests in the browser you have to choose which browser you want it to run if unless using chrome, which is the default. You can do this by  \nBuild Validation  \nTo automate this process in Azure Devops you can add the following snippet to your pipeline definition yaml file. This will lint any scripts in the ./scripts/ folder.  \n{% raw %}  \nyaml\n- task: Npm@1\ndisplayName: 'Lint'\ninputs:\ncommand: 'custom'\ncustomCommand: 'run lint'\nworkingDir: './scripts/'  \n{% endraw %}  \nPre-commit hooks  \nAll developers should run eslint in a pre-commit hook to ensure standard formatting. We highly recommend using an editor integration like vscode-eslint to provide realtime feedback.  \nUnder .git/hooks rename pre-commit.sample to pre-commit  \nRemove the existing sample code in that file  \nThere are many examples of scripts for this on gist, like pre-commit-eslint  \nModify accordingly to include TypeScript files (include ts extension and make sure typescript-eslint is set up)  \nMake the file executable: chmod +x .git/hooks/pre-commit  \nAs an alternative husky can be considered to simplify pre-commit hooks.  \nCode Review Checklist  \nIn addition to the Code Review Checklist you should also look for these JavaScript and TypeScript specific code review items.  \nJavascript / Typescript Checklist  \n[ ] Does the code stick to our formatting and code standards? Does running prettier and ESLint over the code should yield no warnings or errors respectively?  \n[ ] Does the change re-implement code that would be better served by pulling in a well known module from the ecosystem?  \n[ ] Is \"use strict\"; used to reduce errors with undeclared variables?  \n[ ] Are unit tests used where possible, also for APIs?  \n[ ] Are tests arranged correctly with the Arrange/Act/Assert pattern and properly documented in this way?  \n[ ] Are best practices for error handling followed, as well as try catch finally statements?  \n[ ] Are the doWork().then(doSomething).then(checkSomething) properly followed for async calls, including expect, done?  \n[ ] Instead of using raw strings, are constants used in the main class? Or if these strings are used across files/classes, is there a static class for the constants?  \n[ ] Are magic numbers explained? There should be no number in the code without at least a comment of why it is there. If the number is repetitive, is there a constant/enum or equivalent?  \n[ ] If there is an asynchronous method, does the name of the method end with the Async suffix?  \n[ ] Is a minimum level of logging in place? Are the logging levels used sensible?  \n[ ] Is document fragment manipulation limited to when you need to manipulate multiple sub elements?  \n[ ] Does TypeScript code compile without raising linting errors?  \n[ ] Instead of using raw strings, are constants used in the main class? Or if these strings are used across files/classes, is there a static class for the constants?  \n[ ] Are magic numbers explained? There should be no number in the code without at least a comment of why it is there. If the number is repetitive, is there a constant/enum or equivalent?  \n[ ] Is there a proper /* */ in the various classes and methods?  \n[ ] Are heavy operations implemented in the backend, leaving the controller as thin as possible?  \n[ ] Is event handling on the html efficiently done?",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\recipes\\javascript-and-typescript.md"
    },
    {
        "chunkId": "chunk77_0",
        "chunkContent": "Markdown Code Reviews  \nStyle Guide  \nDevelopers should treat documentation like other source code and follow the same rules and checklists when reviewing documentation as code.  \nDocumentation should both use good Markdown syntax to ensure it's properly parsed, and follow good writing style guidelines to ensure the document is easy to read and understand.  \nMarkdown  \nMarkdown is a lightweight markup language that you can use to add formatting elements to plaintext text documents. Created by John Gruber in 2004, Markdown is now one of the world\u2019s most popular markup languages.  \nUsing Markdown is different from using a WYSIWYG editor. In an application like Microsoft Word, you click buttons to format words and phrases, and the changes are visible immediately. Markdown isn\u2019t like that. When you create a Markdown-formatted file, you add Markdown syntax to the text to indicate which words and phrases should look different.  \nYou can find more information and full documentation here.  \nLinters  \nMarkdown has specific way of being formatted. It is important to respect this formatting, otherwise some interpreters which are strict won't properly display the document. Linters are often used to help developers properly create documents by both verifying proper Markdown syntax, grammar and proper English language.  \nA good setup includes a markdown linter used during editing and PR build verification, and a grammar linter used while editing the document. The following are a list of linters that could be used in this setup.  \nmarkdownlint  \nmarkdownlint is a linter for markdown that verifies Markdown syntax, and also enforces rules that make the text more readable. Markdownlint-cli is an easy-to-use CLI based on Markdownlint.  \nIt's available as a ruby gem, an npm package, a Node.js CLI and a VS Code extension. The VS Code extension Prettier also catches all markdownlint errors.  \nInstalling the Node.js CLI  \n{% raw %}  \nbash\nnpm install -g markdownlint-cli  \n{% endraw %}  \nRunning markdownlint on a Node.js project  \n{% raw %}  \nbash\nmarkdownlint **/*.md --ignore node_modules  \n{% endraw %}  \nFixing errors automatically  \n{% raw %}  \nbash\nmarkdownlint **/*.md --ignore node_modules --fix  \n{% endraw %}  \nA comprehensive list of markdownlint rules is available here.  \nwrite-good  \nwrite-good is a linter for English text that helps writing better documentation.  \n{% raw %}  \nbash\nnpm install -g write-good  \n{% endraw %}  \nRun write-good  \n{% raw %}  \nbash\nwrite-good *.md  \n{% endraw %}  \nRun write-good without installing it  \n{% raw %}  \nbash\nnpx write-good *.md  \n{% endraw %}  \nWrite Good is also available as an extension for VS Code  \nVS Code Extensions  \nWrite Good Linter  \nThe Write Good Linter Extension integrates with VS Code to give grammar and language advice while editing the document.  \nmarkdownlint extension  \nThe markdownlint extension examines the Markdown documents, showing warnings for rule violations while editing.  \nBuild Validation  \nLinting  \nTo automate linting with markdownlint for PR validation in GitHub actions,\nyou can either use linters aggregator as we do with MegaLinter in this repository or use the following YAML.  \n{% raw %}  \n```yaml\nname: Markdownlint\n\non:\npush:\npaths:\n- \"/*.md\"\npull_request:\npaths:\n- \"/*.md\"\n\njobs:\nlint:\n\n```  \n{% endraw %}  \nChecking Links  \nTo automate link check in your markdown files add markdown-link-check action to your validation pipeline:  \n{% raw %}  \nyaml\nmarkdown-link-check:\nruns-on: ubuntu-latest\nsteps:\n- uses: actions/checkout@master\n- uses: gaurav-nelson/github-action-markdown-link-check@v1  \n{% endraw %}  \nMore information about markdown-link-check action options can be found at markdown-link-check home page  \nCode Review Checklist  \nIn addition to the Code Review Checklist you should also look for these documentation specific code review items  \n[ ] Is the document easy to read and understand and does it follow good writing guidelines?  \n[ ] Is there a single source of truth or is content repeated in more than one document?  \n[ ] Is the documentation up to date with the code?  \n[ ] Is the documentation technically, and ethically correct?  \nWriting Style Guidelines  \nThe following are some examples of writing style guidelines.  \nAgree in your team which guidelines you should apply to your project documentation.\nSave your guidelines together with your documentation, so they are easy to refer back to.  \nWording  \nUse inclusive language, and avoid jargon and uncommon words. The docs should be easy to understand  \nBe clear and concise, stick to the goal of the document  \nUse active voice  \nSpell check and grammar check the text  \nAlways follow chronological order  \nVisit Plain English for tips on how to write documentation that is easy to understand.  \nDocument Organization  \nOrganize documents by topic rather than type, this makes it easier to find the documentation  \nEach folder should have a top-level README.md and any other documents within that folder should link directly or indirectly from that README.md  \nDocument names with more than one word should use underscores instead of spaces, for example machine_learning_pipeline_design.md. The same applies to images  \nHeadings  \nStart with a H1 (single # in markdown) and respect the order H1 > H2 > H3 etc  \nFollow each heading with text before proceeding with the next heading  \nAvoid putting numbers in headings. Numbers shift, and can create outdated titles  \nAvoid using symbols and special characters in headers, this causes problems with anchor links  \nAvoid links in headers  \nLinks  \nAvoid duplication of content, instead link to the single source of truth  \nLink but don't summarize. Summarizing content on another page leads to the content living in two places  \nUse meaningful anchor texts, e.g. instead of writing Follow the instructions [here](../recipes/markdown.md) write Follow the [Markdown guidelines](../recipes/markdown.md)  \nMake sure links to Microsoft docs do not contain the language marker /en-us/ or /fr-fr/, as this is automatically determined by the site itself.  \nLists  \nList items should start with capital letters if possible  \nUse ordered lists when the items describe a sequence to follow, otherwise use unordered lists  \nFor ordered lists, prefix each item with 1. When rendered, the list items will appear with sequential numbering. This avoids number-gaps in list  \nDo not add commas , or semicolons ; to the end of list items, and avoid periods . unless the list item represents a complete sentence  \nImages  \nPlace images in a separate directory named img  \nName images appropriately, avoiding generic names like screenshot.png  \nAvoid adding large images or videos to source control, link to an external location instead  \nEmphasis and special sections  \nUse bold or italic to emphasize\nFor sections that everyone reading this document needs to be aware of, use blocks  \nUse backticks for code, a single backtick for inline code like pip install flake8 and 3 backticks for code blocks followed by the language for syntax highlighting  \n{% raw %}  \npython\ndef add(num1: int, num2: int):\nreturn num1 + num2  \n{% endraw %}  \nUse check boxes for task lists  \n[ ] Item 1  \n[ ] Item 2  \n[x] Item 3  \nAdd a References section to the end of the document with links to external references  \nPrefer tables to lists for comparisons and reports to make research and results more readable  \nOption Pros Cons Option 1 Some pros Some cons Option 2 Some pros Some cons  \nGeneral  \nAlways use Markdown syntax, don't mix with HTML  \nMake sure the extension of the files is .md - if the extension is missing, a linter might ignore the files",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\recipes\\markdown.md"
    },
    {
        "chunkId": "chunk78_0",
        "chunkContent": "Python Code Reviews  \nStyle Guide  \nDevelopers should follow the PEP8 style guide with type hints. The use of type hints throughout paired with linting and type hint checking avoids common errors that are tricky to debug.  \nProjects should check Python code with automated tools.  \nLinting should be added to build validation, and both linting and code formatting can be added to your pre-commit hooks and VS Code.  \nCode Analysis / Linting  \nThe 2 most popular python linters are Pylint and Flake8. Both check adherence to PEP8 but vary a bit in what other rules they check. In general Pylint tends to be a bit more stringent and give more false positives but both are good options for linting python code.  \nBoth Pylint and Flake8 can be configured in VS Code using the VS Code python extension.  \nFlake8  \nFlake8 is a simple and fast wrapper around Pyflakes (for detecting coding errors) and pycodestyle (for pep8).  \nInstall Flake8  \n{% raw %}  \nbash\npip install flake8  \n{% endraw %}  \nAdd an extension for the pydocstyle (for doc strings) tool to flake8.  \n{% raw %}  \nbash\npip install flake8-docstrings  \n{% endraw %}  \nAdd an extension for pep8-naming (for naming conventions in pep8) tool to flake8.  \n{% raw %}  \nbash\npip install pep8-naming  \n{% endraw %}  \nRun Flake8  \n{% raw %}  \nbash\nflake8 .    # lint the whole project  \n{% endraw %}  \nPylint  \nInstall Pylint  \n{% raw %}  \nbash\npip install pylint  \n{% endraw %}  \nRun Pylint  \n{% raw %}  \nbash\npylint src  # lint the source directory  \n{% endraw %}  \nAutomatic Code Formatting  \nBlack  \nBlack is an unapologetic code formatting tool. It removes all need from pycodestyle nagging about formatting, so the team can focus on content vs style. It's not possible to configure black for your own style needs.  \n{% raw %}  \nbash\npip install black  \n{% endraw %}  \nFormat python code  \n{% raw %}  \nbash\nblack [file/folder]  \n{% endraw %}  \nAutopep8  \nAutopep8 is more lenient and allows more configuration if you want less stringent formatting.  \n{% raw %}  \nbash\npip install autopep8  \n{% endraw %}  \nFormat python code  \n{% raw %}  \nbash\nautopep8 [file/folder] --in-place  \n{% endraw %}  \nyapf  \nyapf Yet Another Python Formatter is a python formatter from Google based on ideas from gofmt.  This is also more configurable, and a good option for automatic code formatting.  \n{% raw %}  \nbash\npip install yapf  \n{% endraw %}  \nFormat python code  \n{% raw %}  \nbash\nyapf [file/folder] --in-place  \n{% endraw %}  \nVS Code Extensions  \nPython  \nThe Python language extension is the base extension you should have installed for python development with VS Code. It enables intellisense, debugging, linting (with the above linters), testing with pytest or unittest, and code formatting with the formatters mentioned above.  \nPyright  \nThe Pyright extension augments VS Code with static type checking when you use type hints  \n{% raw %}  \npython\ndef add(first_value: int, second_value: int) -> int:\nreturn first_value + second_value  \n{% endraw %}  \nBuild validation  \nTo automate linting with flake8 and testing with pytest in Azure Devops you can add the following snippet to you azure-pipelines.yaml file.  \n{% raw %}  \n```yaml\ntrigger:\nbranches:\ninclude:\n- develop\n- master\npaths:\ninclude:\n- src/*\n\npool:\nvmImage: 'ubuntu-latest'\n\njobs:\n- job: LintAndTest\ndisplayName: Lint and Test\n\nsteps:\n\ncheckout: self\nlfs: true\n\ntask: UsePythonVersion@0\ndisplayName: 'Set Python version to 3.6'\ninputs:\nversionSpec: '3.6'\n\nscript: pip3 install --user -r requirements.txt\ndisplayName: 'Install dependencies'\n\nscript: |\n# Install Flake8\npip3 install --user flake8\n# Install PyTest\npip3 install --user pytest\ndisplayName: 'Install Flake8 and PyTest'\n\nscript: |\npython3 -m flake8\ndisplayName: 'Run Flake8 linter'\n\nscript: |\n# Run PyTest tester\npython3 -m pytest --junitxml=./test-results.xml\ndisplayName: 'Run PyTest Tester'\n\ntask: PublishTestResults@2\ndisplayName: 'Publish PyTest results'\ncondition: succeededOrFailed()\ninputs:\ntestResultsFiles: '*/test-.xml'\ntestRunTitle: 'Publish test results for Python $(python.version)'\n```  \n{% endraw %}  \nTo perform a PR validation on GitHub you can use a similar YAML configuration with GitHub Actions  \nPre-commit hooks  \nPre-commit hooks allow you to format and lint code locally before submitting the pull request.  \nAdding pre-commit hooks for your python repository is easy using the pre-commit package  \nInstall pre-commit and add to the requirements.txt\n{% raw %}\nbash\npip install pre-commit\n{% endraw %}  \nAdd a .pre-commit-config.yaml file in the root of the repository, with the desired pre-commit actions\n{% raw %}\nyaml\nrepos:\n-   repo: https://github.com/ambv/black\nrev: stable\nhooks:\n- id: black\nlanguage_version: python3.6\n-   repo: https://github.com/pre-commit/pre-commit-hooks\nrev: v1.2.3\nhooks:\n- id: flake8\n{% endraw %}  \nEach individual developer that wants to set up pre-commit hooks can then run\n{% raw %}\nbash\npre-commit install\n{% endraw %}  \nAt the next attempted commit any lint failures will block the commit.  \nNote: Installing pre-commit hooks is voluntary and done by each developer individually. Thus, it's not a replacement for build validation on the server  \nCode Review Checklist  \nIn addition to the Code Review Checklist you should also look for these python specific code review items  \n[ ] Are all new packages used included in requirements.txt  \n[ ] Does the code pass all lint checks?  \n[ ] Do functions use type hints, and are there any type hint errors?  \n[ ] Is the code readable and using pythonic constructs wherever possible.",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\recipes\\python.md"
    },
    {
        "chunkId": "chunk79_0",
        "chunkContent": "Language Specific Guidance  \nBash  \nC#  \nGo  \nJava  \nJavaScript and TypeScript  \nMarkdown  \nPython  \nTerraform  \nYAML (Azure Pipelines)",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\recipes\\README.md"
    },
    {
        "chunkId": "chunk80_0",
        "chunkContent": "Terraform Code Reviews  \nStyle Guide  \nDevelopers should follow the terraform style guide.  \nProjects should check Terraform scripts with automated tools.  \nCode Analysis / Linting  \nTFLint  \nTFLint is a Terraform linter focused on possible errors, best practices, etc. Once TFLint installed in the environment, it can be invoked using the VS Code terraform extension.  \nVS Code Extensions  \nThe following VS Code extensions are widely used.  \nTerraform extension  \nThis extension provides syntax highlighting, linting, formatting and validation capabilities.  \nAzure Terraform extension  \nThis extension provides Terraform command support, resource graph visualization and CloudShell integration inside VS Code.  \nBuild Validation  \nEnsure you enforce the style guides during build. The following example script can be used to install terraform, and a linter that\nthen checks for formatting and common errors.  \n{% raw %}  \n```shell\n\n! /bin/bash\n\nset -e\n\nSCRIPT_DIR=$(dirname \"$BASH_SOURCE\")\ncd \"$SCRIPT_DIR\"\n\nTF_VERSION=0.12.4\nTF_LINT_VERSION=0.9.1\n\necho -e \"\\n\\n>>> Installing Terraform 0.12\"\n\nInstall terraform tooling for linting terraform\n\nwget -q https://releases.hashicorp.com/terraform/${TF_VERSION}/terraform_${TF_VERSION}_linux_amd64.zip -O /tmp/terraform.zip\nsudo unzip -q -o -d /usr/local/bin/ /tmp/terraform.zip\n\necho \"\"\necho -e \"\\n\\n>>> Install tflint (3rd party)\"\nwget -q https://github.com/wata727/tflint/releases/download/v${TF_LINT_VERSION}/tflint_linux_amd64.zip -O /tmp/tflint.zip\nsudo unzip -q -o -d /usr/local/bin/ /tmp/tflint.zip\n\necho -e \"\\n\\n>>> Terraform version\"\nterraform -version\n\necho -e \"\\n\\n>>> Terraform Format (if this fails use 'terraform fmt -recursive' command to resolve\"\nterraform fmt -recursive -diff -check\n\necho -e \"\\n\\n>>> tflint\"\ntflint\n\necho -e \"\\n\\n>>> Terraform init\"\nterraform init\n\necho -e \"\\n\\n>>> Terraform validate\"\nterraform validate\n```  \n{% endraw %}  \nCode Review Checklist  \nIn addition to the Code Review Checklist you should also look for these Terraform specific code review items  \nProviders  \n[ ] Are all providers used in the terraform scripts versioned to prevent breaking changes in the future?  \nRepository Organization  \n[ ] The code split into reusable modules?  \n[ ] Modules are split into separate .tf files where appropriate?  \n[ ] The repository contains a README.md describing the architecture provisioned?  \n[ ] If Terraform code is mixed with application source code, the Terraform code isolated into a dedicated folder?  \nTerraform state  \n[ ] The Terraform project configured using Azure Storage as remote state backend?  \n[ ] The remote state backend storage account key stored a secure location (e.g. Azure Key Vault)?  \n[ ] The project is configured to use state files based on the environment, and the deployment pipeline is configured to supply the state file name dynamically?  \nVariables  \n[ ] If the infrastructure will be different depending on the environment (e.g. Dev, UAT, Production), the environment specific parameters are supplied via a .tfvars file?  \n[ ] All variables have type information. E.g. a list(string) or string.  \n[ ] All variables have a description stating the purpose of the variable and its usage.  \n[ ] default values are not supplied for variables which must be supplied by a user.  \nTesting  \n[ ] Unit and integration tests covering the Terraform code exist (e.g. Terratest, terratest-abstraction)?  \nNaming and code structure  \n[ ] Resource definitions and data sources are used correctly in the Terraform scripts?  \nresource: Indicates to Terraform that the current configuration is in charge of managing the life cycle of the object  \ndata: Indicates to Terraform that you only want to get a reference to the existing object, but don\u2019t want to manage it as part of this configuration  \n[ ] The resource names start with their containing provider's name followed by an underscore? e.g. resource from the provider postgresql might be named as postgresql_database?  \n[ ] The try function is only used with simple attribute references and type conversion functions? Overuse of the try function to suppress errors will lead to a configuration that is hard to understand and maintain.  \n[ ] Explicit type conversion functions used to normalize types are only returned in module outputs? Explicit type conversions are rarely necessary in Terraform because it will convert types automatically where required.  \n[ ] The Sensitive property on schema set to true for the fields that contains sensitive information? This will prevent the field's values from showing up in CLI output.  \nGeneral recommendations  \nTry avoiding nesting sub configuration within resources. Create a separate resource section for resources even though they can be declared as sub-element of a resource. For example, declaring subnets within virtual network vs declaring subnets as a separate resources compared to virtual network on Azure.  \nNever hard-code any value in configuration. Declare them in locals section if a variable is needed multiple times as a static value and are internal to the configuration.  \nThe names of the resources created on Azure should not be hard-coded or static. These names should be dynamic and user-provided using variable block. This is helpful especially in unit testing when multiple tests are running in parallel trying to create resources on Azure but need different names (few resources in Azure need to be named uniquely e.g. storage accounts).  \nIt is a good practice to output the ID of resources created on Azure from configuration. This is especially helpful when adding dynamic blocks for sub-elements/child elements to the parent resource.  \nUse the required_providers block for establishing the dependency for providers along with pre-determined version.  \nUse the terraform block to declare the provider dependency with exact version and also the terraform CLI version needed for the configuration.  \nValidate the variable values supplied based on usage and type of variable. The validation can be done to variables by adding validation block.  \nValidate that the component SKUs are the right ones, e.g. standard vs premium.",
        "source": "..\\data\\docs\\code-with-engineering\\code-reviews\\recipes\\terraform.md"
    },
    {
        "chunkId": "chunk81_0",
        "chunkContent": "Continuous Delivery  \nThe inspiration behind continuous delivery is constantly delivering valuable software to users and developers more frequently. Applying the principles and practices laid out in this readme will help you reduce risk, eliminate manual operations and increase quality and confidence.  \nDeploying software involves the following principles:  \nProvision and manage the cloud environment runtime for your application (cloud resources, infrastructure, hardware, services, etc).  \nInstall the target application version across your cloud environments.  \nConfigure your application, including any required data.  \nA continuous delivery pipeline is an automated manifestation of your process to streamline these very principles in a consistent and repeatable manner.  \nGoal  \nFollow industry best practices for delivering software changes to customers and developers.  \nEstablish consistency for the guiding principles and best practices when assembling continuous delivery workflows.  \nGeneral Guidance  \nDefine a Release Strategy  \nIt's important to establish a common understanding between the Dev Lead and application stakeholder(s) around the release strategy / design  during the planning phase of a project. This common understanding includes the deployment and maintenance of the application throughout its SDLC.  \nRelease Strategy Principles  \nContinuous Delivery by Jez Humble, David Farley cover the key considerations to follow when creating a release strategy:  \nParties in charge of deployments to each environment, as well as in charge of the release.  \nAn asset and configuration management strategy.  \nAn enumeration of the environments available for acceptance, capacity, integration, and user acceptance testing, and the process by which builds will be moved through these environments.  \nA description of the processes to be followed for deployment into testing and production environments, such as change requests to be opened and approvals that need to be granted.  \nA discussion of the method by which the application\u2019s deploy-time and runtime configuration will be managed, and how this relates to the automated deployment process.  \n_Description of the integration with any external systems. At what stage and how are they tested as part of a release? How does the technical operator communicate with the provider in the event of a problem?  \n_A disaster recovery plan so that the application\u2019s state can be recovered following a disaster. Which steps will need to be in place to restart or redeploy the application should it fail.  \n_Production sizing and capacity planning: How much data will your live application create? How many log files or databases will you need? How much bandwidth and disk space will you need? What latency are clients expecting?  \nHow the initial deployment to production works.  \nHow fixing defects and applying patches to the production environment will be handled.  \nHow upgrades to the production environment will be handled, including data migration. How will upgrades be carried out to the application without destroying its state.  \nApplication Release and Environment Promotion  \nYour release manifestation process should take the deployable build artifact created from your commit stage and deploy them across all cloud environments, starting with your test environment.  \nThe test environment (often called Integration) acts as a gate to validate if your test suite completes successfully for all release candidates. This validation should always begin in a test environment while inspecting the deployed release integrated from the feature / release branch containing your code changes.  \nCode changes released into the test environment typically targets the main branch (when doing trunk) or release branch (when doing gitflow).  \nThe First Deployment  \nThe very first deployment of any application should be showcased to the customer in a production-like environment (UAT) to solicit feedback early. The UAT environment is used to obtain product owner sign-off acceptance to ultimately promote the release to production.  \nCriteria for a production-like environment  \nRuns the same operating system as production.  \nHas the same software installed as production.  \nIs sized and configured the same way as production.  \nMirrors production's networking topology.  \nSimulated production-like load tests are executed following a release to surface any latency or throughput degradation.  \nModeling your Release Pipeline  \nIt's critical to model your test and release process to establish a common understanding between the application engineers and customer stakeholders. Specifically aligning expectations for how many cloud environments need to be pre-provisioned as well as defining sign-off gate roles and responsibilities.  \nRelease Pipeline Modeling Considerations  \nDepict all stages an application change would have to go through before it is released to production.  \nDefine all release gate controls.  \nDetermine customer-specific Cloud RBAC groups which have the authority to approve release candidates per environment.  \nRelease Pipeline Stages  \nThe stages within your release workflow are ultimately testing a version of your application to validate it can be released in accordance to your acceptance criteria. The release pipeline should account for the following conditions:  \nRelease Selection: The developer carrying out application testing should have the capability to select which release version to deploy to the testing environment.  \nDeployment - Release the application deployable build artifact (created from the CI stage) to the target cloud environment.  \nConfiguration - Applications should be configured consistently across all your environments. This configuration is applied at the time of deployment.  Sensitive data like app secrets and certificates should be mastered in a fully managed PaaS key and secret store (eg Key Vault, KMS). Any secrets used by the application should be sourced internally within the application itself. Application Secrets should not be exposed within the runtime environment. We encourage 12 Factor principles, especially when it comes to configuration management.  \nData Migration - Pre populate application state and/or data records which is needed for your runtime environment. This may also include test data required for your end-to-end integration test suite.  \nDeployment smoke test. Your smoke test should also verify that your application is pointing to the correct configuration (e.g. production pointing to a UAT Database).  \nPerform any manual or automated acceptance test scenarios.  \nApprove the release gate to promote the application version to the target cloud environment. This promotion should also include the environment's configuration state (e.g. new env settings, feature flags, etc).  \nLive Release Warm Up  \nA release should be running for a period of time before it's considered live and allowed to accept user traffic. These warm up activities may include application server(s) and database(s) pre-fill any dependent cache(s) as well as establish all service connections (eg connection pool allocations, etc).  \nPre-production releases  \nApplication release candidates should be deployed to a staging environment similar to production for carrying out final manual/automated tests (including capacity testing). Your production and staging / pre-prod cloud environments should be setup at the beginning of your project.  \nApplication warm up should be a quantified measurement that's validated as part of your pre-prod smoke tests.  \nRolling-Back Releases  \nYour release strategy should account for rollback scenarios in the event of unexpected failures following a deployment.  \nRolling back releases can get tricky, especially when database record/object changes occur in result of your deployment (either inadvertently or intentionally). If there are no data changes which need to be backed out, then you can simply trigger a new release candidate for the last known production version and promote that release along your CD pipeline.  \nFor rollback scenarios involving data changes, there are several approaches to mitigating this which fall outside the scope of this guide. Some involve database record versioning, time machining database records / objects, etc. All data files and databases should be backed up prior to each release so they could be restored. The mitigation strategy for this scenario will vary across our projects. The expectation is that this mitigation strategy should be covered as part of your release strategy.  \nAnother approach to consider when designing your release strategy is deployment rings. This approach simplifies rollback scenarios while limiting the impact of your release to end-users by gradually deploying and validating your changes in production.  \nZero Downtime Releases  \nA hot deployment follows a process of switching users from one release to another with no impact to the user experience. As an example, Azure managed app services allows developers to validate app changes in a staging deployment slot before swapping it with the production slot. App Service slot swapping can also be fully automated once the source slot is fully warmed up (and auto swap is enabled). Slot swapping also simplifies release rollbacks once a technical operator restores the slots to their pre-swap states.  \nKubernetes natively supports rolling updates.  \nBlue-Green Deployments  \nBlue / Green is a deployment technique which reduces downtime by running two identical instances of a production environment called Blue and Green.  \nOnly one of these environments accepts live production traffic at a given time.  \nIn the above example, live production traffic is routed to the Green environment. During application releases, the new version is deployed to the blue environment which occurs independently from the Green environment. Live traffic is unaffected from Blue environment releases. You can point your end-to-end test suite against the Blue environment as one of your test checkouts.  \nMigrating users to the new application version is as simple as changing the router configuration to direct all traffic to the Blue environment.  \nThis technique simplifies rollback scenarios as we can simply switch the router back to Green.  \nDatabase providers like Cosmos and Azure SQL natively support data replication to help enable fully synchronized Blue Green database environments.  \nCanary Releasing  \nCanary releasing enables development teams to gather faster feedback when deploying new features to production. These releases are rolled out to a subset of production nodes (where no users are routed to) to collect early insights around capacity testing and functional completeness and impact.  \nOnce smoke and capacity tests are completed, you can route a small subset of users to the production nodes hosting the release candidate.  \nCanary releases simplify rollbacks as you can avoid routing users to bad application versions.  \nTry to limit the number of versions of your application running parallel in production, as it can complicate maintenance and monitoring controls.  \nLow code solutions  \nLow code solutions have increased their participation in the applications and processes and because of that it is required that a proper conjunction of disciplines improve their development.  \nHere is a guide for continuous deployment for Low Code Solutions.  \nReferences  \nContinuous Delivery by Jez Humble, David Farley.  \nContinuous integration vs. continuous delivery vs. continuous deployment  \nDeployment Rings  \nTools  \nCheck out the below tools to help with some CD best practices listed above:  \nFlux for gitops  \nCI/CD workflow using GitOps  \nTekton for Kubernetes native pipelines  \nNote Jenkins-X uses Tekton under the hood.  \nArgo Workflows  \nFlagger for powerful, Kubernetes native releases including blue/green, canary, and A/B testing.  \nNot quite CD related, but checkout jsonnet, a templating language to reduce boilerplate and increase sharing between your yaml/json manifests.",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-delivery\\README.md"
    },
    {
        "chunkId": "chunk82_0",
        "chunkContent": "Runtime Variables in GitHub Actions  \nObjective  \nWhile GitHub Actions is a popular choice for writing and running CI/CD pipelines, especially for open source projects hosted on GitHub, it lacks specific quality of life features found in other CI/CD environments. One key feature that GitHub Actions has not yet implemented is the ability to mock and inject runtime variables into a workflow, in order to test the pipeline itself.  \nThis provides a bridge between a pre-existing feature in Azure DevOps, and one that has not yet released inside GitHub Actions.  \nTarget Audience  \nThis guide assumes that you are familiar with CI/CD, and understand the security implications of CI/CD pipelines. We also assume basic knowledge with GitHub Actions, including how to write and run a basic CI/CD pipeline, checkout repositories inside the action, use Marketplace Actions with version control, etc.  \nWe assume that you, as a CI/CD engineer, want to inject environment variables or environment flags into your pipelines and workflows in order to test them, and are using GitHub Actions to accomplish this.  \nUsage Scenario  \nMany integration or end-to-end workflows require specific environment variables that are only available at runtime. For example, a workflow might be doing the following:  \nIn this situation, testing the pipeline is extremely difficult without having to make external calls to the resource. In many cases, making external calls to the resource can be expensive or time-consuming, significantly slowing down inner loop development.  \nAzure DevOps, as an example, offers a way to define pipeline variables on a manual trigger:  \nGitHub Actions does not do so yet.  \nSolution  \nTo workaround this, the easiest solution is to add runtime variables to either commit messages or the PR Body, and grep for the variable. GitHub Actions provides grep functionality natively using a contains function, which is what we shall be specifically using.  \nIn scope:  \nWe will scope this to injecting a single environment variable into a pipeline, with a previously known key and value.  \nOut of Scope:  \nWhile the solution is obviously extensible using shell scripting or any other means of creating variables, this solution serves well as the proof of the basic concept. No such scripting is provided in this guide.  \nAdditionally, teams may wish to formalize this process using a PR Template that has an additional section for the variables being provided. This is not however included in this guide.  \nSecurity Warning:  \nThis is NOT for injecting secrets as the commit messages and PR body can be retrieved by a third party, are stored in  \ngit log, and can otherwise be read by a malicious individual using a variety of tools. Rather, this is for testing a workflow that needs simple variables to be injected into it, as above.  \nIf you need to retrieve secrets or sensitive information, use the  \nGitHub Action for Azure Key Vault or some other similar secret storage and retrieval service.  \nCommit Message Variables  \nHow to inject a single variable into the environment for use, with a specified key and value. In this example, the key is COMMIT_VAR and the value is [commit var].  \nPre-requisites:  \nPipeline triggers are correctly set up to trigger on pushed commits (Here we will use actions-test-branch as the branch of choice)  \nCode Snippet:  \n{% raw %}  \n```yaml\non:\npush:\nbranches:\n- actions-test-branch\n\njobs:\nEcho-On-Commit:\nruns-on: ubuntu-latest\nsteps:\n- name: \"Checkout Repository\"\nuses: actions/checkout@v2\n\n```  \n```\n\n{% endraw %}\n\nAvailable as a .YAML here.\n\nCode Explanation:\n\nThe first part of the code is setting up Push triggers on the working branch and checking out the repository, so we will not explore that in detail.\n\n{% raw %}\n\nyaml\n- name: \"Set flag from Commit\"\nenv:\nCOMMIT_VAR: ${{ contains(github.event.head_commit.message, '[commit var]') }}\n\n{% endraw %}\n\nThis is a named step inside the only Job in our GitHub Actions pipeline. Here, we set an environment variable for the step: Any code or action that the step calls will now have the environment variable available.\n\n{% raw %}\n\nyaml\nrun: |\nif ${COMMIT_VAR} == true; then\necho \"flag=true\" >> $GITHUB_ENV\necho \"flag set to true\"\nelse\necho \"flag=false\" >> $GITHUB_ENV\necho \"flag set to false\"\nfi\n\n{% endraw %}\n\nThe run command here checks to see if the COMMIT_VAR variable has been set to true, and if it has, it sets a secondary flag to true, and echoes this behavior. It does the same if the variable is false.\n\nThe specific reason to do this is to allow for the flag variable to be used in further steps instead of having to reuse the COMMIT_VAR in every step. Further, it allows for the flag to be used in the if step of an action, as in the next part of the snippet.\n\n{% raw %}\n\nyaml\n- name: \"Use flag if true\"\nif: env.flag\nrun: echo \"Flag is available and true\"\n\n{% endraw %}\n\nIn this part of the snippet, the next step in the same job is now using the flag that was set in the previous step. This allows the user to:\n\nReuse the flag instead of repeatedly accessing the GitHub Context\n\nSet the flag using multiple conditions, instead of just one. For example, a different step might ALSO set the flag to true or false for different reasons.\n\nChange the variable in exactly one place instead of having to change it in multiple places\n\nShorter Alternative:\n\nThe \"Set flag from commit\" step can be simplified to the following in order to make the code much shorter, although not necessarily more readable:\n\n{% raw %}\n\nyaml\n- name: \"Set flag from Commit\"\nenv:\nCOMMIT_VAR: ${{ contains(github.event.head_commit.message, '[commit var]') }}\nrun: |\necho \"flag=${COMMIT_VAR}\" >> $GITHUB_ENV\necho \"set flag to ${COMMIT_VAR}\"\n\n{% endraw %}\n\nUsage:\n\nIncluding the Variable\n\nPush to branch master:\n\n{% raw %}\n\n```cmd  \ngit add.\ngit commit -m \"Running GitHub Actions Test [commit var]\"\ngit push\n```\n\n{% endraw %}\n\nThis triggers the workflow (as will any push). As the [commit var] is in the commit message, the ${COMMIT_VAR} variable in the workflow will be set to true and result in the following:\n\nNot Including the Variable\n\nPush to branch master:\n\n{% raw %}\n\n```cmd  \ngit add.\ngit commit -m \"Running GitHub Actions Test\"\ngit push\n```\n\n{% endraw %}\n\nThis triggers the workflow (as will any push). As the [commit var] is not in the commit message, the ${COMMIT_VAR} variable in the workflow will be set to false and result in the following:\n\nPR Body Variables\n\nWhen a PR is made, the PR Body can also be used to set up variables. These variables can be made available to all the workflow runs that stem from that PR, which can help ensure that commit messages are more informative and less cluttered, and reduces the work on the developer.\n\nOnce again, this for an expected key and value. In this case, the key is PR_VAR and the value is [pr var].\n\nPre-requisites:\n\nPipeline triggers are correctly set up to trigger on a pull request into a specific branch. (Here we will use master as the destination branch.)\n\nCode Snippet:\n\n{% raw %}\n\n```yaml\non:\npull_request:\nbranches:\n- master  \njobs:\nEcho-On-PR:\nruns-on: ubuntu-latest\nsteps:\n- name: \"Checkout Repository\"\nuses: actions/checkout@v2  \n```\n\n```  \n{% endraw %}  \nAvailable as a .YAML here.  \nCode Explanation:  \nThe first part of the YAML file simply sets up the Pull Request Trigger. The majority of the following code is identical, so we will only explain the differences.  \n{% raw %}  \nyaml\n- name: \"Set flag from PR\"\nenv:\nPR_VAR: ${{ contains(github.event.pull_request.body, '[pr var]') }}  \n{% endraw %}  \nIn this section, the PR_VAR environment variable is set to true or false depending on whether the [pr var] string is in the PR Body.  \nShorter Alternative:  \nSimilarly to the above, the YAML step can be simplified to the following in order to make the code much shorter, although not necessarily more readable:  \n{% raw %}  \nyaml\n- name: \"Set flag from PR\"\nenv:\nPR_VAR: ${{ contains(github.event.pull_request.body, '[pr var]') }}\nrun: |\necho \"flag=${PR_VAR}\" >> $GITHUB_ENV\necho \"set flag to ${PR_VAR}\"  \n{% endraw %}  \nUsage:  \nCreate a Pull Request into master, and include the expected variable in the body somewhere:  \nThe GitHub Action will trigger automatically, and since [pr var] is present in the PR Body, it will set the flag to true, as shown below:  \nReal World Scenarios  \nThere are many real world scenarios where controlling environment variables can be extremely useful. Some are outlined below:  \nAvoiding Expensive External Calls  \nDeveloper A is in the process of writing and testing an integration pipeline. The integration pipeline needs to make a call to an external service such as Azure Data Factory or Databricks, wait for a result, and then echo that result. The workflow could look like this:  \nThe workflow inherently takes time and is expensive to run, as it involves maintaining a Databricks cluster while also waiting for the response. This external dependency can be removed by essentially mocking the response for the duration of writing and testing other parts of the workflow, and mocking the response in situations where the actual response either does not matter, or is not being directly tested.  \nSkipping Long CI processes  \nDeveloper B is in the process of writing and testing a CI/CD pipeline. The pipeline has multiple CI stages, each of which runs sequentially. The workflow might look like this:  \nIn this case, each CI stage needs to run before the next one starts, and errors in the middle of the process can cause the entire pipeline to fail. While this might be intended behavior for the pipeline in some situations (Perhaps you don't want to run a more involved, longer build or run a time-consuming test coverage suite if the CI process is failing), it means that steps need to be commented out or deleted when testing the pipeline itself.  \nInstead, an additional step could check for a [skip ci $N] tag in either the commit messages or PR Body, and skip a specific stage of the CI build. This ensures that the final pipeline does not have changes committed to it that render it broken, as sometimes happens when commenting out/deleting steps. It additionally allows for a mechanism to repeatedly test individual steps by skipping the others, making developing the pipeline significantly easier.",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-delivery\\devops-provider-recipes\\github-actions\\runtime-variables\\README.md"
    },
    {
        "chunkId": "chunk83_0",
        "chunkContent": "Deploying with GitOps  \nWhat is GitOps?  \nGitOps is a way of managing your infrastructure and applications so that the whole system is described declaratively and version controlled (most likely in a Git repository), and having an automated process that ensures that the deployed environment matches the state specified in a repository.\"- WeaveWorks  \nWhy should I use GitOps?  \nGitOps simply allows faster deployments by having git repositories in the center offering a clear audit trail via git commits and no direct environment access. Read more on Why should I use GitOps?  \nThe below diagram compares traditional CI/CD vs GitOps workflow:  \nTools for GitOps  \nSome popular GitOps frameworks for Kubernetes backed by CNCF community:  \nFlux V2  \nArgo CD  \nRancher Fleet  \nDeploying using GitOps  \nGitOps with Flux v2 can be enabled in Azure Kubernetes Service (AKS) managed clusters or Azure Arc-enabled Kubernetes connected clusters as a cluster extension. After the microsoft.flux cluster extension is installed, you can create one or more fluxConfigurations resources that sync your Git repository sources to the cluster and reconcile the cluster to the desired state. With GitOps, you can use your Git repository as the source of truth for cluster configuration and application deployment.  \nTutorial: Deploy configurations using GitOps on an Azure Arc-enabled Kubernetes cluster  \nTutorial: Implement CI/CD with GitOps  \nMulti-cluster and multi-tenant environment with Flux v2",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-delivery\\gitops\\deploying\\README.md"
    },
    {
        "chunkId": "chunk84_0",
        "chunkContent": "Azure DevOps: Managing Settings on a Per-Branch Basis  \nWhen using Azure DevOps Pipelines for CI/CD, it's convenient to leverage the built-in pipeline variables for secrets management, but using pipeline variables for secrets management has its disadvantages:  \nPipeline variables are managed outside the code that references them. This makes it easy to introduce drift between the source code and the secrets, e.g. adding a reference to a new secret in code but forgetting to add it to the pipeline variables (leads to confusing build breaks), or deleting a reference to a secret in code and forgetting to remote it from the pipeline variables (leads to confusing pipeline variables).  \nPipeline variables are global shared state. This can lead to confusing situations and hard to debug problems when developers make concurrent changes to the pipeline variables which may override each other. Having a single global set of pipeline variables also makes it impossible for secrets to vary per environment (e.g. when using a branch-based deployment model where 'master' deploys using the production secrets, 'development' deploys using the staging secrets, and so forth).  \nA solution to these limitations is to manage secrets in the Git repository jointly with the project's source code. As described in secrets management, don't check secrets into the repository in plain text. Instead we can add an encrypted version of our secrets to the repository and enable our CI/CD agents and developers to decrypt the secrets for local usage with some pre-shared key. This gives us the best of both worlds: a secure storage for secrets as well as side-by-side management of secrets and code.  \n{% raw %}  \n```sh\n\nfirst, make sure that we never commit our plain text secrets and generate a strong encryption key\n\necho \".env\" >> .gitignore\nENCRYPTION_KEY=\"$(LC_ALL=C < /dev/urandom tr -dc '_A-Z-a-z-0-9' | head -c128)\"\n\nnow let's add some secret to our .env file\n\necho \"MY_SECRET=...\" >> .env\n\nalso update our secrets documentation file\n\ncat >> .env.template <<< \"\n\nenter description of your secret here\n\nMY_SECRET=\n\"\n\nnext, encrypt the plain text secrets; the resulting .env.enc file can safely be committed to the repository\n\necho \"${ENCRYPTION_KEY}\" | openssl enc -aes-256-cbc -md sha512 -pass stdin -in .env -out .env.enc\ngit add .env.enc .env.template\ngit commit -m \"Update secrets\"\n```  \n{% endraw %}  \nWhen running the CI/CD, the build server can now access the secrets by decrypting them. E.g. for Azure DevOps, configure ENCRYPTION_KEY as a secret pipeline variable and then add the following step to azure-pipelines.yml:  \n{% raw %}  \nyaml\nsteps:\n- script: echo \"$(ENCRYPTION_KEY)\" | openssl enc -aes-256-cbc -md sha512 -pass stdin -in .env.enc -out .env -d\ndisplayName: Decrypt secrets  \n{% endraw %}  \nYou can also use variable groups linked directly to Azure key vault for your pipelines to manage all secrets in one location.",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-delivery\\gitops\\secret-management\\azure-devops-secret-management-per-branch.md"
    },
    {
        "chunkId": "chunk85_0",
        "chunkContent": "Secret management with GitOps  \nGitOps projects have git repositories in the center that are considered a source of truth for managing both infrastructure and application. This infrastructure and application will require secured access to other resources of the system through secrets.\nCommitting clear-text secrets into git repositories is unacceptable even if the repositories are private to your team and organization. Teams need a secure way to handle secrets when using GitOps.  \nThere are many ways to manage secrets with GitOps and at high level can be categorized into:  \nEncrypted secrets in git repositories  \nReference to secrets stored in the external key vault  \nTLDR: Referencing secrets in an external key vault is the recommended approach. It is easier to orchestrate secret rotation and more scalable with multiple clusters and/or teams.  \nEncrypted secrets in git repositories  \nIn this approach, Developers manually encrypt secrets using a public key, and the key can only be decrypted by the custom Kubernetes controller running in the target cluster. Some popular tools for his approach are Bitnami Sealed Secrets, Mozilla SOPS  \nAll the secret encryption tools share the following:  \nSecret changes are managed by making changes within the GitOps repository which provides great traceability  \nAll secrets can be rotated by making changes in GitOps, without accessing the cluster  \nThey support fully disconnected gitops scenarios  \nSecrets are stored encrypted in the gitops repository, if the private encryption key is leaked and the attacker has access to the repo, all secrets can be decrypted  \nBitnami Sealed Secrets  \nSealed Secrets use asymmetric encryption to encrypt secrets. A Kubernetes controller generates a key-pair (private-public) and stores the private key in the cluster's etcd database as a Kubernetes secret. Developers use Kubeseal CLI to seal secrets before committing to the git repo.  \nSome of the key points of using Sealed Secrets are:  \nSupport automatic key rotation for the private key and can be used to enforce re-encryption of secrets  \nDue to automatic renewal of the sealing key, the key needs to be prefetched from the cluster or cluster set up to store the sealing key on renewal in a secondary location  \nMulti-tenancy support at the namespace level can be enforced by the controller  \nWhen sealing secrets developers need a connection to the cluster control plane to fetch the public key or the public key has to be explicitly shared with the developer  \nIf the private key in the cluster is lost for some reason all secrets need to be re-encrypted followed by a new key-pair generation  \nDoes not scale with multi-cluster, because every cluster will require a controller having its own key pair  \nCan only encrypt secret resource type  \nThe\u00a0Flux\u00a0documentation has\u00a0inconsistences\u00a0in\u00a0the\u00a0Azure\u00a0Key Vault\u00a0examples  \nMozilla SOPS  \nSOPS: Secrets OPerationS is an encryption tool that supports YAML, JSON, ENV, INI, and BINARY formats and encrypts with AWS KMS, GCP KMS, Azure Key Vault, age, and PGP and is not just limited to Kubernetes. It supports integration with some common key management systems including Azure Key Vault, where one or more key management system is used to store the encryption key for encrypting secrets and not the actual secrets.  \nSome of the key points of using SOPS are:  \nFlux has native support for SOPS with cluster-side decryption  \nProvides an added layer of security as the private key used for decryption is protected in an external key vault  \nTo use the Helm CLI for encryption the (Helm Secrets) plugin is needed  \nNeeds (KSOPS)(kustomize-sopssecretgenerator) plugin to work with Kustomization  \nDoes not scale with larger teams as each developer has to encrypt the secrets  \nThe public key is sufficient for creating brand new files. The secret key is required for decrypting and editing existing files because SOPS computes a MAC on all values.\u00a0When using the public key solely to add or remove a field, the whole file should be deleted and recreated  \nSupports\u00a0several\u00a0types\u00a0of\u00a0keys\u00a0that\u00a0can\u00a0be\u00a0used\u00a0in\u00a0both\u00a0connected\u00a0and\u00a0disconnected\u00a0state.\u00a0A\u00a0secret\u00a0can\u00a0have\u00a0a\u00a0list\u00a0of\u00a0keys\u00a0and\u00a0will\u00a0try\u00a0do\u00a0decrypt\u00a0with\u00a0all\u00a0of\u00a0them.  \nReference to secrets stored in an external key vault (recommended)  \nThis approach relies on a key management system like Azure Key Vault to hold the secrets and the git manifest in the repositories has reference to the key vault secrets. Developers do not perform any cryptographic operations with files in repositories. Kubernetes operators running in the target cluster are responsible for pulling the secrets from the key vault and making them available either as Kubernetes secrets or secrets volume mounted to the pod.  \nAll the below tools share the following:  \nSecrets are not stored in the repository  \nSupports Prometheus metrics for observability  \nSupports sync with Kubernetes Secrets  \nSupports Linux and Windows containers  \nProvides enterprise-grade external secret management  \nEasily scalable with multi-cluster and larger teams  \nBoth solutions support either Azure Active Directory (Azure AD) service principal or managed identity for authentication with the Key Vault.  \nFor secret rotation ideas, see Secrets Rotation on Environment Variables and Mounted Secrets  \nFor how to authenticate private container registries with a service principal see: Authenticated Private Container Registry  \nAzure Key Vault Provider for Secrets Store CSI Driver  \nAzure Key Vault Provider (AKVP) for Kubernetes secret store CSI Driver allows you to get secret contents stored in an Azure Key Vault instance and use the Secrets Store CSI driver interface to mount them into Kubernetes pods. Mounts secrets/keys/certs to pod using a CSI Inline volume.  \nAzure Key Vault Provider for Secrets Store CSI Driver install guide.  \nCSI driver will need access to Azure Key Vault either through a service principal or managed identity (recommended). To make this access secure you can leverage Azure AD Workload Identity(recommended) or AAD Pod Identity. Please note AAD pod identity will soon be replaced by workload identity.  \nProduct Group Links provided for AKVP with SSCSID:  \nDifferences between ESO / SSCSID (GitHub Issue)  \nSecrets Management on K8S talk here (Native Secrets, Vault.io, and ESO vs. SSCSID)  \nAdvantages:  \nSupports pod portability with the SecretProviderClass CRD  \nSupports auto rotation of secrets with customizable sync intervals per cluster.  \nSeems to be the MSFT choice (Secrets Store CSI driver is heavily contributed by MSFT and Kubernetes-SIG)  \nDisadvantages:  \nMissing disconnected scenario support: When the node is offline the SSCSID fails to fetch the secret and thus mounting the volume fails, making scaling and restarting pods not possible while being offline  \nAKVP can only access Key Vault from a non-Azure environment using a service principal  \nThe Kubernetes Secret containing the service principal credentials need to be created as a secret in the same namespace as the application pod. If pods in multiple namespaces need to use the same SP to access Key Vault, this Kubernetes Secret needs to be created in each namespace.  \nThe GitOps repo must contain the name of the Key Vault within the SecretProviderClass  \nMust mount secrets as volumes to allow syncing into Kubernetes Secrets  \nUses more resources (4 pods; CSI Storage driver and provider) and is a daemonset - not test on RPS / resource usage  \nExternal Secrets Operator with Azure Key Vault  \nThe External Secrets Operator (ESO) is an open-sourced Kubernetes operator that can read secrets from external secret stores (e.g., Azure Key Vault) and sync those into Kubernetes Secrets. In contrast to the CSI Driver, the ESO controller creates the secrets on the cluster as K8s secrets, instead of mounting them as volumes to pods.  \nDocs on using ESO Azure Key vault provider here.  \nESO will need access to Azure Key Vault either through the use of a service principal or managed identity (via Azure AD Workload Identity(recommended) or AAD Pod Identity).  \nAdvantages:  \nSupports auto rotation of secrets with customizable sync intervals per secret.  \nComponents are split into different CRDs for namespace (ExternalSecret, SecretStore) and cluster-wide (ClusterSecretStore, ClusterExternalSecret) making syncing more manageable i.r.t. different deployments/pods etc.  \nService Principal secret for the (Cluster)SecretStores could placed in a namespaced that only the ESO can access (see Shared ClusterSecretStore).  \nResource efficient (single pod) - not test on RPS / resource usage.  \nOpen source and high contributions, (GitHub)  \nMounting Secrets as volumes is supported via K8S's APIs (see here)  \nPartial disconnected scenario support: As ESO is using native K8s secrets the cluster can be offline, and it does not have any implications towards restarting and scaling pods while being offline  \nDisadvantages:  \nThe GitOps repo must contain the name of the Key Vault within the SecretStore / ClusterSecretStore or a ConfigMap linking to it  \nMust create secrets as K8s secrets  \nImportant Links  \nSealed Secrets with Flux v2  \nMozilla SOPS with Flux v2  \nSecret Management with Argo CD  \nSecret management Workflow  \nAppendix  \nAuthenticated Private Container Registry  \nAn option on how to authenticate private container registries (e.g., ACR):  \nUse a dockerconfigjson Kubernetes Secret on Pod-Level with ImagePullSecret (This can be also defined on namespace-level)",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-delivery\\gitops\\secret-management\\README.md"
    },
    {
        "chunkId": "chunk86_0",
        "chunkContent": "Secrets rotation of environment variables and mounted secrets in pods  \nThis document covers some ways you can do secret rotation with environment variables and mounted secrets in Kubernetes pods  \nMapping Secrets via secretKeyRef with environment variables  \nIf we map a K8s native secret via a secretKeyRef into an environment variable and we rotate keys the environment variable is not updated even though the K8s native secret has been updated. We need to restart the Pod so changes get populated. Reloader solves this issue with a K8S controller.  \n{% raw %}  \nyaml\n...\nenv:\n- name: EVENTHUB_CONNECTION_STRING\nvalueFrom:\nsecretKeyRef:\nname: poc-creds\nkey: EventhubConnectionString\n...  \n{% endraw %}  \nMapping Secrets via volumeMounts (ESO way)  \nIf we map a K8s native secret via a volume mount and we rotate keys the file gets updated. The application needs to then be able pick up the changes without a restart (requiring most likely custom logic in the application to support this). Then no restart of the application is required.  \n{% raw %}  \nyaml\n...\nvolumeMounts:\n- name: mounted-secret\nmountPath: /mnt/secrets-store\nreadOnly: true\nvolumes:\n- name: mounted-secret\nsecret:\nsecretName: poc-creds\n...  \n{% endraw %}  \nMapping Secrets via volumeMounts (AKVP SSCSID way)  \nSSCSID focuses on mounting external secrets into the CSI. Thus if we rotate keys the file gets updated. The application needs to then be able pick up the changes without a restart (requiring most likely custom logic in the application to support this). Then no restart of the application is required.  \n{% raw %}  \nyaml\n...\nvolumeMounts:\n- name: app-secrets-store-inline\nmountPath: \"/mnt/app-secrets-store\"\nreadOnly: true\nvolumes:\n- name: app-secrets-store-inline\ncsi:\ndriver: secrets-store.csi.k8s.io\nreadOnly: true\nvolumeAttributes:\nsecretProviderClass: akvp-app\nnodePublishSecretRef:\nname: secrets-store-sp-creds\n...  \n{% endraw %}",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-delivery\\gitops\\secret-management\\secret-rotation-in-pods.md"
    },
    {
        "chunkId": "chunk87_0",
        "chunkContent": "Continuous delivery on low-code and no-code solutions  \nLow-code and no-code platforms have taken a spot in a wide variety of Business Solutions involving process automation, AI models, Bots, Business Applications and Business Intelligence. The scenarios enabled by these platforms are constantly evolving and opening a spot for productive roles. This has been exactly the reason why bringing more professional tools to their development have become necessary such as controlled and automated delivery.  \nIn the case of Power Platform products, the adoption of a CI/CD process may seem to increase the development complexity to a solution oriented to Citizen Developers it is more important to make the development process more scalable and capable of dealing with new features and bug corrections in a faster way.  \nEnvironments in Power Platform Solutions  \nEnvironments are spaces where Power Platform Solutions exists. They store, manage and share everything related to the solution like data, apps, chat bots, flows and models. They also serve as containers to separate apps that might have different roles, security requirements or just target audiences. They can be used to create different stages of the solution development process, the expected model of working with environments in a CI/CD process will be as the following image suggests.  \nEnvironments considerations  \nWhenever an environment has been created, its resources can be only accessed by users within the same tenant which is an Azure Active Directory tenant in fact. When you create an app in an environment that app can only interact with data sources that are also deployed in that same environment, this includes connections, flows and Dataverse databases. This is an important consideration when dealing with a CD process.  \nDeployment strategy  \nWith three environments already created to represent the stages of the deployment, the goal now is to automate the deployment from one environment to another. Each environment will require the creation of its own solution: business logic and data.  \nStep 1  \nDevelopment team will be working in a Dev environment. These environments according to the team could be one for the team or one for each developer.  \nOnce changes have been made, the first step will be packaging the solution and export it into source control.  \nStep 2  \nSecond step is about the solution, you need to have a managed solution to deploy to other environments such as Stage or Production so now you should use a JIT environment where you would import your unmanaged solution and export them as managed. These solution files won't be checked into source control but will be stored as a build artifact in the pipeline making them available to be deployed in the release pipeline. This is where the second environment will be used. This second environment will be responsible of receiving the output managed solution coming from the artifact.  \nStep 3  \nThird and final step will import the solution into the production environment, this means that this stage will take the artifact from last step and will export it. When working in this environment you can also version your product in order to make a better trace of the product.  \nTools  \nMost used tools to get this process completed are:  \nPower Platform Build Tools.  \nThere is also a non graphical tool that could be used to work with this CD process. The Power CLI tool.  \nReferences  \nApplication lifecycle management with Microsoft Power Platform",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-delivery\\low-code-solutions\\README.md"
    },
    {
        "chunkId": "chunk88_0",
        "chunkContent": "Recipes  \nGithub  \nGithub workflows  \nTerraform  \nSave output to variable group  \nShare common variables naming conventions  \nTerraform structure guidelines",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-delivery\\recipes\\README.md"
    },
    {
        "chunkId": "chunk89_0",
        "chunkContent": "GitHub Workflows  \nA workflow is a configurable automated process made up of one or more jobs where each of these jobs can be an action in GitHub. Currently, a YAML file format is supported for defining a workflow in GitHub.  \nAdditional information on GitHub actions and GitHub Workflows in the links posted in the references section below.  \nWorkflow Per Environment  \nThe general approach is to have one pipeline, where the code is built, tested and deployed, and the artifact is then promoted to the next environment, eventually to be deployed into production.  \nThere are multiple ways in GitHub that an environment setup can be achieved. One way it can be done is to have one workflow for multiple environments, but the complexity increases as additional processes and jobs are added to a workflow, which does not mean it cannot be done for small pipelines. The plus point of having one workflow is that, when an artifact flows from one environment to another the state and environment values between the deployment environments can be passed easily.  \nOne way to get around the complexity of a single workflow is to have separate workflows for different environments, making sure that only the artifacts created and validated are promoted from one environment to another, as well as, the workflow is small enough, to debug any issues seen in any of the workflows. In this case, the state and environment values need to be passed from one deployment environment to another. Multiple workflows also helps to keep the deployments to the environments independent thus reducing the time to deploy and find issues earlier than later in the process. Also, since the environments are independent of each other, any failures in deploying to one environment does not block deployments to other environments. One tradeoff in this method, is that with different workflows for each environment, the maintenance increases as the complexity of workflows increase over time.  \nReferences  \nGitHub Actions  \nGitHub Workflows",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-delivery\\recipes\\github-workflows\\README.md"
    },
    {
        "chunkId": "chunk90_0",
        "chunkContent": "Terraform recipes  \nSave output to variable group  \nShare common variables naming conventions  \nTerraform structure guidelines",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-delivery\\recipes\\terraform\\README.md"
    },
    {
        "chunkId": "chunk91_0",
        "chunkContent": "Save terraform output to a variable group (Azure DevOps)  \nThis recipe applies only to terraform usage with Azure DevOps. It assumes your familiar with terraform commands and Azure Pipelines.  \nContext  \nWhen terraform is used to automate the provisioning of the infrastructure, an Azure Pipeline is generally dedicated to apply terraform configuration files. It will create, update, delete Azure resources to provision your infrastructure changes.  \nOnce files are applied, some Output Values (for instance resource group name, app service name) can be referenced and outputted by terraform. These values must be generally retrieved afterwards, used as input variables for the deployment of services happening in separate pipelines.  \n{% raw %}  \n```tf\noutput \"core_resource_group_name\" {\ndescription = \"The resource group name\"\nvalue       = module.core.resource_group_name\n}\n\noutput \"core_key_vault_name\" {\ndescription = \"The key vault name.\"\nvalue       = module.core.key_vault_name\n}\n\noutput \"core_key_vault_url\" {\ndescription = \"The key vault url.\"\nvalue       = module.core.key_vault_url\n}\n```  \n{% endraw %}  \nThe purpose of this recipe is to answer the following statement: How to make terraform output values available across multiple pipelines ?  \nSolution  \nOne suggested solution is to store outputted values in the Library with a Variable Group. Variable groups is a convenient way store values you might want to be passed into a YAML pipeline. In addition, all assets defined in the Library share a common security model. You can control who can define new items in a library, and who can use an existing item.  \nFor this purpose, we are using the following commands:  \nterraform output to extract the value of an output variable from the state file (provided by Terraform CLI)  \naz pipelines variable-group to manage variable groups (provided by Azure DevOps CLI)  \nYou can use the following script once terraform apply is completed to create/update the variable group.  \nScript (update-variablegroup.sh)  \nParameters  \nName Description DEVOPS_ORGANIZATION The URI of the Azure DevOps organization. DEVOPS_PROJECT The name or id of the Azure DevOps project. GROUP_NAME The name of the variable group targeted.  \nImplementation choices:  \nIf a variable group already exists, a valid option could be to delete and rebuild the group from scratch. However, as authorization could have been updated at the group level, we prefer to avoid this option. The script remove instead all variables in the targeted group and add them back with latest values. Permissions are not impacted.  \nA variable group cannot be empty. It must contains at least one variable. A temporary uuid value is created to mitigate this issue, and removed once variables are updated.  \n{% raw %}  \n```bash\n\n!/bin/bash\n\nset -e\n\nexport DEVOPS_ORGANIZATION=$1\nexport DEVOPS_PROJECT=$2\nexport GROUP_NAME=$3\n\nconfigure the azure devops cli\n\naz devops configure --defaults organization=${DEVOPS_ORGANIZATION} project=${DEVOPS_PROJECT} --use-git-aliases true\n\nget the variable group id (if already exists)\n\ngroup_id=$(az pipelines variable-group list --group-name ${GROUP_NAME} --query '[0].id' -o json)\n\nif [ -z \"${group_id}\" ]; then\n# create a new variable group\ntf_output=$(terraform output -json | jq -r 'to_entries[] | \"(.key)=(.value.value)\"')\naz pipelines variable-group create --name ${GROUP_NAME} --variables ${tf_output} --authorize true\nelse\n# get existing variables\nvar_list=$(az pipelines variable-group variable list --group-id ${group_id})\n\nfi\n```  \n{% endraw %}  \nAuthenticate with Azure DevOps  \nMost commands used in previous script interact with Azure DevOps and do require authentication. You can authenticate using the System.AccessToken security token used by the running pipeline, by assigning it to an environment variable named AZURE_DEVOPS_EXT_PAT, as shown in the following example (see Azure DevOps CLI in Azure Pipeline YAML for additional information).  \nIn addition, you can notice we are also using predefined variables to target the Azure DevOps organization and project (respectively System.TeamFoundationCollectionUri and System.TeamProjectId).  \n{% raw %}  \nyaml\n- task: Bash@3\ndisplayName: 'Update variable group using terraform outputs'\ninputs:\ntargetType: filePath\narguments: $(System.TeamFoundationCollectionUri) $(System.TeamProjectId) \"Platform-VG\"\nworkingDirectory: $(terraformDirectory)\nfilePath: $(scriptsDirectory)/update-variablegroup.sh\nenv:\nAZURE_DEVOPS_EXT_PAT: $(System.AccessToken)  \n{% endraw %}  \nSystem variables Description System.AccessToken Special variable that carries the security token used by the running build. System.TeamFoundationCollectionUri The URI of the Azure DevOps organization. System.TeamProjectId The ID of the project that this build belongs to.  \nLibrary security  \nRoles are defined for Library items, and membership of these roles governs the operations you can perform on those items.  \nRole for library item Description Reader Can view the item. User Can use the item when authoring build or release pipelines. For example, you must be a 'User' for a variable group to use it in a release pipeline. Administrator Can also manage membership of all other roles for the item. The user who created an item gets automatically added to the Administrator role for that item. By default, the following groups get added to the Administrator role of the library: Build Administrators, Release Administrators, and Project Administrators. Creator Can create new items in the library, but this role doesn't include Reader or User permissions. The Creator role can't manage permissions for other users.  \nWhen using System.AccessToken, service account <ProjectName> Build Service identity will be used to access the Library.  \nPlease ensure in Pipelines > Library > Security section that this service account has Administrator role at the Library or Variable Group level to create/update/delete variables (see. Library of assets for additional information)).",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-delivery\\recipes\\terraform\\save-output-to-variable-group.md"
    },
    {
        "chunkId": "chunk92_0",
        "chunkContent": "Sharing Common Variables / Naming Conventions Between Terraform Modules  \nWhat are we trying to solve?  \nWhen deploying infrastructure using code, it's common practice to split the code into different modules that are responsible for the deployment of a part or a component of the infrastructure. In Terraform, this can be done by using modules.  \nIn this case, it is useful to be able to share some common variables as well as centralize naming conventions of the different resources, to ensure it will be easy to refactor when it has to change, despite the dependencies that exist between modules.  \nFor example, let's consider 2 modules:  \nNetwork module, responsible for deploying Virtual Network, Subnets, NSGs and Private DNS Zones  \nAzure Kubernetes Service module responsible for deploying AKS cluster  \nThere are dependencies between these modules, like the Kubernetes cluster that will be deployed into the virtual network from the Network module. To do that, it must reference the name of the virtual network, as well as the resource group it is deployed in. And ideally, we would like these dependencies to be loosely coupled, as much as possible, to keep agility in how the modules are deployed and keep independent lifecycle.  \nThis page explains a way to solve this with Terraform.  \nHow to do it?  \nContext  \nLet's consider the following structure for our modules:  \n{% raw %}  \nconsole\nmodules\n\u251c\u2500\u2500 kubernetes\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 provider.tf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\n\u251c\u2500\u2500 network\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 provider.tf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf  \n{% endraw %}  \nNow, assume that you deploy a virtual network for the development environment, with the following properties:  \nname: vnet-dev  \nresource group: rg-dev-network  \nThen at some point, you need to inject these values into the Kubernetes module, to get a reference to it through a data source, for example:  \n{% raw %}  \nhcl\ndata \"azurem_virtual_network\" \"vnet\" {\nname                = var.vnet_name\nresource_group_name = var.vnet_rg_name\n}  \n{% endraw %}  \nIn the snippet above, the virtual network name and resource group are defined through variable. This is great, but if this changes in the future, then the values of these variables must change too. In every module they are used.  \nBeing able to manage naming in a central place will make sure the code can easily be refactored in the future, without updating all modules.  \nAbout Terraform variables  \nIn Terraform, every input variable must be defined at the configuration (or module) level, using the variable block. By convention, this is often done in a variables.tf file, in the module. This file contains variable declaration and default values. Values can be set using variables configuration files (.tfvars), environment variables or CLI arg when using the terraform plan or apply commands.  \nOne of the limitation of the variables declaration is that it's not possible to compose variables, locals or Terraform built-in functions are used for that.  \nCommon Terraform module  \nOne way to bypass this limitations is to introduce a \"common\" module, that will not deploy any resources, but just compute / calculate and output the resource names and shared variables, and be used by all other modules, as a dependency.  \n{% raw %}  \nconsole\nmodules\n\u251c\u2500\u2500 common\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 output.tf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\n\u251c\u2500\u2500 kubernetes\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 provider.tf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\n\u251c\u2500\u2500 network\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 provider.tf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf  \n{% endraw %}  \nvariables.tf:  \n{% raw %}  \n```hcl\nvariable \"environment_name\" {\ntype = string\ndescription = \"The name of the environment.\"\n}\n\nvariable \"location\" {\ntype = string\ndescription = \"The Azure region where the resources will be created. Default is westeurope.\"\ndefault = \"westeurope\"\n}\n```  \n{% endraw %}  \noutput.tf:  \n{% raw %}  \n```hcl\n\nShared variables\n\noutput \"location\" {\nvalue = var.location\n}\n\noutput \"subscription\" {\nvalue = var.subscription\n}\n\nVirtual Network Naming\n\noutput \"vnet_rg_name\" {\nvalue = \"rg-network-${var.environment_name}\"\n}\n\noutput \"vnet_name\" {\nvalue = \"vnet-${var.environment_name}\"\n}\n\nAKS Naming\n\noutput \"aks_rg_name\" {\nvalue = \"rg-aks-${var.environment_name}\"\n}\n\noutput \"aks_name\" {\nvalue = \"aks-${var.environment_name}\"\n}\n```  \n{% endraw %}  \nNow, if you execute the Terraform apply for the common module, you get all the shared/common variables in outputs:  \n{% raw %}  \n```console\n$ terraform plan -var environment_name=\"dev\" -var subscription=\"$(az account show --query id -o tsv)\"\n\nChanges to Outputs:\n+ aks_name     = \"aks-dev\"\n+ aks_rg_name  = \"rg-aks-dev\"\n+ location     = \"westeurope\"\n+ subscription = \"01010101-1010-0101-1010-010101010101\"\n+ vnet_name    = \"vnet-dev\"\n+ vnet_rg_name = \"rg-network-dev\"\n\nYou can apply this plan to save these new output values to the Terraform state, without changing any real infrastructure.\n```  \n{% endraw %}  \nUse the common Terraform module  \nUsing the common Terraform module in any other module is super easy. For example, this is what you can do in the Azure Kubernetes module main.tf file:  \n{% raw %}  \n```hcl\nmodule \"common\" {\nsource           = \"../common\"\nenvironment_name = var.environment_name\nsubscription     = var.subscription\n}\n\ndata \"azurerm_subnet\" \"aks_subnet\" {\nname                 = \"AksSubnet\"\nvirtual_network_name = module.common.vnet_name\nresource_group_name  = module.common.vnet_rg_name\n}\n\nresource \"azurerm_kubernetes_cluster\" \"aks\" {\nname                = module.common.aks_name\nresource_group_name = module.common.aks_rg_name\nlocation            = module.common.location\ndns_prefix          = module.common.aks_name\n\nidentity {\ntype = \"SystemAssigned\"\n}\n\ndefault_node_pool {\nname           = \"default\"\nvm_size        = \"Standard_DS2_v2\"\nvnet_subnet_id = data.azurerm_subnet.aks_subnet.id\n}\n}\n```  \n{% endraw %}  \nThen, you can execute the terraform plan and terraform apply commands to deploy!  \n{% raw %}  \n```console\nterraform plan -var environment_name=\"dev\" -var subscription=\"$(az account show --query id -o tsv)\"\ndata.azurerm_subnet.aks_subnet: Reading...\ndata.azurerm_subnet.aks_subnet: Read complete after 1s [id=/subscriptions/01010101-1010-0101-1010-010101010101/resourceGroups/rg-network-dev/providers/Microsoft.Network/virtualNetworks/vnet-dev/subnets/AksSubnet]\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n+ create\n\nTerraform will perform the following actions:\n\n# azurerm_kubernetes_cluster.aks will be created\n+ resource \"azurerm_kubernetes_cluster\" \"aks\" {\n+ dns_prefix                          = \"aks-dev\"\n+ fqdn                                = (known after apply)\n+ id                                  = (known after apply)\n+ kube_admin_config                   = (known after apply)\n+ kube_admin_config_raw               = (sensitive value)\n+ kube_config                         = (known after apply)\n+ kube_config_raw                     = (sensitive value)\n+ kubernetes_version                  = (known after apply)\n+ location                            = \"westeurope\"\n+ name                                = \"aks-dev\"\n+ node_resource_group                 = (known after apply)\n+ portal_fqdn                         = (known after apply)\n+ private_cluster_enabled             = (known after apply)\n+ private_cluster_public_fqdn_enabled = false\n+ private_dns_zone_id                 = (known after apply)\n+ private_fqdn                        = (known after apply)\n+ private_link_enabled                = (known after apply)\n+ public_network_access_enabled       = true\n+ resource_group_name                 = \"rg-aks-dev\"\n+ sku_tier                            = \"Free\"\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n```  \n{% endraw %}  \nNote: the usage of a common module is also valid if you decide to deploy all your modules in the same operation from a main Terraform configuration file, like:  \n{% raw %}  \n```hcl\nmodule \"common\" {\nsource           = \"./common\"\nenvironment_name = var.environment_name\nsubscription     = var.subscription\n}\n\nmodule \"network\" {\nsource           = \"./network\"\nvnet_name        = module.common.vnet_name\nvnet_rg_name     = module.common.vnet_rg_name\n}\n\nmodule \"kubernetes\" {\nsource           = \"./kubernetes\"\naks_name         = module.common.aks_name\naks_rg           = module.common.aks_rg_name\n}\n```  \n{% endraw %}  \nCentralize input variables definitions  \nIn case you chose to define variables values directly in the source control (e.g. gitops scenario) using variables definitions files (.tfvars), having a common module will also help to not have to duplicate the common variables definitions in all modules. Indeed, it is possible to have a global file that is defined once, at the common module level, and merge it with a module-specific variables definitions files at Terraform plan or apply time.  \nLet's consider the following structure:  \n{% raw %}  \nconsole\nmodules\n\u251c\u2500\u2500 common\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dev.tfvars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 prod.tfvars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 output.tf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\n\u251c\u2500\u2500 kubernetes\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dev.tfvars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 prod.tfvars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 provider.tf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\n\u251c\u2500\u2500 network\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 dev.tfvars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 prod.tfvars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 provider.tf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf  \n{% endraw %}  \nThe common module as well as all other modules contain variables files for dev and prod environment. tfvars files from the common module will define all the global variables that will be shared with other modules (like subscription, environment name, etc.) and .tfvars files of each module will define only the module-specific values.  \nThen, it's possible to merge these files when running the terraform apply or terraform plan command, using the following syntax:  \n{% raw %}  \nbash\nterraform plan -var-file=<(cat ../common/dev.tfvars ./dev.tfvars)  \n{% endraw %}  \nNote: using this, it is really important to ensure that you have not the same variable names in both files, otherwise that will generate an error.  \nConclusion  \nBy having a common module that owns shared variables as well as naming convention, it is now easier to refactor your Terraform configuration code base. Imagine that for some reason you need change the pattern that is used for the virtual network name: you change it in the common module output files, and just have to re-apply all modules!",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-delivery\\recipes\\terraform\\share-common-variables-naming-conventions.md"
    },
    {
        "chunkId": "chunk93_0",
        "chunkContent": "Guidelines on Structuring and Testing the Terraform Configuration  \nContext  \nWhen creating an infrastructure configuration, it is important to follow a consistent and organized structure to ensure maintainability, scalability and reusability of the code. The goal of this section is to briefly describe how to structure your Terraform configuration in order to achieve this.  \nStructuring the Terraform configuration  \nThe recommended structure is as follows:  \nPlace each component you want to configure in its own module folder. Analyze your infrastructure code and identify the logical components that can be separated into reusable modules. This will give you a clear separation of concerns and will make it straight forward to include new resources, update existing ones or reuse them in the future. For more details on modules and when to use them, see the Terraform guidance.  \nPlace the .tf module files at the root of each folder and make sure to include a README file in a markdown format which can be automatically generated based on the module code. It's recommended to follow this approach as this file structure will be automatically picked up by the Terraform Registry.  \nUse a consistent set of files to structure your modules. While this can vary depending on the specific needs of the project, one good example can be the following:  \nprovider.tf: defines the list of providers according to the plugins used  \ndata.tf: defines information read from different data sources  \nmain.tf: defines the infrastructure objects needed for your configuration (e.g. resource group, role assignment, container registry)  \nbackend.tf: backend configuration file  \noutputs.tf: defines structured data that is exported  \nvariables.tf: defines static, reusable values  \nInclude in each module sub folders for documentation, examples and tests.\nThe documentation includes basic information about the module: what is it installing, what are the options, an example use case and so on. You can also add here any other relevant details you might have.\nThe example folder can include one or more examples of how to use the module, each example having the same set of configuration files decided on the previous step. It's recommended to also include a README providing a clear understanding of how it can be used in practice.\nThe tests folder includes one or more files to test the example module together with a documentation file with instructions on how these tests can be executed.  \nPlace the root module in a separate folder called main: this is the primary entry point for the configuration. Like for the other modules, it will contain its corresponding configuration files.  \nAn example configuration structure obtained using the guidelines above is:  \n{% raw %}  \nconsole\nmodules\n\u251c\u2500\u2500 mlops\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 doc\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 example\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 test\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 backend.tf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 data.tf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 outputs.tf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 provider.tf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 variables.tf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 common\n\u251c\u2500\u2500 main  \n{% endraw %}  \nTesting the configuration  \nTo test Terraform configurations, the Terratest library is utilized. A comprehensive guide to best practices with Terratest, including unit tests, integration tests, and end-to-end tests, is available for reference here.  \nTypes of tests  \nUnit Test for Module / Resource: Write unit tests for individual modules / resources to ensure that each module behaves as expected in isolation. They are particularly valuable in larger, more complex Terraform configurations where individual modules can be reused and are generally quicker in terms of execution time.  \nIntegration Test: These tests verify that the different modules and resources work together as intended.  \nFor simple Terraform configurations, extensive unit testing might be overkill. Integration tests might be sufficient in such cases. However, as the complexity grows, unit tests become more valuable.  \nKey aspects to consider  \nSyntax and validation: Use terraform fmt and terraform validate to check the syntax and validate the Terraform configuration during development or in the deployment script / pipeline. This ensures that the configuration is correctly formatted and free of syntax errors.  \nDeployment and existence: Terraform providers, like the Azure provider, perform certain checks during the execution of terraform apply. If Terraform successfully applies a configuration, it typically means that the specified resources were created or modified as expected. In your code you can skip this validation and focus on particular resource configurations that are more critical, described in the next points.  \nResource properties that can break the functionality: The expectation here is that we're not interested in testing each property of a resource, but to identify the ones that could cause an issue in the system if they are changed, such as access or network policies, service principal permissions and others.  \nValidation of Key Vault contents: Ensuring the presence of necessary keys, certificates, or secrets in the Azure Key Vault that are stored as part of resource configuration.  \nProperties that can influence the cost or location: This can be achieved by asserting the locations, service tiers, storage settings, depending on the properties available for the resources.  \nNaming convention  \nWhen naming Terraform variables, it's essential to use clear and consistent naming conventions that are easy to understand and follow. The general convention is to use lowercase letters and numbers, with underscores instead of dashes, for example: \"azurerm_resource_group\".\nWhen naming resources, start with the provider's name, followed by the target resource, separated by underscores. For instance, \"azurerm_postgresql_server\" is an appropriate name for an Azure provider resource. When it comes to data sources, use a similar naming convention, but make sure to use plural names for lists of items. For example, \"azurerm_resource_groups\" is a good name for a data source that represents a list of resource groups.\nVariable and output names should be descriptive and reflect the purpose or use of the variable. It's also helpful to group related items together using a common prefix. For example, all variables related to storage accounts could start with \"storage_\". Keep in mind that outputs should be understandable outside of their scope. A useful naming pattern to follow is \"{name}_{attribute}\", where \"name\" represents a resource or data source name, and \"attribute\" is the attribute returned by the output. For example, \"storage_primary_connection_string\" could be a valid output name.  \nMake sure you include a description for outputs and variables, as well as marking the values as 'default' or 'sensitive' when the case. This information will be captured in the generated documentation.  \nGenerating the documentation  \nThe documentation can be automatically generated based on the configuration code in your modules with the help of terraform-docs. To generate the Terraform module documentation, go to the module folder and enter this command:  \n{% raw %}  \nsh\nterraform-docs markdown table --output-file README.md --output-mode inject .  \n{% endraw %}  \nThen, the documentation will be generated inside the component root directory.  \nConclusion  \nThe approach presented in this section is designed to be flexible and easy to use, making it straight forward to add new resources or update existing ones. The separation of concerns also makes it easy to reuse existing components in other projects, with all the information (modules, examples, documentation and tests) located in one place.  \nReferences and Further Reading  \nTerraform-docs  \nTerraform Registry  \nTerraform Module Guidance  \nTerratest  \nTesting HashiCorp Terraform  \nBuild Infrastructure - Terraform Azure Example",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-delivery\\recipes\\terraform\\terraform-structure-guidelines.md"
    },
    {
        "chunkId": "chunk94_0",
        "chunkContent": "Secrets Management  \nSecrets Management refers to the way in which we protect configuration settings and other sensitive data which, if\nmade public, would allow unauthorized access to resources. Examples of secrets are usernames, passwords, api keys, SAS\ntokens etc.  \nWe should assume any repo we work on may go public at any time and protect our secrets, even if\nthe repo is initially private.  \nGeneral Approach  \nThe general approach is to keep secrets in separate configuration files that are not checked in\nto the repo. Add the files to the .gitignore to prevent that they're checked in.  \nEach developer maintains their own local version of the file or, if required, circulate them via private channels e.g. a Teams chat.  \nIn a production system, assuming Azure, create the secrets in the environment of the running process. We can do this by manually editing the 'Applications Settings' section of the resource, but a script using\nthe Azure CLI to do the same is a useful time-saving utility. See az webapp config appsettings for more details.  \nIt's best practice to maintain separate secrets configurations for each environment that you run. e.g. dev, test, prod, local etc  \nThe secrets-per-branch recipe describes a simple way to manage separate secrets configurations for each environment.  \nNote: even if the secret was only pushed to a feature branch and never merged, it's still a part of the git history. Follow these instructions to remove any sensitive data and/or regenerate any keys and other sensitive information added to the repo. If a key or secret made it into the code base, rotate the key/secret so that it's no longer active  \nKeeping Secrets Secret  \nThe care taken to protect our secrets applies both to how we get and store them, but also to how we use them.  \nDon't log secrets  \nDon't put them in reporting  \nDon't send them to other applications, as part of URLs, forms, or in any other way other than to make a request to the service that requires that secret  \nEnhanced-Security Applications  \nThe techniques outlined below provide good security and a common pattern for a wide range of languages. They rely on\nthe fact that Azure keeps application settings (the environment) encrypted until your app runs.  \nThey do not prevent secrets from existing in plaintext in memory at runtime. In particular, for garbage collected languages those values may exist for longer than the lifetime of the variable, and may be visible when debugging a memory dump of the process.  \nIf you are working on an application with enhanced security requirements you should consider using additional techniques to maintain encryption on secrets throughout the application lifetime.  \nAlways rotate encryption keys on a regular basis.  \nTechniques for Secrets Management  \nThese techniques make the loading of secrets  transparent to the developer.  \nC#/.NET  \nModern .NET Solution  \nFor .NET SDK (version 2.0 or higher) we have dotnet secrets, a tool provided by the .NET SDK that allows you to manage and protect sensitive information, such as API keys, connection strings, and other secrets, during development. The secrets are stored securely on your machine and can be accessed by your .NET applications.  \n{% raw %}  \n```shell\n\nInitialize dotnet secret\n\ndotnet user-secrets init\n\nAdding secret\n\ndotnet user-secrets set\n\ndotnet user-secrets set ExternalServiceApiKey my-api-key-12345\n\nUpdate Secret\n\ndotnet user-secrets set ExternalServiceApiKey updated-api-key-67890\n\n```  \n{% endraw %}  \nTo access the secrets;  \n{% raw %}  \n```csharp\nusing Microsoft.Extensions.Configuration;\n\nvar builder = new ConfigurationBuilder()\n.AddUserSecrets();\n\nvar configuration = builder.Build();\nvar externalServiceApiKey = configuration[\"ExternalServiceApiKey\"];\n\n```  \n{% endraw %}  \nDeployment Considerations  \nWhen deploying your application to production, it's essential to ensure that your secrets are securely managed. Here are some deployment-related implications:  \nRemove Development Secrets: Before deploying to production, remove any development secrets from your application configuration. You can use environment variables or a more secure secret management solution like Azure Key Vault or AWS Secrets Manager in production.  \nSecure Deployment: Ensure that your production server is secure, and access to secrets is controlled. Never store secrets directly in source code or configuration files.  \nKey Rotation: Consider implementing a secret rotation policy to regularly update your secrets in production.  \n.NET Framework Solution  \nUse the file attribute of the appSettings element to load secrets from a local file.  \n{% raw %}  \n```xml\n\n\u2026\n\n\n\n\n\u2026\n\n```  \n{% endraw %}  \nAccess secrets:  \n{% raw %}  \nC#\nstatic void Main(string[] args)\n{\nString mySecret = System.Configuration.ConfigurationManager.AppSettings[\"mySecret\"];\n}  \n{% endraw %}  \nWhen running in Azure, ConfigurationManager will load these settings from the process environment. We don't need to upload secrets files to the server or change any code.  \nNode  \nStore secrets in environment variables or in a .env file  \n{% raw %}  \nbash\n$ cat .env\nMY_SECRET=mySecret  \n{% endraw %}  \nUse the dotenv package to load and access environment variables  \n{% raw %}  \nnode\nrequire('dotenv').config()\nlet mySecret = process.env(\"MY_SECRET\")  \n{% endraw %}  \nPython  \nStore secrets in environment variables or in a .env file  \n{% raw %}  \nbash\n$ cat .env\nMY_SECRET=mySecret  \n{% endraw %}  \nUse the dotenv package to load and access environment variables  \n{% raw %}  \n```Python\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nmy_secret = os.getenv('MY_SECRET')\n```  \n{% endraw %}  \nAnother good library for reading environment variables is environs  \n{% raw %}  \n```Python\nfrom environs import Env\n\nenv = Env()\nenv.read_env()\nmy_secret = os.environ[\"MY_SECRET\"]\n```  \n{% endraw %}  \nDatabricks  \nDatabricks has the option of using dbutils as a secure way to retrieve credentials and not reveal them within the notebooks running on Databricks  \nThe following steps lay out a clear pathway to creating new secrets and then utilizing them within a notebook on Databricks:  \nInstall and configure the Databricks CLI on your local machine  \nGet the Databricks personal access token  \nCreate a scope for the secrets  \nCreate secrets  \nValidation  \nAutomated credential scanning can be performed on the code regardless of the programming language. Read more about it here",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-delivery\\secrets-management\\README.md"
    },
    {
        "chunkId": "chunk95_0",
        "chunkContent": "Continuous Integration and Delivery  \nContinuous Integration is the engineering practice of frequently committing code in a shared repository, ideally several times a day, and performing an automated build on it. These changes are built with other simultaneous changes to the system, which enables early detection of integration issues between multiple developers working on a project. Build breaks due to integration failures are treated as the highest priority issue for all the developers on a team and generally work stops until they are fixed.  \nPaired with an automated testing approach, continuous integration also allows us to also test the integrated build such that we can verify that not only does the code base still build correctly, but also is still functionally correct. This is also a best practice for building robust and flexible software systems.  \nContinuous Delivery takes the Continuous Integration concept further to also test deployments of the integrated code base on a replica of the environment it will be ultimately deployed on. This enables us to learn early about any unforeseen operational issues that arise from our changes as quickly as possible and also learn about gaps in our test coverage.  \nThe goal of all of this is to ensure that the main branch is always shippable, meaning that we could, if we needed to, take a build from the main branch of our code base and ship it on production.  \nIf these concepts are unfamiliar to you, take a few minutes and read through Continuous Integration and Continuous Delivery.  \nOur expectation is that CI/CD should be used in all the engineering projects that we do with our customers and that we are building, testing, and deploying each change we make to any software system that we are building.  \nFor a much deeper understanding of all of these concepts, the books Continuous Integration and Continuous Delivery provide a comprehensive background.  \nTools  \nAzure Pipelines  \nOur tooling at Microsoft has made setting up integration and delivery systems like this easy. If you are unfamiliar with it, take a few moments now to read through Azure Pipelines (Previously VSTS) and for a practical walkthrough of how this works in practice, one example you can read through is CI/CD on Kubernetes with VSTS.  \nJenkins  \nJenkins is one of the most commonly used tools across the open source community. It is well-known with hundreds of plugins for every build requirement.\nJenkins is free but requires a dedicated server.\nYou can easily create a Jenkins VM using this template  \nTravisCI  \nTravis CI can be used for open source projects at no cost but developers must purchase an enterprise plan for private projects.\nThis service is ideal for validation of PR's on GitHub because it is lightweight and easy to set up with no need for dedicated server setup.\nIt also supports a Build matrix feature which allows accelerating the build and testing process by breaking them into parts.  \nCircleCI  \nCircleCI is a free service for open source projects with no dedicated server required. It is also ideal for validation of PR's on GitHub.\nCircleCI also allows workflows, parallelism and splitting your tests across any number of containers with a wide array of packages pre-installed on the build containers.  \nAppVeyor  \nAppVeyor is another free CI service for open source projects which also supports Windows-based builds.",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\CICD.md"
    },
    {
        "chunkId": "chunk96_0",
        "chunkContent": "Inclusive Linting  \nAs software professionals we should strive to promote an inclusive work environment, which naturally extends to the code and documentation we write. It's important to keep the use of inclusive language consistent across an entire project or repository.  \nTo achieve this, we recommend using a text file analysis tool such as an inclusive linter and including this as a step in your CI pipelines.  \nWhat to Lint for  \nThe primary goal of an inclusive linter is to flag any occurrences of non-inclusive language within source code (and optionally suggest some alternatives). Non-inclusive words or phrases in a project can be found anywhere from comments and documentation to variable names.  \nAn inclusive linter may include its own dictionary of \"default\" non-inclusive words and phrases to run against as a good starting point. These tools can also be customizable, oftentimes offering the ability to omit some terms and/or add your own.  \nThe ability to add additional terms to your linter has the added benefit of enabling linting of sensitive language on top of inclusive linting. This can prevent things such as customer names or other non-public information from making it into your git history, for instance.  \nGetting Started with an Inclusive Linter  \n[woke]  \nOne inclusive linter we recommend is woke. It is a language-agnostic CLI tool that detects non-inclusive language in your source code and recommends alternatives. While woke automatically applies a default ruleset with non-inclusive terms to lint for, you can also apply a custom rule config (via a yaml file) with additional terms to lint for. See [example.yaml] for an example of adding custom rules.  \nRunning the tool locally on a file or directory is relatively straightforward:  \n{% raw %}  \n```sh\n$ woke test.txt\n\ntest.txt:2:2-6: guys may be insensitive, use folks, people instead (warning)\n* guys\n^\n```  \n{% endraw %}  \nwoke can be run locally on your machine or CI/CD system via CLI and is also available as a two GitHub Actions:  \nRun woke  \nRun woke with Reviewdog  \nTo use the standard \"Run woke\" GitHub Action with the default ruleset in a CI pipeline:  \nAdd the woke action as a step in your project's CI pipeline yaml:\n{% raw %}\n```yaml\nname: ci\non:\n- pull_request\njobs:\nwoke:\nname: woke\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout\nuses: actions/checkout@v2\n- name: woke\nuses: get-woke/woke-action@v0\nwith:\n# Cause the check to fail on any broke rules\nfail-on-error: true\n\n```\n{% endraw %}  \nRun your pipeline  \nView the output in the \"Actions\" tab in the main repository view  \nFor more information about additional configuration and usage, see the official docs.",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\inclusive-linting.md"
    },
    {
        "chunkId": "chunk97_0",
        "chunkContent": "Continuous Integration  \nWe encourage engineering teams to make an upfront investment during Sprint 0 of a project to establish an automated and repeatable pipeline which continuously integrates code and releases system executable(s) to target cloud environments. Each integration should be verified by an automated build process that asserts a suite of validation tests pass and surface any errors across the developer team.  \nWe encourage teams to implement the CI/CD pipelines before any service code is written for customers, which usually happens in Sprint 0(N). This way, the engineering team can develop and test their work in isolation without impacting other developers and promote a consistent devops workflow throughout the engagement.  \nThese principles map directly agile software development lifecycle practices.  \nGoals  \nContinuous integration automation is an integral part of the software development lifecycle intended to reduce build integration errors and maximize velocity across a dev crew.  \nA robust build automation pipeline will:  \nAccelerate team velocity  \nPrevent integration problems  \nAvoid last minute chaos during release dates  \nProvide a quick feedback cycle for system-wide impact of local changes  \nSeparate build and deployment stages  \nMeasure and report metrics around build failures / success(s)  \nIncrease visibility across the team enabling tighter communication  \nReduce human errors, which is probably the most important part of automating the builds  \nBuild Definition Managed in Git  \nCode / manifest artifacts required to build your project should be maintained in within your project(s) git repository(s)  \nCI provider-specific build pipeline definition(s) should reside within your project(s) git repository(s).  \nBuild Automation  \nAn automated build should encompass the following principles:  \nBuild Task  \nA single step within your build pipeline that compiles your code project into a single build artifact.  \nUnit Testing  \nYour build definition includes validation steps to execute a suite of automated unit tests to ensure that application components meets its design and behaves as intended.  \nCode Style Checks  \nCode across an engineering team must be formatted to agreed coding standards. Such standards keep code consistent, and most importantly easy for the team and customer(s) to read and refactor. Code styling consistency encourages collective ownership for project scrum teams and our partners.  \nThere are several open source code style validation tools available to choose from (code style checks, StyleCop). The Code Review recipes section of the playbook has suggestions for linters and preferred styles for a number of languages.  \nYour code and documentation should avoid the use of non-inclusive language wherever possible. Follow the Inclusive Linting section to ensure your project promotes an inclusive work environment for both the team and for customers.  \nWe recommend incorporating security analysis tools within the build stage of your pipeline such as: code credential scanner, security risk detection, static analysis, etc. For Azure DevOps, you can add a security scan task to your pipeline by installing the Microsoft Security Code Analysis Extension. GitHub Actions supports a similar extension with the RIPS security scan solution.  \nCode standards are maintained within a single configuration file. There should be a step in your build pipeline that asserts code in the latest commit conforms to the known style definition.  \nBuild Script Target  \nA single command should have the capability of building the system. This is also true for builds running on a CI server or on a developers local machine.  \nNo IDE Dependencies  \nIt's essential to have a build that's runnable through standalone scripts and not dependent on a particular IDE. Build pipeline targets can be triggered locally on their desktops through their IDE of choice. The build process should maintain enough flexibility to run within a CI server as well. As an example, dockerizing your build process offers this level of flexibility as VSCode and IntelliJ supports docker plugin extensions.  \nDevOps security checks  \nIntroduce security to your project at early stages. Follow the DevSecOps section to introduce security practices, automation, tools and frameworks as part of the CI.  \nBuild Environment Dependencies  \nAutomated local environment setup  \nWe encourage maintaining a consistent developer experience for all team members. There should be a central automated manifest / process that streamlines the installation and setup of any software dependencies. This way developers can replicate the same build environment locally as the one running on a CI server.  \nBuild automation scripts often require specific software packages and version pre-installed within the runtime environment of the OS. This presents some challenges as build processes typically version lock these dependencies.  \nAll developers on the team should be able to emulate the build environment from their local desktop regardless of their OS.  \nFor projects using VS Code, leveraging Dev Containers can really help standardize the local developer experience across the team.  \nWell established software packaging tools like Docker, Maven, npm, etc should be considered when designing your build automation tool chain.  \nDocument local setup  \nThe setup process for setting up a local build environment should be well documented and easy for developers to follow.  \nInfrastructure as Code  \nManage as much of the following as possible, as code:  \nConfiguration Files  \nConfiguration Management(ie environment variable automation via terraform)  \nSecret Management(ie creating Azure secrets via terraform)  \nCloud Resource Provisioning  \nRole Assignments  \nLoad Test Scenarios  \nAvailability Alerting / Monitoring Rules and Conditions  \nDecoupling infrastructure from the application codebase simplifies engineering teams move to cloud native applications.  \nTerraform resource providers like Azure DevOps is making it easier for developers to manage build pipeline variables, service connections and CI/CD pipeline definitions.  \nSample DevOps Workflow using Terraform and Cobalt  \nWhy  \nRepeatable and auditable changes to infrastructure make it easier to roll back to known good configurations and to rapidly expand to new stages and regions without having to hand-wire cloud resources  \nBattle tested and templated IAC reference projects like Cobalt and Bedrock enable more engineering teams deploy secure and scalable solutions at a much more rapid pace  \nSimplify \u201clift and shift\u201d scenarios by abstracting the complexities of cloud-native computing away from application developer teams.  \nIAC DevOPS: Operations by Pull Request  \nThe Infrastructure deployment process built around a repo that holds the current expected state of the system / Azure environment.  \nOperational changes are made to the running system by making commits on this repo.  \nGit also provides a simple model for auditing deployments and rolling back to a previous state.  \nInfrastructure Advocated Patterns  \nYou define infrastructure as code in Terraform / ARM / Ansible templates  \nTemplates are repeatable cloud resource stacks with a focus on configuration sets aligned with app scaling and throughput needs.  \nIAC Principles  \nAutomate the Azure Environment  \nAll cloud resources are provisioned through a set of infrastructure as code templates. This also includes secrets, service configuration settings, role assignments and monitoring conditions.  \nAzure Portal should provide a read-only view on environment resources. Any change applied to the environment should be made through the IAC CI tool-chain only.  \nProvisioning cloud environments should be a repeatable process that's driven off the infrastructure code artifacts checked into our git repository.  \nIAC CI Workflow  \nWhen the IAC template files change through a git-based workflow, A CI build pipeline builds, validates and reconciles the target infrastructure environment's current state with the expected state. The infrastructure execution plan candidate for these fixed environments are reviewed by a cloud administrator as a gate check prior to the deployment stage of the pipeline applying the execution plan.  \nDeveloper Read-Only Access to Cloud Resources  \nDeveloper accounts in the Azure portal should have read-only access to IAC environment resources in Azure.  \nSecret Automation  \nIAC templates are deployed via a CI/CD system that has secrets automation integrated. Avoid applying changes to secrets and/or certificates directly in the Azure Portal.  \nInfrastructure Integration Test Automation  \nEnd-to-end integration tests are run as part of your IAC CI process to inspect and validate that an azure environment is ready for use.  \nInfrastructure Documentation  \nThe deployment and cloud resource template topology should be documented and well understood within the README of the IAC git repo.  \nLocal environment and CI workflow setup steps should be documented.  \nConfiguration Validation  \nApplications use configuration to allow different runtime behaviors and it\u2019s quite common to use files to store these settings. As developers, we might introduce errors while editing these files which would cause issues for the application to start and/or run correctly. By applying validation techniques on both syntax and semantics of our configuration, we can detect errors before the application is deployed and execute, improving the developer (user) experience.  \nApplication Configuration Files Examples  \nJSON, with support for complex data types and data structures  \nYAML, a super set of JSON with support for complex data types and structures  \nTOML, a super set of JSON and a formally specified configuration file format  \nWhy Validate Application Configuration as a Separate Step?  \nEasier Debugging & Time saving - With a configuration validation step in our pipeline, we can avoid running the application just to find it fails. It saves time on having to deploy & run, wait and then realize something is wrong in configuration. In addition, it also saves time on going through the logs to figure out what failed and why.  \nBetter user/developer experience - A simple reminder to the user that something in the configuration isn't in the right format can make all the difference between the joy of a successful deployment process and the intense frustration of having to guess what went wrong. For example, when there is a Boolean value expected, it can either be a string value like \"True\" or \"False\" or an integer value such as \"0\" or \"1\" . With configuration validation we make sure the meaning is correct for our application.  \nAvoid data corruption and security breaches - Since the data arrives from an untrusted source, such as a user or an external webservice, it\u2019s particularly important to validate the input . Otherwise, it will run at the risk of performing errors, corrupting data, or, worse, be vulnerable to a whole array of injection attacks.  \nWhat is Json Schema?  \nJSON-Schema is the standard of JSON documents that describes the structure and the requirements of your JSON data. Although it is called JSON-Schema, it also common to use this method for YAMLs, as it is a super set of JSON.\nThe schema is very simple; point out which fields might exist, which are required or optional, what data format they use. Other validation rules can be added on top of that basic premise, along with human-readable information. The metadata lives in schemas which are .json files as well.\nIn addition, schema has the widest adoption among all standards for JSON validation as it covers a big part of validation scenarios. It uses easy-to-parse JSON documents for schemas and is easily extensible.  \nHow to Implement Schema Validation?  \nImplementing schema validation is divided in two - the generation of the schemas and the validation of yaml/json files with those schemas.  \nGeneration  \nThere are two options to generate a schema:  \nFrom code - we can leverage the existing models and objects in the code and generate a customized schema.  \nFrom data - we can take yaml/json samples which reflect the configuration in general and use the various online tools to generate a schema.  \nValidation  \nThe schema has 30+ validators for different languages, including 10+ for JavaScript, so no need to code it yourself.  \nIntegration Validation  \nAn effective way to identify bugs in your build at a rapid pace is to invest early into a reliable suite of automated tests that validate the baseline functionality of the system:  \nEnd to end integration tests  \nInclude tests in your pipeline to validate the build candidate conforms to automated business functionality assertions. Any bugs or broken code should be reported in the test results including the failed test and relevant stack trace. All tests should be invoked through a single command.  \nKeep the build fast. Consider automated test runtime when deciding to pull in dependencies like databases, external services and mock data loading into your test harness. Slow builds often become a bottleneck for dev teams when parallel builds on a CI server are not an option. Consider adding max timeout limits for lengthy validations to fail fast and maintain high velocity across the team.  \nAvoid checking in broken builds  \nAutomated build checks, tests, lint runs, etc should be validated locally before committing your changes to the scm repo. Test Driven Development is a practice dev crews should consider to help identify bugs and failures as early as possible within the development lifecycle.  \nReporting build failures  \nIf the build step happens to fail then the build pipeline run status should be reported as failed including relevant logs and stack traces.  \nTest Automation Data Dependencies  \nAny mocked dataset(s) used for unit and end-to-end integration tests should be checked into the mainline repository. Minimize any external data dependencies with your build process.  \nCode Coverage Checks  \nWe recommend integrating code coverage tools within your build stage. Most coverage tools fail builds when the test coverage falls below a minimum threshold(80% coverage). The coverage report should be published to your CI system to track a time series of variations.  \nGit Driven Workflow  \nBuild on commit  \nEvery commit to the baseline repository should trigger the CI pipeline to create a new build candidate.  \nBuild artifact(s) are built, packaged, validated and deployed continuously into a non-production environment per commit. Each commit against the repository results into a CI run which checks out the sources onto the integration machine, initiates a build, and notifies the committer of the result of the build.  \nAvoid commenting out failing tests  \nAvoid commenting out tests in the mainline branch. By commenting out tests, we get an incorrect indication of the status of the build.  \nBranch policy enforcement  \nProtected branch policies should be setup on the main branch to ensure that CI stage(s) have passed prior to starting a code review. Code review approvers will only start reviewing a pull request once the CI pipeline run passes for the latest pushed git commit.  \nBroken builds should block pull request reviews.  \nPrevent commits directly into main branch.  \nBranch strategy  \nRelease branches should auto trigger the deployment of a build artifact to its target cloud environment. You can find additional guidance on the Azure DevOps documentation site under the Manage deployments section  \nDeliver Quickly and Daily  \n\"By committing regularly, every committer can reduce the number of conflicting changes. Checking in a week's worth of work runs the risk of conflicting with other features and can be very difficult to resolve. Early, small conflicts in an area of the system cause team members to communicate about the change they are making.\"  \nIn the spirit of transparency and embracing frequent communication across a dev crew, we encourage developers to commit code on a daily cadence. This approach provides visibility to feature progress and accelerates pair programming across the team. Here are some principles to consider:  \nEveryone commits to the git repository each day  \nEnd of day checked-in code should contain unit tests at the minimum.  \nRun the build locally before checking in to avoid CI pipeline failure saturation. You should verify what caused the error, and try to solve it as soon as possible instead of committing your code. We encourage developers to follow a lean SDLC principles.  \nIsolate work into small chunks which ties directly to business value and refactor incrementally.  \nIsolated Environments  \nOne of the key goals of build validation is to isolate and identify failures in staging environment(s) and minimize any disruption to live production traffic. Our E2E automated tests should run in an environment which mimics our production environment(as much as possible). This includes consistent software versions, OS, test data volume simulations, network traffic parity with production, etc.  \nTest in a clone of production  \nThe production environment should be duplicated into a staging environment(QA and/or Pre-Prod) at a minimum.  \nPull request update(s) trigger staged releases  \nNew commits related to a pull request should trigger a build / release into an integration environment. The production environment should be fully isolated from this process.  \nPromote infrastructure changes across fixed environments  \nInfrastructure as code changes should be tested in an integration environment and promoted to all staging environment(s) then migrated to production with zero downtime for system users.  \nTesting in production  \nThere are various approaches with safely carrying out automated tests for production deployments. Some of these may include:  \nFeature flagging  \nA/B testing  \nTraffic shifting  \nDeveloper Access to the Latest Release Artifacts  \nOur devops workflow should enable developers to get, install and run the latest system executable. Release executable(s) should be auto generated as part of our CI/CD pipeline(s).  \nDevelopers can access latest executable  \nThe latest system executable is available for all developers on the team. There should be a well-known place where developers can reference the release artifact.  \nRelease artifact is published for each pull request or merges into main branch  \nIntegration Observability  \nApplied state changes to the mainline build should be made available and communicated across the team. Centralizing logs and status(s) from build and release pipeline failures are essential for developers investigating broken builds.  \nWe recommend integrating Teams or Slack with CI/CD pipeline runs which helps keep the team continuously plugged into failures and build candidate status(s).  \nContinuous integration top level dashboard  \nModern CI providers have the capability to consolidate and report build status(s) within a given dashboard.  \nYour CI dashboard should be able to correlate a build failure with a git commit.  \nBuild status badge in project readme  \nThere should be a build status badge included in the root README of the project.  \nBuild notifications  \nYour CI process should be configured to send notifications to messaging platforms like Teams / Slack once the build completes. We recommend creating a separate channel to help consolidate and isolate these notifications.  \nResources  \nMartin Fowler's Continuous Integration Best Practices  \nBedrock Getting Started Quick Guide  \nCobalt Quick Start Guide  \nTerraform Azure DevOps Provider  \nAzure DevOps multi stage pipelines  \nAzure Pipeline Key Concepts  \nAzure Pipeline Environments  \nArtifacts in Azure Pipelines  \nAzure Pipeline permission and security roles  \nAzure Environment approvals and checks  \nTerraform Getting Started Guide with Azure  \nTerraform Remote State Azure Setup  \nTerratest - Unit and Integration Infrastructure Framework",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\README.md"
    },
    {
        "chunkId": "chunk98_0",
        "chunkContent": "Data Science Pipeline  \nAs Azure DevOps doesn't allow code reviewers to comment directly in Jupyter Notebooks, Data Scientists(DSs) have\nto convert the notebooks to scripts before they commit and push these files to the repository.  \nThis document aims to automate this process in Azure DevOps, so the DSs don't need to execute anything locally.  \nProblem statement  \nA Data Science repository has this folder structure:  \n{% raw %}  \n```bash\n\n```  \n{% endraw %}  \nThe python files are needed to allow Pull Request reviewers to add comments to the notebooks, they can add comments\nto the Python scripts and we apply these comments to the notebooks.  \nSince we have to run this process manually before we add files to a commit, this manual process is error prone, e.g.\nIf we create a notebook, generate the script from it, but later make some changes and forget to generate a new script\nfor the changes.  \nSolution  \nOne way to avoid this is to create the scripts in the repository from the commit. This document will describe this\nprocess.  \nWe can add a pipeline with the following steps to the repository to run in ipynb files:  \nGo to the Project Settings -> Repositories -> Security -> User Permissions  \nAdd the Build Service in Users the permission to Contribute  \nCreate a new pipeline.  \nIn the newly created pipeline we add:  \nTrigger to run on ipynb files:\n{% raw %}\nyml\ntrigger:\npaths:\ninclude:\n- '*.ipynb'\n- '**/*.ipynb'\n{% endraw %}  \nSelect the pool as Linux:\n{% raw %}\nyml\npool:\nvmImage: ubuntu-latest\n{% endraw %}  \nSet the directory where we want to store the scripts:\n{% raw %}\nyml\nvariables:\nREPO_URL: # Azure DevOps URL in the format: dev.azure.com/<Organization>/<Project>/_git/<RepoName>\n{% endraw %}  \nNow we will start the core of the pipeline:  \nUpgrade pip  \n{% raw %}\n```yml\n- script: |\npython -m pip install --upgrade pip\ndisplayName: 'Upgrade pip'\n```\n{% endraw %}  \nInstall nbconvert and ipython:  \n{% raw %}\nyml\n- script: |\npip install nbconvert ipython\ndisplayName: 'install nbconvert & ipython'\n{% endraw %}  \nInstall pandoc:  \n{% raw %}\nyml\n- script: |\nsudo apt install -y pandoc\ndisplayName: \"Install pandoc\"\n{% endraw %}  \nFind the notebook files (ipynb) in the last commit to the repo and convert it to scripts (py):  \n{% raw %}\nyml\n- task: Bash@3\ninputs:\ntargetType: 'inline'\nscript: |\nIPYNB_PATH=($(git diff-tree --no-commit-id --name-only -r $(Build.SourceVersion) | grep '[.]ipynb$'))\necho $IPYNB_PATH\n[ -z \"$IPYNB_PATH\" ] && echo \"Nothing to convert\" || jupyter nbconvert --to script $IPYNB_PATH\ndisplayName: \"Convert Notebook to script\"\n{% endraw %}  \nCommit these changes to the repository:  \n{% raw %}\nyml\n- bash: |\ngit config --global user.email \"build@dev.azure.com\"\ngit config --global user.name \"build\"\ngit add .\ngit commit -m 'Convert Jupyter notebooks' || echo \"No changes to commit\" && NO_CHANGES=1\n[ -z \"$NO_CHANGES\" ] || git push https://$(System.AccessToken)@$(REPO_URL) HEAD:$(Build.SourceBranchName)\ndisplayName: \"Commit notebook to repository\"\n{% endraw %}  \nNow we have a pipeline that will generate the scripts as we commit our notebooks.",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\ci-in-data-science\\working-with-notebooks\\README.md"
    },
    {
        "chunkId": "chunk99_0",
        "chunkContent": "DevSecOps  \nThe concept of DevSecOps  \nDevSecOps or DevOps security is about introducing security earlier in the life cycle of application development (a.k.a shift-left), thus minimizing the impact of vulnerabilities and bringing security closer to development team.  \nWhy  \nBy embracing shift-left mentality, DevSecOps encourages organizations to bridge the gap that often exists between development and security teams to the point where many of the security processes are automated and are effectively handled by the development team.  \nDevSecOps Practices  \nThis section covers different tools, frameworks and resources allowing introduction of DevSecOps best practices to your project at early stages of development.\nTopics covered:  \nCredential Scanning - automatically inspecting a project to ensure that no secrets are included in the project's source code.  \nSecrets Rotation - automated process by which the secret, used by the application, is refreshed and replaced by a new secret.  \nStatic Code Analysis - analyze source code or compiled versions of code to help find security flaws.  \nPenetration Testing - a simulated attack against your application to check for exploitable vulnerabilities.  \nContainer Dependencies Scanning - search for vulnerabilities in container operating systems, language packages and application dependencies.",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\dev-sec-ops\\README.md"
    },
    {
        "chunkId": "chunk100_0",
        "chunkContent": "Azure DevOps  \nWrite something about Azure DevOps here.  \nTable of Contents  \nService connection security",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\dev-sec-ops\\azure-devops\\README.md"
    },
    {
        "chunkId": "chunk101_0",
        "chunkContent": "Azure DevOps Service Connection Security  \nService Connections are used in Azure DevOps Pipelines to connect to external services, like Azure, GitHub, Docker, Kubernetes, and many other services. Service Connections can be used to authenticate to these external services and to invoke diverse types of commands, like create and update resources in Azure, upload container images to Docker, or deploy applications to Kubernetes.  \nTo be able to invoke these commands, Service Connections need to have the right permissions to do so, for most types of Service Connections the permissions can be scoped to a subset of resources to limit the access they have. To improve the principle of least privilege, it's often very common to have separate Service Connections for different environments like Dev/Test/QA/Prod.  \nSecure Service Connection  \nSecuring Service Connections can be achieved by using several methods.  \nUser permissions can be configured to ensure only the correct users can create, view, use, and manage the Service Connection.  \nPipeline-level permissions can be configured to ensure only approved YAML pipelines are able to use the Service Connection.  \nProject permissions can be configured to ensure only certain Azure DevOps projects are able to use the Service Connection.  \nAfter using the above methods, what is secured is who can use the Service Connections.\nWhat still isn't secured however, is what can be done with the Service Connections.  \nBecause Service Connections have all the necessary permissions in the external services, it is crucial to secure Service Connections so they cannot be misused by accident or by malicious users.  \nAn example of this is a Azure DevOps Pipeline that uses a Service Connection to an Azure Resource Group (or entire subscription) to list all resources and then delete those resources.  Without the correct security in place, it could be possible to execute this Pipeline, without any validation or reviews being done.  \n{% raw %}  \n```yaml\npool:\nvmImage: ubuntu-latest\n\nsteps:\n- task: AzureCLI@2\ninputs:\nazureSubscription: 'Production Service Connection'\nscriptType: 'pscore'\nscriptLocation: 'inlineScript'\ninlineScript: |\n$resources = az resource list\nforeach ($resource in $resources) {\naz resource delete --ids $resource.id\n}\n```  \n{% endraw %}  \nPipeline Security caveat  \nYAML pipelines can be triggered without the need for a pull request, this introduces a security risk.  \nIn good practice,  \nPull Requests and  \nCode Reviews should be used to ensure the code that is being deployed, is being reviewed by a second person and potentially automatically being checked for vulnerabilities and other security issues.  \nHowever, YAML Pipelines can be executed without the need for a Pull Request and Code Reviews. This allows the (malicious) user to make changes using the Service Connection which would normally require a reviewer.  \nThe configuration of when a pipeline should be triggered is specified in the YAML Pipeline itself and therefore a pipeline can be configured to execute on changes in a temporary branch. In this temporary branch, any changes made to the pipeline itself will be executed without being reviewed.  \nIf the given pipeline has been granted Pipeline-level permissions to use a specific Service Connection, any command can be executed using that Service Connection, without anyone reviewing the command.\nSince Service Connections can have a lot of permissions in the external service, executing any pipeline without review could potentially have big consequences.  \nService Connection Checks  \nTo prevent accidental mis-use of Service Connections there are several checks that can be configured. These checks are configured on the Service Connection itself and therefore can only be configured by the owner or administrator of that Service Connection. A user of a certain YAML Pipeline cannot modify these checks since the checks are not defined in the YAML file itself.\nConfiguration can be done in the Approvals and Checks menu on the Service Connection.  \nBranch Control  \nBy configuring Branch Control on a Service Connection, you can control that the Service Connection can only be used in a YAML Pipeline if the pipeline is running from a specific branch.  \nBy configuring Branch Control to only allow the main branch (and potentially release branches) you can ensure a YAML Pipeline can only use the Service Connection after any changes to that pipeline have been merged into the main branch, and therefore has passed any Pull Requests checks and Code Reviews.\nAs an additional check, Branch Control can verify if Branch Protections (like required Pull Requests and Code Reviews) are actually configured on the allowed branches.  \nWith Branch Control in place, in combination with Branch Protections, it is not possible anymore to run any commands against a Service Connection without having multiple persons review the commands. Therefore accidental, or malicious, mis-use of the permissions a Service Connection has is not possible anymore.  \nNote: When setting a wildcard for the Allowed Branches, anyone could still create a branch matching that wildcard and would be able to use the Service Connection. Using git permissions it can be configured so only administrators are allowed to create certain branches, like release branches.",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\dev-sec-ops\\azure-devops\\service-connection-security.md"
    },
    {
        "chunkId": "chunk102_0",
        "chunkContent": "Dependency and Container Scanning  \nDependency and Container scanning is performed in order to search for vulnerabilities in operating systems, language and application packages.  \nWhy Dependency and Container Scanning  \nContainer images are standard application delivery format in cloud-native environments.\nHaving a broad selection of images from the community, we often choose a community base image, and then add packages that we need to it, which might also come from community sources.\nThose arbitrary dependencies might introduce vulnerabilities to our image and application.  \nApplying Dependency and Container Scanning  \nImages that contain software with security vulnerabilities become exploitable at runtime. When building an image in your CI pipeline, image scanning must be a requirement for a build to pass. Images that did not pass scanning should never be pushed to your production-accessible container registry.  \nDependency and Container scanning best practices:  \nBase Image - if your image is built on top of a third-party base image, validate the following:  \nThe image comes from a well-known company or open-source group.  \nIt is hosted on a reputable registry.  \nThe Dockerfile is available, and check for dependencies installed in it.  \nThe image is frequently updated - old images might not contain the latest security updates.  \nRemove Non-Essential Software - Start with a minimal base image and install only the tools, libraries and configuration files that are required by your application.\nAvoid installing the following tools or remove them if present:\nNetwork tools and clients: e.g., wget, curl, netcat, ssh.\nShells: e.g. sh, bash. Note that removing shells also prevents the use of shell scripts at runtime. Instead, use an executable when possible.\nCompilers and debuggers. These should be used only in build and development containers, but never in production containers.  \nContainer images should be immutable - download and include all the required dependencies during the image build.  \nScan for vulnerabilities in software dependencies -  today there is likely no software project without some form of external libraries, dependencies or open source.\nWhile it allows the development team to focus on their application code, the dependency brings forth an expected downside where the security posture of the real application is now resting on it.\nTo detect vulnerabilities contained within a project\u2019s dependencies use container scanning tools which as part of their analysis scan the software dependencies (see \"Dependency and Container Scanning Frameworks and Tools\").  \nDependency and Container Scanning Frameworks and Tools  \nTrivy - a simple and comprehensive vulnerability scanner for containers (doesn't support Windows containers)  \nAqua - dependency and container scanning for applications running on AKS, ACI and Windows Containers. Has an integration with AzDO pipelines.  \nDependency-Check Plugin for SonarQube - OnPrem dependency scanning  \nMend (previously WhiteSource) - Open Source Scanning Software  \nConclusion  \nA powerful technology such as containers should be used carefully. Install the minimal requirements needed for your application, be aware of the software dependencies your application is using and make sure to maintain it over time by using container and dependencies scanning tools.",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\dev-sec-ops\\dependency-container-scanning\\README.md"
    },
    {
        "chunkId": "chunk103_0",
        "chunkContent": "Penetration Testing  \nA penetration test is a simulated attack against your application to check for exploitable security issues.  \nWhy Penetration Testing  \nPenetration testing performed on a running application. As such, it tests the application E2E with all of its layers. It's output is a real simulated attack on the application that succeeded, therefore it is a critical issue in your application and should be addressed as soon as possible.  \nApplying Penetration Testing  \nMany organizations perform manual penetration testing. But new vulnerabilities found every day. Therefore, it is a good practice to have an automated penetration testing performed.\nTo achieve this automation use penetration testing tools to uncover vulnerabilities, such as unsanitized inputs that are susceptible to code injection attacks.\nInsights provided by the penetration test can then be used to fine-tune your WAF security policies and patch detected vulnerabilities.  \nPenetration Testing Frameworks and Tools  \nOWASP Zed Attack Proxy (ZAP) - OWASP penetration testing tool for web applications.  \nConclusion  \nPenetration testing is essential to check for vulnerabilities in your application and protect it from simulated attacks. Insights provided by Penetration testing can identify weak spots in an organization's security posture, as well as measure the compliance of its security policy, test the staff's awareness of security issues and determine whether -- and how -- the organization would be subject to security disasters.",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\dev-sec-ops\\penetration-testing\\README.md"
    },
    {
        "chunkId": "chunk104_0",
        "chunkContent": "Credential Scanning  \nCredential scanning is the practice of automatically inspecting a project to ensure that no secrets are included in the project's source code. Secrets include database passwords, storage connection strings, admin logins, service principals, etc.  \nWhy Credential scanning  \nIncluding secrets in a project's source code is a significant risk, as it might make those secrets available to unwanted parties. Even if it seems that the source code is accessible to the same people who are privy to the secrets, this situation is likely to change as the project grows. Spreading secrets in different places makes them harder to manage, access control, and revoke efficiently. Secrets that are committed to source control are also harder to discard of, since they will persist in the source's history.  \nAnother consideration is that coupling the project's code to its infrastructure and deployment specifics is limiting and considered a bad practice. From a software design perspective, the code should be independent of the runtime configuration that will be used to run it, and that runtime configuration includes secrets.\nAs such, there should be a clear boundary between code and secrets: secrets should be managed outside of the source code (read more  \nhere) and credential scanning should be employed to ensure that this boundary is never violated.  \nApplying Credential Scanning  \nIdeally, credential scanning should be run as part of a developer's workflow (e.g. via a git pre-commit hook), however, to protect against developer error, credential scanning must also be enforced as part of the continuous integration process to ensure that no credentials ever get merged to a project's main branch.\nTo implement credential scanning for a project, consider the  following:  \nStore secrets in an external secure store that is meant to store sensitive information  \nUse secrets scanning tools to asses your repositories current state by scanning it's full history for secrets  \nIncorporate an automated secrets scanning tool into your CI pipeline to detect unintentional committing of secrets  \nAvoid git add . commands on git  \nAdd sensitive files to .gitignore  \nCredential Scanning Frameworks and Tools  \nRecipes and Scenarios -  \ndetect-secrets is an aptly named module for detecting secrets within a code base.  \nUse detect-secrets inside Azure DevOps Pipeline  \nMicrosoft Security Code Analysis extension  \nAdditional Tools -  \nCodeQL  \u2013 GitHub security. CodeQL lets you query code as if it was data. Write a query to find all variants of a vulnerability  \nGit-secrets  - Prevents you from committing passwords and other sensitive information to a git repository.  \nConclusion  \nSecret management is essential to every project. Storing secrets in external secrets store and incorporating this mindset into your workflow will improve your security posture and will result in cleaner code.",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\dev-sec-ops\\secret-management\\credential_scanning.md"
    },
    {
        "chunkId": "chunk105_0",
        "chunkContent": "Secret management  \nSecret Management refers to the tools and practices used to manage digital authentication credentials (like API keys, tokens, passwords, and certificates). These secrets are used to protect access to sensitive data and services, making their management critical for security.  \nImportance of Secret Management  \nIn modern software development, applications often need to interact with other software components, APIs, and services. These interactions often require authentication, which is typically handled using secrets. If these secrets are not managed properly, they can be exposed, leading to potential security breaches.  \nBest Practices for Secret Management  \nCentralized Secret Storage: Store all secrets in a centralized, encrypted location. This reduces the risk of secrets being lost or exposed.  \nAccess Control: Implement strict access control policies. Only authorized entities should have access to secrets.  \nRotation of Secrets: Regularly change secrets to reduce the risk if a secret is compromised.  \nAudit Trails: Keep a record of when and who accessed which secret. This can help in identifying suspicious activities.  \nAutomated Secret Management: Automate the processes of secret creation, rotation, and deletion. This reduces the risk of human error.  \nRemember, the goal of secret management is to protect sensitive information from unauthorized access and potential security threats.  \nPages  \nCredential Scanning  \nSecrets Rotation  \nStatic code analysis",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\dev-sec-ops\\secret-management\\README.md"
    },
    {
        "chunkId": "chunk106_0",
        "chunkContent": "Secrets Rotation  \nSecret rotation is the process of refreshing the secrets that are used by the application.\nThe best way to authenticate to Azure services is by using a managed identity, but there are some scenarios where that isn't an option. In those cases, access keys or secrets are used. You should periodically rotate access keys or secrets.  \nWhy Secrets Rotation  \nSecrets are an asset and as such have a potential to be leaked or stolen. By rotating the secrets, we are revoking any secrets that may have been compromised. Therefore, secrets should be rotated frequently.  \nManaged Identity  \nAzure Managed identities are automatically issues by Azure in order to identify individual resources, and can be used for authentication in place of secrets and passwords. The appeal in using Managed Identities is the elimination of management of secrets and credentials. They are not required on developers machines or checked into source control, and they don't need to be rotated. Managed identities are considered safer than the alternatives and is the recommended choice.  \nApplying Secrets Rotation  \nIf Azure Managed Identity can't be used. This and the following sections will explain how rotation of secrets can be achieved:  \nTo promote frequent rotation of a secret - define an automated periodic secret rotation process.\nThe secret rotation process might result in a downtime when the application is restarted to introduce the new secret. A common solution for that is to have two versions of secret available, also referred to as Blue/Green Secret rotation. By having a second secret at hand, we can start a second instance of the application with that secret before the previous secret is revoked, thus avoiding any downtime.  \nSecrets Rotation Frameworks and Tools  \nFor rotation of a secret for resources that use one set of authentication credentials click here  \nFor rotation of a secret for resources that have two sets of authentication credentials click here  \nConclusion  \nRefreshing secrets is important to ensure that your secret stays a secret without causing downtime to your application.",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\dev-sec-ops\\secret-management\\secrets_rotation.md"
    },
    {
        "chunkId": "chunk107_0",
        "chunkContent": "Static Code Analysis  \nStatic code analysis is a method of detecting security issues by examining the source code of the application.  \nWhy Static Code Analysis  \nCompared to code reviews, Static code analysis tools are more fast, accurate and through.\nAs it operates on the source code itself, it is a very early indicator for issues, and coding errors found earlier are less costly to fix.  \nApplying Static Code Analysis  \nStatic Code Analysis should be integrated in your build process.\nThere are many tools available for Static Code Analysis, choose the ones that meet your programming language and development techniques.  \nStatic Code Analysis Frameworks and Tools  \nSonarCloud - static code analysis with cloud-based software as a service product.\nOWASP Source code Analysis - OWASP recommendations for source code analysis tools  \nConclusion  \nStatic code analysis is essential to identify potential problems and security issues in the code. It allows you to detect bugs and security issues at an early stage.",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\dev-sec-ops\\secret-management\\static-code-analysis.md"
    },
    {
        "chunkId": "chunk108_0",
        "chunkContent": "Running detect-secrets in Azure DevOps Pipelines  \nOverview  \nIn this article, you can find information on how to integrate YELP detect-secrets into your Azure DevOps Pipeline. The proposed code can be part of the classic CI process or (preferred way) build validation for PRs before merging to the main branch.  \nAzure DevOps Pipeline  \nProposed Azure DevOps Pipeline contains multiple steps described below:  \nSet Python 3 as default  \nInstall detect-secrets using pip  \nRun detect-secrets tool  \nPublish results in the Pipeline Artifact\nNOTE: It's an optional step, but for future investigation .json file with results may be helpful.  \nAnalyzing detect-secrets results\nNOTE: This step does a simple analysis of the .json file. If any secret has been detected, then break the build with exit code 1.  \nNOTE: The below example has 2 jobs: for Linux and Windows agents. You do not have to use both jobs - just adjust the pipeline to your needs.  \nNOTE: Windows example does not use the latest version of detect-secrets. It is related to the bug in the detect-secret tool (see more in Issue#452). It is highly recommended to monitor the fix for the issue and use the latest version if possible by removing version tag ==1.0.3 in the pip install command.  \n{% raw %}  \n```yaml\ntrigger:\n- none\n\njobs:\n- job: ubuntu\ndisplayName: \"detect-secrets on Ubuntu Linux agent\"\npool:\nvmImage: ubuntu-latest\nsteps:\n- task: UsePythonVersion@0\ndisplayName: \"Set Python 3 as default\"\ninputs:\nversionSpec: \"3\"\naddToPath: true\narchitecture: \"x64\"\n\njob: windows\ndisplayName: \"detect-secrets on Windows agent\"\npool:\nvmImage: windows-latest\nsteps:\n\n\ntask: UsePythonVersion@0\ndisplayName: \"Set Python 3 as default\"\ninputs:\nversionSpec: \"3\"\naddToPath: true\narchitecture: \"x64\"\n\n\nscript: pip install detect-secrets==1.0.3\ndisplayName: \"Install detect-secrets using pip\"\n\n\nscript: |\ndetect-secrets --version\ndetect-secrets scan --all-files --force-use-all-plugins > $(Pipeline.Workspace)/detect-secrets.json\ndisplayName: \"Run detect-secrets tool\"\n\n\ntask: PublishPipelineArtifact@1\ndisplayName: \"Publish results in the Pipeline Artifact\"\ninputs:\ntargetPath: \"$(Pipeline.Workspace)/detect-secrets.json\"\nartifact: \"detect-secrets-windows\"\npublishLocation: \"pipeline\"\n\n\npwsh: |\n$dsjson = Get-Content $(Pipeline.Workspace)/detect-secrets.json\nWrite-Output $dsjson\n$dsObj = $dsjson | ConvertFrom-Json\n$count = ($dsObj.results | Get-Member -MemberType NoteProperty).Count\nif ($count -gt 0) {\n$msg = \"Secrets were detected in code. $count file(s) affected. \"\nWrite-Host \"##vso[task.logissue type=error]$msg\"\nWrite-Host \"##vso[task.complete result=Failed;]$msg\"\n}\nelse {\nWrite-Host \"##vso[task.complete result=Succeeded;]No secrets detected.\"\n}\ndisplayName: \"Analyzing detect-secrets results\"\n```  \n{% endraw %}",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\dev-sec-ops\\secret-management\\recipes\\detect-secrets-ado.md"
    },
    {
        "chunkId": "chunk109_0",
        "chunkContent": "Credential Scanning Tool: detect-secrets  \nBackground  \nThe detect-secrets tool is an open source project that uses heuristics and rules to scan for a wide range of secrets. We can extend the tool with custom rules and heuristics via a simple Python plugin API.  \nUnlike other credential scanning tools, detect-secrets does not attempt to check a project's entire git history when invoked, but instead scans the project's current state. This means that the tool runs quickly which makes it ideal for use in continuous integration pipelines.  \ndetect-secrets employs the concept of a \"baseline file\", i.e. a list of known secrets already present in the repository, and we can configure it to ignore any of these pre-existing secrets when running. This makes it easy to gradually introduce the tool into a pre-existing project.  \nThe baseline file also provides a simple and convenient way of handling false positives. We can white-list the false positive in the baseline file to ignore it on future invocations of the tool.  \nSetup  \n{% raw %}  \n```sh\n\ninstall system dependencies: diff, jq, python3 (if on Linux-based OS)\n\napt-get install -y diffutils jq python3 python3-pip\n\ninstall system dependencies: diff, jq, python3 (if on Windows)\n\nwinget install Python.Python.3\nchoco install diffutils jq -y\n\ninstall the detect-secrets tool\n\npython3 -m pip install detect-secrets\n\nrun the tool to establish a list of known secrets\n\nreview this file thoroughly and check it into the repository\n\ndetect-secrets scan > .secrets.baseline\n```  \n{% endraw %}  \nPre-commit hook  \nIt is recommended to use detect-secrets in your development environment as a Git pre-commit hook.  \nFirst, follow the pre-commit installation instructions to install the tool in your development environment.  \nThen, add the following to your .pre-commit-config.yaml:  \n{% raw %}  \nyaml\nrepos:\n-   repo: https://github.com/Yelp/detect-secrets\nrev: v1.4.0\nhooks:\n-   id: detect-secrets\nargs: ['--baseline', '.secrets.baseline']  \n{% endraw %}  \nUsage in CI pipelines  \n{% raw %}  \n```sh\n\nbackup the list of known secrets\n\ncp .secrets.baseline .secrets.new\n\nfind all the secrets in the repository\n\ndetect-secrets scan --baseline .secrets.new $(find . -type f ! -name '.secrets.' ! -path '/.git*')\n\nif there is any difference between the known and newly detected secrets, break the build\n\nlist_secrets() { jq -r '.results | keys[] as $key | \"($key),(.[$key] | .[] | .hashed_secret)\"' \"$1\" | sort; }\n\nif ! diff <(list_secrets .secrets.baseline) <(list_secrets .secrets.new) >&2; then\necho \"Detected new secrets in the repo\" >&2\nexit 1\nfi\n```  \n{% endraw %}",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\dev-sec-ops\\secret-management\\recipes\\detect-secrets.md"
    },
    {
        "chunkId": "chunk110_0",
        "chunkContent": "Recipes  \nDetect secrets  \nDetect secrets on Azure DevOps",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\dev-sec-ops\\secret-management\\recipes\\README.md"
    },
    {
        "chunkId": "chunk111_0",
        "chunkContent": "Reusing dev containers within a pipeline  \nGiven a repository with a local development container aka dev container that contains all the tooling required for development, would it make sense to reuse that container for running the tooling in the Continuous Integration pipelines?  \nOptions for building devcontainers within pipeline  \nThere are three ways to build devcontainers within pipeline:  \nWith GitHub - devcontainers/ci builds the container with the devcontainer.json. Example here: devcontainers/ci \u00b7 Getting Started.  \nWith GitHub - devcontainers/cli, which is the same as the above, but using the underlying CLI directly without tasks.  \nBuilding the DockerFile with docker build. This option excludes all configuration/features specified within the devcontainer.json.  \nConsidered Options  \nRun CI pipelines in native environment  \nRun CI pipelines in the dev container via building image locally  \nRun CI pipelines in the dev container with a container registry  \nHere are below pros and cons for both approaches:  \nRun CI pipelines in native environment  \nPros Cons Can use any pipeline tasks available Need to keep two sets of tooling and their versions in sync No container registry Can take some time to start, based on tools/dependencies required Agent will always be up to date with security patches The dev container should always be built within each run of the CI pipeline, to verify the changes within the branch haven't broken anything  \nRun CI pipelines in the dev container without image caching  \nPros Cons Utilities scripts will work out of the box Need to rebuild the container for each run, given that there may be changes within the branch being built Rules used (for linting or unit tests) will be the same on the CI Not everything in the container is needed for the CI pipeline\u00b9 No surprise for the developers, local outputs (of linting for instance) will be the same in the CI Some pipeline tasks will not be available All tooling and their versions defined in a single place Building the image for each pipeline run is slow\u00b2 Tools/dependencies are already present The dev container is being tested to include all new tooling in addition to not being broken  \n\u00b9: container size can be reduces by exporting the layer that contains only the tooling needed for the CI pipeline  \n\u00b2: could be mitigated via adding image caching without using a container registry  \nRun CI pipelines in the dev container with image registry  \nPros Cons Utilities scripts will work out of the box Need to rebuild the container for each run, given that there may be changes within the branch being built No surprise for the developers, local outputs (of linting for instance) will be the same in the CI Not everything in the container is needed for the CI pipeline\u00b9 Rules used (for linting or unit tests) will be the same on the CI Some pipeline tasks will not be available   \u00b2 All tooling and their versions defined in a single place Require access to a container registry to host the container within the pipeline\u00b3 Tools/dependencies are already present The dev container is being tested to include all new tooling in addition to not being broken Publishing the container built from devcontainer.json allows you to reference it in the cacheFrom in devcontainer.json (see docs ). By doing this, VS Code will use the published image as a layer cache when building  \n\u00b9: container size can be reduces by exporting the layer that contains only the tooling needed for the CI pipeline. This would require building the image without tasks  \n\u00b2: using container jobs in AzDO you can use all tasks (as far as I can tell). Reference: Dockerizing DevOps V2 - AzDO container jobs - DEV Community  \n\u00b3: within GH actions, the default Github Actions token can be used for accessing GHCR without setting up separate registry, see the example below.\nNOTE: This does not build the Dockerfile together with the devcontainer.json  \n{% raw %}  \nyaml\n- uses: whoan/docker-build-with-cache-action@v5\nid: cache\nwith:\nusername: $GITHUB_ACTOR\npassword: \"${{ secrets.GITHUB_TOKEN }}\"\nregistry: docker.pkg.github.com\nimage_name: devcontainer\ndockerfile: .devcontainer/Dockerfile  \n{% endraw %}",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\devcontainers\\README.md"
    },
    {
        "chunkId": "chunk112_0",
        "chunkContent": "CI Pipeline for better documentation  \nIntroduction  \nMost projects start with spikes, where developers and analysts produce lots of documentation.  \nSometimes, these documents don't have a standard and each team member writes them accordingly with their preference. Add to that\nthe time a reviewer will spend confirming grammar, searching for typos or non-inclusive language.  \nThis pipeline helps address that!  \nThe Pipeline  \nThe pipeline uses the following npm modules:  \nmarkdownlint: add standardization using rules  \nmarkdown-link-check: check the links in the documentation and report broken\nones  \nwrite-good: linter for English prose  \nWe have been using this pipeline for more than one year in different engagements and always received great feedback from the\ncustomers!  \nHow does it work  \nTo start using this pipeline:  \nDownload the files from this repository  \nUnzip the folders and files to your repository root if the repository is empty\nif it's not brand new, copy the files and make the required adjustments:\ncheck .azdo so it matches your repository standard\ncheck package.json so you don't overwrite one you already have in the process. Also update the file if you changed\nthe name of the .azdo folder.  \nCreate the pipeline in Azure DevOps or GitHub  \nReferences  \nMarkdown Code Reviews in the Code With Engineering Playbook",
        "source": "..\\data\\docs\\code-with-engineering\\continuous-integration\\markdown-linting\\README.md"
    },
    {
        "chunkId": "chunk113_0",
        "chunkContent": "Design  \nDesigning software well is hard.  \nISE has collected a number of practices which we find help in the design process.\nThis covers not only technical design of software, but also architecture design and non-functional requirements gathering for new projects.  \nGoals  \nProvide recommendations for how to design software for maintainability, ease of extension, adherence to best practices, and sustainability.  \nReference or define process or checklists to help ensure well-designed software.  \nCollate and point to reference sources (guides, repos, articles) that can help shortcut the learning process.  \nSections  \nDiagram Types  \nDesign Patterns  \nDesign Reviews  \nNon-Functional Requirements Guidance  \nSustainable Software Engineering  \nRecipes  \nDesign Recipes  \nCode Examples  \nFolder Structure  \nFolder Structure For Python Repository  \nProject Templates  \nRust\nActix Web, Diesel ORM, Test Containers, Onion Architecture  \nPython\nFlask, SQLAlchemy ORM, Test Containers, Onion Architecture",
        "source": "..\\data\\docs\\code-with-engineering\\design\\readme.md"
    },
    {
        "chunkId": "chunk114_0",
        "chunkContent": "Cloud Resource Design Guidance  \nAs cloud usage scales, considerations for subscription design, management groups, and resource naming/tagging conventions have an impact on governance, operations management, and adoption patterns.  \nNOTE: Always work with the relevant stakeholders to ensure that introducing new patterns provides the intended value.  \nWhen working in an existing cloud environment, it is important to understand any current patterns and how they are used before making a change to them.  \nReferences  \nThe following references can be used to understand the latest best practices in organizing cloud resources:  \nOrganizing Subscriptions  \nResource Tagging Decision Guide  \nResource Naming Conventions  \nRecommended Azure Resource Abbreviations  \nOrganizing Dev/Test/Production Workloads  \nTooling  \nAzure Resource Naming Tool",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-patterns\\cloud-resource-design-guidance.md"
    },
    {
        "chunkId": "chunk115_0",
        "chunkContent": "Data and DataOps Fundamentals  \nMost projects involve some type of data storage, data processing and data ops. For these projects, as with all projects, we follow the general guidelines laid out in other sections around security, testing, observability, CI/CD etc.  \nGoal  \nThe goal of this section is to briefly describe how to apply the fundamentals to data heavy projects or portions of the project.  \nIsolation  \nPlease be cautious of which isolation levels you are using. Even with a database that offers serializability, it is possible that within a transaction or connection you are leveraging a lower isolation level than the database offers. In particular, read uncommitted (or eventual consistency), can have a lot of unpredictable side effects and introduce bugs that are difficult to reason about. Eventually consistent systems should be treated as a last resort for achieving your scalability requirements; batching, sharding, and caching are all recommended solutions to increase your scalability. If none of these options are tenable, consider evaluating the \"New SQL\" databases like CockroachDB or TiDB, before leveraging an option that relies on eventual consistency.  \nThere are other levels of isolation, outside the isolation levels mentioned in the link above. Some of these have nuances different from the 4 main levels, and can be difficult to compare. Snapshot Isolation, strict serializability, \"read your own writes\", monotonic reads, bounded staleness, causal consistency, and linearizability are all other terms you can look into to learn more on the subject.  \nConcurrency Control  \nYour systems should (almost) always leverage some form of concurrency control, to ensure correctness amongst competing requests and to prevent data races. The 2 forms of concurrency control are pessimistic and optimistic.  \nA pessimistic transaction involves a first request to \"lock the data\", and a second request to write the data. In between these requests, no other requests touching that data will succeed. See 2 Phase Locking (also often known as 2 Phase Commit) for more info.  \nThe (more) recommended approach is optimistic concurrency, where a user can read the object at a specific version, and update the object if and only if it hasn't changed. This is typically done via the Etag Header.  \nA simple way to accomplish this on the database side is to increment a version number on each update. This can be done in a single executed statement as:  \nWARNING: the below will not work when using an isolation level at or lower than read uncommitted (eventual consistency).  \n{% raw %}  \n```SQL\n-- Please treat this as pseudo code, and adjust as necessary.\n\nUPDATE\nSET field1 = value1, ..., fieldN = valueN, version = $new_version\nWHERE ID = $id AND version = $version\n```  \n{% endraw %}  \nData Tiering (Data Quality)  \nDevelop a common understanding of the quality of your datasets so that everyone understands the quality of the data, and expected use cases and limitations.  \nA common data quality model is Bronze, Silver, Gold  \nBronze: This is a landing area for your raw datasets with none or minimal data transformations applied, and therefore are optimized for writes / ingestion. Treat these datasets as an immutable, append only store.  \nSilver: These are cleansed, semi-processed datasets. These conform to a known schema and predefined data invariants and might have further data augmentation applied. These are typically used by data scientists.  \nGold: These are highly processed, highly read-optimized datasets primarily for consumption of business users. Typically, these are structured in your standard fact and dimension tables.  \nDivide your data lake into three major areas containing your Bronze, Silver and Gold datasets.  \nNote: Additional storage areas for malformed data, intermediate (sandbox) data, and libraries/packages/binaries are also useful when designing your storage organization.  \nData Validation  \nValidate data early in your pipeline  \nAdd data validation between the Bronze and Silver datasets. By validating early in your pipeline, you can ensure all datasets conform to a specific schema and known data invariants. This can also potentially prevent data pipeline failures in case of unexpected changes to the input data.  \nData that does not pass this validation stage can be rerouted to a record store dedicated for malformed data for diagnostic purposes.  \nIt may be tempting to add validation prior to landing in the Bronze area of your data lake. This is generally not recommended. Bronze datasets are there to ensure you have as close of a copy of the source system data. This can be used to replay the data pipeline for both testing (i.e. testing data validation logic) and data recovery purposes (i.e. data corruption is introduced due to a bug in the data transformation code and thus the pipeline needs to be replayed).  \nIdempotent Data Pipelines  \nMake your data pipelines re-playable and idempotent  \nSilver and Gold datasets can get corrupted due to a number of reasons such as unintended bugs, unexpected input data changes, and more. By making data pipelines re-playable and idempotent, you can recover from this state through deployment of code fixes, and re-playing the data pipelines.  \nIdempotency also ensures data-duplication is mitigated when replaying your data pipelines.  \nTesting  \nEnsure data transformation code is testable  \nAbstracting away data transformation code from data access code is key to ensuring unit tests can be written against data transformation logic. An example of this is moving transformation code from notebooks into packages.  \nWhile it is possible to run tests against notebooks, by extracting the code into packages, you increase the developer productivity by increasing the speed of the feedback cycle.  \nCI/CD, Source Control and Code Reviews  \nAll artifacts needed to build the data pipeline from scratch should be in source control. This included infrastructure-as-code artifacts, database objects (schema definitions, functions, stored procedures etc.), reference/application data, data pipeline definitions and data validation and transformation logic.  \nAny new artifacts (code) introduced to the repository should be code reviewed, both automatically (linting, credential scanning etc.) and peer reviewed.  \nThere should be a safe, repeatable process (CI/CD) to move the changes through dev, test and finally production.  \nSecurity and Configuration  \nMaintain a central, secure location for sensitive configuration such as database connection strings that can be accessed by the appropriate services within the specific environment.  \nOn Azure this is typically solved through securing secrets in a Key Vault per environment, then having the relevant services query KeyVault for the configuration  \nObservability  \nMonitor infrastructure, pipelines and data  \nA proper monitoring solution should be in-place to ensure failures are identified, diagnosed and addressed in a timely manner. Aside from the base infrastructure and pipeline runs, data should also be monitored. A common area that should have data monitoring is the malformed record store.  \nEnd to End and Azure Technology Samples  \nThe DataOps for the Modern Data Warehouse repo contains both end-to-end and technology specific samples on how to implement DataOps on Azure.  \nImage: CI/CD for Data pipelines on Azure - from DataOps for the Modern Data Warehouse repo",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-patterns\\data-heavy-design-guidance.md"
    },
    {
        "chunkId": "chunk116_0",
        "chunkContent": "Distributed System Design Reference  \nDistributed systems introduce new and interesting problems that need addressing.\nSoftware engineering as a field has dealt with these problems for years, and there are phenomenal resources available for reference when creating a new distributed system.\nSome that we recommend are as follows:  \nMartin Fowler's Patterns of Distributed Systems  \nmicroservices.io  \nAzure's Cloud Design Patterns",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-patterns\\distributed-system-design-reference.md"
    },
    {
        "chunkId": "chunk117_0",
        "chunkContent": "Network Architecture Guidance for Azure  \nThe following are some best practices when setting up and working with network resources in Azure Cloud environments.  \nNOTE: When working in an existing cloud environment, it is important to understand any current patterns, and how they are used, before making a change to them. You should also work with the relevant stakeholders to make sure that any new patterns you introduce provide enough value to make the change.  \nNetworking and VNet setup  \nHub-and-spoke Topology  \nA hub-and-spoke network topology is a common architecture pattern used in Azure for organizing and managing network resources. It is based on the concept of a central hub that connects to various spoke networks. This model is particularly useful for organizing resources, maintaining security, and simplifying network management.  \nThe hub-and-spoke model is implemented using Azure Virtual Networks (VNet) and VNet peering.  \nThe hub: The central VNet acts as a hub, providing shared services such as network security, monitoring, and connectivity to on-premises or other cloud environments. Common components in the hub include Network Virtual Appliances (NVAs), Azure Firewall, VPN Gateway, and ExpressRoute Gateway.  \nThe spokes: The spoke VNets represent separate units or applications within an organization, each with its own set of resources and services. They connect to the hub through VNet peering, which allows for communication between the hub and spoke VNets.  \nImplementing a hub-and-spoke model in Azure offers several benefits:  \nIsolation and segmentation: By dividing resources into separate spoke VNets, you can isolate and segment workloads, preventing any potential issues or security risks from affecting other parts of the network.  \nCentralized management: The hub VNet acts as a single point of management for shared services, making it easier to maintain, monitor, and enforce policies across the network.  \nSimplified connectivity: VNet peering enables seamless communication between the hub and spoke VNets without the need for complex routing or additional gateways, reducing latency and management overhead.  \nScalability: The hub-and-spoke model can easily scale to accommodate additional spokes as the organization grows or as new applications and services are introduced.  \nCost savings: By centralizing shared services in the hub, organizations can reduce the costs associated with deploying and managing multiple instances of the same services across different VNets.  \nRead more about hub-and-spoke topology  \nWhen deploying hub/spoke, it is recommended that you do so in connection with landing zones. This ensures consistency across all environments as well as guardrails to ensure a high level of security while giving developers freedom within development environments.  \nFirewall and Security  \nWhen using a hub-and-spoke topology it is possible to deploy a centralized firewall in the Hub that all outgoing traffic or traffic to/from certain VNets, this allows for centralized threat protection while minimizing costs.  \nDNS  \nThe best practices for handling DNS in Azure, and in cloud environments in general, include using managed DNS services. Some of the benefits of using managed DNS services is that the resources are designed to be secure, easy to deploy and configure.  \nDNS forwarding: Set up DNS forwarding between your on-premises DNS servers and Azure DNS servers for name resolution across environments.  \nUse Azure Private DNS zones for Azure resources: Configure Azure Private DNS zones for your Azure resources to ensure name resolution is kept within the virtual network.  \nRead more about Hybrid/Multi-Cloud DNS infrastructure and Azure DNS infrastructure  \nIP Allocation  \nWhen allocating IP address spaces to Azure Virtual Networks (VNets), it's essential to follow best practices for proper management, and scalability.  \nHere are some recommendations for IP allocation to VNets:  \nReserve IP addresses: Reserve IP addresses in your address space for critical resources or services.  \nPublic IP allocation: Minimize the use of public IP addresses and use Azure Private Link when possible to access services over a private connection.  \nIP address management (IPAM): Use IPAM solutions to manage and track IP address allocation across your hybrid environment.  \nPlan your address space: Choose an appropriate private address space (from RFC 1918) for your VNets that is large enough to accommodate future growth. Avoid overlapping with on-premises or other cloud networks.  \nUse CIDR notation: Use Classless Inter-Domain Routing (CIDR) notation to define the VNet address space, which allows more efficient allocation and prevents wasting IP addresses.  \nUse subnets: Divide your VNets into smaller subnets based on security, application, or environment requirements. This allows for better network management and security.  \nConsider leaving a buffer between VNets: While it's not strictly necessary, leaving a buffer between VNets can be beneficial in some cases, especially when you anticipate future growth or when you might need to merge VNets. This can help avoid re-addressing conflicts when expanding or merging networks.  \nReserve IP addresses: Reserve a range of IP addresses within your VNet address space for critical resources or services. This ensures that they have a static IP address, which is essential for specific services or applications.  \nPlan for hybrid scenarios: If you're working in a hybrid environment with on-premises or multi-cloud networks, ensure that you plan for IP address allocation across all environments. This includes avoiding overlapping address spaces and reserving IP addresses for specific resources like VPN gateways or ExpressRoute circuits.  \nRead more at azure-best-practices/plan-for-ip-addressing  \nResource Allocation  \nFor resource allocation the best practices from Cloud Resource Design Guidance should be followed.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-patterns\\network-architecture-guidance-for-azure.md"
    },
    {
        "chunkId": "chunk118_0",
        "chunkContent": "Network Architecture Guidance for Hybrid  \nThe following are best practices around how to design and configure resources, used for Hybrid and Multi-Cloud environments.  \nNOTE: When working in an existing hybrid environment, it is important to understand any current patterns, and how they are used before making any changes.  \nHub-and-spoke Topology  \nThe hub-and-spoke topology doesn't change much when using cloud/hybrid if configured correctly, The main different is that the hub VNet is peering to the on-prem network via a ExpressRoute and that all traffic from Azure might exit via the ExpressRoute and the on-prem internet connection.  \nThe generalized best practices are in  Network Architecture Guidance for Azure#Hub and Spoke topology  \nIP Allocation  \nWhen working with Hybrid deployment, take extra care when planning IP allocation as there is a much greater risk of overlapping network ranges.  \nThe general best practices are available in the Network Architecture Guidance for Azure#ip-allocation  \nRead more about this in Azure Best Practices Plan for IP Addressing  \nExpressRoute  \nEnvironments using Express often tunnel all traffic from Azure via a private link (ExpressRoute) to an on-prem location. This imposes a few problems when working with PAAS offerings as not all of them connect via their respective private endpoint and instead use an external IP for outgoing connections, or some PAAS to PASS traffic occur internally in azure and won't function with disabled public networks.  \nTwo notable services here are data planes copies of storage accounts and a lot of the services not supporting private endpoints.  \nChoose the right ExpressRoute circuit: Select an appropriate SKU (Standard or Premium) and bandwidth based on your organization's requirements.\nRedundancy: Ensure redundancy by provisioning two ExpressRoute circuits in different peering locations.\nMonitoring: Use Azure Monitor and Network Performance Monitor (NPM) to monitor the health and performance of your ExpressRoute circuits.  \nDNS  \nGeneral best practices are available in Network Architecture Guidance for Azure#dns  \nWhen using Azure DNS in a hybrid or multi-cloud environment it is important to ensure a consistent DNS and forwarding configuration which ensures that records are automatically updated and that all DNS servers are aware of each other and know which server is the authoritative for the different records.  \nRead more about Hybrid/Multi-Cloud DNS infrastructure  \nResource Allocation  \nFor resource allocation the best practices from Cloud Resource Design Guidance should be followed.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-patterns\\network-architecture-guidance-for-hybrid.md"
    },
    {
        "chunkId": "chunk119_0",
        "chunkContent": "Non-Functional Requirements Capture  \nGoals  \nIn software engineering projects, important characteristics of the system providing for necessary e.g., testability, reliability, scalability, observability, security, manageability are best considered as first-class citizens in the requirements gathering process.\nBy defining these non-functional requirements in detail early in the engagement, they can be properly evaluated when the cost of their impact on subsequent design decisions is comparatively low.  \nTo support the process of capturing a project's comprehensive non-functional requirements, this document offers a taxonomy for non-functional requirements and provides a framework for their identification, exploration, assignment of customer stakeholders, and eventual codification into formal engineering requirements as input to subsequent solution design.  \nAreas of Investigation  \nEnterprise Security  \nPrivacy  \nPII  \nHIPAA  \nEncryption  \nData mobility  \nat rest  \nin motion  \nin process/memory  \nKey Management  \nresponsibility\nplatform\nBYOK\nCMK  \nINFOSEC regulations/standards  \ne.g., FIPS-140-2\nLevel 2\nLevel 3  \nISO 27000 series  \nNIST  \nOther  \nNetwork security  \nPhysical/Logical traffic boundaries/flow topology\nAzure <-- --> On-prem\nPublic <-- --> Azure\nVNET\nPIP\nFirewalls\nVPN\nExpressRoute\nTopology\nSecurity  \nCertificates\nIssuer\nCA\nSelf-signed\nRotation/expiry  \nINFOSEC Incident Response  \nProcess  \nPeople  \nResponsibilities  \nSystems  \nLegal/Regulatory/Compliance  \nEnterprise AuthN/AuthZ  \nUsers  \nServices  \nAuthorities/directories  \nMechanisms/handshakes  \nActive Directory  \nSAML  \nOAuth  \nOther  \nRBAC  \nPerms inheritance model  \nEnterprise Monitoring/Operations  \nLogging  \nOperations  \nReporting  \nAudit  \nMonitoring  \nDiagnostics/Alerts  \nOperations  \nHA/DR  \nRedundancy  \nRecovery/Mitigation  \nPractices  \nPrinciple of least-privilege  \nPrinciple of separation-of-responsibilities  \nOther standard Enterprise technologies/practices  \nDeveloper ecosystem  \nPlatform/OS\nHardened\nApproved base images\nImage repository  \nTools, languages\nApproval process  \nCode repositories\nSecrets management patterns\nEnv var\nConfig file(s)\nSecrets retrieval API  \nPackage manager source(s)\nPrivate\nPublic\nApproved/Trusted  \nCI/CD  \nArtifact repositories  \nProduction ecosystem  \nPlatform/OS  \nHardened  \nApproved base images  \nImage repository  \nDeployment longevity/volatility  \nAutomation  \nReproducibility\nIaC\nScripting\nOther  \nOther areas/topics not addressed above (requires customer input to comprehensively enumerate)  \nInvestigation Process  \nIdentify/brainstorm likely areas/topics requiring further investigation/definition  \nIdentify customer stakeholder(s) responsible for each identified area/topic  \nSchedule debrief/requirements definition session(s) with each stakeholder  \nas necessary to achieve sufficient understanding of the probable impact of each requirement to the project  \nboth current/initial milestone and long-term/road map  \nDocument requirements/dependencies identified and related design constraints  \nEvaluate current/near-term planned milestone(s) through the lens of the identified requirements/constraints  \nCategorize each requirement as affecting immediate/near-term milestone(s) or as applicable instead to the longer-term road map/subsequent milestones  \nAdapt plans for current/near-term milestone(s) to accommodate immediate/near-term-categorized requirements  \nStructure of Outline/Assignment of Responsible Stakeholder  \nIn the following outline, assign name/email of 'responsible stakeholder' for each element after the appropriate level in the outline hierarchy. Assume inheritance model of responsibility assignment: stakeholder at any ancestor (parent) level is also responsible for descendent (child) elements unless overridden at the descendent level).  \ne.g.,  \nParent1 [Susan/susan@domain.com]  \nchild1  \nchild2 [John/john@domain.com]\ngrandchild1  \nchild3  \nParent2 [Sam/sam@domain.com]  \nchild1  \nchild2  \nIn the preceding example, 'Susan' is responsible for Parent1 and all of its descendants except for Parent1/child2 and Parent1/child2/grandchild1 (for which 'John' is the stakeholder). 'Sam' is responsible for the entirety of Parent2 and all of its descendants.  \nThis approach permits the retention of the logical hierarchy of elements themselves while also flexibly interleaving the 'stakeholder' identifications within the hierarchy of topics if/when they may need to diverge due to e.g., customer organizational nuances.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-patterns\\non-functional-requirements-capture-guide.md"
    },
    {
        "chunkId": "chunk120_0",
        "chunkContent": "Object-Oriented Design Reference  \nWhen writing software for large projects, the hardest part is often communication and maintenance.\nFollowing proven design patterns can optimize for maintenance, readability, and ease of extension.\nIn particular, object-oriented design patterns are well-established in the industry.  \nPlease refer to the following resources to create strong object-oriented designs:  \nDesign Patterns Wikipedia  \nObject Oriented Design Website",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-patterns\\object-oriented-design-reference.md"
    },
    {
        "chunkId": "chunk121_0",
        "chunkContent": "Design Patterns  \nThe design patterns section recommends patterns of software and architecture design.\nThis section provides a curated list of commonly used patterns from trusted sources.\nRather than duplicate or replace the cited sources, this section aims to compliment them with suggestions, guidance, and learnings based on firsthand experiences.  \nSubsections  \nData Heavy Design Guidance  \nObject Oriented Design Reference  \nDistributed System Design Reference  \nREST API Design Guidance  \nCloud Resource Design Guidance  \nNetwork Architecture Guidance for Azure  \nNetwork Architecture Guidance for Hybrid",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-patterns\\README.md"
    },
    {
        "chunkId": "chunk122_0",
        "chunkContent": "REST API Design Guidance  \nGoals  \nElevate Microsoft's published REST API design guidelines.  \nHighlight common design decisions and factors to consider when designing.  \nProvide additional resources to inform API design in areas not directly addressed by the Microsoft guidelines.  \nCommon API Design Decisions  \nThe Microsoft REST API guidelines provide design guidance covering a multitude of use-cases.\nThe following sections are a good place to start as they are likely required considerations by any REST API design:  \nURL Structure  \nHTTP Methods  \nHTTP Status Codes  \nCollections  \nJSON Standardizations  \nVersioning  \nNaming  \nCreating API Contracts  \nAs different development teams expose APIs to access various REST based services, it's important to have an API contract to share between the producer and consumers of APIs. Open API format is one of the most popular API description format. This Open API document can be produced in two ways:  \nDesign-First - Team starts developing APIs by first describing API designs as an Open API document and later generates server side boilerplate code with the help of this document.  \nCode-First - Team starts writing the server side API interface code e.g. controllers, DTOs etc. and later generates and Open API document from it.  \nDesign-First Approach  \nA Design-First approach means that APIs are treated as \"first-class citizens\" and everything about a project revolves around the idea that at the end these APIs will be consumed by clients. So based on the business requirements API development team first start describing API designs as an Open API document and collaborate with the stakeholders to gather feedback.  \nThis approach is quite useful if a project is about developing externally exposed set of APIs which will be consumed by partners. In this approach, we first agree upon an API contract (Open API document) creating clear expectations on both API producer and consumer sides so both teams can begin work in parallel as per the pre-agreed API design.  \nKey Benefits of this approach:  \nEarly API design feedback.  \nClearly established expectations for both consumer & producer as both have agreed upon an API contract.  \nDevelopment teams can work in parallel.  \nTesting team can use API contracts to write early tests even before business logic is in place. By looking at different models, paths, attributes and other aspects of the API testing can provide their input which can be very valuable.  \nDuring an agile development cycle API definitions are not impacted by incremental dev changes.  \nAPI design is not influenced by actual implementation limitations & code structure.  \nServer side boilerplate code e.g. controllers, DTOs etc. can be auto generated from API contracts.  \nMay improve collaboration between API producer & consumer teams.  \nPlanning a Design-First Development:  \nIdentify use cases & key services which API should offer.  \nIdentify key stakeholders of API and try to include them during API design phase to get continuous feedback.  \nWrite API contract definitions.  \nMaintain consistent style for API status codes, versioning, error responses etc.  \nEncourage peer reviews via pull requests.  \nGenerate server side boilerplate code & client SDKs from API contract definitions.  \nImportant Points to consider:  \nIf API requirements changes often during initial development phase, than a Design-First approach may not be a good fit as this will introduce additional overhead, requiring repeated updates & maintenance to the API contract definitions.  \nIt might be worthwhile to first try out your platform specific code generator and evaluate how much more additional work will be required in order to meet your project requirements and coding guidelines because it is possible that a particular platform specific code generator might not be able to generate a flexible & maintainable implementation of actual code. For instance If your web framework requires annotations to be present on your controller classes (e.g. for API versioning or authentication), make sure that the code generation tool you use fully supports them.  \nMicrosoft TypeSpec is a valuable tool for developers who are working on complex APIs. By providing reusable patterns it can streamline API development and promote best practices. We have put together some samples about how to enforce an API design-first approach in a GitHub CI/CD pipeline to help accelerate it's adoption in a Design-First Development.  \nCode-First Approach  \nA Code-First approach means that development teams first implements server side API interface code e.g. controllers, DTOs etc. and than generates API contract definitions out of it. In current times this approach is more widely popular within developer community than Design-First Approach.  \nThis approach has the advantages of allowing the team to quickly implement APIs and also providing the flexibility to react very quickly to any unexpected API requirement changes.  \nKey Benefits of this approach:  \nRapid development of APIs as development team can start implementing APIs much faster directly after understanding key requirements & use cases.  \nDevelopment team has better control & flexibility to implement server side API interfaces in a way which best suited for project structure.  \nMore popular among development teams so its easier to get consensus on a related topic and also has more ready to use code examples available on various blogs or developer forums regarding how to generate Open API definitions out of actual code.  \nDuring initial phase of development where both API producer & consumers requirements might change often this approach is better as it provides flexibility to quickly react on such changes.  \nImportant Points to consider:  \nA generated Open API definition can become outdated, so its important to have automated checks to avoid this otherwise generated client SDKs will be out of sync and may cause issues for API consumers.  \nWith Agile development, it is hard to ensure that definitions embedded in runtime code remain stable, especially across rounds of refactoring and when serving multiple concurrent API versions.  \nIt might be useful to regularly generate Open API definition and store it in version control system otherwise generating the OpenAPI definition at runtime might makes it more complex in scenarios where that definition is required at development/CI time.  \nHow to Interpret and Apply The Guidelines  \nThe API guidelines document includes a section on how to apply the guidelines depending on whether the API is new or existing.\nIn particular, when working in an existing API ecosystem, be sure to align with stakeholders on a definition of what constitutes a breaking change to understand the impact of implementing certain best practices.  \nWe do not recommend making a breaking change to a service that predates these guidelines simply for the sake of compliance.  \nAdditional Resources  \nMicrosoft's Recommended Reading List for REST APIs  \nDocumentation - Guidance - REST APIs  \nDetailed HTTP status code definitions  \nSemantic Versioning  \nOther Public API Guidelines  \nMicrosoft TypeSpec  \nMicrosoft TypeSpec GitHub Workflow samples",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-patterns\\rest-api-design-guidance.md"
    },
    {
        "chunkId": "chunk123_0",
        "chunkContent": "Design Reviews  \nTable of Contents  \nGoals  \nMeasures  \nImpact  \nParticipation  \nFacilitation Guidance  \nTechnical Spike  \nGoals  \nReduce technical debt for our customers  \nContinue to iterate on design after Game Plan review  \nGenerate useful technical artifacts that can be referenced by Microsoft and customers  \nMeasures  \nCost of Change  \nWhen incorporating design reviews as part of the engineering process, decisions are front-loaded before implementation begins. Making a decision of using Azure Kubernetes Service instead of App Services at the design phase likely only requires updating documentation. However, making this pivot after implementation has started or after a solution is in use is much more costly.  \nAre these changes occurring before or after implementation? How large of effort are they typically?  \nReviewer Participation  \nHow many individuals participate across the designs created? Cumulatively if this is a larger number this would indicate a wider contribution of ideas and perspectives. A lower number (i.e. same 2 individuals only on every review) might indicate a limited set of perspectives. Is anyone participating from outside the core development team?  \nTime To Potential Solutions  \nHow long does it typically take to go from requirements to solution options (multiple)?  \nThere is a healthy balancing act between spending too much or too little time evaluating different potential solutions. Too little time puts higher risk of costly changes required after implementation. Too much time delays target value from being delivered; as well as subsequent features in queue. However, the faster the team can identify the most critical information necessary to make an informed decision, the faster value can be provided with lower risk of costly changes down the road.  \nTime to Decisions  \nHow long does it take to make a decision on which solution to implement?  \nThere is also a healthy balancing act in supporting a healthy debate while not hindering the team's delivery. The ideal case is for a team to quickly digest the solution options presented, ask questions, and debate before finally reaching quorum on a particular approach. In cases where no quorum can be reached, the person with the most context on the problem (typically story owner) should make the final decision. Prioritize delivering value and learning. Disagree and commit!  \nImpact  \nSolutions can be quickly be operated into customer's production environment  \nEasier for other dev crews to leverage your teams work  \nEasier for engineers to ramp up on projects  \nIncrease team velocity by front-loading changes and decisions when they cost the least  \nIncreased team engagement and transparency by soliciting wide reviewer participation  \nParticipation  \nDev Crew  \nThe dev crew should always participate in all design review sessions  \nISE Engineering  \nCustomer Engineering  \nDomain Experts  \nDomain experts should participate in design review sessions as needed  \nISE Tech Domains  \nCustomer subject-matter experts (SME)  \nSenior Leadership  \nFacilitation Guidance  \nRecipes  \nPlease see our Design Review Recipes for guidance on design process.  \nSync Design Reviews via in-person / virtual meetings  \nJoint meetings with dev crew, subject-matter experts (SMEs) and customer engineers  \nAsync Design Reviews via Pull-Requests  \nSee the async design review recipe for guidance on facilitating async design reviews. This can be useful for teams that are geographically distributed across different time-zones.  \nTechnical Spike  \nA technical spike is most often used for evaluating the impact new technology has on the current implementation. Please read more here.  \nDesign Documentation  \nDocument and update the architecture design in the project design documentation  \nTrack and document design decisions in a decision log  \nDocument decision process in trade studies when multiple solutions exist for the given problem  \nEarly on in engagements, the team must decide where to land artifacts generated from design reviews.\nTypically, we meet the customer where they are at (for example, using their Confluence instance to land documentation if that is their preferred process).\nHowever, similar to storing decision logs, trade studies, etc. in the development repo, there are also large benefits to maintaining design review artifacts in the repo as well.\nUsually these artifacts can be further added to root level documentation directory or even at the root of the corresponding project if the repo is monolithic.\nIn adding them to the project repo, these artifacts must similarly be reviewed in Pull Requests (typically preceding but sometimes accompanying implementation) which allows async review/discussion.\nFurthermore, artifacts can then easily link to other sections of the repo and source code files (via markdown links).",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\README.md"
    },
    {
        "chunkId": "chunk124_0",
        "chunkContent": "Design Decision Log  \nNot all requirements can be captured in the beginning of an agile project during one or more design sessions. The initial architecture design can evolve or change during the project, especially if there are multiple possible technology choices that can be made. Tracking these changes within a large document is in most cases not ideal, as one can lose oversight over the design changes made at which point in time. Having to scan through a large document to find a specific content takes time, and in many cases the consequences of a decision is not documented.  \nWhy is it important to track design decisions  \nTracking an architecture design decision can have many advantages:  \nDevelopers and project stakeholders can see the decision log and track the changes, even as the team composition changes over time.  \nThe log is kept up-to-date.  \nThe context of a decision including the consequences for the team are documented with the decision.  \nIt is easier to find the design decision in a log than having to read a large document.  \nWhat is a recommended format for tracking decisions  \nIn addition to incorporating a design decision as an update of the overall design documentation of the project, the decisions could be tracked as Architecture Decision Records as Michael Nygard proposed in his blog.  \nThe effort invested in design reviews and discussions can be different throughout the course of a project. Sometimes decisions are made quickly without having to go into a detailed comparison of competing technologies. In some cases, it is necessary to have a more elaborate study of advantages and disadvantages, as is described in the documentation of Trade Studies. In other cases, it can be helpful to conduct Engineering Feasibility Spikes. An ADR can incorporate each of these different approaches.  \nArchitecture Decision Record (ADR)  \nAn architecture decision record has the structure  \n[Ascending number]. [Title of decision]\nThe title should give the reader the information on what was decided upon.\nExample:  \n001. App level logging with Serilog and Application Insights  \nDate:\nThe date the decision was made.  \nStatus:\nProposed/Accepted/Deprecated/Superseded\nA proposed design can be reviewed by the development team prior to accepting it. A previous decision can be superseded by a new one, or the ADR record marked as deprecated in case it is not valid anymore.  \nContext:\nThe text should provide the reader an understanding of the problem, or as Michael Nygard puts it, a value-neutral [an objective] description of the forces at play.\nExample:  \nDue to the microservices design of the platform, we need to ensure consistency of logging throughout each service so tracking of usage, performance, errors etc. can be performed end-to-end. A single logging/monitoring framework should be used where possible to achieve this, whilst allowing the flexibility for integration/export into other tools at a later stage. The developers should be equipped with a simple interface to log messages and metrics.  \nIf the development team had a data-driven approach to back the decision, i.e. a study that evaluates the potential choices against a set of objective criteria by following the guidance in Trade Studies, the study should be referred to in this section.  \nDecision:\nThe decision made, it should begin with 'We will...' or 'We have agreed to ....\nExample:  \nWe have agreed to utilize Serilog as the Dotnet Logging framework of choice at the application level, with integration into Log Analytics and Application Insights for analysis.  \nConsequences:\nThe resulting context, after having applied the decision.\nExample:  \nSampling will need to be configured in Application Insights so that it does not become overly-expensive when ingesting millions of messages, but also does not prevent capture of essential information. The team will need to only log what is agreed to be essential for monitoring as part of design reviews, to reduce noise and unnecessary levels of sampling.  \nWhere to store ADRs  \nADRs can be stored and tracked in any version control system such as git. As a recommended practice, ADRs can be added as pull request in the proposed status to be discussed by the team until it is updated to accepted to be merged with the main branch. They are usually stored in a folder structure doc/adr or doc/arch. Additionally, it can be useful to track ADRs in a decision-log.md to provide useful metadata in an obvious format.  \nDecision Logs  \nA decision log is a Markdown file containing a table which provides executive summaries of the decisions contained in ADRs, as well as some other metadata. You can see a template table at doc/decision-log.md.  \nWhen to track ADRs  \nArchitecture design decisions are usually tracked whenever significant decisions are made that affect the structure and characteristics of the solution or framework we are building. ADRs can also be used to document results of spikes when evaluating different technology choices.  \nExamples of ADRs  \nThe first ADR could be the decision to use ADRs to track design decisions,  \n0001-record-architecture-decisions.md,  \nfollowed by actual decisions in the engagement as in the example used above,  \n0002-app-level-logging.md.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\decision-log\\README.md"
    },
    {
        "chunkId": "chunk125_0",
        "chunkContent": "Decision Log  \nThis document is used to track key decisions that are made during the course of the project.\nThis can be used at a later stage to understand why decisions were made and by whom.  \nDecision Date Alternatives Considered Reasoning Detailed doc Made By Work Required A one-sentence summary of the decision made. Date the decision was made. A list of the other approaches considered. A two to three sentence summary of why the decision was made. A link to the ADR with the format [Title] DR. Who made this decision? A link to the work item for the linked ADR.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\decision-log\\doc\\decision-log.md"
    },
    {
        "chunkId": "chunk126_0",
        "chunkContent": "1. Record architecture decisions  \nDate: 2020-03-20  \nStatus  \nAccepted  \nContext  \nWe need to record the architectural decisions made on this project.  \nDecision  \nWe will use Architecture Decision Records, as described by Michael Nygard.  \nConsequences  \nSee Michael Nygard's article, linked above. For a lightweight ADR tool set, see Nat Pryce's adr-tools.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\decision-log\\doc\\adr\\0001-record-architecture-decisions.md"
    },
    {
        "chunkId": "chunk127_0",
        "chunkContent": "2. App-level Logging with Serilog and Application Insights  \nDate: 2020-04-08  \nStatus  \nAccepted  \nContext  \nDue to the microservices design of the platform, we need to ensure consistency of logging throughout each service so tracking of usage, performance, errors etc. can be performed end-to-end. A single logging/monitoring framework should be used where possible to achieve this, whilst allowing the flexibility for integration/export into other tools at a later stage. The developers should be equipped with a simple interface to log messages and metrics.  \nDecision  \nWe have agreed to utilize Serilog as the Dotnet Logging framework of choice at the application level, with integration into Log Analytics and Application Insights for analysis.  \nConsequences  \nSampling will need to be configured in Application Insights so that it does not become overly-expensive when ingesting millions of messages, but also does not prevent capture of essential information.\nThe team will need to only log what is agreed to be essential for monitoring as part of design reviews, to reduce noise and unnecessary levels of sampling.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\decision-log\\doc\\adr\\0002-app-level-logging.md"
    },
    {
        "chunkId": "chunk128_0",
        "chunkContent": "Decision Log  \nThis document is used to track key decisions that are made during the course of the project.\nThis can be used at a later stage to understand why decisions were made and by whom.  \nDecision Date Alternatives Considered Reasoning Detailed doc Made By Work Required Use Architecture Decision Records 01/25/2021 Standard Design Docs An easy and low cost solution of tracking architecture decisions over the lifetime of a project Record Architecture Decisions Dev Team #21654 Use ArgoCD 01/26/2021 FluxCD ArgoCD is more feature rich, will support more scenarios, and will be a better tool to put in our tool belts. So we have decided at this point to go with ArgoCD GitOps Trade Study Dev Team #21672 Use Helm 01/28/2021 Kustomize, Kubes, Gitkube, Draft Platform maturity, templating, ArgoCD support K8s Package Manager Trade Study Dev Team #21674 Use CosmosDB 01/29/2021 Blob Storage, CosmosDB, SQL Server, Neo4j, JanusGraph, ArangoDB CosmosDB has better Azure integration, managed identity, and the Gremlin API is powerful. Graph Storage Trade Study and Decision Dev Team #21650 Use Azure Traffic Manager 02/02/2021 Azure Front Door A lightweight solution to route traffic between multiple k8s regional clusters Routing Trade Study Dev Team #21673 Use Linkerd + Contour 02/02/2021 Istio, Consul, Ambassador, Traefik A CNCF backed cloud native k8s stack to deliver service mesh, API gateway and ingress Routing Trade Study Dev Team #21673 Use ARM Templates 02/02/2021 Terraform, Pulumi, Az CLI Azure Native, Az Monitoring and incremental updates support Automated Deployment Trade Study Dev Team #21651 Use 99designs/gqlgen 02/04/2021 graphql, graphql-go, thunder Type safety, auto-registration and code generation GraphQL Golang Trade Study Dev Team #21775 Create normalized role data model 03/25/2021 Career Stage Profiles (CSP), Microsoft Role Library Requires a data model that support the data requirements of both role systems Role Data Model Schema Dev Team #22035 Design for edges and vertices 03/25/2021 N/A N/A Data Model Dev Team #21976 Use grammes 03/29/2021 Gremlin, gremgo, gremcos Balance of documentation and maturity Gremlin API library Trade Study Dev Team #21870 Design for Gremlin implementation 04/02/2021 N/A N/A Gremlin Dev Team #21980 Design for Gremlin implementation 04/02/2021 N/A N/A Gremlin Dev Team #21980 Expose 1:1 data model from API to DB 04/02/2021 Exposing a minified version of data model contract Team decided that there were no pieces of data that we can rule out as being useful. Will update if data model becomes too complex API README Dev Team #21658 Deprecate SonarCloud 04/05/2021 Checkstyle, PMD, FindBugs Requires paid plan to use in a private repo Code Quality & Security Dev Team #22090 Adopted Stable Tagging Strategy 04/08/2021 N/A Team aligned on the proposed docker container tagging strategy Tagging Strategy Dev Team #22005",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\decision-log\\examples\\memory\\Decision-Log.md"
    },
    {
        "chunkId": "chunk129_0",
        "chunkContent": "Memory  \nThese examples were taken from the Memory project, an internal tool for tracking an individual's accomplishments.  \nThe main example here is the Decision Log.\nSince this log was used from the start, the decisions are mostly based on technology choices made in the start of the project.\nAll line items have a link out to the trade studies done for each technology choice.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\decision-log\\examples\\memory\\README.md"
    },
    {
        "chunkId": "chunk130_0",
        "chunkContent": "Data Model  \nTable of Contents  \nGraph vertices and edges  \nGraph Properties  \nVertex Descriptions  \nFull Role JSON Example  \nGraph Model  \nGraph Vertices and Edges  \nThe set of vertices (entities) and edges (relationships) of the graph model  \nVertex (Source) Edge Type Relationship Type Vertex (Target) Notes Required Profession Applies 1:many Discipline Top most level of categorization * Discipline Defines 1:many Role Groups of related roles within a profession * AppliedBy 1:1 Profession 1 Role Requires 1:many Responsibility Individual role mapped to an employee 1+ Requires 1:many Competency 1+ RequiredBy 1:1 Discipline 1 Succeeds 1:1 Role Supports career progression between roles 1 Precedes 1:1 Role Supports career progression between roles 1 AssignedTo 1:many User Profile * Responsibility Expects 1:many Key Result A group of expected outcomes and key results for employees within a role 1+ ExpectedBy 1:1 Role 1 Competency Describes 1:many Behavior A set of behaviors that contribute to success 1+ DescribedBy 1:1 Role 1 Key Result ExpectedBy 1:1 Responsibility The expected outcome of performing a responsibility 1 Behavior ContributesTo 1:1 Competency The way in which one acts or conducts oneself 1 User Profile Fulfills many:1 Role 1+ Authors 1:many Entry * Reads many:many Entry * Entry SharedWith many:many User Profile Business logic should add manager to this list by default. These users should only have read access. * Demonstrates many:many Competency * Demonstrates many:many Behavior * Demonstrates many:many Responsibility * Demonstrates many:many Result * AuthoredBy many:1 UserProfile 1+ DiscussedBy 1:many Commentary * References many:many Artifact * Competency DemonstratedBy many:many Entry * Behavior DemonstratedBy many:many Entry * Responsibility DemonstratedBy many:many Entry * Result DemonstratedBy many:many Entry * Commentary Discusses many:1 Entry * Artifact ReferencedBy many:many Entry 1+  \nGraph Properties  \nThe full set of data properties available on each vertex and edge  \nVertex/Edge Property Data Type Notes Required (Any) ID guid 1 Profession Title String 1 Description String 0 Discipline Title String 1 Description String 0 Role Title String 1 Description String 0 Level Band String SDE, SDE II, Senior, etc 1 Responsibility Title String 1 Description String 0 Competency Title String 1 Description String 0 Key Result Description String 1 Behavior Description String 1 User Profile Theme selection string there are only 2: dark, light 1 PersonaId guid[] there are only 2: User, Admin 1+ UserId guid Points to AAD object 1 DeploymentRing string[] Is used to deploy new versions 1 Project string[] list of user created projects * Entry Title string 1 DateCreated date 1 ReadyToShare boolean false if draft 1 AreaOfImpact string[] 3 options: self, contribute to others, leverage others * Commentary Data string 1 DateCreated date 1 Artifact Data string 1 DateCreated date 1 ArtifactType string describes the artifact type: markdown, blob link 1  \nVertex Descriptions  \nProfession  \nTop most level of categorization  \n{% raw %}  \njson\n{\n\"title\": \"Software Engineering\",\n\"description\": \"Description of profession\",\n\"disciplines\": []\n}  \n{% endraw %}  \nDiscipline  \nGroups of related roles within a profession  \n{% raw %}  \njson\n{\n\"title\": \"Site Reliability Engineering\",\n\"description\": \"Description of discipline\",\n\"roles\": []\n}  \n{% endraw %}  \nRole  \nIndividual role mapped to an employee  \n{% raw %}  \njson\n{\n\"title\": \"Site Reliability Engineering IC2\",\n\"description\": \"Detailed description of role\",\n\"responsibilities\": [],\n\"competencies\": []\n}  \n{% endraw %}  \nResponsibility  \nA group of expected outcomes and key results for employees within a role  \n{% raw %}  \njson\n{\n\"title\": \"Technical Knowledge and Domain Specific Expertise\",\n\"results\": []\n}  \n{% endraw %}  \nCompetency  \nA set of behaviors that contribute to success  \n{% raw %}  \njson\n{\n\"title\": \"Adaptability\",\n\"behaviors\": []\n}  \n{% endraw %}  \nKey Result  \nThe expected outcome of performing a responsibility  \n{% raw %}  \njson\n{\n\"description\": \"Develops a foundational understanding of distributed systems design...\"\n}  \n{% endraw %}  \nBehavior  \nThe way in which one acts or conducts oneself  \n{% raw %}  \njson\n{\n\"description\": \"Actively seeks information and tests assumptions.\"\n}  \n{% endraw %}  \nUser  \nThe user object refers to whom a person is.\nWe do not store our own rather use Azure OIDs.  \nUser Profile  \nThe user profile contains any user settings and edges specific to Memory.  \nPersona  \nA user may hold multiple personas.  \nEntry  \nThe same entry object can hold many kinds of data, and at this stage of the project we decide that we will not store external data, so it's up to the user to provide a link to the data for a reader to click into and get redirected to a new tab to open.  \nNote: This means that in the web app, we will need to ensure links are opened in new tabs.  \nProject  \nProjects are just string fields to represent what a user wants to group their entries under.  \nArea of Impact  \nThis refers to the 3 areas of impact in the venn-style diagram in the HR tool.\nThe options are: self, contributing to impact of others, building on others' work.  \nCommentary  \nA comment is essentially a piece of text.\nHowever, anyone that an entry is shared with can add commentary on an entry.  \nArtifact  \nThe artifact object contains the relevant data as markdown, or a link to the relevant data.  \nFull Role JSON Example  \n{% raw %}  \njson\n{\n\"id\": \"abc123\",\n\"title\": \"Site Reliability Engineering IC2\",\n\"description\": \"Detailed description of role\",\n\"responsibilities\": [\n{\n\"id\": \"abc123\",\n\"title\": \"Technical Knowledge and Domain Specific Expertise\",\n\"results\": [\n{\n\"description\": \"Develops a foundational understanding of distributed systems design...\"\n},\n{\n\"description\": \"Develops an understanding of the code, features, and operations of specific products...\"\n}\n]\n},\n{\n\"id\": \"abc123\",\n\"title\": \"Contributions to Development and Design\",\n\"results\": [\n{\n\"description\": \"Develops and tests basic changes to optimize code...\"\n},\n{\n\"description\": \"Supports ongoing engagements with product engineering teams...\"\n}\n]\n}\n],\n\"competencies\": [\n{\n\"id\": \"abc123\",\n\"title\": \"Adaptability\",\n\"behaviors\": [\n{ \"description\": \"Actively seeks information and tests assumptions.\" },\n{\n\"description\": \"Shifts his or her approach in response to the demands of a changing situation.\"\n}\n]\n},\n{\n\"id\": \"abc123\",\n\"title\": \"Collaboration\",\n\"behaviors\": [\n{\n\"description\": \"Removes barriers by working with others around a shared need or customer benefit.\"\n},\n{\n\"description\": \" Incorporates diverse perspectives to thoroughly address complex business issues.\"\n}\n]\n}\n]\n}  \n{% endraw %}  \nAPI Data Model  \nBecause there is no internal edges or vertices that need to be hidden from API consumers, the API will expose a 1:1 mapping of the current data model for consumption.\nThis is subject to change if our data model becomes too complex for downstream users.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\decision-log\\examples\\memory\\Architecture\\Data-Model.md"
    },
    {
        "chunkId": "chunk131_0",
        "chunkContent": "Application Deployment  \nThe Memory application leverages Azure DevOps for work item tracking as well as continuous integration (CI) and continuous deployment (CD).  \nEnvironments  \nThe Memory project uses multiple environments to isolate and test changes before promoting releases to the global user base.  \nNew environment rollouts are automatically triggered based upon a successful deployment of the previous stage /environment.  \nThe development, staging and production environments leverage slot deployment during an environment rollout.\nAfter a new release is deployed to a staging slot, it is validated through a series of functional integration tests.\nUpon a 100% pass rate of all tests the staging & production slots are swapped effectively making updates to the environment available.  \nAny errors or failed tests halt the deployment in the current stage and prevent changes to further environments.  \nEach deployed environment is completely isolated and does not share any components.\nThey each have unique resource instances of Azure Traffic Manager, Cosmos DB, etc.  \nDeployment Dependencies  \nDevelopment Staging Production CI Quality Gates Development Staging Manual Approval  \nLocal  \nThe local environment is used by individual software engineers during the development of new features and components.  \nEngineers leverage some components from the deployed development environment that are not available on certain platforms or are unable to run locally.  \nCosmosDB  \nEmulator only exists for Windows  \nThe local environment also does not use Azure Traffic Manager.\nThe frontend web app directly communicates to the backend REST API typically running on a separate localhost port mapping.  \nDevelopment  \nThe development environment is used as the first quality gate.\nAll code that is checked into the main branch is automatically deployed to this environment after all CI quality gates have passed.  \nDev Regions  \nWest US (westus)  \nStaging  \nThe staging environment is used to validate new features, components and other changes prior to production rollout.\nThis environment is primarily used by developers, QA and other company stakeholders.  \nStaging Regions  \nWest US (westus)  \nEast US (eastus)  \nProduction  \nThe production environment is used by the worldwide user base.\nChanges to this environment are gated by manual approval by your product's leadership team in addition to other automatic quality gates.  \nProduction Regions  \nWest US (westus)  \nCentral US (centralus)  \nEast US (eastus)  \nEnvironment Variable Group  \nInfrastructure Setup (memory-common)  \nappName  \nbusinessUnit  \nserviceConnection  \nsubscriptionId  \nDevelopment Setup (memory-dev)  \nenvironmentName (placeholder)  \nStaging Setup (memory-staging)  \nenvironmentName (placeholder)  \nProduction Setup (memory-prod)  \nenvironmentName (placeholder)",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\decision-log\\examples\\memory\\Deployment\\Environments.md"
    },
    {
        "chunkId": "chunk132_0",
        "chunkContent": "Trade Study: GitOps  \nConducted by: Tess and Jeff  \nBacklog Work Item: #21672  \nDecision Makers: Wallace, whole team  \nOverview  \nFor Memory, we will be creating a cloud native application with infrastructure as code.\nWe will use GitOps for Continuous Deployment through pull requests infrastructure changes to be reflected.  \nOverall, between our two options, one is more simple and targeted in a way that we believe would meet the requirements for this project.\nThe other does the same, with additional features that may or may not be worth the extra configuration and setup.  \nEvaluation Criteria  \nRepo style: mono versus multi  \nPolicy Enforcement  \nDeployment Methods  \nDeployment Monitoring  \nAdmission Control  \nAzure Documentation availability  \nMaintainability  \nMaturity  \nUser Interface  \nSolutions  \nFlux  \nFlux is a tool created by Waveworks and is built on top of Kubernetes' API extension system, supports multi-tenancy, and integrates seamlessly with popular tools like Prometheus.  \nFlux Acceptance Criteria Evaluation  \nRepo style: mono versus multi  \nFlux supports both as of v2  \nPolicy Enforcement  \nAzure Policy is in Preview  \nDeployment Methods  \nDefine a Helm release using Helm Controllers  \nKustomization describes deployments  \nDeployment Monitoring  \nFlux works with Prometheus for deployment monitoring as well as Grafana dashboards  \nAdmission Control  \nFlux uses RBAC from Kubernetes to lock down sync permissions.  \nUses the service account to access image pull secrets  \nAzure Documentation availability  \nGreat, better when using Helm Operators  \nMaintainability  \nManage via YAML files in git repo  \nMaturity  \nv2 is published under Apache license in GitHub, it works with Helm v3, and has PR commits from as recently as today  \n945 stars, 94 forks  \nUser Interface  \nCLI, the simplest lightweight option  \nOther features to call out (see more on website)  \nFlux only supports Pull-based deployments which means it must be paired with an operator  \nFlux can send notifications and receive webhooks for syncing  \nHealth assessments  \nDependency management  \nAutomatic deployment  \nGarbage collection  \nDeploy on commit  \nVariations  \nControllers  \nBoth Controller options are optional.  \nThe Helm Controller additionally fetches helm artifacts to publish, see below diagram.  \nThe Kustomize Controller manages state and continuous deployment.  \nWe will not decide between the controller to use here, as that's a separate trade study, however we will note that Helm is more widely documented within Flux documentation.  \nFlux v1  \nFlux v1 is only in maintenance mode and should not be used anymore.\nSo this section does not consider the v1 option a valid option.  \nGitOps Toolkit  \nFlux v2 is built on top of the GitOps Toolkit, however we do not evaluate using the GitOps Toolkit alone as that is for when you want to make your own CD system, which is not what we want.  \nArgoCD with Helm Charts  \nArgoCD is a declarative, GitOps-based Continuous Delivery (CD) tool for Kubernetes.  \nArgoCD with Helm Acceptance Criteria Evaluation  \nRepo style: mono versus multi  \nArgoCD supports both  \nPolicy Enforcement  \nAzure Policy is in Preview  \nDeployment Methods  \nDeploy with Helm Chart  \nUse Kustomize to apply some post-rendering to the Helm release templates  \nDeployment Monitoring  \nArgo CD expose two sets of Prometheus metrics (application metrics and API server metrics) for deployment monitoring.  \nAdmission Control  \nArgoCD use RBAC feature.\nRBAC requires SSO configuration or one or more local users setup.\nOnce SSO or local users are configured, additional RBAC roles can be defined  \nArgo CD does not have its own user management system and has only one built-in user admin.\nThe admin user is a superuser, and it has unrestricted access to the system  \nAuthorization is handled via JWT tokens and checking group claims in them  \nAzure Documentation availability  \nArgo has documentation on Azure AD  \nMaturity  \nHas PR commits from as recently as today  \n5,000 stars, 1,100 forks  \nMaintainability  \nCan use GitOps to manage it  \nUser Interface  \nArgoCD has a GUI and can be used across clusters  \nOther features to call out (see more on website)  \nArgoCD support both pull model and push model for continuous delivery  \nArgo can send notifications, but you need a separate tool for it  \nArgo can receive webhooks  \nHealth assessments  \nPotentially much more useful multi-tenancy tools.\nManages multiple projects, maps them to teams, etc.  \nSSO Integration  \nGarbage collection  \nResults  \nThis section should contain a table that has each solution rated against each of the evaluation criteria:  \nSolution Repo style Policy Enforcement Deployment Methods Deployment Monitoring Admission Control Azure Doc Maintainability Maturity UI Flux mono, multi Azure Policy, preview Helm, Kustomize Prometheus, Grafana RBAC Yes on Azure YAML in git repo 945 stars, 94 forks, currently maintained CLI ArgoCD mono, multi Azure Policy, preview Helm, Kustomize, KSonnet, ... Prometheus, Grafana RBAC Only in their own docs manifests in git repo 5,000 stars, 1,100 forks GUI, multiple clusters in same GUI  \nDecision  \nArgoCD is more feature rich, will support more scenarios, and will be a better tool to put in our tool belts.\nSo we have decided at this point to go with ArgoCD.  \nReferences  \nGitOps  \nEnforcement  \nMonitoring  \nPolicies  \nDeployment  \nPush with ArgoCD in Azure DevOps",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\decision-log\\examples\\memory\\Trade-Studies\\GitOps.md"
    },
    {
        "chunkId": "chunk133_0",
        "chunkContent": "Async Design Reviews  \nGoals  \nAllow team members to review designs as their work schedule allows.  \nImpact  \nThis in turn results in the following benefits:  \nHigher Participation & Accessibility. They do not need to be online and available at the same time as others to review.  \nReduced Time Constraint. Reviewers can spend longer than the duration of a single meeting to think through the approach and provide feedback.  \nMeasures  \nThe metrics and/or KPIs used for design reviews overall would still apply. See design reviews for measures guidance.  \nParticipation  \nThe participation should be same as any design review. See design reviews for participation guidance.  \nFacilitation Guidance  \nThe concept is to have the design follow the same workflow as any code changes to implement story or task. Rather than code however, the artifacts being added or changed are Markdown documents as well as any other supporting artifacts (prototypes, code samples, diagrams, etc).  \nPrerequisites  \nSource Controlled Design Docs  \nDesign documentation must live in a source control repository that supports pull requests (i.e. git). The following guidelines can be used to determine what repository houses the docs  \nKeeping docs in the same repo as the affected code allows for the docs to be updated atomically alongside code within the same pull request.  \nIf the documentation represents code that lives in many different repositories, it may make more sense to keep the docs in their own repository.  \nPlace the docs so that they do not trigger CI builds for the affected code (assuming the documentation was the only change). This can be done by placing them in an isolated directory should they live alongside the code they represent. See directory structure example below.  \n{% raw %}  \ntext\n-root\n--src\n--docs <-- exclude from ci build trigger\n--design  \n{% endraw %}  \nWorkflow  \nThe designer branches the repo with the documentation.  \nThe designer works on adding or updating documentation relevant to the design.  \nThe designer submits pull request and requests specific team members to review.  \nReviewers provide feedback to Designer who incorporates the feedback.  \n(OPTIONAL) Design review meeting might be held to give deeper explanation of design to reviewers.  \nDesign is approved/accepted and merged to main branch.  \nTips for Faster Review Cycles  \nTo make sure a design is reviewed in a timely manner, it's important to directly request reviews from team members. If team members are assigned without asking, or if no one is assigned it's likely the design will sit for longer without review. Try the following actions:  \nMake it the designer's responsibility to find reviewers for their design  \nThe designer should ask a team member directly (face-to-face conversation, async messaging, etc) if they are available to review. Only if they agree, then assign them as a reviewer.  \nIndicate if the design is ready to be merged once approved.  \nIndicate Design Completeness  \nIt helps the reviewer to understand if the design is ready to be accepted or if its still a work-in-progress. The level and type of feedback the reviewer provides will likely be different depending on its state. Try the following actions to indicate the design state  \nMark the PR as a Draft. Some ALM tools support opening a pull request as a Draft such as Azure DevOps.  \nPrefix the title with \"DRAFT\", \"WIP\", or \"work-in-progress\".  \nSet the pull request to automatically merge after approvals and checks have passed. This can indicate to the reviewer the design is complete from the designer's perspective.  \nPractice Inclusive Behaviors  \nThe designated reviewers are not the only team members that can provide feedback on the design. If other team members voluntarily committed time to providing feedback or asking questions, be sure to respond. Utilize face-to-face conversation (in person or virtual) to resolve feedback or questions from others as needed. This aids in building team cohesiveness in ensuring everyone understands and is willing to commit to a given design. This practice demonstrates inclusive behavior; which will promote trust and respect within the team.  \nRespond to all PR comments objectively and respectively irrespective of the authors level, position, or title.  \nAfter two round trips of question/response, resort to synchronous communication for resolution (i.e. virtual or physical face-to-face conversation).",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\recipes\\async-design-reviews.md"
    },
    {
        "chunkId": "chunk134_0",
        "chunkContent": "Incorporating Design Reviews into an Engagement  \nIntroduction  \nDesign reviews should not feel like a burden. Design reviews can be easily incorporated into the dev crew process with minimal overhead.  \nOnly create design reviews when needed. Not every story or task requires a complete design review.  \nLeverage this guidance to make changes that best fit in with the team. Every team works differently.  \nLeverage Microsoft subject-matter experts (SME) as needed during design reviews. Not every story needs SME or leadership sign-off. Most design reviews can be fully executed within a dev crew.  \nUse diagrams to visualize concepts and architecture.  \nThe following guidelines outline how Microsoft and the customer together can incorporate design reviews into their day-to-day agile processes.  \nEnvisioning / Architecture Design Session (ADS)  \nEarly in an engagement Microsoft works with customers to understand their unique goals and objectives and establish a definition of done. Microsoft dives deep into existing customer infrastructure and architecture to understand potential constraints. Additionally, we seek to understand and uncover specific non-functional requirements that influence the solution.  \nDuring this time the team uncovers many unknowns, leveraging all new-found information, in order to help generate an impactful design that meets customer goals. After ADS it can be helpful to conduct Engineering Feasibility Spikes to further de-risk technologies being considered for the engagement.  \nTip: All unknowns have not been addressed at this point.  \nSprint Planning  \nIn many engagements Microsoft works with customers using a SCRUM agile development process which begins with sprint planning. Sprint planning is a great opportunity to dive deep into the next set of high priority work. Some key points to address are the following:  \nIdentify stories that require design reviews  \nSeparate design from implementation for complex stories  \nAssign an owner to each design story  \nStories that will benefit from design reviews have one or more of the following in common:  \nThere are many unknown or unclear requirements  \nThere is a wide distribution of anticipated workload, or story pointing, across the dev crew  \nThe developer cannot clearly illustrate all tasks required for the story  \nTip: After sprint planning is complete the team should consider hosting an initial design review discussion to dive deep in the design requirement of the stories that were identified. This will provide more clarity so that the team can move forward with a design review, synchronously or asynchronously, and complete tasks.  \nSprint Backlog Refinement  \nIf your team is not already hosting a Sprint Backlog Refinement session at least once per week you should consider it. It is a great opportunity to:  \nKeep the backlog clean  \nRe-prioritize work based on shifting business priorities  \nFill in missing descriptions and acceptance criteria  \nIdentify stories that require design reviews  \nThe team can follow the same steps from sprint planning to help identify which stories require design reviews. This can often save much time during the actual sprint planning meetings to focus on the task at hand.  \nSprint Retrospectives  \nSprint retrospectives are a great time to check in with the dev team, identify what is working or not working, and propose changes to keep improving.  \nIt is also a great time to check in on design reviews  \nDid any of the designs change from last sprint?  \nHow have design changes impacted the engagement?  \nHave previous design artifacts been updated to reflect new changes?  \nAll design artifacts should be treated as a living document. As requirements change or uncover more unknowns the dev crew should retroactively update all design artifacts. Missing this critical step may cause the customer to incur future technical debt. Artifacts that are not up to date are bugs in the design.  \nTip: Keep your artifacts up to date by adding it to your teams Definition of Done for all user stories.  \nSync Design Reviews  \nIt is often helpful to schedule 1-2 design sessions per sprint as part of the normal aforementioned meeting cadence.\nThroughout the sprint, folks can add design topics to the meeting agenda and if there is nothing to discuss for a particular meeting occurrence, it can simply be cancelled.\nWhile these sessions may not always be used, they help project members align on timing and purpose early on and establish precedence, often encouraging participation so design topics don't slip through the cracks.\nOftentimes, it is helpful for those project members intending to present their design to the wider group to distribute documentation on their design prior to the session so that other participants can come prepared with context heading into the session.  \nIt should be noted that the necessity of these sessions certainly evolves over the course of the engagement.\nEarly on, or in other times of more ambiguity, these meetings are typically used more often and more fully.  \nLastly, while it is suggested that sync design reviews are scheduled during the normal sprint cadence, scheduling ad-hoc sessions should not be discouraged - even if these reviews are limited to the participants of a specific workstream.  \nWrap-up Sprints  \nWrap-up sprints are a great time to tie up loose ends with the customer and hand-off solution. Customer hand-off becomes a lot easier when there are design artifacts to reference and deliver alongside the completed solution.  \nDuring your wrap-up sprints the dev crew should consider the following:  \nAre the design artifacts up to date?  \nAre the design artifacts stored in an accessible location?",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\recipes\\engagement-process.md"
    },
    {
        "chunkId": "chunk135_0",
        "chunkContent": "Engineering Feasibility Spikes: identifying and mitigating risk  \nIntroduction  \nSome engagements require more de-risking than others. Even after Architectural Design Sessions (ADS) an engagement may still have substantial technical unknowns. These types of engagements warrant an exploratory/validation phase where Engineering Feasibility Spikes can be conducted immediately after envisioning/ADS and before engineering sprints.  \nEngineering feasibility spikes  \nAre regimented yet collaborative time-boxed investigatory activities conducted in a feedback loop to capitalize on individual learnings to inform the team.  \nIncrease the team\u2019s knowledge and understanding while minimizing engagement risks.  \nThe following guidelines outline how Microsoft and the customer can incorporate engineering feasibility spikes into the day-to-day agile processes.  \nPre-Mortem  \nA good way to gauge what engineering spikes to conduct is to do a pre-mortem.  \nWhat is a pre-mortem?  \nA 90-minute meeting after envisioning/ADS that includes the entire team (and can also include the customer) which answers \"Imagine the project has failed. What problems and challenges caused this failure?\"  \nAllows the entire team to initially raise concerns and risks early in the engagement.  \nThis input is used to decide which risks to pursue as engineering spikes.  \nSharing Learnings & Current Progress  \nFeedback loop  \nThe key element from conducting the engineering feasibility spikes is sharing the outcomes in-flight.  \nThe team gets together and shares learning on a weekly basis (or more frequently if needed).  \nThe sharing is done via a 30-minute call.  \nEveryone on the Dev Crew joins the call (even if not everyone is assigned an engineering spike story or even if the spike work was underway and not fully completed).  \nThe feedback loop is significantly tighter/shorter than in sprint-based agile process. Instead of using the Sprint as the forcing function to adjust/pivot/re-prioritize, the interim sharing sessions were the trigger.  \nRe-prioritizing the next spikes  \nAfter the team shares current progress, another round of planning is done. This allows the team to  \nEstablish a very tight feedback loop.  \nRe-prioritize the next spike(s) because of the outcome from the current engineering feasibility spikes.  \nAdjusting based on context  \nDuring the sharing call, and when the team believes it has enough information, the team sometimes comes to the realization that the original spike acceptance criteria is no longer valid. The team pivots into another area that provides more value.  \nA decision log can be used to track outcomes.  \nEngineering Feasibility Sprints Diagram  \nThe process is depicted in the diagram below.  \nBenefits  \nCreating code samples to prove out ideas  \nIt is important to note to be intentional about the spikes not aiming to produce production-level code.  \nThe team sometimes must write code to arrive at the technical learning.  \nThe team must be cognizant that the code written for the spikes is not going to serve as the code for the final solution.  \nThe code written is just enough to drive the investigation forward with greater confidence.  \nFor example, supposed the team was exploring the API choreography of creating a Graph client with various Azure Active Directory (AAD) authentication flows and permissions. The code to demonstrate this is implemented in a console app, but it could have been done via an Express server, etc. The fact that it was a console app was not important, but rather the ability of the Graph client to be able to do operations against the Graph API endpoint with the minimal number of permissions is the main learning goal.  \nTargeted conversations  \nBy sharing the progress of the spike, the team\u2019s collective knowledge increases.  \nThe spikes allow the team to drive succinct conversations with various Product Groups (PGs) and other subject matter experts (SMEs).  \nRather than speaking at a hypothetical level, the team playbacks project/architecture concerns and concretely points out why something is a showstopper or not a viable way forward.  \nIncreased customer trust  \nThis process leads to increased customer trust.  \nUsing this process, the team  \nBrings the customer along in the decision-making process and guides them how to go forward.  \nProvides answers with confidence and suggests sound architectural designs.  \nConducting engineering feasibility spikes sets the team and the customer up for success, especially if it highlights technology learnings that help the customer fully understand the feasibility/viability of an engineering solution.  \nSummary of key points  \nA pre-mortem can involve the whole team in surfacing business and technical risks.  \nThe key purpose of the engineering feasibility spike is learning.  \nLearning comes from both conducting and sharing insights from spikes.  \nUse new spike infused learnings to revise, refine, re-prioritize, or create the next set of spikes.  \nWhen spikes are completed, look for new weekly rhythms like adding a \u2018risk\u2019 column to the retro board or raising topics at daily standup to identify emerging risks.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\recipes\\engineering-feasibility-spikes.md"
    },
    {
        "chunkId": "chunk136_0",
        "chunkContent": "Your Feature or Story Design Title Here (prefix with DRAFT/WIP to indicate level of completeness)  \nDoes the feature re-use or extend existing patterns / interfaces that have already been established for the project?\nDoes the feature expose new patterns or interfaces that will establish a new standard for new future development?  \nFeature/Story Name  \nEngagement: [Engagement]  \nCustomer: [Customer]  \nAuthors: [Author1, Author2, etc.]  \nOverview/Problem Statement  \nIt can also be a link to the work item.  \nDescribe the feature/story with a high-level summary.  \nConsider additional background and justification, for posterity and historical context.  \nList any assumptions that were made for this design.  \nGoals/In-Scope  \nList the goals that the feature/story will help us achieve that are most relevant for the design review discussion.  \nThis should include acceptance criteria required to meet definition of done.  \nNon-goals / Out-of-Scope  \nList the non-goals for the feature/story.  \nThis contains work that is beyond the scope of what the feature/component/service is intended for.  \nProposed Design  \nBriefly describe the high-level architecture for the feature/story.  \nRelevant diagrams (e.g. sequence, component, context, deployment) should be included here.  \nTechnology  \nDescribe the relevant OS, Web server, presentation layer, persistence layer, caching, eventing/messaging/jobs, etc. \u2013 whatever is applicable to the overall technology solution and how are they going to be used.  \nDescribe the usage of any libraries of OSS components.  \nBriefly list the languages(s) and platform(s) that comprise the stack.  \nNon-Functional Requirements  \nWhat are the primary performance and scalability concerns for this feature/story?  \nAre there specific latency, availability, and RTO/RPO objectives that must be met?  \nAre there specific bottlenecks or potential problem areas? For example, are operations CPU or I/O (network, disk) bound?  \nHow large are the data sets and how fast do they grow?  \nWhat is the expected usage pattern of the service? For example, will there be peaks and valleys of intense concurrent usage?  \nAre there specific cost constraints? (e.g. $ per transaction/device/user)  \nDependencies  \nDoes this feature/story need to be sequenced after another feature/story assigned to the same team and why?  \nIs the feature/story dependent on another team completing other work?  \nWill the team need to wait for that work to be completed or could the work proceed in parallel?  \nRisks & Mitigation  \nDoes the team need assistance from subject-matter experts?  \nWhat security and privacy concerns does this milestone/epic have?  \nIs all sensitive information and secrets treated in a safe and secure manner?  \nOpen Questions  \nList any open questions/concerns here.  \nAdditional References  \nList any additional references here including links to backlog items, work items or other documents.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\recipes\\feature-story-design-review-template.md"
    },
    {
        "chunkId": "chunk137_0",
        "chunkContent": "High Level / Game Plan Design Recipe  \nWhy is this valuable?  \nDesign at macroscopic level shows the interactions between systems and services that will be used to accomplish the project. It is intended to ensure there is high level understanding of the plan for what to build, which off-the-shelf components will be used, and which external components will need to interact with the deliverable.  \nThings to keep in mind  \nAs with all other aspects of the project, design reviews must provide a friendly and safe environment so that any team member feels comfortable proposing a design for review and can use the opportunity to grow and learn from the constructive / non-judgemental feedback from peers and subject-matter experts (see Team Agreements).  \nAttempt to illustrate different personas involved in the use cases and how/which boxes are their entry points.  \nPrefer pictures over paragraphs. The diagrams aren't intended to generate code, so they should be fairly high level.  \nArtifacts should indicate the direction of calls (are they outbound, inbound, or bidirectional?) and call out system boundaries where ports might need to be opened or additional infrastructure work may be needed to allow calls to be made.  \nSequence diagrams are helpful to show the flow of calls among components + systems.  \nGeneric box diagrams depicting data flow or call origination/destination are useful. However, the title should clearly define what the arrows show indicate. In most cases, a diagram will show either data flow or call directions but not both.  \nVisualize the contrasting aspects of the system/diagram for ease of communication. e.g. differing technologies employed, modified vs. untouched components, or internet vs. local cloud components. Colors, grouping boxes, and iconography can be used for differentiating.  \nPrefer ease-of-understanding for communicating ideas over strict UML correctness.  \nDesign reviews should be lightweight and should not feel like an additional process overhead.  \nExamples",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\recipes\\high-level-design-recipe.md"
    },
    {
        "chunkId": "chunk138_0",
        "chunkContent": "Milestone / Epic Design Review Recipe  \nWhy is this valuable?  \nDesign at epic/milestone level can help the team make better decisions about prioritization by summarizing the value, effort, complexity, risks, and dependencies. This brief document can help the team align on the selected approach and briefly explain the rationale for other teams, subject-matter experts, project advisors, and new team members.  \nThings to keep in mind  \nAs with all other aspects of the project, design reviews must provide a friendly and safe environment so that any team member feels comfortable proposing a design for review and can use the opportunity to grow and learn from the constructive / non-judgemental feedback from peers and subject-matter experts (see Team Agreements).  \nDesign reviews should be lightweight and should not feel like an additional process overhead.  \nDev Lead can usually provide guidance on whether a given epic/milestone needs a design review and can help other team members in preparation.  \nThis is not a strict template that must be followed and teams should not be bogged down with polished \"design presentations\".  \nThink of the recipe below as a \"menu of options\" for potential questions to think through in designing this epic. Not all sections are required for every epic. Focus on sections and questions that are most relevant for making the decision and rationalizing the trade-offs.  \nMilestone/epic design is considered high-level design but is usually more detailed than the design included in the Game Plan, but will likely re-use some technologies, non-functional requirements, and constraints mentioned in the Game Plan.  \nAs the team learned more about the project and further refined the scope of the epic, they may specifically call out notable changes to the overall approach and, in particular, highlight any unique deployment, security, private, scalability, etc. characteristics of this milestone.  \nTemplate  \nYou can download the Milestone/Epic Design Review Template, copy it into your project, and use it as described in the async design review recipe.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\recipes\\milestone-epic-design-review-recipe.md"
    },
    {
        "chunkId": "chunk139_0",
        "chunkContent": "Your Milestone/Epic Design Title Here (prefix with DRAFT/WIP to indicate level of completeness)  \nPlease refer to https://microsoft.github.io/code-with-engineering-playbook/design/design-reviews/recipes/milestone-epic-design-review-recipe/ for things to keep in mind when using this template.  \nMilestone / Epic: Name  \nProject / Engagement: [Project Engagement]  \nAuthors: [Author1, Author2, etc.]  \nOverview / Problem Statement  \nDescribe the milestone/epic with a high-level summary and a problem statement. Consider including or linking to any additional background (e.g. Game Plan or Checkpoint docs) if it is useful for historical context.  \nGoals / In-Scope  \nList a few bullet points of goals that this milestone/epic will achieve and that are most relevant for the design review discussion. You may include acceptable criteria required to meet the Definition of Done.  \nNon-goals / Out-of-Scope  \nList a few bullet points of non-goals to clarify the work that is beyond the scope of the design review for this milestone/epic.  \nProposed Design / Suggested Approach  \nTo optimize the time investment, this should be brief since it is likely that details will change as the epic/milestone is further decomposed into features and stories. The goal being to convey the vision and complexity in something that can be understood in a few minutes and can help guide a discussion (either asynchronously via comments or in a meeting).  \nA paragraph to describe the proposed design / suggested approach for this milestone/epic.  \nA diagram (e.g. architecture, sequence, component, deployment, etc.) or pseudo-code snippet to make it easier to talk through the approach.  \nList a few of the alternative approaches that were considered and include the brief key Pros and Cons used to help rationalize the decision. For example:  \nPros Cons Simple to implement Creates secondary identity system Repeatable pattern/code artifact Deployment requires admin credentials  \nTechnology  \nBriefly list the languages(s) and platform(s) that comprise the stack. This may include anything that is needed to understand the overall solution: OS, web server, presentation layer, persistence layer, caching, eventing, etc.  \nNon-Functional Requirements  \nWhat are the primary performance and scalability concerns for this milestone/epic?  \nAre there specific latency, availability, and RTO/RPO objectives that must be met?  \nAre there specific bottlenecks or potential problem areas? For example, are operations CPU or I/O (network, disk) bound?  \nHow large are the data sets and how fast do they grow?  \nWhat is the expected usage pattern of the service? For example, will there be peaks and valleys of intense concurrent usage?  \nAre there specific cost constraints? (e.g. $ per transaction/device/user)  \nOperationalization  \nAre there any specific considerations for the CI/CD setup of milestone/epic?  \nIs there a process (manual or automated) to promote builds from lower environments to higher ones?  \nDoes this milestone/epic require zero-downtime deployments, and if so, how are they achieved?  \nAre there mechanisms in place to rollback a deployment?  \nWhat is the process for monitoring the functionality provided by this milestone/epic?  \nDependencies  \nDoes this milestone/epic need to be sequenced after another epic assigned to the same team and why?  \nIs the milestone/epic dependent on another team completing other work?  \nWill the team need to wait for that work to be completed or could the work proceed in parallel?  \nRisks & Mitigations  \nDoes the team need assistance from subject-matter experts?  \nWhat security and privacy concerns does this milestone/epic have?  \nIs all sensitive information and secrets treated in a safe and secure manner?  \nOpen Questions  \nInclude any open questions and concerns.  \nAdditional References  \nInclude any additional references including links to work items or other documents.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\recipes\\milestone-epic-design-review-template.md"
    },
    {
        "chunkId": "chunk140_0",
        "chunkContent": "Preferred Diagram Tooling  \nAt each stage in the engagement process, diagrams are a key part of the design review.\nThe preferred tooling for creating and maintaining diagrams is to choose one of the following:  \nMicrosoft Visio  \nMicrosoft PowerPoint  \nThe .drawio.png (or .drawio) format from diagrams.net (formerly draw.io)  \nIn all cases, we recommend storing the exported PNG images from these diagrams in the repo along with the source files so they can easily be referenced in documentation and more easily reviewed during PRs. The .drawio.png format stores both at once.  \nMicrosoft Visio  \nIt contains a lot of shapes out of the box, including Azure icons, the desktop app exists on PC, and there's a great Web app. Most diagrams in the Azure Architecture Center are Visio diagrams.  \nMicrosoft PowerPoint  \nDiagrams can be easily reused in presentations, a PowerPoint license is pretty common, the desktop app exists on PC and on the Mac, and there's a great Web app.  \n.drawio.png  \nThere are different desktop, web apps and VS Code extensions.\nThis tooling can be used like Visio or LucidChart, without the licensing/remote storage concerns.\nFurthermore, Diagrams.net has a collection of Azure/Office/Microsoft icons, as well as other well-known tech, so it is not only useful for swimlanes and flow diagrams, but also for architecture diagrams.  \n.drawio.png should be preferred over the .drawio format.\nThe .drawio.png format uses the metadata layer within the PNG file-format to hide SVG vector graphics representation, then renders the .png when saving.\nThis clever use of both the meta layer and image layer allows anyone to further edit the PNG file.\nIt also renders like a normal PNG in browsers and other viewers, making it easy to transfer and embed.\nFurthermore, it can be edited within VSCode very easily using the Draw.io Integration VSCode Extension.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\recipes\\preferred-diagram-tooling.md"
    },
    {
        "chunkId": "chunk141_0",
        "chunkContent": "Design Review Recipes  \nDesign reviews come in all shapes and sizes. There are also different items to consider when creating a design at different stages during an engagement  \nDesign Review Process  \nIncorporate design reviews throughout the lifetime of an engagement  \nDesign Review Templates  \nGame Plan  \nThe same template already in use today  \nHigh level architecture and design  \nIncludes technologies, languages & products to complete engagement objective  \nMilestone / Epic Design Review  \nShould be considered when an engagement contains multiple milestones or epics  \nDesign should be more detailed than game plan  \nMay require unique deployment, security and/or privacy characteristics from other milestones  \nFeature/story design review  \nDesign for complex features or stories  \nWill reuse deployment, security and other characteristics defined within game plan or milestone  \nMay require new libraries, OSS or patterns to accomplish goals  \nTask design review  \nHighly detailed design for a complex tasks with many unknowns  \nWill integrate into higher level feature/component designs",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\recipes\\README.md"
    },
    {
        "chunkId": "chunk142_0",
        "chunkContent": "Spike: {Name}  \nConducted by: {Names and at least one email address for follow-up questions}  \nBacklog Work Item: {Link to the work item to provide more context}  \nSprint: {Which sprint did the study take place. Include sprint start date}  \nGoal  \nDescribe what question(s) the spike intends to answer and why.  \nMethod  \nDescribe how the team will uncover the answer to the question(s) the spike intends to answer. For example:  \nBuild prototype to test.  \nResearch existing documents and samples.  \nDiscuss with subject matter experts.  \nEvidence  \nDocument the evidence collected that informed the conclusions below. Examples may include:  \nRecorded or live demos of a prototype providing the desired capabilities  \nMetrics collected while testing the prototype  \nDocumentation that indicates the solution can provided the desired capabilities  \nConclusions  \nWhat was the answer to the question(s) outlined at the start of the spike? Capture what was learned that will inform future work.  \nNext Steps  \nWhat work is expected as an outcome of the learning within this spike. Was there work that was blocked or dependent on the learning within this spike?",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\recipes\\sprint-spike-template.md"
    },
    {
        "chunkId": "chunk143_0",
        "chunkContent": "Your Task Design Title Here (prefix with DRAFT/WIP to indicate level of completeness)  \nWhen developing a design document for a new task, it should contain a detailed design proposal demonstrating how it will solve the goals outlined below.  \nNot all tasks require a design review, but when they do it is likely that there many unknowns, or the solution may be more complex.\nThe design should include diagrams, pseudocode, interface contracts as needed to provide a detailed understanding of the proposal.  \nTask Name  \nStory Name  \nEngagement: [Engagement]  \nCustomer: [Customer]  \nAuthors: [Author1, Author2, etc.]  \nOverview/Problem Statement  \nIt can also be a link to the work item.  \nDescribe the task with a high-level summary.  \nConsider additional background and justification, for posterity and historical context.  \nGoals/In-Scope  \nList a few bullet points of what this task will achieve and that are most relevant for the design review discussion.  \nThis should include acceptance criteria required to meet the definition of done.  \nNon-goals / Out-of-Scope  \nList a few bullet points of non-goals to clarify the work that is beyond the scope of the design review for this task.  \nProposed Options  \nDescribe the detailed design to accomplish the proposed task.  \nWhat patterns & practices will be used and why were they chosen.  \nWere any alternate proposals considered?  \nWhat new components are required to be developed?  \nAre there any existing components that require updates?  \nRelevant diagrams (e.g. sequence, component, context, deployment) should be included here.  \nTechnology Choices  \nDescribe any libraries and OSS components that will be used to complete the task.  \nBriefly list the languages(s) and platform(s) that comprise the stack.  \nOpen Questions  \nList any open questions/concerns here.  \nAdditional References  \nList any additional references here including links to backlog items, work items or other documents.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\recipes\\task-design-review-template.md"
    },
    {
        "chunkId": "chunk144_0",
        "chunkContent": "Technical Spike  \nFrom Wikipedia...  \nA spike in a sprint can be used in a number of ways:  \nAs a way to familiarize the team with new hardware or software  \nTo analyze a problem thoroughly and assist in properly dividing work among separate team members.  \nSpike tests can also be used to mitigate future risk, and may uncover additional issues that have escaped notice.  \nA distinction can be made between technical spikes and functional spikes. The technical spike is used more often for evaluating the impact new technology has on the current implementation. A functional spike is used to determine the interaction with a new feature or implementation.  \nEngineering feasibility spikes can also be conducted to de-risk an engagement and increase the team's understanding.  \nDeliverable  \nGenerally the deliverable from a Technical Spike should be a document detailing what was evaluated and the outcome of that evaluation. The specifics contained in the document will vary, but there are some general principles that might be helpful.  \nProblem Statement/Goals: Be sure to include a section that clearly details why an evaluation is being done and what the outcome of this evaluation should be. This is helpful to ensure that the technical spike was productive and advanced the overall project in some way.  \nMake sure it is repeatable: Detail the components used, installation instructions, configuration, etc. required to build the environment that was used for evaluation and testing. If any testing is performed, make sure to include the scripts, links to the applications, configuration options, etc. so that testing could be performed again.\nThere are many reasons that the evaluation environment may need to be rebuilt. For example:  \nAnother scenario needs to be tested.  \nA new version of the technology has been released.  \nThe technology needs to be tested on a new platform.  \nFact-Finding: The goal of a spike should be fact-finding, not decision-making or recommendation. Ideally, the technology spike digs into a number of technical questions and gets answers so that the broader project team can then come back together and agree on an appropriate course forward.  \nEvidence: Generally you will use sections to summarize the results of testing which do not include the potentially hundreds of detailed results, however, you should include all detailed testing results in an appendix or an attachment. Having full results detailed somewhere will help the team trust the results. In addition, data can be interpreted lots of different ways, and it may be necessary to go back to the original data for a new interpretation.  \nOrganization: The technical documentation can be lengthy. It is generally a good idea to organize sections with headers and include a table of contents. Generally sections towards the beginning of the document should summarize data and use one or more appendices for more details.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\recipes\\technical-spike.md"
    },
    {
        "chunkId": "chunk145_0",
        "chunkContent": "Trade Studies  \nTrade studies are a tool for selecting the best option out of several possible options for a given problem (for example: compute, storage).\nThey evaluate potential choices against a set of objective criteria/requirements to clearly lay out the benefits and limitations\nof each solution.  \nTrade studies are a concept from systems engineering that we adapted for software projects. Trade\nstudies have proved to be a critical tool to drive alignment with the stakeholders, earn credibility while doing so and ensure our decisions\nwere backed by data and not bias.  \nWhen to use the tool  \nTrade studies go hand in hand with high level architecture design. This usually occurs as project requirements are solidifying, before\ncoding begins. Trade studies continue to be useful throughout the project any time there are multiple options that need\nto be selected from. New decision point could occur from changing requirements, getting results of a research spike, or identifying\nchallenges that were not originally seen.  \nTrade studies should be avoided if there is a clear solution choice. Because they require each solution to be fully thought out, they\nhave the potential to take a lot of time to complete. When there is a clear design, the trade study should be omitted, and an entry\nshould be made in the Decision Log documenting the decision.  \nWhy Trade Studies  \nTrade studies are a way of formalizing the design process and leaving a documentation record for why the decision was made. This gives a few advantages:  \nThe trade study template guides a user through the design process. This provides structure to the design stage.  \nHaving a uniform design process aids splitting work amongst team members. We have had success with engineers pairing to define requirements, evaluation criteria, and brainstorming possible solutions. Then they can each split to review solutions in parallel, before rejoining to make the final decision.  \nThe completed trade study document helps drive alignment across the team and decision makers. For presenting results of the study, the document itself can be used to highlight the main points. Alternatively, we have extracted requirements, diagrams for each solution, and the results table into a slide deck to give high level overviews of the results.  \nThe completed trade study gets checked into the code repository, providing documentation of the decision process. This leaves a history of the requirements at the time that lead to each decision. Also, the results table gives a quick reference for how the decision would be impacted if requirements change as the project proceeds.  \nFlow of a Trade Study  \nTrade studies can vary widely in scope; however, they follow the common pattern below:  \nSolidify the requirements \u2013 Work with the stakeholders to agree on the requirements for the functionality that you are trying to build.  \nCreate evaluation criteria \u2013 This is a set of qualitative and quantitative assessment points that represent the requirements. Taken together, they become an easy to measure stand-in for the potentially abstract requirements.  \nBrainstorm solutions \u2013 Gather a list of possible solutions to the problem. Then, use your best judgement to pick the 2-4 solutions that seem most promising. For assistance narrowing solutions, remember to reach out to subject-matter experts and other teams who may have gone through a similar decision.  \nEvaluate shortlisted solutions \u2013 Dive deep into each solution and measure it against the evaluation criteria. In this stage, time box your research to avoid overly investing in any given area.  \nCompare results and choose solution - Align the decision with the team. If you are unable to decide, then a clear list of action items and owners to drive the final decision must be produced.  \nTemplate  \nSee template.md for an example of how to structure the above information. This template was created to guide a user\nthrough conducting a trade study. Once the decision has been made we recommend adding an entry to the\nDecision Log that has references back to the full text of the trade study.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\trade-studies\\README.md"
    },
    {
        "chunkId": "chunk146_0",
        "chunkContent": "Trade Study Template  \nThis generic template can be used for any situation where we have a set of requirements that can be satisfied\nby multiple solutions. They can range in scope from choice of which open source package to use, to full\narchitecture designs.  \nTrade Study/Design: {study name goes here}  \nConducted by: {Names of those that can answer follow-up questions and at least one email address}  \nBacklog Work Item: {Link to the work item to provide more context}  \nSprint: {Which sprint did the study take place? Include sprint start date}  \nDecision: {Solution chosen to proceed with}  \nDecision Makers:  \nIMPORTANT Designs should be completed within a sprint. Most designs will benefit from brevity. To accomplish this:  \nNarrow the scope of the design.  \nNarrow evaluation to 2 to 3 solutions.  \nDesign experiments to collect evidence as fast as possible.  \nOverview  \nDescription of the problem we are solving. This should include:  \nAssumptions about the rest of the system  \nConstraints that apply to the system, both business and technical  \nRequirements for the functionality that needs to be implemented, including possible inputs and outputs  \n(optional) A diagram showing the different pieces  \nDesired Outcomes  \nThe following section should establish the desired capabilities of the solution for it to be successful. This can be done by answering the following questions either directly or via link to related artifact (i.e. PBI or Feature description).  \nAcceptance: What capabilities should be demonstrable for a stakeholder to accept the solution?  \nJustification: How does this contribute to the broader project objectives?  \nIMPORTANT This is not intended to define outcomes for the design activity itself. It is intended to define the outcomes for the solution being designed.  \nAs mentioned in the User Interface section, if the trade study is analyzing an application development solution, make use of the persona stories to derive desired outcomes. For example, if a persona story exemplifies a certain accessibility requirement, the parallel desired outcome may be \"The application must be accessible for people with vision-based disabilities\".  \nEvaluation Criteria  \nThe former should be condensed down to a set of \"evaluation criteria\" that we can rate any potential solutions\nagainst. Examples of evaluation criteria:  \nRuns on Windows and Linux - Binary response  \nCompute Usage - Could be categories that effectively rank different options: High, Medium, Low  \nCost of the solution \u2013 An estimated numeric field  \nThe results section contains a table evaluating each solution against the evaluation criteria.  \nKey Metrics (Optional)  \nIf available, describe any measurable metrics that are important to the success of the solution. Examples include, but are not limited to:  \nPerformance & Scale targets such as, Requests/Second, Latency, and Response time (at a given percentile).  \nAzure consumption cost budget. For example, given certain usage, solution expected to cost X dollars per month.  \nAvailability uptime of XX% over X time period.  \nConsistency. Writes available for read within X milliseconds.  \nRecovery point objective (RPO) & Recovery time objective (RTO).  \nConstraints (Optional)  \nIf applicable, describe the boundaries from which we have to design the solution. This could be thought of as the \"box\" the team has to work within. This box may be defined as:  \nTechnologies, services, and languages an organization is comfortable operating/managing.  \nDevices, operating systems, and/or browsers that must be supported.  \nBackward Compatibility. For example, public interfaces consumed by client or third party apps cannot introduce breaking changes.  \nIntegrations or dependencies with other systems. For example, push notifications to client apps must be done via existing websockets channel.  \nAccessibility  \nAccessibility is never optional. Microsoft has made a public commitment to always produce accessible applications. For more information visit the official Microsoft accessibility site and read the Accessibility page.  \nConsider the following prompts when determining application accessibility requirements:  \nDoes the application meet industry accessibility standards?  \nAre training, support, and documentation resources accessible?  \nIs the application designed to be inclusive for people will a broad range of abilities, languages, and cultures?  \nSolution Hypotheses  \nEnumerate the solutions that are believed to deliver the outcomes defined above.  \nNOTE: Limiting the evaluated solutions to 2 or 3 potential candidates can help manage the time spent on the evaluation. If there are more than 3 candidates, prioritize what the team feels are the top 3. If appropriate, the eliminated candidates can be mentioned to capture why they were eliminated. Additionally, there should be at least two options compared, otherwise you didn't need a trade study.  \n{Solution 1} - Short and easily recognizable name  \nAdd a brief description of the solution and how its expected to produce the desired outcomes. If appropriate, illustrations/diagrams can be used to reduce the amount of text explanation required to describe the solution.  \nNOTE: Using present tense language to describe the solution can help avoid confusion between current state and future state. For example, use \"This solution works by doing...\" vs. \"This solution would work by doing...\".  \nEach solution section should contain the following:  \nDescription of the solution  \n(optional) A diagram to quickly reference the solution  \nPossible variations - things that are small variations on the main solution can be grouped together  \nEvaluation of the idea based on the evaluation criteria above  \nThe depth, detail, and contents of these sections will vary based on the complexity of the functionality\nbeing developed.  \nExperiment(s)  \nDescribe how the solution will be evaluated to prove or dis-prove that it will produce the desired outcomes. This could take many forms such as building a prototype and researching existing documentation and sample solutions.  \nAdditionally, document any assumptions made as part of the experiment.  \nNOTE: Time boxing these experiments can be beneficial to make sure the team is making the best use of the time by focusing on collecting key evidence in the simplest/fastest way possible.  \nEvidence  \nPresent the evidence collected during experimentation that supports the hypothesis that this solution will meet the desired outcomes. Examples may include:  \nRecorded or live demos of a prototype providing the desired capabilities  \nMetrics collected while testing the prototype  \nDocumentation that indicates the solution can provide the desired capabilities  \nNOTE: Evidence is not required for every capability, metric, or constraint for the design to be considered done. Instead, focus on presenting evidence that is most relevant and impactful towards supporting or eliminating the hypothesis.  \n{Solution 2}  \n...  \n{Solution N}  \n...  \nResults  \nThis section should contain a table that has each solution rated against each of the evaluation criteria:  \nSolution Evaluation Criteria 1 Evaluation Criteria 2 ... Evaluation Criteria N Solution 1 Solution 2 ... Solution M  \nNote: The formatting of the table can change. In the past, we have had success with qualitative descriptions\nin the table entries and color coding the cells to represent good, fair, bad.  \nDecision  \nThe chosen solution, or a list of questions that need to be answered before the decision can be made.  \nIn the latter case, each question needs an action item and an assigned person for answering the question. Once those questions are answered, the document must be updated to reflect the answers, and the final decision.  \nIn the first case, describe which solution was chosen and why. Summarize what evidence informed the decision and how that evidence mapped to the desired outcomes.  \nIMPORTANT: Decisions should be made with the understanding that they can change as the team learns more. It's a starting point, not a contract.  \nNext Steps  \nWhat work is expected once a decision has been reached? Examples include but are not limited to:  \nCreating new PBI's or modifying existing ones  \nFollow up spikes  \nCreating specification for public interfaces and integrations between other work streams.  \nDecision Log Entry",
        "source": "..\\data\\docs\\code-with-engineering\\design\\design-reviews\\trade-studies\\template.md"
    },
    {
        "chunkId": "chunk147_0",
        "chunkContent": "Diagram Types  \nCreating and maintaining diagrams is a challenge for any team. Common reasons across these challenges include:  \nNot leveraging tools to assist in generating diagrams  \nUncertainty on what to include in a diagram and when to create one  \nOvercoming these challenges and effectively using design diagrams can amplify a team's ability to execute throughout the entire Software Development Lifecycle, from the design phase when proposing various designs to leveraging it as documentation as part of the maintenance phase.  \nThis section will share sample tools for diagram generation, provide a high level overview of the different types of diagrams and provide examples of some of these types.  \nThere are two primary classes of diagrams:  \nStructural  \nBehavior  \nWithin each of these classes, there are many types of diagrams, each intended to convey specific types of information. When different types of diagrams are effectively used in a solution, system, or repository, one can deliver a cohesive and incrementally detailed design.  \nSample Design Diagrams  \nThis section contains educational material and examples for the following design diagrams:  \nClass Diagrams - Useful to document the structural design of a codebase's relationship between classes, and their corresponding methods  \nComponent Diagrams - Useful to document a high level structural overview of all the components and their direct \"touch points\" with other Components  \nSequence Diagrams - Useful to document a behavior overview of the system, capturing the various \"use cases\" or \"actions\" that triggers the system to perform some business logic  \nDeployment Diagram - Useful in order to document the networking and hosting environments where the system will operate in  \nSupplemental Resources  \nEach of the above types of diagrams will provide specific resources related to its type. Below are the generic resources:  \nVisual Paradigm UML Structural vs Behavior Diagrams  \nPlantUML - requires a generator from code to PlantUML syntax to generate diagrams  \nC# to PlantUML  \nDrawing manually",
        "source": "..\\data\\docs\\code-with-engineering\\design\\diagram-types\\README.md"
    },
    {
        "chunkId": "chunk148_0",
        "chunkContent": "Class Diagrams  \nPurpose  \nThis document is intended to provide a baseline understanding for what, why, and how to incorporate Class Diagrams as part of your engagement. Regarding the how, the section at the bottom will provide tools and plugins to automate as much as possible when generating Class Diagrams through VSCode.  \nWikipedia defines UML Class Diagrams as:  \na type of static structure diagram that describes the structure of a system by showing the system's classes, their attributes, operations (or methods), and the relationships among objects.  \nThe key terms to make a note of here are:  \nstatic structure  \nshowing the system's classes, attributes, operations, and relationships  \nClass Diagrams are a type of a static structure because it focuses on the properties, and relationships of classes. It is not supposed to inform about the data flow, the caller or callee responsibilities, the request flows, nor any other \"behavior\" related characteristics.  \nEssential Takeaways  \nEach \"Component\" (Stand alone piece of software - think datastores, microservices, serverless functions, user interfaces, etc...) of a Product or System will have it's own Class Diagram.  \nClass Diagrams should tell a \"story\", where each Diagram will require Engineers to really think about:  \nThe responsibility / operations of each class. What can (should) the class perform?  \nThe class' attributes and properties. What can be set by an implementor of this class? What are all (if any) universally static properties?  \nThe visibility or accessibility that a class' operation may have to other classes  \nThe relationship between each class or the various instances  \nWhen to Create?  \nBecause Class Diagrams represent one of the more granular depiction of what a \"product\" or \"system\" is composed of, it is recommended to begin the creation of these diagrams at the beginning and throughout the engineering portions of an engagement.  \nThis does mean that any code change (new feature, enhancement, code refactor) might involve updating one or many Class Diagrams. Although this might seem like a downside of Class Diagrams, it actually can become a very strong benefit.  \nBecause Class Diagrams tell a \"story\" for each Component of a product (see the previous section), it requires a substantial amount of upfront thought and design considerations. This amount of upfront thought ultimately results in making more effective code changes, and may even minimize the level of refactors in future stages of the engagement.  \nClass Diagrams also provides quick \"alert indicators\" when a refactor might be necessary. Reasons could be due to seeing that a particular class might be doing too much, have too many dependencies, or when the codebase might produce a very \"messy\" or \"chaotic\" Class Diagram. If the Class Diagram is unreadable, the code will probably be unreadable  \nExamples  \nOne can find many examples online such as at UML Diagrams.  \nBelow are some basic examples:  \nVersioning  \nBecause Class Diagrams will be changing rapidly, essentially anytime a class is changed in the code, and because it might be very large in size, it's recommended to \"publish\" an image of the generated diagram periodically. The frequency might vary as the engagement proceeds.  \nThe below approach can be used to assist the team on how often to update the published version of the diagram:  \nWait until the engagement progresses (maybe 10-20% completion) before publishing a Class Diagram. It is not worth publishing a Class Diagram from the beginning as it will be changing daily  \nOnce the most crucial classes are developed, update the published diagram periodically. Ideally whenever a large refactor or net new class is introduced. If the team uses an IDE plugin to automatically generate the diagram from their development environment, this becomes more of a documentation task rather than a necessity  \nAs the engagement approaches its end (90-100% completion), update the published diagram whenever a change to an existing class as part of a feature or story acceptance criteria  \nDepending on the tool being used, automatic versioning might be performed whenever an update to the Diagram is performed. If not, it is recommended to capture distinct versions whenever there is a particular customer need to have a snapshot of the project at a particular point in time. The hard requirement is that the latest diagram should be published and everyone should know how to access it as\nthe customer hand-off approaches.  \nResources  \nWikipedia  \nVisual Paradigm  \nVS Code Plugins:  \nC#, Visual Basic, C++ using Class Designer Component  \nTypeScript classdiagram-ts  \nPlantUML - requires a generator from code to PlantUML syntax to generate diagrams\nPlantUML Syntax\nC# to PlantUML\nDrawing manually",
        "source": "..\\data\\docs\\code-with-engineering\\design\\diagram-types\\DesignDiagramsTemplates\\classDiagrams.md"
    },
    {
        "chunkId": "chunk149_0",
        "chunkContent": "Component Diagrams  \nPurpose  \nThis document is intended to provide a baseline understanding for what, why, and how to incorporate Component Diagrams\nas part of your engagement. Regarding the how, the section at the bottom will provide tools and plugins to streamline as much as possible when generating Component Diagrams through VSCode.  \nWikipedia defines UML Component Diagrams as:  \na component diagram depicts how components are wired together to form larger components or software systems.  \nComponent Diagrams are a type of a static structure because it focuses on the responsibility and relationships between components as part of the overall system or solution.  \nIt is not supposed to inform about the data flow, the caller or callee responsibilities, the request flows, nor any other \"behavior\" related characteristics.  \n...Hold on a second... what is a Component?  \nA Component is a runnable solution that performs a set of operations and can possibly be interfaced through a particular API. One can think of Components as a \"stand alone\" piece of software - think datastores, microservices, serverless functions, user interfaces, etc...  \nEssential Takeaways  \nThe primary two takeaways from a Component Diagram should be:  \nA quick view of all the various components (User Interface, Service, Data Storage) involved in the system  \nThe immediate \"touch points\" that a particular Component has with other Components, including how that \"touch point\" is accomplished (HTTP, FTP, etc...)  \nDepending on the complexity of the system, a team might decide to create several Component Diagrams. Where there is one diagram per Component (depicting all it's immediate \"touch points\" with other Components).  \nOr if a system is simple, the team might decide to create a single Component Diagram capturing all Components in the diagram.  \nWhen to Create?  \nBecause Component Diagrams represent a high level overview of the entire system from a Component focus, it is recommended to begin the creation of this diagram from the beginning of an engagement, and update it as the various Components are identified, developed, and introduced into the system. Otherwise, if this is left till later, then there is risk that:  \nthe team won't be able to identify areas of improvement  \nthe team or other necessary stakeholders won't have a full understanding on how the system works as it is being developed  \nBecause of the inherent granularity of the system, the Component Diagrams won't have to be updated as often as Class Diagrams. Things that might merit updating a Component Diagram could be:  \nA deletion or addition of a new Component into the system  \nA change to a system Component's interaction APIs  \nA change to a system Component's immediate \"touch points\" with other Components  \nBecause Component Diagrams focuses on informing the various \"touch points\" between Components, it requires some upfront thought in order to determine what Components are needed and what interaction mechanisms are most effective per the system requirements. This amount of upfront thought should be approached in a pragmatic manner - as the design may evolve over time, and that is perfectly fine,\nas long as changes are influenced based on functional requirements and non-functional requirements.  \nExamples  \nBelow are some basic examples:  \nVersioning  \nBecause Component Diagrams will be changing periodically, it's recommended to \"publish\" an image of the generated diagram periodically. The frequency might vary as the engagement proceeds.  \nThe below approach can be used to assist the team on how often to update the published version of the diagram:  \nAt the beginning of the engagement, publishing an \"envisioned\" version of the Component Diagram will provide a common visual to all engineers when working on the different parts of the solution  \nThroughout the engagement, update the published diagram periodically. Ideally whenever a new Component is introduced into the system, or whenever a new \"touch point\" occurs between Components  \nDepending on the tool being used, automatic versioning might be performed whenever an update to the Diagram is performed. If not, it is recommended to capture distinct versions whenever there is a particular customer need to have a snapshot of the project at a particular point in time. The hard requirement is that the latest diagram should be published and everyone should know how to access it as\nthe customer hand-off approaches.  \nResources  \nWikipedia  \nVisual Paradigm  \nVS Code Plugins:  \nPlantUML - requires a generator from code to PlantUML syntax to generate diagrams\nPlantUML Syntax\nDrawing manually",
        "source": "..\\data\\docs\\code-with-engineering\\design\\diagram-types\\DesignDiagramsTemplates\\componentDiagrams.md"
    },
    {
        "chunkId": "chunk150_0",
        "chunkContent": "Deployment Diagrams  \nPurpose  \nThis document is intended to provide a baseline understanding for what, why, and how to incorporate Deployment Diagrams\nas part of your engagement.  \nWikipedia defines UML Deployment Diagrams as:  \nmodels the physical deployment of artifacts on nodes  \nDeployment Diagrams are a type of a static structure because it focuses on the infrastructure and hosting where all aspects of the system reside in.  \nIt is not supposed to inform about the data flow, the caller or callee responsibilities, the request flows, nor any other \"behavior\" related characteristics.  \nEssential Takeaways  \nThe Deployment diagram should contain all Components identified in the Component Diagram(s), but captured alongside the following elements:  \nFirewalls  \nVNETs and subnets  \nVirtual machines  \nCloud Services  \nData Stores  \nServers (Web, proxy)  \nLoad Balancers  \nThis diagram should inform the audience:  \nwhere things are hosted / running in  \nwhat network boundaries are involved in the system  \nWhen to Create?  \nBecause Deployment Diagrams represent the final \"hosting\" architecture, it's recommended to create the \"final envisioned\" diagram from the beginning of an engagement. This allows the team to have a shared idea on what the team is working towards. Keep in mind that this might change if any non-functional requirement was not considered at the start of the engagement. This is okay, but\nrequires creating the necessary Backlog Items and updating the Deployment diagram in order to capture these changes.  \nIt's also worthwhile to create and maintain a Deployment Diagram depicting the \"current\" state of the system. At times, it may be beneficial for there to be a Deployment Diagram per each environment (Dev, QA, Staging, Prod, etc...). However, this adds to the amount of maintenance required and should only be performed if there are substantial differences across environments.  \nThe \"current\" Deployment diagram should be updated when:  \nA new element has been introduced or removed in the system (see the \"Essential Takeaways\" section for a list of possible elements)  \nExamples  \nBelow are some basic examples:  \nVersioning  \nBecause Deployment Diagrams will be changing periodically, it's recommended to \"publish\" an image of the generated diagram periodically. The frequency might vary as the engagement proceeds.  \nThe below approach can be used to assist the team on how often to update the published version of the diagram:  \nAt the beginning of the engagement, publishing an \"envisioned\" version of the Component Diagram will provide a common visual to all engineers when working on the different parts of the solution  \nThroughout the engagement, update the \"actual / current\" diagram (state represented from the \"main\" branch) periodically. Ideally whenever a new Component is introduced into the system, or whenever a new \"touch point\" occurs between Components.  \nResources  \nWikipedia  \nVisual Paradigm  \nPlantUML - requires a generator from code to PlantUML syntax to generate diagrams\nPlantUML Syntax\nDrawing manually",
        "source": "..\\data\\docs\\code-with-engineering\\design\\diagram-types\\DesignDiagramsTemplates\\deploymentDiagrams.md"
    },
    {
        "chunkId": "chunk151_0",
        "chunkContent": "Sequence Diagrams  \nPurpose  \nThis document is intended to provide a baseline understanding for what, why, and how to incorporate Sequence Diagrams\nas part of an engagement. Regarding the how, the section at the bottom will provide tools and plugins to streamline as much as possible when generating Sequence Diagrams through VSCode.  \nWikipedia defines UML Sequence Diagrams responsible to:  \ndepict the objects involved in the scenario and the sequence of messages exchanged between the objects needed to carry out the functionality of the scenario  \nWhat is a scenario? It can be:  \nan actual user persona performing an action  \na system specific trigger (time based, condition based) that results in an action to occur  \nWhat is a message in this context? It can be:  \na synchronous or asynchronous request  \na transfer of any form of data between any objects  \nWhat is an object in this context? It can be:  \nany specific user persona  \nany service  \nany data store  \na system (black box composed of unknown services, data stores or other components)  \nan abstract sub-scenario (in order to minimize high complexity of a scenario)  \nEssential Takeaways  \nA Sequence Diagram should:  \nstart with a scenario  \nindicate which object or \"actor\" initiated that scenario  \nhave the scenario clearly indicate what the \"end\" state is, even if it doesn't necessarily end back with the object that initiated the scenario  \nIt is okay for a single Sequence Diagram to have many different scenarios if they have some related context that merits them being grouped.  \nAnother important thing to keep in mind, is that the objects involved in a Sequence Diagram should refer to existing Components from a Component Diagram.  \nThere are 2 areas where complexity can result in an overly \"crowded\" Sequence Diagram, making it costly to maintain. They are:  \nLarge number of objects / components involved in a particular scenario  \nCapturing all the possible \"failure\" situations that a scenario may encounter  \nLarge Number of Objects  \nA Sequence Diagram typically starts with an end user persona performing an action, and then shows all the various components and request/data transfers that are involved in that scenario. However, more often than not, the complete end-to-end flow for that scenario may be too complex in order to capture within a single Sequence Diagram.  \nWhen this level of complexity occurs, consider creating separate sub-scenario Sequence Diagrams, and using it as an object in a particular Sequence Diagram. Examples for this are \"Authentication\" or \"Authorization\". Almost all user persona scenarios will have several objects/components involved in either of these sub-scenarios, but it is not necessary to include them in every Sequence Diagram\nonce the sub-scenarios have a stand-alone Sequence Diagram created.  \nBe sure that when using this approach of sub-scenarios to give it a name that encapsulates what the sub-scenarios is performing, and to determine the appropriate \"actor\" and \"action\" that initiates the sub-scenarios.  \nThe combination and story telling between these end user Sequence Diagrams and the sub-scenarios Sequence Diagrams can greatly improve readability by distributing the level of complexity across multiple diagrams and take advantage of reusability of common sub-scenarios.  \nHandling Large Number of Failure Situations  \nAnother factor of high complexity is the possible failure situations that a particular scenario may encounter. Each object / component involved in the scenario could have several different \"failure\" situations, which could result in a very crowded and messy Sequence Diagram.  \nIn order to make it realistic to manage all these scenarios, try to:  \nIdentify the most common failure situations that an \"actor\" may face as part of a scenario. Capturing these in a sequence diagram and documenting the other scenarios without having to manage them in a diagram will accomplish the goal of awareness  \n\"Bubble up\" and \"abstract\" all the vast number of failure situations that can occur downstream in the system, and depict how the object / component closest to the \"actor\" handles all these failures and informs the \"actor\" of them  \nWhen to Create?  \nBecause Sequence Diagrams represent a detailed overview of the behavior of the system, outlining the various messages/requests sent within the system, it is recommended to begin the creation of these diagrams from the beginning of an engagement. While updating it as the various communications between Components are introduced into the system. The risks of not creating Sequence Diagrams\nearly on are that:  \nthe team will not create any because of it being perceived more as a \"chore\" instead of adding value  \nthe team will be unable to gain insights in time, from visualizing the various messages and requests sent between Components, in order to perform any potential refactoring  \nthe team or other necessary stakeholders won't have a complete understanding of the request/message/data flow within the system  \nBecause of the inherent granularity of the system, the Sequence Diagrams won't have to be updated as often as Class Diagrams, but may require more maintenance than Component Diagrams. Things that might merit updating a Sequence Diagram could be:  \nA new request/message/data being sent across Components involved in a scenario  \nA change to one or several Components involved in a Sequence Diagram. Such as splitting a component into multiple ones, or consolidating many Components into a single one  \nThe introduction of a new Use Case or scenario that the system now supports  \nExamples  \nPlace Order Scenario:  \nA \"Member\" user persona places an order, which can be composed of many \"order items\"  \nThe \"Member\" user persona can be either of type \"VIP\" or \"Ordinary\"  \nDepending on the \"Member type\", each \"order item\" will be shipped using either a Courier or via Mail  \nIf the \"Member\" user persona selected the option to be informed once all \"order items\" have been shipped, then the system will send a notification  \nFacebook User Authentication Scenario:  \nA user persona uses a Web Browser to interact with an \"application\" which tries to access a specific \"Facebook resource\"  \nThe \"Facebook Authorization Server\" is involved in order to have the user to authenticate with Facebook  \nThe user persona then receives a \"permission form\" in order to authorize the \"application\" access to the \"Facebook resource\"  \nIf the \"application\" was not authorized, then the \"application\" returns back an error  \nIf the \"application\" was authorized, then the \"application\" retrieves an \"access token\" from the \"Facebook Authorization Server\" and uses it to securely access the \"Facebook resource\" from the \"Facebook Content Server\". Once the content is obtained, the \"application\" sends it to the Web Browser  \nVersioning  \nBecause Sequence Diagrams are more expensive to maintain, it's recommended to \"publish\" an image of the generated diagram often, whenever a new \"use case\" or \"scenario\" is identified as part of the system behavior or requirements.  \nThe most important element to these diagrams is to ensure that the latest version is accurate. If the latest diagram shows a sequence of communication between components that are no longer valid, then the diagram causes more harm than good.  \nThe below approach can be used to assist the team on how often to update the published version of the diagram:  \nAt the beginning of the engagement, publishing an \"envisioned\" version of the Sequence Diagram will provide a common visual to all engineers when working on the different parts of the solution (focusing on the data flow and request flow)  \nThroughout the engagement, update the published diagram periodically. Ideally whenever a new \"use case\" or \"scenario\" is identified, or when a Component is introduced or removed in the system, or when a change in data/request flow is made in the system  \nDepending on the tool being used, automatic versioning might be performed whenever an update to the Diagram is performed. If not, it is recommended to capture distinct versions whenever there is a particular customer need to have a snapshot of the project at a particular point in time. The hard requirement is that the latest diagram should be published and everyone should know how to access it as\nthe customer hand-off approaches.  \nResources  \nWikipedia  \nVisual Paradigm  \nVS Code Plugins:  \nPlantUML - requires a generator from code to PlantUML syntax to generate diagrams\nPlantUML Syntax\nDrawing manually",
        "source": "..\\data\\docs\\code-with-engineering\\design\\diagram-types\\DesignDiagramsTemplates\\sequenceDiagrams.md"
    },
    {
        "chunkId": "chunk152_0",
        "chunkContent": "Exception handling  \nException constructs  \nAlmost all language platforms offer a construct of exception or equivalent to handle error scenarios. The underlying platform, used libraries or the authored code can \"throw\" exceptions to initiate an error flow. Some of the advantages of using exceptions are -  \nAbstract different kind of errors  \nBreaks the control flow from different code structures  \nNavigate the call stack till the right catch block is identified  \nAutomatic collection of call stack  \nDefine different error handling flows thru multiple catch blocks  \nDefine finally block to cleanup resources  \nHere is some guidance on exception handling in .Net  \nC# Exception fundamentals  \nHandling exceptions in .Net  \nCustom exceptions  \nAlthough the platform offers numerous types of exceptions, often we need custom defined exceptions to arrive at an optimal low level design for error handling. The advantages of using custom exceptions are -  \nDefine exceptions specific to business domain of the requirement. E.g. InvalidCustomerException  \nWrap system/platform exceptions to define more generic system exception so that overall code base is more tech stack agnostic. E.g DatabaseWriteException which wraps MongoWriteException.  \nEnrich the exception with more information about the code flow of the error.  \nEnrich the exception with more information about the data context of the error. E.g. RecordId in property in DatabaseWriteException which carries the Id of the record failed to update.  \nDefine custom error message which is more business user friendly or support team friendly.  \nCustom exception hierarchy  \nBelow diagram shows a sample hierarchy of custom exceptions.  \nIt defines a BaseException class which derives from System.Exception class and parent of all custom exceptions. BaseException also has additional properties for ActionCode and ResultCode. ActionCode represents the \"flow\" in which the error happened. ResultCode represents the exact error that happened. These additional properties help in defining different error handling flows in the catch blocks.  \nDefines a number of System exceptions which derive from SystemException class. They will address all the errors generated by the technical aspects of the code. Like connectivity, read, write, buffer overflow etc  \nDefines a number of Business exceptions which derive from BusinessException class. They will address all the errors generated by the business aspects of the code. Like data validations, duplicate rows.  \nError details in API response  \nWhen an error occurs in an API, it has to rendered as response with all the necessary fields. There can be custom response schema drafted for these purposes. But one of the popular formats is the problem detail structure -  \nProblem details  \nThere are inbuilt problem details middleware library built in ASP.Net core. For further details refer to below link  \nProblem details service in ASP.Net core",
        "source": "..\\data\\docs\\code-with-engineering\\design\\exception-handling\\readme.md"
    },
    {
        "chunkId": "chunk153_0",
        "chunkContent": "Sustainable Software Engineering  \nThe choices made throughout the engineering process regarding cloud services, software architecture design and automation can have a big impact on the carbon footprint of a solution.\nSome choices are always beneficial, like turning off unused resources.\nOther choices require a more nuanced understanding of the business case at hand and its potential carbon impact.  \nGoal  \nOne goal of this section is to provide tangible guidance for what sustainable actions you can apply in certain situations and the tools to be able to implement those recommendations.\nAnother goal is to highlight the many resources available to learn about the wider domain of sustainable software.  \nSustainable Engineering Checklist  \nThis checklist should be used to quickly identify scenarios for which common sustainable actions exist.\nCheck the box if the scenario applies to your project, then go through the actions and tools you can use to build more sustainable software for those cases.\nIf there are important nuances to consider, they will be linked in the Disclaimers section.  \nFor readability some considerations are blank, indicating that the action applies to the first consideration above it.  \n\u2705 Consideration Action Principle Tools Disclaimers For any running software/services Shutdown unused resources. Electricity Consumption Identify Unassociated Resources Resize physical or virtual machines to improve utilization. Energy Proportionality Azure Advisor Cost Recommendations Understanding Advisor Recommendations For development and testing VMs Configure VMs to shutdown during off-hours Electricity Consumption Start/Stop VMs during off-hours For VMs with attached volumes Limit the amount of attached storage capacity to what you expect to use and expand as necessary Electricity Consumption Expanding storage of active VMs Understanding the energy cost of storage For systems using object storage (Azure Blob Storage, AWS S3, GCP Cloud Storage, etc) Compress infrequently accessed data Electricity Consumption , Embodied Carbon Compressing and extracting files in .NET Understanding the energy cost of storage Delete data when it is no longer needed Electricity Consumption Configuring a lifecycle management policy Understanding the energy cost of storage For systems running in on-premise data centers Migrate to hyperscale cloud provider Embodied Carbon , Electricity Consumption Cloud Adoption Approaches Carbon benefits of cloud computing For systems migrating to a hyperscale cloud provider Consider physically shipping data to the provider Networking Azure Data Box Understanding data shipping tradeoffs For time-flexible workloads Utilize \"Spot VMs\" for compute Demand Shaping How to use Spot VMs For services with varied utilization patterns Configure Autoscaling Energy Proportionality Autoscaling Documentation Use serverless functions Energy Proportionality Serverless Architecture Design For services with geographically co-located users (EG internal employee apps) Select a data center region that is physically close to them Networking Azure products available by region Consider running edge devices to reduce excessive data transfer Networking Azure Stack Edge Understanding edge tradeoffs For systems sending data over the network Use caching policies to keep data on the local machine Networking HTTP caching APIs , Cache Management in .NET Understanding caching tradeoffs Consider caching data close to end users with a CDN Networking Benefits of a CDN Understanding CDN tradeoffs Send only the data that will be used Networking Compress data to reduce the size Networking Compressing and extracting files in .NET When designing for the end user Consider giving users visibility and control over their energy usage Electricity Consumption Demand Shaping Designing for eco-mode Design and test your application to be compatible for a wide variety of devices, especially older devices Embodied Carbon Extending device lifespan Compatibility Testing When selecting a programming language Consider the energy efficiency of languages Electricity Consumption Reasoning about the energy consumption of programming languages , Programming Language Energy Efficiency (PDF) Making informed programming language choices  \nResources  \nPrinciples of Green Software Engineering  \nGreen Software Foundation  \nMicrosoft Cloud for Sustainability  \nLearning Module: Sustainable Software\nEngineering  \nTools  \nCarbon-Aware SDK  \n\"Awesome List\" of Green Software  \nEmissions Impact  \nAzure GreenAI Carbon-Intensity API  \nProjects  \nSustainability through SpotVMs",
        "source": "..\\data\\docs\\code-with-engineering\\design\\sustainability\\readme.md"
    },
    {
        "chunkId": "chunk154_0",
        "chunkContent": "Disclaimers  \nThe following disclaimers provide more details about how to consider the impact of particular actions recommended by the Sustainable Engineering Checklist.  \nACTION: Resize physical or virtual machines to improve utilization  \nRecommendations from cost-savings tools are usually aligned with carbon-reduction, but as sustainability is not the purpose of such tools, carbon-savings are not guaranteed. How a cloud provider or data center manages unused capacity is also a factor in determining how impactful this action may be. For example:  \nThe sustainable impact of using smaller VMs in the same family are typically beneficial or neutral. When cores are no longer reserved they can be used by others instead of bringing new servers online.  \nThe sustainable impact of changing VM families can be harder to reason about because the underlying hardware and reserved cores may be changing with them.  \nACTION: Migrate to a hyperscale cloud provider  \nCarbon savings from hyperscale cloud providers are generally attributable to four key features: IT operational efficiency, IT equipment efficiency, data center infrastructure efficiency, and renewable electricity. Microsoft Cloud, for example, is between 22 and 93 percent more energy efficient than traditional enterprise data centers, depending on the specific comparison being made. When taking into account renewable energy purchases, the Microsoft Cloud is between 72 and 98 percent more carbon efficient. Source (PDF)  \nACTION: Consider running an edge device  \nRunning an edge device negates many of the benefits of hyperscale compute facilities, so considering the local energy grid mix and the typical timing of the workloads is important to determine if this is beneficial overall.  The larger volume of data that needs to be transmitted, the more this solution becomes appealing. For example, sending large amounts of audio and video content for processing.  \nACTION: Consider physically shipping data to the provider  \nShipping physical items has its own carbon impact, depending on the mode of transportation, which needs to be understood before making this decision.  The larger the volume of data that needs to be transmitted the more this options may be beneficial.  \nACTION: Consider the energy efficiency of languages  \nWhen selecting a programming language, the most energy efficient programming language may not always be the best choice for development speed, maintenance, integration with dependent systems, and other project factors. But when deciding between languages that all meet the project needs, energy efficiency can be a helpful consideration.  \nACTION: Use caching policies  \nA cache provides temporary storage of resources that have been requested by an application. Caching can improve application performance by reducing the time required to get a requested resource. Caching can also improve sustainability by decreasing the amount of network traffic.  \nWhile caching provides these benefits, it also increases the risk that the resource returned to the application is stale, meaning that it is not identical to the resource that would have been sent by the server if caching were not in use. This can create poor user experiences when data accuracy is critical.  \nAdditionally, caching may allow unauthorized users or processes to read sensitive data. An authenticated response that is cached may be retrieved from the cache without an additional authorization. Due to security concerns like this, caching is not recommended for middle tier scenarios.  \nACTION: Consider caching data close to end users with a CDN  \nIncluding CDNs in your network architecture adds many additional servers to your software footprint, each with their own  local energy grid mix.  The details of CDN hardware and the impact of the power that runs it is important to determine if the carbon emissions from running them is lower than the emissions from sending the data over the wire from a more distant source.  The larger the volume of data, distance it needs to travel, and frequency of requests, the more this solution becomes appealing.",
        "source": "..\\data\\docs\\code-with-engineering\\design\\sustainability\\sustainable-action-disclaimers.md"
    },
    {
        "chunkId": "chunk155_0",
        "chunkContent": "Sustainable Principles  \nThe following principle overviews provide the foundations supporting specific actions in the Sustainable Engineering Checklist. More details about each principle can be found by following the links in the headings or visiting the Principles of Green Software Engineering website.  \nElectricity Consumption  \nMost electricity is still produced through the burning of fossil fuels and is responsible for 49% of the carbon emitted into the atmosphere.  \nSoftware consumes electricity in its execution. Running hardware consumes electricity even at zero percent utilization.  Some of the best ways we can reduce electricity consumption and the subsequent emissions of carbon pollution is to make our applications more energy efficient when they are running and limit idle hardware.  \nEnergy Proportionality  \nThe relationship between power and utilization is not proportional.  \nThe more you utilize a computer, the more efficient it becomes at converting electricity to useful computing operations. Running your work on as few servers as possible with the highest utilization rate maximizes their energy efficiency.  \nAn idle computer, even running at zero percent utilization, still draws electricity.  \nEmbodied Carbon  \nEmbodied carbon (otherwise referred to as \"Embedded Carbon\") is the amount of carbon pollution emitted during the creation and disposal of a device. When calculating the total carbon pollution for the computers running your software, account for both the carbon pollution to run the computer and the embodied carbon of the computer. Therefore a great way to reduce embodied carbon is to prevent the need for new devices to be manufactured by extending the usefulness of existing ones.  \nDemand Shaping  \nDemand shaping is a strategy of shaping our demand for resources so it matches the existing supply.  \nIf supply is high, increase the demand by doing more in your applications. If the supply is low, decrease demand.  This means doing less in your applications or delaying work until supply is higher.  \nNetworking  \nA network is a series of switches, routers, and servers. All the computers and network equipment in a network consume electricity and have embedded carbon. The internet is a global network of devices typically run off the standard local grid energy mix.  \nWhen you send data across the internet, you are sending that data through many devices in the network, each one of those devices consuming electricity. As a result, any data you send or receive over the internet emits carbon.  \nThe amount of carbon emitted to send data depends on many factors including:  \nDistance the data travels  \nNumber of hops between network devices  \nEnergy efficiency of the network devices  \nCarbon intensity of energy used by each device at the time the data is transmitted.  \nNetwork protocol used to coordinate data transmission - e.g. multiplex, header compression, TLS/Quic  \nRecent networking studies - Cloud Carbon Footprint",
        "source": "..\\data\\docs\\code-with-engineering\\design\\sustainability\\sustainable-engineering-principles.md"
    },
    {
        "chunkId": "chunk156_0",
        "chunkContent": "Separating client apps from the services they consume during development  \nClient Apps typically rely on remote services to power their apps.\nHowever, development schedules between the client app and the services don't always fully align. For a high velocity inner dev loop, client app development must be decoupled from the backend services while still allowing the app to \"invoke\" the services for local testing.  \nOptions  \nSeveral options exist to decouple client app development from the backend services. The options range from embedding mock implementation of the services into the application, others rely on simplified versions of the services.  \nThis document lists several options and discusses trade-offs.  \nEmbedded Mocks  \nAn embedded mock solution includes classes that implement the service interfaces locally. Interfaces and data classes, also called models or data transfer objects or DTOs, are often generated from the services' API specs using tools like nswag (RicoSuter/NSwag: The Swagger/OpenAPI toolchain for .NET, ASP.NET Core and TypeScript. (github.com)) or autorest (Azure/autorest: OpenAPI (f.k.a Swagger) Specification code generator. Supports C#, PowerShell, Go, Java, Node.js, TypeScript, Python, Ruby (github.com)).  \nA simple service implementation can return a static response. For RESTful services, the JSON responses for the stubs can be stored as application resources or simply as static strings.  \n{% raw %}  \n```cs\npublic Task GetUserAsync(long userId, CancellationToken cancellationToken)\n{\nPetProfile result = Newtonsoft.Json.JsonConvert.DeserializeObject(\nMockUserProfile.UserProfile, new Newtonsoft.Json.JsonSerializerSettings());\n\nreturn Task.FromResult(result);\n}\n```  \n{% endraw %}  \nMore sophisticated can randomly return errors to test the app's resiliency code paths.  \nMocks can be activated via conditional compilation or dynamically via app configuration. In either case, it is recommended to ensure that mocks, service responses and externalized configurations are not included in the final release to avoid confusing behavior and inclusion of potential vulnerabilities.  \nSample: Registering Mocks via Dependency Injection  \nDependency Injection Containers like Unity (Unity Container Introduction | Unity Container) make\nit easy to switch between mock services and real service client implementations. Since both implement the same interface, implementations can be registered with the Unity container.  \n{% raw %}  \n```cs\npublic static void Bootstrap(IUnityContainer container)\n{\n\nif DEBUG\n\ncontainer.RegisterSingleton();\n\nelse\n\ncontainer.RegisterSingleton();\n\nendif\n\n}\n```  \n{% endraw %}  \nConsuming mocks via Dependency Injection  \nThe code consuming the interfaces will not notice the difference.  \n{% raw %}  \n```cs\npublic class UserPageModel\n{\nprivate readonly IUserServiceClient userServiceClient;\n\npublic UserPageModel(IUserServiceClient userServiceClient)\n{\nthis.userServiceClient = userServiceClient;\n}\n\n}\n```  \n{% endraw %}  \nLocal Services  \nThe approach with Locally Running Services is to replace the call in the client from pointing to the actual endpoint (whether dev, QA, prod, etc.) to a local endpoint.  \nThis approach also enables injecting traffic capture and shaping proxies like Postman (Postman API Platform | Sign Up for Free) or Fiddler (Fiddler | Web Debugging Proxy and Troubleshooting Solutions (telerik.com)).  \nThe advantage of this approach is that the APIs are decoupled from the client and can be independently updated/modified (e.g. changing response codes, changing data) without requiring changes to the client. This helps to unlock new development scenarios and provides flexibility during the development phase.  \nThe challenge with this approach is that it does require setup, configuration, and running of the services locally. There are tools that help to simplify that process (e.g. JsonServer, Postman Mock Server).  \nHigh-Fidelity Local Services  \nA local service stub implements the expected APIs. Just like the embedded mock, it can be generated based on existing API contracts (e.g. OpenAPI).  \nA high-fidelity approach packages the real services together with simplified data in docker containers that can be run locally using docker-compose before the client app is started for local debugging and testing. To enable running services fully local the \"local version\" substitutes dependent cloud services with local alternatives, e.g. file storage instead of blobs, locally running SQL Server instead of SQL AzureDB.  \nThis approach also enables full fidelity integration testing without spinning up distributed deployments.  \nStub / Fake Services  \nLower fidelity approaches run stub services, that could be generated from API specs, or run fake servers like JsonServer (JsonServer.io: A fake json server API Service for prototyping and testing.) or Postman. All these services would respond with predetermined and configured JSON messages.  \nHow to decide  \nPros Cons Example when developing for: Example When not to Use Embedded Mocks Simplifies the F5 developer experience Tightly coupled with Client More static type data scenarios Testing  (e.g. unit tests, integration tests) No external dependencies to manage Hard coded data Initial integration with services Mocking via Dependency Injection can be a non-trivial effort High-Fidelity Local Services Loosely Coupled from Client Extra tooling required i.e. local infrastructure overhead URL Routes When API contract are not available Easier to independently modify response Extra setup and configuration of services Independent updates to services Can utilize HTTP traffic Easier to replace with real services at a later time Stub/Fake Services Loosely coupled from client Extra tooling required i.e. local infrastructure overhead Response Codes When API Contracts available Easier to independently modify response Extra setup and configuration of services Complex/variable data scenarios When API Contracts are note available Independent updates to services Might not provide full fidelity of expected API Can utilize HTTP traffic Easier to replace with real services at a later time",
        "source": "..\\data\\docs\\code-with-engineering\\developer-experience\\client-app-inner-loop.md"
    },
    {
        "chunkId": "chunk157_0",
        "chunkContent": "Copilots  \nThere are a number of AI tools that can improve the developer experience. This article will discuss tooling that is available as well as advice on when it might be appropriate to use such tooling.  \nGitHub Copilot  \nThe current version of GitHub Copilot can provide code completion in many popular IDEs. For instance, the VS Code extension that can be installed from the VS Code Marketplace. It requires a GitHub account to use. For more information about what IDEs are supported, what languages are supported, cost, features, etc., please checkout out the information on Copilot and Copilot for Business.  \nSome example use-cases for GitHub Copilot include:  \nWrite Documentation. For example, the above paragraph was written using Copilot.  \nWrite Unit Tests. Given that setup and assertions are often consistent across unit tests, Copilot tends to be very accurate.  \nUnblock. It is often hard start writing when staring at a blank page, Copilot can fill the space with something that may or may not be what you ultimately want to do, but it can help get you in the right head space.  \nIf you want Copilot to write something useful for you, try writing a comment that describes what your code is going to do - it can often take it from there.  \nGitHub Copilot Labs  \nCopilot has a GitHub Copilot labs extension that offers additional features that are not yet ready for prime-time. For VS Code, you can install it from the VS Code Marketplace. These features include:  \nExplain. Copilot can explain what the code is doing in natural language.  \nTranslate. Copilot can translate code from one language to another.  \nBrushes. You can select code that Copilot then modifies inline based on a \"brush\" you select, for example, to make the code more readable, fix bugs, improve debugging, document, etc.  \nGenerate Tests. Copilot can generate unit tests for your code. Though currently this is limited to JavaScript and TypeScript.  \nGitHub Copilot X  \nThe next version of Copilot offers a number of new use-cases beyond code completion. These include:  \nChat. Rather than just providing code completion, Copilot will be able to have a conversation with you about what you want to do. It has context about the code you are working on and can provide suggestions based on that context. Beyond just writing code, consider using chat to:  \nBuild SQL Indexes. Given a query, ChatGPT can generate a SQL index that will improve the performance of the query.  \nWrite Regular Expressions. These are notoriously difficult to write, but ChatGPT can generate them for you if you give some sample input and describe what you want to extract.  \nImprove and Validate. If you are unsure of the implications of writing code a particular way, you can ask questions about it. For instance, you might ask if there is a way to write the code that is more performant or uses less memory. Once it gives you an opinion, you can ask it to provide documentation validating that assertion.  \nExplain. Copilot can explain what the code is doing in natural language.  \nWrite Code. Given prompting by the developer it can write code that you can one-click deploy into existing or new files.  \nDebug. Copilot can analyze your code and propose solutions to fix bugs.  \nIt can do most of what Labs can do with \"brushes\" as \"topics\", but whereas Labs changes the code in your file, the chat functionality just shows what it would change in the window. However, there is also an \"inline mode\" for GitHub Copilot Chat that allows you to make changes to your code inline which does not have this same limitation.  \nChatGPT / Bing Chat  \nFor coding, generic AI chat tools such as ChatGPT and Bing Chat are less useful, but they still have their place. GitHub Copilot will only answer \"questions about coding\" and it's interpretation of that rule can be a little restrictive. Some cases for using ChatGPT or Bing Chat include:  \nWrite Documentation. Copilot can write documentation, but using ChatGPT or Bing Chat, you can expand your documentation to include business information, use-cases, additional context, etc.  \nChange Perspective. ChatGPT can impersonate a persona or even a system and answer questions from that perspective. For example, you can ask it to explain what a particular piece of code does from the perspective of a user. You might have ChatGPT imagine it is a database administrator and ask it to explain how to improve a particular query.  \nWhen using Bing Chat, experiment with modes, sometimes changing to Creative Mode can give the results you need.  \nPrompt Engineering  \nChat AI tools are only as good as the prompts you give them. The quality and appropriateness of the output can vary greatly depending on the prompt. In addition, many of these tools restrict the number of prompts you can send in a given amount of time. To learn more about prompt engineering, you might review some open source documentation here.  \nConsiderations  \nIt is important when using AI tools to understand how the data (including private or commercial code) might be used by the system. Read more about how GitHub Copilot handles your data and code here.",
        "source": "..\\data\\docs\\code-with-engineering\\developer-experience\\copilots.md"
    },
    {
        "chunkId": "chunk158_0",
        "chunkContent": "Cross Platform Tasks  \nThere are several options to alleviate cross-platform compatibility issues.  \nRunning tasks in a container  \nUsing the tasks-system in VS Code which provides options to allow commands to be executed specific to an operating system.  \nDocker or Container based  \nUsing containers as development machines allows developers to get started with minimal setup and abstracts the development environment from the host OS by having it run in a container.\nDevContainers can also help in standardizing the local developer experience across the team.  \nThe following are some good resources to get started with running tasks in DevContainers  \nDeveloping inside a container.  \nTutorial on Development in Containers  \nFor samples projects and dev container templates see VS Code Dev Containers Recipe  \nDev Containers Library  \nTasks in VS Code  \nRunning Node.js  \nThe example below offers insight into running Node.js executable as a command with tasks.json and how it can be treated differently on Windows and Linux.  \n{% raw %}  \njson\n{\n\"label\": \"Run Node\",\n\"type\": \"process\",\n\"windows\": {\n\"command\": \"C:\\\\Program Files\\\\nodejs\\\\node.exe\"\n},\n\"linux\": {\n\"command\": \"/usr/bin/node\"\n}\n}  \n{% endraw %}  \nIn this example, to run Node.js, there is a specific windows command, and a specific linux command. This allows for platform specific properties. When these are defined, they will be used instead of the default properties when the command is executed on the Windows operating system or on Linux.  \nCustom Tasks  \nNot all scripts or tasks can be auto-detected in the workspace. It may be necessary at times to defined your own custom tasks. In this example, we have a script to run in order to set up some environment correctly. The script is stored in a folder inside your workspace and named test.sh for Linux & macOS and test.cmd for Windows. With the tasks.json file, the execution of this script can be made possible with a custom task that defines what to do on different operating systems.  \n{% raw %}  \n```json\n{\n\"version\": \"2.0.0\",\n\"tasks\": [\n{\n\"label\": \"Run tests\",\n\"type\": \"shell\",\n\"command\": \"./scripts/test.sh\",\n\"windows\": {\n\"command\": \".\\scripts\\test.cmd\"\n},\n\"group\": \"test\",\n\"presentation\": {\n\"reveal\": \"always\",\n\"panel\": \"new\"\n}\n}\n]\n}\n\n```  \n{% endraw %}  \nThe command here is a shell command and tells the system to run either the test.sh or test.cmd. By default, it will run test.sh with that given path. This example here also defines Windows specific properties and tells it execute test.cmd instead of the default.  \nReferences  \nVS Code Docs - operating system specific properties",
        "source": "..\\data\\docs\\code-with-engineering\\developer-experience\\cross-platform-tasks.md"
    },
    {
        "chunkId": "chunk159_0",
        "chunkContent": "Dev Containers: Getting Started  \nIf you are a developer and have experience with Visual Studio Code (VS Code) or Docker, then it's probably time you look at development containers (dev containers). This readme is intended to assist developers in the decision-making process needed to build dev containers. The guidance provided should be especially helpful if you are experiencing VS Code dev containers for the first time.  \nNote: This guide is not about setting up a Docker file for deploying a running Python program for CI/CD.  \nPrerequisites  \nExperience with VS Code  \nExperience with Docker  \nWhat are dev containers?  \nDevelopment containers are a VS Code feature that allows developers to package a local development tool stack into the internals of a Docker container while also bringing the VS Code UI experience with them. Have you ever set a breakpoint inside a Docker container? Maybe not. Dev containers make that possible. This is all made possible through a VS Code extension called the Remote Development Extension Pack that works together with Docker to spin-up a VS Code Server within a Docker container. The VS Code UI component remains local, but your working files are volume mounted into the container. The diagram below, taken directly from the official VS Code docs, illustrates this:  \nIf the above diagram is not clear, a basic analogy that might help you intuitively understand dev containers is to think of them as a union between Docker's interactive mode (docker exec -it 987654e0ff32), and the VS Code UI experience that you are used to.  \nTo set yourself up for the dev container experience described above, use your VS Code's Extension Marketplace to install the Remote Development Extension Pack.  \nHow can dev containers improve project collaboration?  \nVS Code dev containers have improved project collaboration between developers on recent team projects by addressing two very specific problems:  \nInconsistent local developer experiences within a team.  \nSlow onboarding of developers joining a project.  \nThe problems listed above were addressed by configuring and then sharing a dev container definition. Dev containers are defined by their base image, and the artifacts that support that base image. The base image and the artifacts that come with it live in the .devcontainer directory. This directory is where configuration begins. A central artifact to the dev container definition is a configuration file called devcontainer.json. This file orchestrates the artifacts needed to support the base image and the dev container lifecycle. Installation of the Remote Development Extension Pack is required to enable this orchestration within a project repo.  \nAll developers on the team are expected to share and use the dev container definition (.devcontainer directory) in order to spin-up a container. This definition provides consistent tooling for locally developing an application across a team.  \nThe code snippets below demonstrate the common location of a .devcontainer directory and devcontainer.json file within a project repository. They also highlight the correct way to reference a Docker file.  \n{% raw %}  \nbash\n$ tree vs-code-remote-try-python  # main repo directory\n\u2514\u2500\u2500\u2500.devcontainers\n\u251c\u2500\u2500\u2500Dockerfile\n\u251c\u2500\u2500\u2500devcontainer.json  \n{% endraw %}  \n{% raw %}  \n```json\n\ndevcontainer.json\n\n{\n\"name\": \"Python 3\",\n\"build\": {\n\"dockerfile\": \"Dockerfile\",\n\"context\": \"..\",\n// Update 'VARIANT' to pick a Python version: 3, 3.6, 3.7, 3.8\n\"args\": {\"VARIANT\": \"3.8\"}\n},\n}\n```  \n{% endraw %}  \nFor a list of devcontainer.json configuration properties, visit VS Code documentation on dev container properties.  \nHow do I decide which dev container is right for my use case?  \nFortunately, VS Code has a repo gallery of platform specific folders that host dev container definitions (.devcontainer directories) to make getting started with dev containers easier. The code snippet below shows a list of gallery folders that come directly from the VS Code dev container gallery repo:  \n{% raw %}  \nbash\n$ tree vs-code-dev-containers  # main repo directory\n\u2514\u2500\u2500\u2500containers\n\u251c\u2500\u2500\u2500dotnetcore\n|   \u2514\u2500\u2500\u2500.devcontainers # dev container\n\u251c\u2500\u2500\u2500python-3\n|   \u2514\u2500\u2500\u2500.devcontainers # dev container\n\u251c\u2500\u2500\u2500ubuntu\n|   \u2514\u2500\u2500\u2500.devcontainers # dev container\n\u2514\u2500\u2500\u2500....  \n{% endraw %}  \nHere are the final high-level steps it takes to build a dev container:  \nDecide which platform you'd like to build a local development tool stack around.  \nBrowse the VS Code provided dev container gallery of project folders that target your platform and choose the most appropriate one.  \nInspect the dev container definitions (.devcontainer directory) of a project for the base image, and the artifacts that support that base image.  \nUse what you've discovered to begin setting up the dev container as it is, extending it or building your own from scratch.  \nGoing further  \nThere are use cases where you would want to go further in configuring your Dev Container. More details here",
        "source": "..\\data\\docs\\code-with-engineering\\developer-experience\\devcontainers.md"
    },
    {
        "chunkId": "chunk160_0",
        "chunkContent": "Executing pipelines locally  \nAbstract  \nHaving the ability to execute pipeline activities locally has been identified as an opportunity to promote positive developer experience.\nIn this document we will explore a solution which will allow us to have the local CI experience to be as similar as possible to the remote process in the CI server.  \nUsing the suggested method will allow us to:  \nBuild  \nLint  \nUnit test  \nE2E test  \nRun Solution  \nBe OS and environment agnostic.  \nEnter Docker Compose  \nDocker Compose allows you to build push or run multi-container Docker applications.  \nMethod of work  \nDockerize your application(s), including a build step if possible.  \nAdd a step in your docker file to execute unit tests.  \nAdd a step in the docker file for linting.  \nCreate a new dockerfile, possibly in a different folder, which executes end-to-end tests against the cluster. Make sure the default endpoints are configurable (This will become handy in your remote CI server, where you will be able to test against a live environment, if you choose to).  \nCreate a docker-compose file which allows you to choose which of the services to run. The default will run all applications and tests, and an optional parameter can run specific services, for example only the application without the tests.  \nPrerequisites  \nDocker  \nOptional: if you clone the sample app, you need to have dotnet core installed.  \nStep by step with examples  \nFor this tutorial we are going to use a sample dotnet core api application.\nHere is the docker file for the sample app:  \n{% raw %}  \n```sh\n\nhttps://hub.docker.com/_/microsoft-dotnet\n\nFROM mcr.microsoft.com/dotnet/sdk:5.0 AS build\nWORKDIR /app\n\ncopy csproj and restore as distinct layers\n\nCOPY ./ ./\nRUN dotnet restore\n\nRUN dotnet test\n\ncopy everything else and build app\n\nCOPY SampleApp/. ./\nRUN dotnet publish -c release -o out --no-restore\n\nfinal stage/image\n\nFROM mcr.microsoft.com/dotnet/aspnet:5.0\nWORKDIR /app\nCOPY --from=build /app/out .\nENTRYPOINT [\"dotnet\", \"SampleNetApi.dll\"]\n```  \n{% endraw %}  \nThis script restores all dependencies, builds and runs tests. The dotnet app includes stylecop which fails the build in case of linting issues.  \nNext we will also create a dockerfile to perform an end-to-end test. Usually this will look like a set of scripts, or a dedicated app which performs actual HTTP calls to a running application.\nFor the sake of simplicity the dockerfile itself will run a simple curl command:  \n{% raw %}  \nsh\nFROM alpine:3.7\nRUN apk --no-cache add curl\nENTRYPOINT [\"curl\",\"0.0.0.0:8080/weatherforecast\"]  \n{% endraw %}  \nNow we are ready to combine both of the dockerfiles in a docker-compose script:  \n{% raw %}  \nsh\nversion: '3'\nservices:\napp:\nimage: app:0.01\nbuild:\ncontext: .\nports:\n- \"8080:80\"\ne2e:\nimage: e2e:0.01\nbuild:\ncontext: ./E2E  \n{% endraw %}  \nThe docker-compose script will launch the 2 dockerfiles, and it will build them if they were not built before.\nThe following command will run docker compose:  \n{% raw %}  \nsh\ndocker-compose up --build -d  \n{% endraw %}  \nOnce the images are up, you can make calls to the service. The e2e image will perform the set of e2e tests.\nIf you want to skip the tests, you can simply tell compose to run a specific service by appending the name of the service, as follows:  \n{% raw %}  \nsh\ndocker-compose up --build -d app  \n{% endraw %}  \nNow you have a local script which builds and tests you application.\nThe next step would be make your CI run the docker-compose script.  \nHere is an example of a yaml file used by Azure DevOps pipelines:  \n{% raw %}  \n```sh\ntrigger:\n- master\n\npool:\nvmImage: 'ubuntu-latest'\n\nvariables:\nsolution: '*/.sln'\nbuildPlatform: 'Any CPU'\nbuildConfiguration: 'Release'\n\nsteps:\n- task: DockerCompose@0\ndisplayName: Build, Test, E2E\ninputs:\naction: Run services\ndockerComposeFile: docker-compose.yml\n- script: dotnet restore SampleApp\n- script: dotnet build --configuration $(buildConfiguration) SampleApp\ndisplayName: 'dotnet build $(buildConfiguration)'\n```  \n{% endraw %}  \nIn this script the first step is docker-compose, which uses the same file we created the previous steps.\nThe next steps, do the same using scripts, and are here for comparison.\nBy the end of this step, your CI effectively runs the same build and test commands you run locally.",
        "source": "..\\data\\docs\\code-with-engineering\\developer-experience\\execute-local-pipeline-with-docker.md"
    },
    {
        "chunkId": "chunk161_0",
        "chunkContent": "Fake Services Inner Dev Loop  \nIntroduction  \nConsumers of remote services often find that their development cycle is not in sync with development of remote services, leaving developers of these consumers waiting for the remote services to \"catch up\". One approach to mitigate this issue and improve the inner dev loop is by decoupling and using Mock Services. Various Mock Service options are detailed here.  \nThis document will focus on providing an example using the Fake Services approach.  \nAPI  \nFor our example API, we will work against a /User endpoint and the properties for User will be:  \nid - int  \nusername - string  \nfirstName - string  \nlastName - string  \nemail - string  \npassword - string  \nphone - string  \nuserStatus - int  \nTooling  \nFor the Fake Service approach, we will be using Json-Server. Json-Server is a tool that provides the ability to fully fake REST APIs and run the server locally. It is designed to spin up REST APIs with CRUD functionality with minimal setup. Json-Server requires NodeJS and is installed via NPM.  \n{% raw %}  \nbash\nnpm install -g json-server  \n{% endraw %}  \nSetup  \nIn order to run Json-Server, it simply requires a source for data and will infer routes, etc. based on the data file. Note that additional customization can be performed for more advanced scenarios (e.g. custom routes). Details can be found here.  \nFor our example, we will use the following data file, db.json:  \n{% raw %}  \ntext\n{\n\"user\": [\n{\n\"id\": 0,\n\"username\": \"user1\",\n\"firstName\": \"Kobe\",\n\"lastName\": \"Bryant\",\n\"email\": \"kobe@example.com\",\n\"password\": \"superSecure1\",\n\"phone\": \"(123) 123-1234\",\n\"userStatus\": 0\n},\n{\n\"id\": 1,\n\"username\": \"user2\",\n\"firstName\": \"Shaquille\",\n\"lastName\": \"O'Neal\",\n\"email\": \"shaq@example.com\",\n\"password\": \"superSecure2\",\n\"phone\": \"(123) 123-1235\",\n\"userStatus\": 0\n}\n]\n}  \n{% endraw %}  \nRun  \nRunning Json-Server can be performed by simply running:  \n{% raw %}  \nbash\njson-server --watch src/db.json  \n{% endraw %}  \nOnce running, the User endpoint can be hit on the default localhost port: http:/localhost:3000/user  \nNote that Json-Server can be configured to use other ports using the following syntax:  \n{% raw %}  \nbash\njson-server --watch db.json --port 3004  \n{% endraw %}  \nEndpoint  \nThe endpoint can be tested by running curl against it and we can narrow down which user object to get back with the following command:  \n{% raw %}  \nbash\ncurl http://localhost:3000/user/1  \n{% endraw %}  \nwhich, as expected, returns:  \n{% raw %}  \ntext\n{\n\"id\": 1,\n\"username\": \"user2\",\n\"firstName\": \"Shaquille\",\n\"lastName\": \"O'Neal\",\n\"email\": \"shaq@example.com\",\n\"password\": \"superSecure2\",\n\"phone\": \"(123) 123-1235\",\n\"userStatus\": 0\n}  \n{% endraw %}",
        "source": "..\\data\\docs\\code-with-engineering\\developer-experience\\fake-services-inner-loop.md"
    },
    {
        "chunkId": "chunk162_0",
        "chunkContent": "Dev Containers: Going further  \nDev Containers allow developers to share a common working environment, ensuring that the runtime and all dependencies versions are consistent for all developers.  \nDev containers also allow us to:  \nLeverage existing tools to enhance the Dev Containers with more features,  \nProvide custom tools (such as scripts) for other developers.  \nExisting tools  \nIn the development phase, you will most probably need to use tools not installed by default in your Dev Container. For instance, if your project's target is to be deployed on Azure, you will need Azure-cli and maybe Terraform for resources and application deployment. You can find such Dev Containers in the VS Code dev container gallery repo.  \nSome other tools may be:  \nLinters for markdown files,  \nLinters for bash scripts,  \nEtc...  \nLinting files that are not the source code can ensure a common format with common rules for each developer. These checks should be also run in a Continuous Integration Pipeline, but it is a good practice to run them prior opening a Pull Request.  \nLimitation of custom tools  \nIf you decide to include Azure-cli in your Dev Container, developers will be able to run commands against their tenant. However, to make the developers' lives easier, we could go further by letting them prefill their connection information, such as the tenant ID and the subscription ID in a secure and persistent way (do not forget that your Dev Container, being a Docker container, might get deleted, or the image could be rebuilt, hence, all customization inside will be lost).  \nOne way to achieve this is to leverage environment variables, with untracked .env file part of the solution being injected in the Dev Container.  \nConsider the following files structure:  \n{% raw %}  \nbash\nMy Application  # main repo directory\n\u2514\u2500\u2500\u2500.devcontainer\n|       \u251c\u2500\u2500\u2500Dockerfile\n|       \u251c\u2500\u2500\u2500devcontainer.json\n\u2514\u2500\u2500\u2500config\n|       \u251c\u2500\u2500\u2500.env\n|       \u251c\u2500\u2500\u2500.env-sample  \n{% endraw %}  \nThe file config/.env-sample is a tracked file where anyone can find environment variables to set (with no values, obviously):  \n{% raw %}  \nbash\nTENANT_ID=\nSUBSCRIPTION_ID=  \n{% endraw %}  \nThen, each developer who clones the repository can create the file config/.env and fills it in with the appropriate values.  \nIn order now to inject the .env file into the container, you can update the file devcontainer.json with the following:  \n{% raw %}  \njson\n{\n...\n\"runArgs\": [\"--env-file\",\"config/.env\"],\n...\n}  \n{% endraw %}  \nAs soon as the Dev Container is started, these environment variables are sent to the container.  \nAnother approach would be to use Docker Compose, a little bit more complex, and probably too much for just environment variables. Using Docker Compose can unlock other settings such as custom dns, ports forwarding or multiple containers.  \nTo achieve this, you need to add a file .devcontainer/docker-compose.yml with the following:  \n{% raw %}  \nyaml\nversion: '3'\nservices:\nmy-workspace:\nenv_file: ../config/.env\nbuild:\ncontext: .\ndockerfile: Dockerfile\ncommand: sleep infinity  \n{% endraw %}  \nTo use the docker-compose.yml file instead of Dockerfile, we need to adjust devcontainer.json with:  \n{% raw %}  \njson\n{\n\"name\": \"My Application\",\n\"dockerComposeFile\": [\"docker-compose.yml\"],\n\"service\": \"my-workspace\"\n...\n}  \n{% endraw %}  \nThis approach can be applied for many other tools by preparing what would be required. The idea is to simplify developers' lives and new developers joining the project.  \nCustom tools  \nWhile working on a project, any developer might end up writing a script to automate a task. This script can be in bash, python or whatever scripting language they are comfortable with.  \nLet's say you want to ensure that all markdown files written are validated against specific rules you have set up. As we have seen above, you can include the tool markdownlint in your Dev Container . Having the tool installed does not mean developer will know how to use it!  \nConsider the following solution structure:  \n{% raw %}  \nbash\nMy Application  # main repo directory\n\u2514\u2500\u2500\u2500.devcontainer\n|       \u251c\u2500\u2500\u2500Dockerfile\n|       \u251c\u2500\u2500\u2500docker-compose.yml\n|       \u251c\u2500\u2500\u2500devcontainer.json\n\u2514\u2500\u2500\u2500scripts\n|       \u251c\u2500\u2500\u2500check-markdown.sh\n\u2514\u2500\u2500\u2500.markdownlint.json  \n{% endraw %}  \nThe file .devcontainer/Dockerfile installs markdownlint  \n{% raw %}  \n```dockerfile\n...\nRUN apt-get update \\\n&& export DEBIAN_FRONTEND=noninteractive \\\n&& apt-get install -y nodejs npm\n\nAdd NodeJS tools\n\nRUN npm install -g markdownlint-cli\n...\n```  \n{% endraw %}  \nThe file .markdownlint.json contains the rules you want to validate in your markdown files (please refer to the markdownlint site for details).  \nAnd finally, the script scripts/check-markdown.sh contains the following code to execute markdownlint:  \n{% raw %}  \n```bash\n\nGet the repository root\n\nrepoRoot=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )/..\" >/dev/null 2>&1 && pwd )\"\n\nExecute markdownlint for the entire solution\n\nmarkdownlint -c \"${repoRoot}\"/.markdownlint.json\n```  \n{% endraw %}  \nWhen the Dev Container is loaded, any developer can now run this script in their terminal:  \n{% raw %}  \nbash\n/> ./scripts/check-markdown.sh  \n{% endraw %}  \nThis is a small use case, there are unlimited other possibilities to capitalize on work done by developers to save time.  \nOther considerations  \nPlatform architecture  \nWhen installing tooling, you also need to ensure that you know what host computers developers are using. All Intel based computers, whether they are running Windows, Linux or MacOs will have the same behavior.\nHowever, the latest Mac architecture (Apple M1/Silicon) being ARM64, means that the behavior is not the same when building Dev Containers.  \nFor instance, if you want to install Azure-cli in your Dev Container, you won't be able to do it the same way you do it for Intel based machines. On Intel based computers you can install the deb package. However, this package is not available on ARM architecture. The only way to install Azure-cli on Linux ARM is via the Python installer pip.  \nTo achieve this you need to check the architecture of the host building the Dev Container, either in the Dockerfile, or by calling an external bash script to install remaining tools not having a universal version.  \nHere is a snippet to call from the Dockerfile:  \n{% raw %}  \n```bash\n\nIf Intel based, then use the deb file\n\nif [[ dpkg --print-architecture == \"amd64\" ]]; then\nsudo curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash;\nelse\n\narm based, install pip (and gcc) then azure-cli\n\nfi\n```  \n{% endraw %}  \nReuse of credentials for GitHub  \nIf you develop inside a Dev Container, you will also want to share your GitHub credentials between your host and the Dev Container. Doing so, you would avoid copying your ssh keys back and forth (if you are using ssh to access your repositories).  \nOne approach would be to mount your local ~/.ssh folder into your Dev Container. You can either use the mounts option of the devcontainer.json, or use Docker Compose  \nUsing mounts:  \n{% raw %}  \njson\n{\n...\n\"mounts\": [\"source=${localEnv:HOME}/.ssh,target=/home/vscode/.ssh,type=bind\"],\n...\n}  \n{% endraw %}  \nAs you can see, ${localEnv:HOME} returns the host home folder, and it maps it to the container home folder.  \nUsing Docker Compose:  \n{% raw %}  \nyaml\nversion: '3'\nservices:\nmy-worspace:\nenv_file: ../configs/.env\nbuild:\ncontext: .\ndockerfile: Dockerfile\nvolumes:\n- \"~/.ssh:/home/alex/.ssh\"\ncommand: sleep infinity  \n{% endraw %}  \nPlease note that using Docker Compose requires to edit the devcontainer.json file as we have seen above.  \nYou can now access GitHub using the same credentials as your host machine, without worrying of persistence.  \nAllow some customization  \nAs a final note, it is also interesting to leave developers some flexibility in their environment for customization.  \nFor instance, one might want to add aliases to their environment. However, changing the ~/.bashrc file in the Dev Container is not a good approach as the container might be destroyed. There are numerous ways to set persistence, here is one approach.  \nConsider the following solution structure:  \n{% raw %}  \nbash\nMy Application  # main repo directory\n\u2514\u2500\u2500\u2500.devcontainer\n|       \u251c\u2500\u2500\u2500Dockerfile\n|       \u251c\u2500\u2500\u2500docker-compose.yml\n|       \u251c\u2500\u2500\u2500devcontainer.json\n\u2514\u2500\u2500\u2500me\n|       \u251c\u2500\u2500\u2500bashrc_extension  \n{% endraw %}  \nThe folder me is untracked in the repository, leaving developers the flexibility to add personal resources. One of these resources can be a .bashrc extension containing customization. For instance:  \n{% raw %}  \n```bash\n\nSample alias\n\nalias gaa=\"git add --all\"\n```  \n{% endraw %}  \nWe can now adapt our Dockerfile to load these changes when the Docker image is built (and of course, do nothing if there is no file):  \n{% raw %}  \ndockerfile\n...\nRUN echo \"[ -f PATH_TO_WORKSPACE/me/bashrc_extension ] && . PATH_TO_WORKSPACE/me/bashrc_extension\" >> ~/.bashrc;\n...  \n{% endraw %}",
        "source": "..\\data\\docs\\code-with-engineering\\developer-experience\\going-further.md"
    },
    {
        "chunkId": "chunk163_0",
        "chunkContent": "Onboarding Guide Template  \nWhen developing an onboarding document for a team, it should contain details of engagement scope, team processes, codebase, coding standards, team agreements, software requirements and setup details. The onboarding guide can be used as an index to project specific content if it already exists elsewhere. Allowing this guide to be utilized as a foundation with the links will help keep the guide concise and effective.  \nOverview and Goals  \nList a few sentences explaining the high-level summary and the scope of the engagement.  \nConsider adding any additional background and context as needed.  \nInclude the value proposition of the project, goals, what success looks like, and what the team is trying to achieve and why.  \nContacts  \nList a few of the main contacts for the team and project overall such as the Dev Lead and Product Owner.  \nConsider including the roles of these main contacts so that the team knows who to reach out to depending on the situation.  \nTeam Agreement and Code of Conduct  \nInclude the team's code of conduct or agreement that defines a set of expectation from each team member and how the team has agreed to operate.  \nWorking Agreement Template - working agreement  \nDev Environment Setup  \nConsider adding steps to run the project end-to-end. This could be in form of a separate wiki page or document that can be linked here.  \nInclude any software that needs to be downloaded and specify if a specific version of the software is needed.  \nProject Building Blocks  \nThis can include a more in depth description with different areas of the project to help increase the project understanding.  \nIt can include different sections on the various components of the project including deployment, e2e testing, repositories.  \nHelpful Resources and Links  \nThis can include any additional links to documents related to the project  \nIt may include links to backlog items, work items, wiki pages or project history.",
        "source": "..\\data\\docs\\code-with-engineering\\developer-experience\\onboarding-guide-template.md"
    },
    {
        "chunkId": "chunk164_0",
        "chunkContent": "Developer Experience (DevEx)  \nDeveloper experience refers to how easy or difficult it is for a developer to perform essential tasks needed to implement a change. A positive developer experience would mean these tasks are relatively easy for the team (see measures below).  \nThe essential tasks are identified below.  \nBuild - Verify that changes are free of syntax error and compile.  \nTest - Verify that all automated tests pass.  \nStart - Launch end-to-end to simulate execution in a deployed environment.  \nDebug - Attach debugger to started solution, set breakpoints, step through code, and inspect variables.  \nIf effort is invested to make these activities as easy as possible, the returns on that effort will increase the longer the project runs, and the larger the team is.  \nDefining End-to-End  \nThis document makes several references to running a solution end-to-end (aka E2E). End-to-end for the purposes of this document is scoped to the software that is owned, built, and shipped by the team. Systems owned by other teams or third-party vendors is not within the E2E scope for the purposes of this document.  \nGoals  \nMaximize the amount of time engineers spend on writing code that fulfills story acceptance and done-done criteria.  \nMinimize the amount of time spent manual setup and configuration of tooling  \nMinimize regressions and new defects by making end-to-end testing easy  \nImpact  \nDeveloper experience can have a significant impact on the efficiency of the day-to-day execution of the team. A positive experience can pay dividends throughout the lifetime of the project; especially as new developers join the team.  \nIncreased Velocity - Team spends less time on non-value-add activities such as dev/local environment setup, waiting on remote environments to test, and rework (fixing defects).  \nImproved Quality - When it's easy to debug and test, developers will do more of it. This will translate to fewer defects being introduced.  \nEasier Onboarding & Adoption - When dev essential tasks are automated, there is less documentation to write and, subsequently, less to read to get started!  \nMost importantly, the customer will continue to accrue these benefits long after the code-with engagement.  \nMeasures  \nTime to First E2E Result (aka F5 Contract)  \nAssuming a laptop/pc that has never run the solution, how long does it take to set up and run the whole system end-to-end and see a result.  \nTime To First Commit  \nHow long does it take to make a change that can be verified/tested locally. A locally verified/tested change is one that passes test cases without introducing regression or breaking changes.  \nParticipation  \nProviding a positive developer experience is a team effort. However, certain members can take ownership of different areas to help hold the entire team accountable.  \nDev Lead - Set the bar  \nThe following are examples of how the Dev Lead might set the bar for dev experience  \nDetermines development environment (suggested IDE, hosting, etc)  \nDetermines source control environment and number of repos required  \nGiven development environment and repo structure, sets expectations for team to meet in terms of steps to perform the essential dev tasks  \nNominates the DevEx Champion  \nIDE choice is NOT intended to mandate that all team members must use the same IDE. However, this choice will direct where tight-integration investment will be prioritized. For example, if Visual Studio Code is the suggested IDE then, the team would focus on integrating VS code tasks and launch configurations over similar integrations for other IDEs. Team members should still feel free to use their preferred IDE as long as it does not negatively impact the team.  \nDevEx Champion - Identify Iterative Improvements  \nThe DevEx champion takes ownership in holding the team accountable for providing a positive developer experience. The following outline responsibilities for the DevEx champion.  \nActively seek opportunities for improving the solution developer experience  \nWork with the Dev Lead to iteratively improve team expectations for developer experience  \nCurate a backlog actionable stories that identify areas for improvement and prioritize with respect to project delivery goals by engaging directly with the Product Owner and Customer.  \nServe as subject-matter expert for the rest of the team. Help the team determine how to implement DevEx expectations and identify deviations.  \nTeam Members - Assert Expectations  \nThe team members of the team can also help hold each other accountable for providing a positive developer experience. The following are examples of areas team members can help identify where the team's DevEx expectations are not being met.  \nPull requests. Try the changes locally to see if they are adhering to the team's DevEx expectations.  \nDesign Reviews. Look for proposals that may negatively affect the solution's DevEx. These might include  \nIntroduction of new tech whose testability is limited to manual steps in a deployed environment.  \nAddition of new repository  \nNew Team Members - Identify Iterative Improvements  \nNew team members are uniquely positioned to identify instances of undocumented Collective Wisdom. The following outlines responsibilities of new team members as it relates to DevEx:  \nIf you come across missing, incomplete or incorrect documentation while onboarding, you should record the issue as a new defect(s) and assign it to the product owner to triage.  \nIf no onboarding documentation exists, note the steps you took in a new user story. Assign the new story to the product owner to triage.  \nFacilitation Guidance  \nThe following outline examples of several strategies that can be adopted to promote a positive developer experience. It is expected that each team should define what a positive dev experience means within the context of their project. Additionally, refine that over time via feedback mechanisms such as sprint and project retrospectives.  \nEstablish Hotkeys  \nAssign hotkeys to each of the essential tasks.  \nTask Windows Build CTRL+SHIFT+B Test CTRL+R,T Start With Debugging F5  \nThe F5 Contract  \nThe F5 contract aims for the ability to run the end-to-end solution with the following steps.  \nClone - git clone [my-repo-url-here]  \nConfigure - set any configuration values that need to be unique to the individual (i.e. update a .env file)  \nPress F5 - launch the solution with debugging attached.  \nMost IDEs have some form of a task runner that can be used to automate the build, execute, and attach steps. Try to leverage these such that the steps can all be run with as few manual steps as possible.  \nDevEx Champion Actively Seek Improvements  \nThe DevEx champion should actively seek areas where the team has opportunity to improve. For example, do they need to deploy their changes to an environment off their laptop before they can validate if what they did worked. Rather than debugging locally, do they have to do this repetitively to get to a working solution? Does this take several minutes each iteration? Does this block other developers due to the contention on the environment?  \nThe following are ceremonies that the DevEx champion can use to find potential opportunities  \nRetrospectives. Is feedback being raised that relates to the essential tasks being difficult or unwieldy?  \nStandup Blockers. Are individuals getting blocked or stumbling on the essential tasks?  \nAs opportunities are identified, the DevEx champion can translate these into actionable stories for the product backlog.  \nMake Tasks Cross Platform  \nFor essential tasks being standardized during the engagement, ensure that different platforms are accounted for. Team members may have different operating systems and ensuring the tasks are cross-platform will provide an additional opportunity to improve the experience.  \nSee the making tasks cross platform recipe for guidance on how tasks can be configured to include different platforms.  \nCreate an Onboarding Guide  \nWhen welcoming new team members to the engagement, there are many areas for them to get adjusted to and bring them up to speed including codebase, coding standards, team agreements, and team culture. By adopting a strong onboarding practice such as an onboarding guide in a centralized location that explains the scope of the project, processes, setup details, and software required, new members can have all the necessary resources for them to be efficient, successful and a valuable team member from the start.  \nSee the onboarding guide recipe for guidance on what an onboarding guide may look like.  \nStandardize Essential Tasks  \nApply a common strategy across solution components for performing the essential tasks  \nStandardize the configuration for solution components  \nStandardize the way tests are run for each component  \nStandardize the way each component is started and stopped locally  \nStandardize how to document the essential tasks for each component  \nThis standardization will enable the team to more easily automate these tasks across all components at the solution level. See Solution-level Essential Tasks below.  \nSolution-level Essential Tasks  \nAutomate the ability to execute each essential task across all solution components. An example would be mapping the build action in the IDE to run the build task for each component in the solution. More importantly, configure the IDE start action to start all components within the solution. This will provide significant efficiency for the engineering team when dealing with multi-component solutions.  \nWhen this is not implemented, the engineers must repeat each of the essential tasks manually for each component in the solution. In this situation, the number of steps required to perform each essential task is multiplied by the number of components in the system  \n[Configuration steps + Build steps + Start/Debug steps + Stop steps + Run test steps + Documenting all of the above] * [many solution components] = TOO MANY STEPS  \nVS.  \n[Configuration steps + Build steps + Start/Debug steps + Stop steps + Run test steps + Documenting all of the above] * [1 solution] = MINIMUM NUMBER OF STEPS  \nObservability  \nObservability alleviates unforeseen challenges for the developer in a complex distributed system. It identifies project bottlenecks quicker and with more precision, enhancing performance as the developer seeks to deploy code changes. Adding observability improves the experience when identifying and resolving bugs or broken code. This results in fewer or less severe current and future production failures.  \nThere are many observability strategies a developer can use alongside best engineering practices. These resources improve the DevEx by ensuring a shared view of the complex system throughout the entire lifecycle. Observability in code via logging, exception handling and exposing of relevant application metrics for example, promotes the consistent visibility of real time performance. The observability pillars, logging, metrics, and tracing, detail when to enable each of the three specific types of observability.  \nMinimize the Number of Repositories  \nSplitting a solution across multiple repositories can negatively impact the above measures. This can also negatively impact other areas such as Pull Requests, Automated Testing, Continuous Integration, and Continuous Delivery. Similar to the IDE instances, the negative impact is multiplied by the number of repositories.  \n[Clone steps + Branching steps + Commit steps + CI steps + Pull Request reviews & merges ] * [many source code repositories] = TOO MANY STEPS  \nVS.  \n[Clone steps + Branching steps + Commit steps + CI steps + Pull Request reviews & merges ] * [1 source code repository] = MINIMUM NUMBER OF STEPS  \nAtomic Pull Requests  \nWhen the solution is encapsulated within a single repository, it also allows pull requests to represent a change across multiple layers. This is especially helpful when a change requires changes to a shared contract between multiple components. For example, a story requires that an api endpoint is changed. With this strategy the api and web client could be updated with the same pull request. This avoids the main branch being broken temporarily while waiting on dependent pull requests to merge.  \nMinimize Remote Dependencies for Local Development  \nThe fewer dependencies on components that cannot run a developer's machine translate to fewer steps required to get started. Therefore, fewer dependencies will positively impact the measures above.  \nThe following strategies can be used to reduce these dependencies  \nUse an Emulator  \nIf available, emulators are implementations of technologies that are typically only available in cloud environments. A good example is the CosmosDB emulator.  \nUse DI + Toggle to Mock Remote Dependencies  \nWhen the solution depends on a technology that cannot be run on a developer's machine, the setup and testing of that solution can be challenging. One strategy that can be employed is to create the ability to swap that dependency for one that can run locally.  \nAbstract the layer that has the remote dependency behind an interface owned by the solution (not the remote dependency). Create an implementation of that interface using a technology that can be run locally. Create a factory that decides which instance to use. This decision could be based on environment configuration (i.e. the toggle). Then, the original class that depends on the remote tech instead should depend on the factory to provide which instance to use.  \nMuch of this strategy can be simplified with proper dependency injection technique and/or framework.  \nSee example below that swaps Azure Service Bus implementation for RabbitMQ which can be run locally.  \n{% raw %}  \ntypescript\ninterface IPublisher {\nsend(message: string): void\n}\nclass RabbitMQPublisher implements IPublisher {\nsend(message: string) {\n//todo: send the message via RabbitMQ\n}\n}\nclass AzureServiceBusPublisher implements IPublisher {\nsend(message: string) {\n//todo: send the message via Azure Service Bus\n}\n}\ninterface IPublisherFactory{\ncreate(): IPublisher\n}\nclass PublisherFactory{\ncreate(): IPublisher {\n// use env var value to determine which instance should be used\nif(process.env.UseAsb){\nreturn new AzureServiceBusPublisher();\n}\nelse{\nreturn new RabbitMqPublisher();\n}\n}\n}\nclass MyService {\n//inject the factory\nconstructor(private readonly publisherFactory: IPublisherFactory){\n}\nsendAMessage(message: string): void{\n//use the factory to determine which instance to use\nconst publisher: IPublisher = this.publisherFactory.create();\npublisher.send(message);\n}\n}  \n{% endraw %}  \nThe recipes section has a more complete discussion on DI as part of a high productivity inner dev loop",
        "source": "..\\data\\docs\\code-with-engineering\\developer-experience\\README.md"
    },
    {
        "chunkId": "chunk165_0",
        "chunkContent": "Toggle VNet on and off for production and development environment  \nProblem Statement  \nWhen deploying resources on Azure in a secure environment, resources are usually created behind a Private Network (VNet), without public access and with private endpoints to consume resources. This is the recommended approach for pre-production or production environments.  \nAccessing protected resources from a local machine implies one of the following options:  \nUse a VPN  \nUse a jump box  \nWith SSH activated (less secure)  \nWith Bastion (recommended approach)  \nHowever, a developer may want to deploy a test environment (in a non-production subscription) for their tests during development phase, without the complexity of networking.  \nIn addition, infrastructure code should not be duplicated: it has to be the same whether resources are deployed in a production like environment or in development environment.  \nOption  \nThe idea is to offer, via a single boolean variable, the option to deploy resources behind a VNet or not using one infrastructure code base. Securing resources behind a VNet usually implies that public accesses are disabled and private endpoints are created. This is something to have in mind because, as a developer, public access must be activated in order to connect to this environment.  \nThe deployment pipeline will set these resources behind a VNet and will secure them by removing public accesses. Developers will be able to run the same deployment script, specifying that resources will not be behind a VNet nor have public accesses disabled.  \nLet's consider the following use case: we want to deploy a VNet, a subnet, a storage account with no public access and a private endpoint for the table.  \nThe magic variable that will help toggling security will be called behind_vnet, of type boolean.  \nLet's implement this use case using Terraform.  \nThe code below does not contain everything, the purpose is to show the pattern and not how to deploy these resources. For more information on Terraform, please refer to the official documentation.  \nThere is no if per se in Terraform to define whether a specific resource should be deployed or not based on a variable value. However, we can use the count meta-argument. The strength of this meta-argument is if its value is 0, the block is skipped.  \nHere is below the code snippets for this deployment:  \nvariables.tf\n{% raw %}\nterraform\nvariable \"behind_vnet\" {\ntype    = bool\n}\n{% endraw %}  \nmain.tf\n{% raw %}\n```terraform\nresource \"azurerm_virtual_network\" \"vnet\" {\ncount = var.behind_vnet ? 1 : 0\nname                = \"MyVnet\"\naddress_space       = [x.x.x.x/16]\nresource_group_name = \"MyResourceGroup\"\nlocation            = \"WestEurope\"\n\n...\n\nsubnet {\nname              = \"subnet_1\"\naddress_prefix    = \"x.x.x.x/24\"\n}\n\n}\nresource \"azurerm_storage_account\" \"storage_account\" {\nname                = \"storage\"\nresource_group_name = \"MyResourceGroup\"\nlocation            = \"WestEurope\"\ntags                = var.tags\n...\n\npublic_network_access_enabled = var.behind_vnet ? false : true\n\n}\nresource \"azurerm_private_endpoint\" \"storage_account_table_private_endpoint\" {\ncount = var.behind_vnet ? 1 : 0\nname                = \"pe-storage\"\nsubnet_id           = azurerm_virtual_network.vnet[0].subnet[0].id\n\n...\n\nprivate_service_connection {\nname                           = \"psc-storage\"\nprivate_connection_resource_id = azurerm_storage_account.storage_account.id\nsubresource_names              = [ \"table\" ]\n...\n}\n\nprivate_dns_zone_group {\nname = \"privateDnsZoneGroup\"\n...\n}\n\n}\n```\n{% endraw %}  \nIf we run  \n{% raw %}  \nbash\nterraform apply -var behind_vnet=true  \n{% endraw %}  \nthen all the resources above will be deployed, and it is what we want on a pre-production or production environment. The instruction count = var.behind_vnet ? 1 : 0 will set count with the value 1, therefore blocks will be executed.  \nHowever, if we run  \n{% raw %}  \nbash\nterraform apply -var behind_vnet=false  \n{% endraw %}  \nThe same pattern can be applied over and over for the entire infrastructure code.  \nConclusion  \nWith this approach, the same infrastructure code base can be used to target a production like environment with secured resources behind a VNet with no public accesses and also a more permissive development environment.  \nHowever, there are a couple of trade-offs with this approach:  \nif a resource has the count argument, it needs to be treated as a list, and not a single item. In the example above, if there is a need to reference the resource azurerm_virtual_network later in the code,\n{% raw %}\nterraform\nazurerm_virtual_network.vnet.id\n{% endraw %}\nwill not work. The following must be used\n{% raw %}\nterraform\nazurerm_virtual_network.vnet[0].id # First (and only) item of the collection\n{% endraw %}  \nThe meta-argument count cannot be used with for_each for a whole block. That means that the use of loops to deploy multiple endpoints for instance will not work. Each private endpoints will need to be deployed individually.",
        "source": "..\\data\\docs\\code-with-engineering\\developer-experience\\toggle-vnet-dev-environment.md"
    },
    {
        "chunkId": "chunk166_0",
        "chunkContent": "Documentation  \nEvery software development project requires documentation. Agile Software Development values working software over comprehensive documentation. Still, projects should include the key information needed to understand the development and the use of the generated software.  \nDocumentation shouldn't be an afterthought. Different written documents and materials should be created during the whole life cycle of the project, as per the project needs.  \nTable of Contents  \nGoals  \nChallenges  \nWhat documentation should exist?  \nBest practices  \nTools  \nRecipes  \nResources  \nGoals  \nFacilitate onboarding of new team members.  \nImprove communication and collaboration between teams (especially when distributed across time zones).  \nImprove the transition of the project to another team.  \nChallenges  \nWhen working in an engineering project, we typically encounter one or more of these challenges related to documentation (including some examples):  \nNon-existent.  \nNo onboarding documentation, so it takes a long time to set up the environment when you join the project.  \nNo document in the wiki explaining existing repositories, so you cannot tell which of the 10 available repositories you should clone.  \nNo main README, so you don't know where to start when you clone a repository.  \nNo \"how to contribute\" section, so you don't know which is the branch policy, where to add new documents, etc.  \nNo code guidelines, so everyone follows different naming conventions, etc.  \nHidden.  \nImpossible to find useful documentation as it\u2019s scattered all over the place. E.g., no idea how to compile, run and test the code as the README is hidden in a folder within a folder within a folder.  \nUseful processes (e.g., grooming process) explained outside the backlog management tool and not linked anywhere.  \nDecisions taken in different channels other than the backlog management tool and not recorded anywhere else.  \nIncomplete.  \nNo clear branch policy, so everyone names their branches differently.  \nMissing settings in the \"how to run this\" document that are required to run the application.  \nInaccurate.  \nDocuments not updated along with the code, so they don't mention the right folders, settings, etc.  \nObsolete.  \nDesign documents that don't apply anymore, sitting next to valid documents. Which one shows the latest decisions?  \nOut of order (subject / date).  \nDocuments not organized per subject/workstream so not easy to find relevant information when you change to a new workstream.  \nDesign decision logs out of order and without a date that helps to determine which is the final decision on something.  \nDuplicate.  \nNo settings file available in a centralized place as a single source of truth, so developers must keep sharing their own versions, and we end up with many files that might or might not work.  \nAfterthought.  \nKey documents created several weeks into the project: onboarding, how to run the app, etc.  \nDocuments created last minute just before the end of a project, forgetting that they also help the team while working on the project.  \nWhat documentation should exist  \nProject and Repositories  \nCommit Messages  \nPull Requests  \nCode  \nWork Items  \nREST APIs  \nEngineering Feedback  \nBest practices  \nEstablishing and managing documentation  \nCreating good documentation  \nReplacing documentation with automation  \nTools  \nWikis  \nLanguages  \nmarkdown  \nmermaid  \nHow to automate simple checks  \nIntegration with Teams/Slack  \nRecipes  \nHow to sync a wiki between repositories  \nUsing DocFx and Companion Tools to generate a Documentation website  \nDeploy the DocFx Documentation website to an Azure Website automatically  \nHow to create a static website for your documentation based on MkDocs and Material for MkDocs  \nResources  \nSoftware Documentation Types and Best Practices",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\README.md"
    },
    {
        "chunkId": "chunk167_0",
        "chunkContent": "Replacing Documentation with Automation  \nYou can document how to set up your dev machine with the right version of the framework required to run the code, which extensions are useful to develop the application with your editor, or how to configure your editor to launch and debug the application. If it is possible, a better solution is to provide the means to automate tool installs, application startup, etc., instead.  \nSome examples are provided below:  \nDev containers in Visual Studio Code  \nThe Visual Studio Code Remote - Containers extension lets you use a Docker container as a full-featured development environment. It allows you to open any folder inside (or mounted into) a container and take advantage of Visual Studio Code's full feature set.  \nAdditional information: Developing inside a Container.  \nLaunch configurations and Tasks in Visual Studio Code  \nLaunch configurations allows you to configure and save debugging setup details.  \nTasks can be configured to run scripts and start processes so that many of these existing tools can be used from within VS Code without having to enter a command line or write new code.",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\best-practices\\automation.md"
    },
    {
        "chunkId": "chunk168_0",
        "chunkContent": "Establishing and Managing Documentation  \nDocumentation should be source-controlled. Pull Requests can be used to tell others about the changes, so they can be reviewed and discussed. E.g., Async Design Reviews.  \nTools:  \nWikis.",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\best-practices\\establish-and-manage.md"
    },
    {
        "chunkId": "chunk169_0",
        "chunkContent": "Creating Good Documentation  \nReview the Documentation Review Checklist for advice on how to write good documentation.  \nGood documentation should follow good writing guidelines: Writing Style Guidelines.",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\best-practices\\good-documentation.md"
    },
    {
        "chunkId": "chunk170_0",
        "chunkContent": "Best Practices  \nReplacing Documentation with Automation  \nEstablishing and Managing Documentation  \nCreating Good Documentation",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\best-practices\\README.md"
    },
    {
        "chunkId": "chunk171_0",
        "chunkContent": "Code  \nYou might have heard more than once that you should write self-documenting code. This doesn't mean that you should never comment your code.  \nThere are two types of code comments, implementation comments and documentation comments.  \nImplementation comments  \nThey are used for internal documentation, and are intended for anyone who may need to maintain the code in the future, including your future self.  \nThere can be single line and multi-line comments (e.g., C# Comments). Comments are human-readable and not executed, thus ignored by the compiler. So you could potentially add as many as you want.  \nNow, the use of these comments is often considered a code smell. If you need to clarify your code, that may mean the code is too complex. So you should work towards the removal of the clarification by making the code simpler, easier to read, and understand. Still, these comments can be useful to give overviews of the code, or provide additional context information that is not available in the code itself.  \nExamples of useful comments:  \nSingle line comment in C# that explains why that piece of code is there (from a private method in System.Text.Json.JsonSerializer):  \n{% raw %}  \ncsharp\n// For performance, avoid obtaining actual byte count unless memory usage is higher than the threshold.\nSpan<byte> utf8 = json.Length <= (ArrayPoolMaxSizeBeforeUsingNormalAlloc / JsonConstants.MaxExpansionFactorWhileTranscoding) ? ...  \n{% endraw %}  \nMulti-line comment in C# that provides additional context (from a private method in System.Text.Json.Utf8JsonReader):  \n{% raw %}  \n```csharp\n// Transcoding from UTF-16 to UTF-8 will change the length by somewhere between 1x and 3x.\n// Un-escaping the token value will at most shrink its length by 6x.\n// There is no point incurring the transcoding/un-escaping/comparing cost if:\n// - The token value is smaller than charTextLength\n// - The token value needs to be transcoded AND unescaped and it is more than 6x larger than charTextLength\n//      - For an ASCII UTF-16 characters, transcoding = 1x, escaping = 6x => 6x factor\n//      - For non-ASCII UTF-16 characters within the BMP, transcoding = 2-3x, but they are represented as a single escaped hex value, \\uXXXX => 6x factor\n//      - For non-ASCII UTF-16 characters outside of the BMP, transcoding = 4x, but the surrogate pair (2 characters) are represented by 16 bytes \\uXXXX\\uXXXX => 6x factor\n// - The token value needs to be transcoded, but NOT escaped and it is more than 3x larger than charTextLength\n//      - For an ASCII UTF-16 characters, transcoding = 1x,\n//      - For non-ASCII UTF-16 characters within the BMP, transcoding = 2-3x,\n//      - For non-ASCII UTF-16 characters outside of the BMP, transcoding = 2x, (surrogate pairs - 2 characters transcode to 4 UTF-8 bytes)\n\nif (sourceLength < charTextLength\n|| sourceLength / (_stringHasEscaping ? JsonConstants.MaxExpansionFactorWhileEscaping : JsonConstants.MaxExpansionFactorWhileTranscoding) > charTextLength)\n{\n```  \n{% endraw %}  \nDocumentation comments  \nDoc comments are a special kind of comment, added above the definition of any user-defined type or member, and are intended for anyone who may need to use those types or members in their own code.  \nIf, for example, you are building a library or framework, doc comments can be used to generate their documentation. This documentation should serve as API specification, and/or programming guide.  \nDoc comments won't be included by the compiler in the final executable, as with single and multi-line comments.  \nExample of a doc comment in C# (from Deserialize method in System.Text.Json.JsonSerializer):  \n{% raw %}  \ncsharp\n/// <summary>\n/// Parse the text representing a single JSON value into a <typeparamref name=\"TValue\"/>.\n/// </summary>\n/// <returns>A <typeparamref name=\"TValue\"/> representation of the JSON value.</returns>\n/// <param name=\"json\">JSON text to parse.</param>\n/// <param name=\"options\">Options to control the behavior during parsing.</param>\n/// <exception cref=\"System.ArgumentNullException\">\n/// <paramref name=\"json\"/> is <see langword=\"null\"/>.\n/// </exception>\n/// <exception cref=\"JsonException\">\n/// The JSON is invalid.\n///\n/// -or-\n///\n/// <typeparamref name=\"TValue\" /> is not compatible with the JSON.\n///\n/// -or-\n///\n/// There is remaining data in the string beyond a single JSON value.</exception>\n/// <exception cref=\"NotSupportedException\">\n/// There is no compatible <see cref=\"System.Text.Json.Serialization.JsonConverter\"/>\n/// for <typeparamref name=\"TValue\"/> or its serializable members.\n/// </exception>\n/// <remarks>Using a <see cref=\"string\"/> is not as efficient as using the\n/// UTF-8 methods since the implementation natively uses UTF-8.\n/// </remarks>\n[RequiresUnreferencedCode(SerializationUnreferencedCodeMessage)]\npublic static TValue? Deserialize<TValue>(string json, JsonSerializerOptions? options = null)\n{  \n{% endraw %}  \nIn C#, doc comments can be processed by the compiler to generate XML documentation files. These files can be distributed alongside your libraries so that Visual Studio and other IDEs can use IntelliSense to show quick information about types or members. Additionally, these files can be run through tools like DocFx to generate API reference websites.  \nMore information:  \nRecommended XML tags for C# documentation comments.  \nIn other languages, you may require external tools. For example, Java doc comments can be processed by Javadoc tool to generate HTML documentation files.  \nMore information:  \nHow to Write Doc Comments for the Javadoc Tool  \nJavadoc Tool",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\guidance\\code.md"
    },
    {
        "chunkId": "chunk172_0",
        "chunkContent": "Engineering Feedback  \nGood engineering feedback is:  \nActionable  \nSpecific  \nDetailed  \nIncludes assets (script, data, code, etc.) to reproduce scenario and validate solution  \nIncludes details about the customer scenario / what the customer was trying to achieve  \nRefer to Microsoft Engineering Feedback for more details, including guidance, FAQ and examples.",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\guidance\\engineering-feedback.md"
    },
    {
        "chunkId": "chunk173_0",
        "chunkContent": "Projects and Repositories  \nEvery source code repository should include documentation that is specific to it (e.g., in a Wiki within the repository), while the project itself should include general documentation that is common to all its associated repositories (e.g., in a Wiki within the backlog management tool).  \nDocumentation specific to a repository  \nIntroduction  \nGetting started  \nOnboarding  \nSetup: programming language, frameworks, platforms, tools, etc.  \nSandbox environment  \nWorking agreement  \nContributing guide  \nStructure: folders, projects, etc.  \nHow to compile, test, build, deploy the solution/each project  \nDifferent OS versions  \nCommand line + editors/IDEs  \nDesign Decision Logs  \nArchitecture Decision Record (ADRs)  \nTrade Studies  \nSome sections in the documentation of the repository might point to the project\u2019s documentation (e.g., Onboarding, Working Agreement, Contributing Guide).  \nCommon documentation to all repositories  \nIntroduction  \nProject  \nStakeholders  \nDefinitions  \nRequirements  \nOnboarding  \nRepository guide  \nProduction, Spikes  \nTeam agreements  \nTeam Manifesto\nShort summary of expectations around the technical way of working and supported mindset in the team.\nE.g., ownership, respect, collaboration, transparency.  \nWorking Agreement\nHow we work together as a team and what our expectations and principles are.\nE.g., communication, work-life balance, scrum rhythm, backlog management, code management.  \nDefinition of Done\nList of tasks that must be completed to close a user story, a sprint, or a milestone.  \nDefinition of Ready\nHow complete a user story should be in order to be selected as candidate for estimation in the sprint planning.  \nContributing Guide  \nRepo structure  \nDesign documents  \nBranching and branch name strategy  \nMerge and commit history strategy  \nPull Requests  \nCode Review Process  \nCode Review Checklist\nLanguage Specific Checklists  \nProject Design  \nHigh Level / Game Plan  \nMilestone / Epic Design Review  \nDesign Review Recipes  \nMilestone / Epic Design Review Template  \nFeature / Story Design Review Template  \nTask Design Review Template  \nDecision Log Template  \nArchitecture Decision Record (ADR) Template (Example 1,\nExample 2)  \nTrade Study Template",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\guidance\\project-and-repositories.md"
    },
    {
        "chunkId": "chunk174_0",
        "chunkContent": "Pull Requests  \nWhen we create Pull Requests, we must ensure they are properly documented:  \nTitle and Description  \nPull Request Description  \nPull Request Template  \nLinked worked items  \nComments  \nAs an author, address all comments  \nAs a reviewer, make comments clear",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\guidance\\pull-requests.md"
    },
    {
        "chunkId": "chunk175_0",
        "chunkContent": "REST APIs  \nWhen creating REST APIs, you can leverage the OpenAPI-Specification (OAI) (originally known as the Swagger Specification) to describe them:  \nThe OpenAPI Specification (OAS) defines a standard, programming language-agnostic interface description for HTTP APIs, which allows both humans and computers to discover and understand the capabilities of a service without requiring access to source code, additional documentation, or inspection of network traffic. When properly defined via OpenAPI, a consumer can understand and interact with the remote service with a minimal amount of implementation logic.  \nUse cases for machine-readable API definition documents include, but are not limited to: interactive documentation; code generation for documentation, clients, and servers; and automation of test cases. OpenAPI documents describe an APIs services and are represented in either YAML or JSON formats. These documents may either be produced and served statically or be generated dynamically from an application.  \nThere are implementations available for many languages like C#, including low-level tooling, editors, user interfaces, code generators, etc. Here you can find a list of known tooling for the different languages: OpenAPI-Specification/IMPLEMENTATIONS.md.  \nUsing Microsoft TypeSpec  \nWhile the OpenAPI-Specification (OAI) is a popular method for defining and documenting RESTful APIs, there are other languages available that can simplify and expedite the documentation process. Microsoft TypeSpec is one such language that allows for the description of cloud service APIs and the generation of API description languages, client and service code, documentation, and other assets.  \nMicrosoft TypeSpec is a highly extensible language that offers a set of core primitives that can describe API shapes common among REST, OpenAPI, GraphQL, gRPC, and other protocols. This makes it a versatile option for developers who need to work with a range of different API styles and technologies.  \nMicrosoft TypeSpec is a widely adopted tool within Azure teams, particularly for generating OpenAPI Specifications in complex and interconnected APIs that span multiple teams. To ensure consistency across different parts of the API, teams commonly leverage shared libraries which contain reusable patterns. This makes easier to follow best practices rather than deviating from them. By promoting highly regular API designs that adhere to best practices by construction, TypeSpec can help improve the quality and consistency of APIs developed within an organization.  \nReferences  \nASP.NET Core web API documentation with Swagger / OpenAPI.  \nMicrosoft TypeSpec.  \nDesign Patterns - REST API Guidance",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\guidance\\rest-apis.md"
    },
    {
        "chunkId": "chunk176_0",
        "chunkContent": "Work Items  \nWhile many teams can work with a flat list of items, sometimes it helps to group related items into a hierarchical structure. You can use portfolio backlogs to bring more order to your backlog.  \nAgile process backlog work item hierarchy:  \nScrum process backlog work item hierarchy:  \nBugs can be set at the same level as User Stories / Product Backlog Items or Tasks.  \nEpics and Features  \nUser stories / Product Backlog Items roll up into Features, which typically represent a shippable deliverable that addresses a customer need e.g., \"Add shopping cart\". And Features roll up into Epics, which represent a business initiative to be accomplished e.g., \"Increase customer engagement\". Take that into account when naming them.  \nEach Feature or Epic should include as much detail as the team needs to:  \nUnderstand the scope.  \nEstimate the work required.  \nDevelop tests.  \nEnsure the end product meets acceptance criteria.  \nDetails that should be added:  \nValue Area: Business (directly deliver customer value) vs. Architectural (technical services to implement business features).  \nEffort / Story Points / Size: Relative estimate of the amount of work required to complete the item.  \nBusiness Value: Priority of an item compared to other items of the same type.  \nTime Criticality: Higher values indicate an item is more time critical than items with lower values.  \nTarget Date by which the feature should be implemented.  \nYou may use work item tags to support queries and filtering.  \nUser Stories / Product Backlog Items  \nEach User Story / Product Backlog Item should be sized so that they can be completed within a sprint.  \nYou should add the following details to the items:  \nTitle: Usually expressed as \"As a [persona], I want [to perform an action], so that [I can achieve an end result].\".  \nDescription: Provide enough detail to create shared understanding of scope and support estimation efforts. Focus on the user, what they want to accomplish, and why. Don't describe how to develop the product. Provide enough details so the team can write tasks and test cases to implement the item.  \nInclude Design Reviews.  \nAcceptance Criteria: Define what \"Done\" means.  \nActivity: Deployment, Design, Development, Documentation, Requirements, Testing.  \nEffort / Story Points / Size: Relative estimate of the amount of work required to complete the item.  \nBusiness Value: Priority of an item compared to other items of the same type.  \nOriginal Estimate: The amount of estimated work required to complete a task.  \nRemember to use the Discussion section of the items to keep track of related comments, and mention individuals, groups, work items or pull requests when required.  \nTasks  \nEach Task should be sized so that they can be completed within a day.  \nYou should at least add the following details to the items:  \nTitle.  \nDescription: Provide enough detail to create shared understanding of scope. Any developer should be able to take the item and know what needs to be implemented.  \nInclude Design Reviews.  \nReference to the working branch in related code repository.  \nRemember to use the Discussion section of the tasks to keep track of related comments.  \nBugs  \nYou should use bugs to capture both the initial issue and ongoing discoveries.  \nYou should at least add the following details to the bug items:  \nTitle.  \nDescription.  \nSteps to Reproduce.  \nSystem Info / Found in Build: Software and system configuration that is relevant to the bug and tests to apply.  \nAcceptance Criteria: Criteria to meet so the bug can be closed.  \nIntegrated in Build: Name of the build that incorporates the code that fixes the bug.  \nPriority:  \n1: Product should not ship without the successful resolution of the work item. The bug should be addressed as soon as possible.  \n2: Product should not ship without the successful resolution of the work item, but it does not need to be addressed immediately.  \n3: Resolution of the work item is optional based on resources, time, and risk.  \nSeverity:  \n1 - Critical: Must fix. No acceptable alternative methods.  \n2 - High: Consider fix. An acceptable alternative method exists.  \n3 - Medium: (Default).  \n4 - Low.  \nIssues / Impediments  \nDon't confuse with bugs. They represent unplanned activities that may block work from getting done. For example: feature ambiguity, personnel or resource issues, problems with environments, or other risks that impact scope, quality, or schedule.  \nIn general, you link these items to user stories or other work items.  \nActions from Retrospectives  \nAfter a retrospective, every action that requires work should be tracked with its own Task or Issue / Impediment. These items might be unparented (without link to parent backlog item or user story).  \nRelated information  \nBest practices for Agile project management - Azure Boards | Microsoft Docs.  \nDefine features and epics, organize backlog items - Azure Boards | Microsoft Docs.  \nCreate your product backlog - Azure Boards | Microsoft Docs.  \nAdd tasks to support sprint planning - Azure Boards | Microsoft Docs.  \nDefine, capture, triage, and manage bugs or code defects - Azure Boards | Microsoft Docs.  \nAdd and manage issues or impediments - Azure Boards | Microsoft Docs.",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\guidance\\work-items.md"
    },
    {
        "chunkId": "chunk177_0",
        "chunkContent": "Deploy the DocFx Documentation website to an Azure Website automatically  \nIn the article Using DocFx and Companion Tools to generate a Documentation website the process is described to generate content of a documentation website using DocFx. This document describes how to setup an Azure Website to host the content and automate the deployment to it using a pipeline in Azure DevOps.  \nThe QuickStart sample that is provided for a quick setup of DocFx generation also contains the files explained in this document. Especially the .pipelines and infrastructure folders.  \nThe following steps can be followed when using the Quick Start folder. In the infrastructure folder you can find the Terraform files to create the website in an Azure environment. Out of the box, the script will create a website where the documentation content can be deployed to.  \n1. Install Terraform  \nYou can use tools like Chocolatey to install Terraform:  \n{% raw %}  \nshell\nchoco install terraform  \n{% endraw %}  \n2. Set the proper variables  \nConfigure Azure AD authentication - Azure App Service.  \nIf you want to set a custom domain for your documentation website with an SSL certificate you have to do some extra steps. You have to create a Key Vault and store the certificate there. Next step is to uncomment and set the values in variables.tf. You also have to uncomment the necessary steps in main.tf. All is indicated by comment-boxes. For more information see Add a TLS/SSL certificate in Azure App Service.  \nSome extra information on SSL certificate, custom domain and Azure App Service can be found in the following paragraphs. If you are familiar with that or don't need it, go ahead and continue with Step 3.  \nSSL Certificate  \nTo secure a website with a custom domain name and a certificate, you can find the steps to take in the article Add a TLS/SSL certificate in Azure App Service. That article also contains a description of ways to obtain a certificate and the requirements for a certificate. Usually you'll get a certificate from the customers IT department. If you want to start with a development certificate to test the process, you can create one yourself. You can do that in PowerShell with the script below. Replace:  \n[YOUR DOMAIN] with the domain you would like to register, e.g. docs.somewhere.com  \n[PASSWORD] with a password of the certificate. It's required for uploading a certificate in the Key Vault to have a password. You'll need this password in that step.  \n[FILENAME] for the output file name of the certificate. You can even insert the path here where it should be store on your machine.  \nYou can store this script in a PowerShell script file (ps1 extension).  \n{% raw %}  \npowershell\n$cert = New-SelfSignedCertificate -CertStoreLocation cert:\\currentuser\\my -Subject \"cn=[YOUR DOMAIN]\" -DnsName \"[YOUR DOMAIN]\"\n$pwd = ConvertTo-SecureString -String '[PASSWORD]' -Force -AsPlainText\n$path = 'cert:\\currentuser\\my\\' + $cert.thumbprint\nExport-PfxCertificate -cert $path -FilePath [FILENAME].pfx -Password $pwd  \n{% endraw %}  \nThe certificate needs to be stored in the common Key Vault. Go to Settings > Certificates in the left menu of the Key Vault and click Generate/Import. Provide these details:  \nMethod of Certificate Creation: Import  \nCertificate name: e.g. ssl-certificate  \nUpload Certificate File: select the file on disc for this.  \nPassword: this is the [PASSWORD] we reference earlier.  \nCustom domain registration  \nTo use a custom domain a few things need to be done. The process in the Azure portal is described in the article Tutorial: Map an existing custom DNS name to Azure App Service. An important part is described under the header Get a domain verification ID. This ID needs to be registered with the DNS description as a TXT record.  \nImportant to know is that this Custom Domain Verification ID is the same for all web resources in the same Azure subscription. See this StackOverflow issue. This means that this ID needs to be registered only once for one Azure Subscription. And this enables (re)creation of an App Service with the custom domain though script.  \nAdd Get-permissions for Microsoft Azure App Service  \nThe Azure App Service needs to access the Key Vault to get the certificate. This is needed for the first run, but also when the certificate is renewed in the Key Vault. For this purpose the Azure App Service accesses the Key Vault with the App Service resource provided identity. This identity can be found with the service principal name abfa0a7c-a6b6-4736-8310-5855508787cd or Microsoft Azure App Service and is of type Application. This ID is the same for all Azure subscriptions. It needs to have Get-permissions on secrets and certificates. For more information see this article Import a certificate from Key Vault.  \nAdd the custom domain and SSL certificate to the App Service  \nOnce we have the SSL certificate and there is a complete DNS registration as described, we can uncomment the code in the Terraform script from the Quick Start folder to attach this to the App Service. In this script you need to reference the certificate in the common Key Vault and use it in the custom hostname binding. The custom hostname is assigned in the script as well. The settings ssl_state needs to be SniEnabled if you're using an SSL certificate. Now the creation of the authenticated website with a custom domain is automated.  \n3. Deploy Azure resources from your local machine  \nOpen up a command prompt. For the commands to be executed, you need to have a connection to your Azure subscription. This can be done using Azure Cli. Type this command:  \n{% raw %}  \nshell\naz login  \n{% endraw %}  \nThis will use the web browser to login to your account. You can check the connected subscription with this command:  \n{% raw %}  \nshell\naz account show  \n{% endraw %}  \nIf you have to change to another subscription, use this command where you replace [id] with the id of the subscription to select:  \n{% raw %}  \nshell\naz account set --subscription [id]  \n{% endraw %}  \nOnce this is done run this command to initialize:  \n{% raw %}  \nshell\nterraform init  \n{% endraw %}  \nNow you can run the command to plan what the script will do. You run this command every time changes are made to the terraform scripts:  \n{% raw %}  \nshell\nterraform plan  \n{% endraw %}  \nInspect the result shown. If that is what you expect, apply these changes with this command:  \n{% raw %}  \nshell\nterraform apply  \n{% endraw %}  \nWhen asked for approval, type \"yes\" and ENTER. You can also add the -auto-approve flag to the apply command.  \nThe deployment using Terraform is not included in the pipeline from the Quick Start folder as described in the next step, as that asks for more configuration. But of course that can always be added.  \n4. Deploy the website from a pipeline  \nThe best way to create the resources and deploy to it, is to do this automatically in a pipeline. For this purpose the .pipelines/documentation.yml pipeline is provided. This pipeline is built for an Azure DevOps environment. Create a pipeline and reference this YAML file.  \nIMPORTANT: the Quick Start folder contains a web.config that is needed for deployment to IIS or Azure App Service. This enables the use of the json file for search requests. If you don't have this in place, the search of text will never return anything and result in 404's under the hood.  \nYou have to create a Service Connection in your DevOps environment to connect to the Azure Subscription you want to deploy to.  \nIMPORTANT: set the variables AzureConnectionName to the name of the Service Connection and the AzureAppServiceName to the name you determined in the infrastructure/variables.tf.  \nIn the Quick Start folder the pipeline uses master as trigger, which means that any push being done to master triggers the pipeline. You will probably change this to another branch.",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\recipes\\deploy-docfx-azure-website.md"
    },
    {
        "chunkId": "chunk178_0",
        "chunkContent": "Recipes  \ndeploy-docfx-azure-website  \nstatic-website-with-mkdocs  \nsync-wiki-between-repos  \nusing-docfx-and-tools",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\recipes\\README.md"
    },
    {
        "chunkId": "chunk179_0",
        "chunkContent": "How to create a static website for your documentation based on mkdocs and mkdocs-material  \nMkDocs is a tool built to create static websites from raw markdown files. Other alternatives include Sphinx, and Jekyll.  \nWe used MkDocs to create ISE Code-With Engineering Playbook static website from the contents in the GitHub repository. Then we deployed it to GitHub Pages.  \nWe found MkDocs to be a good choice since:  \nIt's easy to set up and looks great even with the vanilla version.  \nIt works well with markdown, which is what we already have in the Playbook.  \nIt uses a Python stack which is friendly to many contributors of this Playbook.  \nFor comparison, Sphinx mainly generates docs from restructured-text (rst) format, and Jekyll is written in Ruby.  \nTo setup an MkDocs website, the main assets needed are:  \nAn mkdocs.yaml file, similar to the one we have in the Playbook. This is the configuration file that defines the appearance of the website, the navigation, the plugins used and more.  \nA folder named docs (the default value for the directory) that contains the documentation source files.  \nA GitHub Action for automatically generating the website (e.g. on every commit to main), similar to this one from the Playbook.  \nA list of plugins used during the build phase of the website. We specified ours here. And these are the plugins we've used:  \nMaterial for MkDocs: Material design appearance and user experience.\npymdown-extensions: Improves the appearance of markdown based content.\nmdx_truly_sane_lists: For defining the indent level for lists without having to refactor the entire documentation we already had in the Playbook.  \nSetting up locally is very easy. See Getting Started with MkDocs for details.  \nFor publishing the website, there's a good integration with GitHub for storing the website as a GitHub Page.  \nAdditional links  \nMkDocs Plugins  \nThe best MkDocs plugins and customizations",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\recipes\\static-website-with-mkdocs.md"
    },
    {
        "chunkId": "chunk180_0",
        "chunkContent": "How to Sync a Wiki between Repositories  \nThis is a quick guide to mirroring a Project Wiki to another repository.  \n{% raw %}  \n```bash\n\nClone the wiki\n\ngit clone\n\nAdd mirror repository as a remote\n\ncd\ngit remote add mirror\n```  \n{% endraw %}  \nNow each time you wish to sync run the following to get latest from the source wiki repo:  \n{% raw %}  \n```bash\n\nGet everything\n\ngit pull -v\n```  \n{% endraw %}  \nWarning: Check that the output of the pull shows \"From source repo URL\". If this shows the mirror repo url then you've forgotten to reset the tracking. Run git branch -u origin/wikiMaster then continue.  \nThen run this to push it to the mirror repo and reset the branch to track the source repo again:  \n{% raw %}  \n```bash\n\nPush all branches up to mirror remote\n\ngit push -u mirror\n\nReset local to track source remote\n\ngit branch -u origin/wikiMaster\n\n```  \n{% endraw %}  \nYour output should look like this when run:  \n{% raw %}  \n```powershell\nPS C:\\Git\\MyProject.wiki> git pull -v\nPOST git-upload-pack (909 bytes)\nremote: Azure Repos\nremote: Found 5 objects to send. (0 ms)\nUnpacking objects: 100% (5/5), done.\nFrom https://.....  wikiMaster -> origin/wikiMaster\nUpdating 7412b94..a0f543b\nFast-forward\n.../dffffds.md | 4 ++++\n1 file changed, 4 insertions(+)\n\nPS C:\\Git\\MyProject.wiki> git push -u mirror\nEnumerating objects: 9, done.\nCounting objects: 100% (9/9), done.\nDelta compression using up to 8 threads\nCompressing objects: 100% (5/5), done.\nWriting objects: 100% (5/5), 2.08 KiB | 2.08 MiB/s, done.\nTotal 5 (delta 4), reused 0 (delta 0)\nremote: Analyzing objects... (5/5) (6 ms)\nremote: Storing packfile... done (48 ms)\nremote: Storing index... done (59 ms)\nTo https://......\n7412b94..a0f543b  wikiMaster -> wikiMaster\nBranch 'wikiMaster' set up to track remote branch 'wikiMaster' from 'mirror'.\n\nPS C:\\Git\\MyProject.wiki> git branch -u origin/wikiMaster\nBranch 'wikiMaster' set up to track remote branch 'wikiMaster' from 'origin'.\n```  \n{% endraw %}",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\recipes\\sync-wiki-between-repos.md"
    },
    {
        "chunkId": "chunk181_0",
        "chunkContent": "Using DocFx and Companion Tools to generate a Documentation website  \nIf you want an easy way to have a website with all your documentation coming from Markdown files and comments coming from code, you can use DocFx. The website generated by DocFx also includes fast search capabilities. There are some gaps in the DocFx solution, but we've provided companion tools that help you fill those gaps. Also see the blog post Providing quality documentation in your project with DocFx and Companion Tools for more explanation about the solution.  \nPrerequisites  \nThis document is followed best by cloning the sample from https://github.com/mtirionMSFT/DocFxQuickStart first. Copy the contents of the QuickStart folder to the root of your own repository to get started in your own environment.  \nQuick Start  \nTLDR;  \nIf you want a really quick start using Azure DevOps and Azure App Service without reading the what and how, follow these steps:  \nAzure DevOps: If you don't have it yet, create a project in Azure DevOps and create a Service Connection to your Azure environment. Clone the repository.  \nQuickStart folder: Copy the contents of the QuickStart folder in there repository that can be found on  https://github.com/mtirionMSFT/DocFxQuickStart to the root of the repository.  \nAzure: Create a resource group in your Azure environment where the documentation website resources should be created.  \nCreate Azure resources: Fill in the default values in infrastructure/variables.tf and run the commands from Step 3 - Deploy Azure resources from your local machine to create the Azure Resources.  \nPipeline: Fill in the variables in .pipelines/documentation.yml, commit the changes and push the contents of the repository to your branch (possibly through a PR).\nNow you can create a pipeline in your Azure DevOps project that uses the .pipelines/documentation.yml and run it.  \nDocuments and projects folder structure  \nThe easiest is to work with a mono repository where documentation and code live together. If that's not the case in your situation but you still want to combine multiple repositories into one documentation website, you'll have to clone all repositories first to be able to combine the information. In this recipe we'll assume a monorepo is used.  \nIn the steps below we'll consider the generation of the documentation website from this content structure:  \n{% raw %}  \nxaml\n\u251c\u2500\u2500 .pipelines             // Azure DevOps pipeline for automatic generation and deployment\n\u2502\n\u251c\u2500\u2500 docs                     // all documents\n\u2502   \u251c\u2500\u2500 .attachments  // all images and other attachments used by documents\n\u2502\n\u251c\u2500\u2500 infrastructure       // Terraform scripts for creation of the Azure website\n\u2502\n\u251c\u2500\u2500 src                        // all projects\n\u2502   \u251c\u2500\u2500 build              // build settings\n\u2502          \u251c\u2500\u2500 dotnet     // .NET build settings\n\u2502   \u251c\u2500\u2500 Directory.Build.props   // project settings for all .NET projects in sub folders\n\u2502   \u251c\u2500\u2500 [Project folders]\n\u2502\n\u251c\u2500\u2500 x-cross\n\u2502   \u251c\u2500\u2500 toc.yml              // Cross reference definition (optional)\n\u2502\n\u251c\u2500\u2500 .markdownlint.json // Markdownlinter settings\n\u251c\u2500\u2500 docfx.json               // DocFx configuration\n\u251c\u2500\u2500 index.md                 // Website landing page\n\u251c\u2500\u2500 toc.yml                    // Definition of the website header content links\n\u251c\u2500\u2500 web.config              // web.config to enable search in deployed website  \n{% endraw %}  \nWe'll be using the DocLinkChecker tool to validate all links in documentation and for orphaned attachments. That's the reason we have all attachments in the .attachments folder.  \nIn the generated website from the QuickStart folder you'll see that the hierarchies of documentation and references is combined in the left table of contents. This is achieved by the definition and use of x-cross\\toc.yml. If you don't want the hierarchies combined, just remove the  folder and file from your environment and (re)generate the website.  \nA .markdownlint.json is included with the contents below. The MD013 setting is set to false to prevent checking for maximum line length. You can modify this file to your likings to include or exclude certain tests.  \n{% raw %}  \njson\n{\n\"MD013\": false\n}  \n{% endraw %}  \nThe contents of the .pipelines and infrastructure folders are explained in the recipe Deploy the DocFx Documentation website to an Azure Website automatically.  \nReference documentation from source code  \nDocFx can generate reference documentation from code, where C# and Typescript are supported best at the moment. In the QuickStart folder we only used C# projects. For DocFx to generate quality reference documentation, quality triple slash-comments are required. See Triple-slash (///) Code Comments Support. To enforce this, it's a good idea to enforce the use of StyleCop. There are a few steps that will give you an easy start with this.  \nFirst, you can use the Directory.Build.props file in the /src folder in combination with the files in the build/dotnet folder. By having this, you enforce StyleCop in all Visual Studio project files in it's sub folders with a configuration of which rules should be used or ignored. You can tailor this to your needs of course. For more information, see Customize your build and Use rule sets to group code analysis rules.  \nTo make sure developers are forced to add the triple-slash comments by throwing compiler errors and to have the proper settings for the generation of documentation XML-files, add the TreatWarningsAsErrors and GenerateDocumentationFile settings to every .csproj file. You can add that in the first PropertyGroup settings like this:  \n{% raw %}  \n```xml\n\n...\ntrue\ntrue\n\n```  \n{% endraw %}  \nNow you are all set to generate documentation from your C# code. For more information about languages supported by DocFx and how to configure it, see Introduction to Multiple Languages Support.  \nNOTE: You can also add a PropertyGroup definition with the two settings in Directory.Build.props to have that settings in all projects. But in that case it will also be inherited in your Test projects.  \n1. Install DocFx and markdownlint-cli  \nGo to the DocFx website to the Download section and download the latest version of DocFx. Go to the github page of markdownlint-cli to find download and install options.  \nYou can also use tools like Chocolatey to install:  \n{% raw %}  \nbash\nchoco install docfx\nchoco install markdownlint-cli  \n{% endraw %}  \n2. Configure DocFx  \nConfiguration for DocFx is done in a docfx.json file. Store this file in the root of your repository.  \nNOTE: You can store the docfx.json somewhere else in the hierarchy, but then you need to provide the path of the file as an argument to the docfx command so it can be located.  \nBelow is a good configuration to start with, where documentation is in the /docs folder and the sources are in the /src folder:  \n{% raw %}  \njson\n{\n\"metadata\": [\n{\n\"src\": [\n{\n\"files\": [ \"src/**.csproj\" ],\n\"exclude\": [ \"_site/**\", \"**/bin/**\", \"**/obj/**\", \"**/[Tt]ests/**\" ]\n}\n],\n\"dest\": \"reference\",\n\"disableGitFeatures\": false\n}\n],\n\"build\": {\n\"content\": [\n{ \"files\": [ \"reference/**\" ] },\n{\n\"files\": [ \"**.md\", \"**/toc.yml\" ],\n\"exclude\": [ \"_site/**\", \"**/bin/**\", \"**/obj/**\", \"**/[Tt]ests/**\" ]\n}\n],\n\"resource\": [\n{ \"files\": [\"docs/.attachments/**\"] },\n{ \"files\": [\"web.config\"] }\n],\n\"template\": [ \"templates/cse\" ],\n\"globalMetadata\": {\n\"_appTitle\": \"CSE Documentation\",\n\"_enableSearch\": true\n},\n\"markdownEngineName\": \"markdig\",\n\"dest\": \"_site\",\n\"xrefService\": [\"https://xref.learn.microsoft.com/query?uid={uid}\"]\n}\n}  \n{% endraw %}  \n3. Setup some basic documents  \nWe suggest starting with a basic documentation structure in the /docs folder. In the provided QuickStart folder we have a basic setup:  \n{% raw %}  \nxaml\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 .attachments                     // All images and other attachments used by documents\n\u2502\n\u2502   \u251c\u2500\u2500 architecture-decisions\n\u2502           \u2514\u2500\u2500 .order\n\u2502           \u2514\u2500\u2500 decision-log.md       // Sample index into all ADRs\n\u2502           \u2514\u2500\u2500 README.md          // Landing page architecture decisions\n\u2502\n\u2502   \u251c\u2500\u2500 getting-started\n\u2502           \u2514\u2500\u2500 .order\n\u2502           \u2514\u2500\u2500 README.md          // This recipe document. Replace the content with something meaningful to the project\n\u2502\n\u2502   \u251c\u2500\u2500 guidelines\n\u2502           \u2514\u2500\u2500 .order\n\u2502           \u2514\u2500\u2500 docs-guidelines.md  // General documentation guidelines\n\u2502           \u2514\u2500\u2500 README.md          // Landing page guidelines\n\u2502\n\u2502   \u251c\u2500\u2500 templates                          // all templates like ADR template and such\n\u2502           \u2514\u2500\u2500 .order\n\u2502           \u2514\u2500\u2500 README.md          // Landing page templates\n\u2502\n\u2502   \u251c\u2500\u2500 working-agreements\n\u2502           \u2514\u2500\u2500 .order\n\u2502           \u2514\u2500\u2500 README.md          // Landing page working agreements\n\u2502\n\u2502   \u251c\u2500\u2500 .order                                // Providing a fixed order of files and directories\n\u2502   \u251c\u2500\u2500 index.md                          // Landing page documentation  \n{% endraw %}  \nYou can use templates like working agreements and such from the ISE Playbook.  \nTo have a proper landing page of your documentation website, you can use a markdown file called INDEX.MD in the root of your repository. Contents can be something like this:  \n{% raw %}  \n```markdown\n\nISE Documentation\n\nThis is the landing page of the ISE Documentation website. This is the page to introduce everything on the website.\n\nYou can add specific links that are important to provide direct access.\n\nTry not to duplicate the links on the top of the page, unless it really makes sense.\n\nTo get started with the setup of this website, read the getting started document with the title Using DocFx and Companion Tools.\n\n```  \n{% endraw %}  \n4. Compile the companion tools and run them  \nNOTE: To explain each step, we'll be going through the various steps in the next few paragraphs. In the provided sample, a batch-file called GenerateDocWebsite.cmd is included. This script will take all the necessary steps to compile the tools, execute the checks, generate the table of contents and execute docfx to generate the website.  \nTo check for proper markdown formatting the markdownlint-cli tool is used. The command takes it's configuration from the .markdownlint.json file in the root of the project. To check all markdown files, simply execute this command:  \n{% raw %}  \nshell\nmarkdownlint **/*.md  \n{% endraw %}  \nIn the QuickStart folder you should have copied in the two companion tools TocDocFxCreation and DocLinkChecker as described in the introduction of this article.  \nYou can compile the tools from Visual Studio, but you can also run dotnet build in both tool folders.  \nThe DocLinkChecker companion tool is used to validate what's in the docs folder. It validates links between documents and attachments in the docs folder and checks if there aren't orphaned attachments. An example of executing this tool, where the check of attachments is included:  \n{% raw %}  \nshell\nDocLinkChecker.exe -d ./docs -a  \n{% endraw %}  \nThe TocDocFxCreation tool is needed to generate a table of contents for your documentation, so users can navigate between folders and documents. If you have compiled the tool, use this command to generate a table of content file toc.yml. To generate a table of contents with the use of the .order files for determining the sequence of articles and to automatically generate index.md documents if no default document is available in a folder, this command can be used:  \n{% raw %}  \nshell\nTocDocFxCreation.exe -d ./docs -sri  \n{% endraw %}  \n5. Run DocFx to generate the website  \nRun the docfx command to generate the website, by default in the _site folder.  \nTIP: If you want to check the website in your local environment, provide the --serve option to either the docfx command or the GenerateDocWebsite script. A small webserver is launched that hosts your website, which is accessible on localhost.  \nStyle of the website  \nIf you started with the QuickStart folder, the website is generated using a custom theme using material design and the Microsoft logo. You can change this to your likings. For more information see How-to: Create A Custom Template | DocFX website (dotnet.github.io).  \nDeploy to an Azure Website  \nAfter you completed the steps, you should have a default website generated in the _site folder. But of course, you want this to be accessible for everyone. So, the next step is to create for instance an Azure Website and have a process to automatically generate and deploy the contents to that website. That process is described in the recipe Deploy the DocFx Documentation website to an Azure Website automatically.  \nReferences  \nDocFX - static documentation generator  \nDeploy the DocFx Documentation website to an Azure Website automatically  \nProviding quality documentation in your project with DocFx and Companion Tools  \nMonorepo For Beginners",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\recipes\\using-docfx-and-tools.md"
    },
    {
        "chunkId": "chunk182_0",
        "chunkContent": "How to Automate Simple Checks  \nIf you want to automate some checks on your Markdown documents, there are several tools that you could leverage. For example:  \nCode Analysis / Linting  \nmarkdownlint to verify Markdown syntax and enforce rules that make the text more readable.  \nmarkdown-link-check to extract links from markdown texts and check whether each link is alive (200 OK) or dead.  \nwrite-good to check English prose.  \nDocker image for node-markdown-spellcheck, a lightweight docker image to spellcheck markdown files.  \nstatic code analysis  \nVS Code Extensions  \nWrite Good Linter to get grammar and language advice while editing a document.  \nmarkdownlint to examine Markdown documents and get warnings for rule violations while editing.  \nAutomation  \npre-commit to use Git hook scripts to identify simple issues before submitting our code or documentation for review.  \nCheck Build validation to automate linting for PRs.  \nCheck CI Pipeline for better documentation for a sample pipeline with markdownlint, markdown-link-check and write-good.  \nSample output:  \nOn linting rules  \nThe team needs to be clear what linting rules are required and shouldn't be overridden with tooling or comments. The team should have consensus on when to override tooling rules.",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\tools\\automation.md"
    },
    {
        "chunkId": "chunk183_0",
        "chunkContent": "Integration with Teams/Slack  \nMonitor your Azure repositories and receive notifications in your channel whenever code is pushed/checked in and whenever a pull request (PR) is created, updated, or a merge is attempted.  \nAzure Repos with Microsoft Teams  \nAzure Repos with Slack",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\tools\\integrations.md"
    },
    {
        "chunkId": "chunk184_0",
        "chunkContent": "Languages  \nMarkdown  \nMarkdown is one of the most popular markup languages to add rich formatting, tables and images to your documentation using plain text documents.  \nMarkdown files (.md) can be source-controlled along with your code.  \nMore information:  \nGetting Started  \nCheat Sheet  \nBasic Syntax  \nExtended Syntax  \nWiki Markdown Syntax  \nTools:  \nMarkdown and Visual Studio Code  \nHow to automate simple checks  \nMermaid  \nMermaid lets you create diagrams using text definitions that can later be rendered with a diagramming and charting tool.  \nMermaid files (.mmd) can be source-controlled along with your code. It's also recommended to include image files (.png) with the rendered diagrams under source control. Your markdown files should link the image files, so they can be read without the need of a Mermaid rendering tool (e.g., during Pull Request review).  \nExample Mermaid diagram  \nThis is an example of a Mermaid flowchart diagram written as code.  \n{% raw %}  \nmermaid\ngraph LR\nA[Diagram Idea] -->|Write mermaid code| B(mermaid.mmd file)\nB -->|Add to source control| C{Code repo}\nB -->|Export as .png| G(.png file of diagram)\nG -->|Add to source control| C  \n{% endraw %}  \nThis is an example of how it can be rendered as an image.  \nMore information:  \nAbout Mermaid  \nDiagram syntax  \nTools:  \nMermaid Live Editor  \nMarkdown Preview Mermaid Support for Visual Studio Code",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\tools\\languages.md"
    },
    {
        "chunkId": "chunk185_0",
        "chunkContent": "Wikis  \nUse a team project wiki to share information with other team members. When you provision a wiki from scratch, a new Git repository stores your Markdown files, images, attachments, and sequence of pages. This wiki supports collaborative editing of its content and structure.  \nIn Azure DevOps, you have the following options for maintaining wiki content:  \nProvision a wiki for your team project. This option supports only one wiki for the team project.  \nPublish Markdown files defined in a Git repository to a wiki. With this option, you can maintain several versioned wikis to support your content needs.  \nMore information:  \nAbout Wikis, READMEs, and Markdown.  \nProvisioned wikis vs. published code as a wiki.  \nCreate a Wiki for your project.  \nManage wikis.  \nWikis vs. digital notebooks (e.g., OneNote)  \nWhen you work on a project, you may decide to document relevant details or record important decisions about the project in a digital notebook. Tools like OneNote allows you to easily organize, navigate and search your notes. You can provide type, highlighting, or ink annotations to your notes. These notes can easily be shared and created together with others. Still, Wikis greatly facilitate the process of establishing and managing documentation by allowing us to source control the documentation.",
        "source": "..\\data\\docs\\code-with-engineering\\documentation\\tools\\wikis.md"
    },
    {
        "chunkId": "chunk186_0",
        "chunkContent": "Engineering Feedback Examples  \nThe following are real-world examples of Engineering Feedback that have led to product improvements and unblocked customers.  \nWindows Server Container support for Azure Kubernetes Service  \nThe Azure Kubernetes Service should have first class Windows container support so solutions that require Windows workloads can be deployed on a wildly popular container orchestration platform. The need was to be able to deploy Windows Server containers on AKS the managed Azure Kubernetes Service. According to this FAQ (and in parallel confirmation) it is not available yet.  \nWe tried to deploy anyway as a test, and it did not work \u2013 the deployment would be pending without success.  \nMore than a dozen large partners/customers are blocked in deploying Windows workloads to AKS due to a lack of support for Windows Server containers. They need this feature so solutions requiring Windows workloads can be deployed to this popular container orchestration platform.  \nWe are seeing an emergence of companies beginning to try Windows containers as an option to move their Windows workloads to the cloud.\u202f Gartner is claiming that 80% of enterprise apps run on Windows. Containers have become the de facto deployment mechanism in the industry, and deployment consistency and speed are a few of the important factors companies are looking for. Enabling Windows applications and ensuring that developers have a good experience when moving their workloads to Azure via Windows containers is key to keeping existing Windows customers within the Azure ecosystem and driving Azure adoption for new workloads.  \nWe are also seeing increased interest, particularly among enterprise customers, in using a single orchestrator control plane for managing both Linux and Windows workloads.  \nThis feedback was created as a high priority feedback and followed up internally until addressed. Here is the announcement.  \nSupport Batch Receiving with Sessions in Azure Functions Service Bus Trigger  \nCustomer scenario was to receive a total of 250 messages per second from 50 producers with requirement for ordering & minimum latency, using a Service Bus topic with sessions enabled for ordering. According to Microsoft documentation, batch receiving is recommended for better performance but this is not currently supported in Azure Functions Service Bus Trigger. The impact (and work around) was choosing containers over Functions. The Acceptance Criteria is for Azure Functions to support Service Bus sessions with batch and non-batch processing for all Azure Functions GA languages.  \nThis feedback was created as a feedback with the Azure Functions product group and also followed up internally until addressed.  \nStream Analytics - No support for zero-downtime scale-down  \nIn order to update the Streaming Unit number in Stream Analytics you need to stop the service and wait for minutes for it to restart. This unacceptable by customers who need near real-time analysis\u200b. In order to have a job re-started, up to 2 minutes are needed and this is not acceptable for a real-time streaming solution. It would also be optimal if scale-up and scale-down could be done automatically, by setting threshold values that when reached increase or decrease automatically the amount of RU available. This feedback is for customers' request for zero down-time scale-down capability in stream analytics.  \nProblem Statement: In order to update the \"Streaming Unit\" number, partners must stop the service and wait until it restarts. The partner needs to be able to update the number without stopping the service.  \nDesired Experience: Partners should be able to update the Streaming Unit number without stopping the associated service.  \nThis feedback was created as a high priority feedback and followed up until addressed in December 2019.  \nPython Support for Azure Functions  \nSeveral customers already use Python as part of their workflow, and would like to be able to use Python for Azure Functions. This is specially true since many of them are already have scripts running on other clouds and services.  \nIn addition, Python support has been in Preview for a very long time, and it's missing a lot of functionality.  \nThis feature request is one of the most asked, and a huge upside potential to pull through Machine Learning (ML) based workloads.  \nThis feedback was created as a feedback with the Azure Functions product group and also followed up internally until addressed. Here is the announcement.",
        "source": "..\\data\\docs\\code-with-engineering\\engineering-feedback\\feedback-examples.md"
    },
    {
        "chunkId": "chunk187_0",
        "chunkContent": "Engineering Feedback Frequently Asked Questions (F.A.Q.)  \nThe questions below are common questions related to the feedback process. The answers are intended to help both Microsoft employees and customers.  \nWhen should I submit feedback versus creating an issue on GitHub, UserVoice, or sending an email directly to a Microsoft employee?  \nIt is appropriate to do both. As a customer or Microsoft employee, you are empowered to create an issue or submit feedback via the medium appropriate for service.  \nIn addition to an issue on GitHub, feedback on UserVoice, or a personal email, Microsoft employees in CSE should submit feedback via CSE Feedback.  In doing so, please reference the GitHub issue, UserVoice feedback, or email by including a link to the item or attaching the email.  \nSubmitting to ISE Feedback allows the ISE Feedback team to coalesce feedback across a wide range of sources, and thus create a unified case to submit to the appropriate Azure engineering team(s).  \nHow can a customer track the status of a specific feedback item?  \nAt this time, customers are not able to directly track the status of feedback submitted via ISE Feedback.  The ISE Feedback process is internal to Microsoft, and as such, available only to Microsoft employees.  Customers may request an update from their ISE engineering partner or Microsoft account representative(s).  \nCustomers can also submit their feedback directly via GitHub or UserVoice (as appropriate for the specific service), and inform their ISE engineering partner.  The ISE engineer should submit the feedback via the ISE Feedback process, and in doing so reference the previously created issue.  Customers can follow the GitHub or UserVoice item to be alerted on updates.  \nHow can a Microsoft employee track the status of a specific feedback item?  \nThe easiest way for a Microsoft employee within ISE to track a specific feedback item is to follow the feedback (a work item) in Azure DevOps.  \nAs a Microsoft employee within ISE, if I submit a feedback and move to another dev crew engagement, how would my customer get an update on that feedback?  \nIf the feedback is also submitted via GitHub or UserVoice, the customer may elect to follow that item for publicly available updates.  The customer may also contact their Microsoft account representative to request an update.  \nAs a Microsoft employee within ISE, what should I expect/do after submitting feedback via ISE Feedback?  \nAfter submitting the feedback, it is recommended to follow the feedback (a work item) in Azure DevOps.  If you have configured Azure DevOps notifications to send an email on work item updates, you will receive an email when the feedback is updated.  \nIf more information about the feedback is needed, a member of the ISE Feedback team will contact you to gather more information.  \nHow/when are feedback aggregated?  \nMembers of the CSE Feedback team will make a best effort to triage and review new CSE Feedback items within two weeks of the original submission date.  \nIf there is similarity across multiple feedback items, a member of the ISE Feedback team may decide to create a new feedback item which is an aggregate of similar items.  This is done to aid in the creation of a unified feedback item to present to the appropriate Microsoft engineering team.  \nOn a monthly basis, the ISE Feedback team will review all feedback and generate a report consisting of the highest priority feedback.  The report is presented to appropriate ISE and Microsoft leadership teams.",
        "source": "..\\data\\docs\\code-with-engineering\\engineering-feedback\\feedback-faq.md"
    },
    {
        "chunkId": "chunk188_0",
        "chunkContent": "Engineering Feedback Guidance  \nThe following guidance provides a minimum set of details that will result in actionable engineering feedback. Ensure that you provide as much detail for each of the following sections as relevant and possible.  \nTitle  \nProvide a meaningful and descriptive title. There is no need to include the Azure service in the title as this will be included as part of the Categorization section.  \nGood examples:  \nSupported X versions not documented  \nRequire all-in-one Y story  \nSummary  \nSummarize the feedback in a short paragraph.  \nCategorization  \nAzure Service  \nWhich Azure service does this feedback item refer to? If there are multiple Azure services involved, pick the primary service and include the details of the others in the Notes section.  \nType  \nSelect one of the following to describe what type of feedback is being provided:  \nBusiness Blocker (e.g. No SLA on X, Service Y not GA, Service A not in Region B)  \nTechnical Blocker (e.g. Accelerated networking not available on Service X)  \nDocumentation (e.g. Instructions for configuring scenario X missing)  \nFeature Request (e.g. Enable simple integration to X on Service Y)  \nStage  \nSelect one of the following to describe the lifecycle stage of the engagement that has generated this feedback:  \nProduction  \nStaging  \nTesting  \nDevelopment  \nImpact  \nDescribe the impact to the customer and engagement that this feedback implies.  \nTime frame  \nProvide a time frame that this feedback item needs to be resolved within (if relevant).  \nPriority  \nPlease provide the customer perspective priority of the feedback.  Feedback is prioritized at one of the following four levels:  \nP0 - Impact is critical and large: Needs to be addressed immediately; impact is critical and large in scope (i.e. major service outage; makes service or functions unusable/unavailable to a high portion of addressable space; no known workaround).  \nP1 - Impact is high and significant: Needs to be addressed quickly; impacts a large percentage of addressable space and impedes progress. A partial workaround exists or is overly painful.  \nP2 - Impact is moderate and varies in scope: Needs to be addressed in a reasonable time frame (i.e. issues that are impeding adoption and usage with no reasonable workarounds). For example, feedback may be related to feature-level issue to solve for friction.  \nP3 - Impact is low: Issue can be address when able or eventually (i.e. relevant to core addressable space but issue does not impede progress or has reasonable workaround). For example, feedback may be related to feature ideas or opportunities.  \nReproduction Steps  \nThe reproduction steps are important since they help confirm and replay the issue, and are essential in demonstrating success once there is a resolution.  \nPre-requisites  \nProvide a clear set of all conditions and pre-requisites required before following the set of reproduction steps. These could include:  \nPlatform (e.g. AKS 1.16.4 cluster with Azure CNI, Ubuntu 19.04 VM)  \nServices (e.g. Azure Key Vault, Azure Monitor)  \nNetworking (e.g. VNET with subnet)  \nSteps  \nProvide a clear set of repeatable steps that will allow for this feedback to be reproduced. This can take the form of:  \nScripts (e.g. bash, PowerShell, terraform, arm template)  \nCommand line instructions (e.g. az, helm, terraform)  \nScreen shots (e.g. azure portal screens)  \nNotes  \nInclude items like architecture diagrams, screenshots, logs, traces etc which can help with understanding your notes and the feedback item. Also include details about the scenario customer/partner verbatim as much as possible in the main content.  \nWhat didn't work  \nDescribe what didn't work or what feature gap you identified.  \nWhat was your expectation or the desired outcome  \nDescribe what you expected to happen. What was the outcome that was expected?  \nDescribe the steps you took  \nProvide a clear description of the steps taken and the outcome/description at each point.",
        "source": "..\\data\\docs\\code-with-engineering\\engineering-feedback\\feedback-guidance.md"
    },
    {
        "chunkId": "chunk189_0",
        "chunkContent": "Microsoft Engineering Feedback  \nWhy is it important to submit Microsoft Engineering Feedback  \nEngineering Feedback captures the \"voice of the customer\" and is an important mechanism to provide actionable insights and help Microsoft product groups continuously improve the platform and cloud services to enable all customers to be as productive as possible.  \nPlease note that Engineering Feedback is an asynchronous (i.e. not real-time) method to capture and aggregate friction points across multiple customers and code-with engagements. Therefore, if you need to report a service outage, or an immediately-blocking bug, you should file an official Azure support ticket and, if possible, reference the ticket id in the feedback that you submit later.  \nEven if the feedback has already been raised directly with a product group or on through online channels like GitHub or Stack Overflow, it is still important to raise it via Microsoft Engineering feedback, so it can be consolidated with other customer projects that have the same feedback to help with prioritization.  \nWhen to submit Engineering Feedback  \nCapturing and providing high-quality actionable Engineering Feedback is an integral ongoing part of all code-with engagements. It is recommended to submit feedback on an ongoing basis instead of batching it up for submission at the end of the engagement.  \nYou should jot down the details of the feedback close to the time when you encounter the specific blockers, challenges, and friction since that is when it is freshest in your mind. The project team can then decide how to prioritize and when to submit the feedback into the official CSE Feedback system (accessible to ISE team members) during each sprint.  \nWhat is good and high-quality Engineering Feedback  \nGood engineering feedback provides enough information for those who are not part of the code-with engagement to understand the customer pain, the associated product issues, the impact and priority of these issues, and any potential workarounds that exist to minimize that impact.  \nHigh-Quality Engineering Feedback is  \nGoal Oriented - states what the customer is trying to accomplish  \nSpecific - details the scenario, observation, or challenge faced by the customer  \nActionable - includes the necessary clarifying information to enable a decision  \nExamples of Good Engineering Feedback  \nFor example, here is an evolution of transforming a fictitious feedback with the above high-quality engineering feedback guidance in mind:  \nStage Feedback Evolution Initial feedback Azure Functions Service Bus Trigger is slow for in-order scenarios Making it Goal Oriented Customer requests batch receiving for Azure Functions Service Bus trigger with sessions enabled to better support higher throughput messaging. They want to use Azure Functions to process as many messages per second as possible with minimum latency and in a given order. Adding Specifics Customer scenario was to receive a total of 250 messages/second from 50 producers with requirement for ordering per producer & minimum latency, using a Service Bus topic with sessions enabled for ordering. Batch receiving is not supported in Azure Functions Service Bus Trigger. Making it Actionable Customer scenario was to receive a total of 250 messages/second from 50 producers with requirement for ordering per producer & minimum latency, using a Service Bus topic with sessions enabled for ordering. According to Microsoft documentation , batch receiving is recommended for better performance but this is not currently supported in the Azure Functions Service Bus Trigger. The impact and workaround was choosing containers over Functions. The desired outcome is for Azure Functions to support Service Bus sessions with batch and non-batch processing for all Azure Functions GA languages.  \nFor real-world examples please follow Feedback Examples.  \nHow to submit Engineering Feedback  \nPlease follow the Engineering Feedback Guidance to ensure that you provide feedback that can be triaged and processed most efficiently.  \nPlease review the Frequently Asked Questions page for additional information on the engineering feedback process.",
        "source": "..\\data\\docs\\code-with-engineering\\engineering-feedback\\README.md"
    },
    {
        "chunkId": "chunk190_0",
        "chunkContent": "Data Exploration  \nAfter envisioning, and typically as part of the ML feasibility study, the next step is to confirm resource access and then dive deep into the available data through data exploration workshops.  \nPurpose of the Data Exploration Workshop  \nThe purpose of the data exploration workshop is as follows:  \nEnsure the team can access the data and compute resources that are necessary for the ML feasibility study  \nEnsure that the data provided is of quality and is relevant to the ML solution  \nMake sure that the project team has a good understanding of the data  \nMake sure that the SMEs (Subject Matter Experts) needed are present for Data Exploration Workshop  \nList people needed for the data exploration workshop  \nAccessing Resources  \nPrior to diving into data exploration workshops, it is important to confirm that you have access to the necessary resources (including data).  \nBelow is an example list of questions to consider before starting a data exploration workshop.  \nWhat are the requirements for an account to be set up in order for the team to access data and compute resources?  \nAre there security requirements around accessing resources (Subscriptions, Azure Resources, project management, etc.) such as VPN, 2FA, jump boxes, etc.?  \nData access:\nIs it on-prem or on Azure already?\nIf it is on-prem, can we move the needed data to Azure under the appropriate subscription? Who has permission to move the data?\nIs the data access approved from a legal/compliance perspective?  \nComputation:\nIs a VPN needed for the project team to access these computation nodes (Virtual Machines, Databricks clusters, etc) from their work PCs/Macs?\nAny restrictions on accessing the source data system from these computation nodes?\nIf we want to create some compute resources, who has permissions to do so?  \nSource code repository:\nDo you have any preference on source code repository location?  \nBacklog management and work planning:\nDo you have any preference on backlog management and work planning, such as Azure DevOps, Jira or anything else?\nIf an existing system, are special accounts / system setups required to access?  \nProgramming Language:\nIs Python/PySpark a preferred language?\nIs there any internal approval processes for the Python/PySpark libraries we want to use for this engagement?  \nData Exploration Workshop  \nKey objectives of the exploration workshops include the following:  \nUnderstand and document the features, location, and availability of the data.  \nWhat order of magnitude is the current data (e.g., GB, TB)? Is this all relevant?  \nHow does the organization decide when to collect additional data or purchase external data? Are there any examples of this?  \nUnderstand the quality of the data. Is there already a data validation strategy in place?  \nWhat data has been used so far to analyze recent data-driven projects? What has been found to be most useful? What was not useful? How was this judged?  \nWhat additional internal data may provide insights useful for data-driven decision-making for proposed projects? What external data could be useful?  \nWhat are the possible constraints or challenges in accessing or incorporating this data?  \nHow was the data collected? Are there any obvious biases due to how the data was collected?  \nWhat changes to data collection, coding, integration, etc has occurred in the last 2 years that may impact the interpretation or availability of the collected data",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\ml-data-exploration.md"
    },
    {
        "chunkId": "chunk191_0",
        "chunkContent": "Generic Envisioning Summary  \nPurpose of this template  \nThis is an example of an envisioning summary completed after envisioning sessions have concluded. It summarizes the materials reviewed, application scenarios discussed and decided, and the next steps in the process.  \nSummary of Envisioning  \nIntroduction  \nThis document is to summarize what we have discussed in these envisioning sessions, and what we have decided to work on in this machine learning (ML) engagement. With this document, we hope that everyone can be on the same page regarding the scope of this ML engagement, and will ensure a successful start for the project.  \nMaterials Shared with the team  \nList materials shared with you here. The list below contains some examples. You will want to be more specific.  \nBusiness vision statement  \nSample Data  \nCurrent problem statement  \nAlso discuss:  \nHow the current solution is built and implemented  \nDetails about the current state of the systems and processes.  \nApplications Scenarios that Can Help [People] Achieve [Task]  \nThe following application scenarios were discussed:  \nScenario 1:  \nScenario 2:  \nAdd more scenarios as needed  \nFor each scenario, provide an appropriately descriptive name and then follow up with more details.  \nFor each scenario, discuss:  \nWhat problem statement was discussed  \nHow we propose to solve the problem (there may be several proposals)  \nWho would use the solution  \nWhat would it look like to use our solution? An example of how it would bring value to the end user.  \nSelected Scenario for this ML Engagement  \nWhich scenario was selected?  \nWhy was this scenario prioritised over the others?  \nWill other scenarios be considered in the future? When will we revisit them / what conditions need to be met to pursue them?  \nMore Details of the Scope for Selected Scenario  \nWhat is in scope?  \nWhat data is available?  \nWhich performance metric to use?  \nBar of performance metrics  \nWhat are deliverables?  \nWhat\u2019s Next?  \nLegal documents to be signed  \nState documents and timeline  \nResponsible AI Review  \nPlan when to conduct a responsible AI process. What are the prerequisites to start this process?  \nData Exploration Workshop  \nA data exploration workshop is planned for DATE RANGE. This data exploration workshops will be X-Y days, not including the time to gain access resources. The purpose of the data exploration workshop is as follows:  \nEnsure the team can access the data and compute resources that are necessary for the ML feasibility study  \nEnsure that the data provided is of quality and is relevant to the ML solution  \nMake sure that the project team has a good understanding of the data  \nMake sure that the SMEs (Subject Matter Experts) needed are present for Data Exploration Workshop  \nList people needed for the data exploration workshop  \nML Feasibility Study till [date]  \nObjectives  \nState what we expect to be the objective in the feasibility study  \nTimeline  \nGive a possible timeline for the feasibility study  \nPersonnel needed  \nWhat sorts of people/roles are needed for the feasibility study?  \nWhat\u2019s After ML Feasibility Study  \nDetail here  \nSummary of Timeline  \nBelow is a high-level summary of the upcoming timeline:  \nDiscuss dates for the data exploration workshop, and feasibility study along with any to-do items such as starting responsible AI process, identifying engineering resources. We suggest using a concise bulleted list or a table to easily convey the information.",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\ml-envisioning-summary-template.md"
    },
    {
        "chunkId": "chunk192_0",
        "chunkContent": "Model Experimentation  \nOverview  \nMachine learning model experimentation involves uncertainty around the expected model results and future operationalization.\nTo handle this uncertainty as much as possible, we propose a semi-structured process, balancing between engineering/research best practices and rapid model/data exploration.  \nModel experimentation goals  \nPerformance: Find the best performing solution  \nOperationalization: Keep an eye towards production, making sure that operationalization is feasible  \nCode quality Maintain code and artifacts quality  \nReproducibility: Keep research active by allowing experiment tracking and reproducibility  \nCollaboration: Foster the collaboration and joint work of multiple people on the team  \nModel experimentation challenges  \nTrial and error process: Difficult to plan and estimate durations and capacity.  \nQuick and dirty: We want to fail fast and get a sense of what\u2019s working efficiently.  \nCollaboration: How do we form a team-wide trial and error process and effective brainstorming.  \nCode quality: How do we maintain the quality of non-production code during research.  \nOperationalization: Switching between approaches might have a significant impact on operationalization (e.g. GPU/CPU, batch/online, parallel/sequential, runtime environments).  \nCreating an experimentation framework which facilitates rapid experimentation, collaboration,\nexperiment and model reproducibility, evaluation  and defined APIs,\nand lets each team member focus on the model development and improvement,\nwhile trusting the framework to do the rest.  \nThe following tools and guidelines are aimed at achieving experimentation goals as well as addressing the aforementioned challenges.  \nTools and guidelines for successful model experimentation  \nVirtual environments  \nSource control and folder/package structure  \nExperiment tracking  \nDatasets and models abstractions  \nModel evaluation  \nVirtual environments  \nIn languages like Python and R, it is always advised to employ virtual environments. Virtual environments facilitate reproducibility, collaboration and productization.\nVirtual environments allow us to be consistent across our local dev envs as well as with compute resources. These environments' configuration files can be used to build the code from source in an consistent way.\nFor more details on why we need virtual environments visit this blog post.  \nWhich virtual environment framework should I choose  \nAll virtual environments frameworks create isolation, some also propose dependency management and additional features. Decision on which framework to use depends on the complexity of the development environment (dependencies and other required resources) and on the ease of use of the framework.  \nTypes of virtual environments  \nIn ISE, we often choose from either venv, Conda or Poetry, depending on the project requirements and complexity.  \nvenv is included in Python, is the easiest to use, but lacks more advanced features like dependency management.  \nConda is a popular package, dependency and environment management framework. It supports multiple stacks (Python, R) and multiple versions of the same environment (e.g. multiple Python versions). Conda maintains its own package repository, therefore some packages might not be downloaded and managed directly through Conda.  \nPoetry is a Python dependency management system which manages dependencies in a standard way using pyproject.toml files and lock files. Similar to Conda, Poetry's dependency resolution process is sometimes slow (see FAQ), but in cases where dependency issues are common or tricky, it provides a robust way to create reproducible and stable environments.  \nExpected outcomes for virtual environments setup  \nDocumentation describing how to create the selected virtual environment and how to install dependencies.  \nEnvironment configuration files if applicable (e.g. requirements.txt for venv, environment.yml for Conda or pyrpoject.toml for Poetry).  \nVirtual environments benefits  \nProductization  \nCollaboration  \nReproducibility  \nSource control and folder or package structure  \nApplied ML projects often contain source code, notebooks, devops scripts, documentation, scientific resources, datasets and more. We recommend coming up with an agreed folder structure to keep resources tidy. Consider deciding upon a generic folder structure for projects (e.g. which contains the folders data, src, docs and notebooks), or adopt popular structures like the CookieCutter Data Science folder structure.  \nSource control should be applied to allow collaboration, versioning, code reviews, traceability and backup. In data science projects, source control should be used for code, and the storing and versioning of other  artifacts (e.g. data, scientific literature) should be decided upon depending on the scenario.  \nFolder structure and source control expected outcomes  \nDefined folder structure for all users to use, pushed to the repo.  \n.gitignore file determining which folders should be synced with git and which should be kept locally. For example, this one.  \nDetermine how notebooks are stored and versioned (e.g. strip output from Jupyter notebooks)  \nSource control and folder structure benefits  \nCollaboration  \nReproducibility  \nCode quality  \nExperiment tracking  \nExperiment tracking tools allow data scientists and researchers to keep track of previous experiments for better understanding of the experimentation process and for the reproducibility of experiments or models.  \nTypes of experiment tracking frameworks  \nExperiment tracking frameworks differ by the set of features they provide for collecting experiment metadata, and comparing and analyzing experiments. In ISE, we mainly use MLFlow on Databricks or Azure ML Experimentation. Note that some experiment tracking frameworks require a deployment, while others are SaaS.  \nExperiment tracking outcomes  \nDecide on an experiment tracking framework  \nEnsure it is accessible to all users  \nDocument set-up on local environments  \nDefine datasets and evaluation in a way which will allow the comparison of all experiments. Consistency across datasets and evaluation is paramount for experiment comparison.  \nEnsure full reproducibility by assuring that all required details are tracked (i.e. dataset names and versions, parameters, code, environment)  \nExperiment tracking benefits  \nModel performance  \nReproducibility  \nCollaboration  \nCode quality  \nDatasets and models abstractions  \nBy creating abstractions to building blocks (e.g., datasets, models, evaluators),\nwe allow the easy introduction of new logic into the experimentation pipeline while keeping the agreed upon experimentation flow intact.  \nThese abstractions can be created using different mechanisms.\nFor example, we can use Object-Oriented Programming (OOP) solutions like abstract classes:  \nAn example from scikit-learn describing the creation of new estimators compatible with the API.  \nAn example from PyTorch on extending the abstract Dataset class.  \nAbstraction outcomes  \nDifferent building blocks have defined APIs allowing them to be replaced or extended.  \nReplacing building blocks does not break the original experimentation flow.  \nMock building blocks are used for unit tests  \nAPIs/mocks are shared with the engineering teams for integration with other modules.  \nAbstraction benefits  \nCollaboration  \nCode quality  \nReproducibility  \nOperationalization  \nModel performance  \nModel evaluation  \nWhen deciding on the evaluation of the ML model/process, consider the following checklist:  \n[ ] Evaluation logic is approved by all stakeholders.  \n[ ] Relationship between evaluation logic and business KPIs is analyzed and decided.  \n[ ] Evaluation flow is applicable for all present and future models (i.e. does not assume some prediction structure or method-specific process).  \n[ ] Evaluation code is unit-tested and reviewed by all team members.  \n[ ] Evaluation flow facilitates further results and error analysis.  \nEvaluation development process outcomes  \nEvaluation strategy is agreed upon all stakeholders  \nResearch and discussion on various evaluation methods and metrics is documented.  \nThe code holding the logic and data structures for evaluation is reviewed and tested.  \nDocumentation on how to apply evaluation is reviewed.  \nPerformance metrics are automatically tracked into the experiment tracker.  \nEvaluation development process benefits  \nModel performance  \nCode quality  \nCollaboration  \nReproducibility",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\ml-experimentation.md"
    },
    {
        "chunkId": "chunk193_0",
        "chunkContent": "Feasibility Studies  \nThe main goal of feasibility studies is to assess whether it is feasible to solve the problem satisfactorily using ML with the available data. We want to avoid investing too much in the solution before we have:  \nSufficient evidence that a solution would be the best technical solution given the business case  \nSufficient evidence that a solution is compatible with the problem context  \nSufficient evidence that a solution is possible  \nSome vetted direction on what a solution should look like  \nThis effort ensures quality solutions backed by the appropriate, thorough amount of consideration and evidence.  \nWhen are feasibility studies useful?  \nEvery engagement can benefit from a feasibility study early in the project.  \nArchitectural discussions can still occur in parallel as the team works towards gaining a solid understanding and definition of what will be built.  \nFeasibility studies can last between 4-16 weeks, depending on specific problem details, volume of data, state of the data etc. Starting with a 4-week milestone might be useful, during which it can be determined how much more time, if any, is required for completion.  \nWho collaborates on feasibility studies?  \nCollaboration from individuals with diverse skill sets is desired at this stage, including data scientists, data engineers, software engineers, PMs, human experience researchers, and domain experts. It embraces the use of engineering fundamentals, with some flexibility. For example, not all experimentation requires full test coverage and code review. Experimentation is typically not part of a CI/CD pipeline. Artifacts may live in the main branch as a folder excluded from the CI/CD pipeline, or as a separate experimental branch, depending on customer/team preferences.  \nWhat do feasibility studies entail?  \nProblem definition and desired outcome  \nEnsure that the problem is complex enough that coding rules or manual scaling is unrealistic  \nClear definition of the problem from business and technical perspectives  \nDeep contextual understanding  \nConfirm that the following questions can be answered based on what was learned during the Discovery Phase of the project. For items that can not be satisfactorily answered, undertake additional investigation to answer.  \nUnderstanding the people who are using and/or affected by the solution  \nUnderstanding the contextual forces at play around the problem, including goals, culture, and historical context  \nTo accomplish this a researcher will:  \nCollaborate with customers and colleagues to explore the landscape of people who relate to and may be affected by the problem space being explored (Users, stakeholders, subject matter experts, etc)  \nFormulate the research question(s) to be addressed  \nSelect and design research to best serve the research question(s)  \nIdentify and select representative research participants across the problem space with whom to conduct the research  \nConstruct a research plan and necessary preparation documents for the selected research method(s)  \nConduct research activity with the participants via the selected method(s)  \nSynthesize, analyze, and interpret research findings  \nWhere relevant, build frameworks, artifacts and processes that help explore the findings and implications of the research across the team  \nShare what was uncovered and understood, and the implications thereof across the engagement team and relevant stakeholders.  \nIf the above research was conducted during the Discovery phase, it should be reviewed, and any substantial knowledge gaps should be identified and filled by following the above process.  \nData access  \nVerify that the full team has access to the data  \nSet up a dedicated and/or restricted environment if required  \nPerform any required de-identification or redaction of sensitive information  \nUnderstand data access requirements (retention, role-based access, etc.)  \nData discovery  \nHold a data exploration workshop and deep dive with domain experts  \nUnderstand data availability and confirm the team's access  \nUnderstand the data dictionary, if available  \nUnderstand the quality of the data. Is there already a data validation strategy in place?  \nEnsure required data is present in reasonable volumes  \nFor supervised problems (most common), assess the availability of labels or data that can be used to effectively approximate labels  \nIf applicable, ensure all data can be joined as required and understand how  \nIdeally obtain or create an entity relationship diagram (ERD)  \nPotentially uncover new useful data sources  \nArchitecture discovery  \nClear picture of existing architecture  \nInfrastructure spikes  \nConcept ideation and iteration  \nDevelop value proposition(s) for users and stakeholders based on the contextual understanding developed through the discovery process (e.g. key elements of value, benefits)  \nAs relevant, make use of  \nCo-creation with team  \nCo-creation with users and stakeholders  \nAs relevant, create vignettes, narratives or other materials to communicate the concept  \nIdentify the next set of hypotheses or unknowns to be tested (see concept testing)  \nRevisit and iterate on the concept throughout discovery as understanding of the problem space evolves  \nExploratory data analysis (EDA)  \nData deep dive  \nUnderstand feature and label value distributions  \nUnderstand correlations among features and between features and labels  \nUnderstand data specific problem constraints like missing values, categorical cardinality, potential for data leakage etc.  \nIdentify any gaps in data that couldn't be identified in the data discovery phase  \nPave the way of further understanding of what techniques are applicable  \nEstablish a mutual understanding of what data is in or out of scope for feasibility, ensuring that the data in scope is significant for the business  \nData pre-processing  \nHappens during EDA and hypothesis testing  \nFeature engineering  \nSampling  \nScaling and/or discretization  \nNoise handling  \nHypothesis testing  \nDesign several potential solutions using theoretically applicable algorithms and techniques, starting with the simplest reasonable baseline  \nTrain model(s)  \nEvaluate performance and determine if satisfactory  \nTweak experimental solution designs based on outcomes  \nIterate  \nThoroughly document each step and outcome, plus any resulting hypotheses for easy following of the decision-making process  \nConcept testing  \nWhere relevant, to test the value proposition, concepts or aspects of the experience  \nPlan user, stakeholder and expert research  \nDevelop and design necessary research materials  \nSynthesize and evaluate feedback to incorporate into concept development  \nContinue to iterate and test different elements of the concept as necessary, including testing to best serve RAI goals and guidelines  \nEnsure that the proposed solution and framing are compatible with and acceptable to affected people  \nEnsure that the proposed solution and framing is compatible with existing business goals and context  \nRisk assessment  \nIdentification and assessment of risks and constraints  \nResponsible AI  \nConsideration of responsible AI principles  \nUnderstanding of users and stakeholders\u2019 contexts, needs and concerns to inform development of RAI  \nTesting AI concept and experience elements with users and stakeholders  \nDiscussion and feedback from diverse perspectives around any responsible AI concerns  \nOutput of a feasibility study  \nPossible outcomes  \nThe main outcome is a feasibility study report, with a recommendation on next steps:\n* If there is not enough evidence to support the hypothesis that this problem can be solved using ML, as aligned with the pre-determined performance measures and business impact:  \nWe detail the gaps and challenges that prevented us from reaching a positive outcome  \nWe may scope down the project, if applicable  \nWe may look at re-scoping the problem taking into account the findings of the feasibility study  \nWe assess the possibility to collect more data or improve data quality  \nIf there is enough evidence to support the hypothesis that this problem can be solved using ML  \nProvide recommendations and technical assets for moving to the operationalization phase",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\ml-feasibility-study.md"
    },
    {
        "chunkId": "chunk194_0",
        "chunkContent": "ML Fundamentals Checklist  \nThis checklist helps ensure that our ML projects meet our ML Fundamentals. The items below are not sequential, but rather organized by different parts of an ML project.  \nData Quality and Governance  \n[ ] There is access to data.  \n[ ] Labels exist for dataset of interest.  \n[ ] Data quality evaluation.  \n[ ] Able to track data lineage.  \n[ ] Understanding of where the data is coming from and any policies related to data access.  \n[ ] Gather Security and Compliance requirements.  \nFeasibility Study  \n[ ] A feasibility study was performed to assess if the data supports the proposed tasks.  \n[ ] Rigorous Exploratory data analysis was performed (including analysis of data distribution).  \n[ ] Hypotheses were tested producing sufficient evidence to either support or reject that an ML approach is feasible to solve the problem.  \n[ ] ROI estimation and risk analysis was performed for the project.  \n[ ] ML outputs/assets can be integrated within the production system.  \n[ ] Recommendations on how to proceed have been documented.  \nEvaluation and Metrics  \n[ ] Clear definition of how performance will be measured.  \n[ ] The evaluation metrics are somewhat connected to the success criteria.  \n[ ] The metrics can be calculated with the datasets available.  \n[ ] Evaluation flow can be applied to all versions of the model.  \n[ ] Evaluation code is unit-tested and reviewed by all team members.  \n[ ] Evaluation flow facilitates further results and error analysis.  \nModel Baseline  \n[ ] Well-defined baseline model exists and its performance is calculated. (More details on well defined baselines)  \n[ ] The performance of other ML models can be compared with the model baseline.  \nExperimentation setup  \n[ ] Well-defined train/test dataset with labels.  \n[ ] Reproducible and logged experiments in an environment accessible by all data scientists to quickly iterate.  \n[ ] Defined experiments/hypothesis to test.  \n[ ] Results of experiments are documented.  \n[ ] Model hyper parameters are tuned systematically.  \n[ ] Same performance evaluation metrics and consistent datasets are used when comparing candidate models.  \nProduction  \n[ ] Model readiness checklist reviewed.  \n[ ] Model reviews were performed (covering model debugging, reviews of training and evaluation approaches, model performance).  \n[ ] Data pipeline for inferencing, including an end-to-end tests.  \n[ ] SLAs requirements for models are gathered and documented.  \n[ ] Monitoring of data feeds and model output.  \n[ ] Ensure consistent schema is used across the system with expected input/output defined for each component of the pipelines (data processing as well as models).  \n[ ] Responsible AI reviewed.",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\ml-fundamentals-checklist.md"
    },
    {
        "chunkId": "chunk195_0",
        "chunkContent": "ML model production checklist  \nThe purpose of this checklist is to make sure that:  \nThe team assessed if the model is ready for production before moving to the scoring process  \nThe team has prepared a production plan for the model  \nThe checklist provides guidelines for creating this production plan. It should be used by teams/organizations that already built/trained an ML model and are now considering putting it into production.  \nChecklist  \nBefore putting an individual ML model into production, the following aspects should be considered:  \n[ ] Is there a well defined baseline? Is the model performing better than the baseline?  \n[ ] Are machine learning performance metrics defined for both training and scoring?  \n[ ] Is the model benchmarked?  \n[ ] Can ground truth be obtained or inferred in production?  \n[ ] Has the data distribution of training, testing and validation sets been analyzed?  \n[ ] Have goals and hard limits for performance, speed of prediction and costs been established so they can be considered if trade-offs need to be made?  \n[ ] How will the model be integrated into other systems, and what impact will it have?  \n[ ] How will incoming data quality be monitored?  \n[ ] How will drift in data characteristics be monitored?  \n[ ] How will performance be monitored?  \n[ ] Have any ethical concerns been taken into account?  \nPlease note that there might be scenarios where it is not possible to check all the items on this checklist. However, it is advised to go through all items and make informed decisions based on your specific use case.  \nWill your model performance be different in production than during training phase  \nOnce deployed into production, the model might be performing much worse than expected. This poor performance could be a result of:  \nThe data to be scored in production is significantly different from the train and test datasets  \nThe feature engineering steps are different or inconsistent in production compared to the training process  \nThe performance measure is not consistent (for example your test set covers several months of data where the performance metric for production has been calculated for one month of data)  \nIs there a well-defined baseline? Is the model performing better than the baseline?  \nA good way to think of a model baseline is the simplest model one can come up with: either a simple threshold, a random guess or a very basic linear model. This baseline is the reference point your model needs to outperform. A well-defined baseline is different for each problem type and there is no one size fits all approach.  \nAs an example, let's consider some common types of machine learning problems:  \nClassification: Predicting between a positive and a negative class. Either the class with the most observations or a simple logistic regression model can be the baseline.  \nRegression: Predicting the house prices in a city. The average house price for the last year or last month, a simple linear regression model, or the previous median house price in a neighborhood could be the baseline.  \nImage classification: Building an image classifier to distinguish between cats and no cats in an image. If your classes are unbalanced: 70% cats and 30% no cats and if you always predict cats, your naive classifier has 70% accuracy and this can be your baseline. If your classes are balanced: 52% cats and 48% no cats, then a simple convolutional architecture can be the baseline (1 conv layer + 1 max pooling + 1 dense). Additionally, human accuracy at labelling can also be the baseline in an image classification scenario.  \nSome questions to ask when comparing to a baseline:  \nHow does your model compare to a random guess?  \nHow does your model performance compare to applying a simple threshold?  \nHow does your model compare with always predicting the most common value?  \nNote: In some cases, human parity might be too ambitious as a baseline, but this should be decided on a case by case basis. Human accuracy is one of the available options, but not the only one.  \nResources:  \n\"How To Get Baseline Results And Why They Matter\" article  \n\"Always start with a stupid model, no exceptions.\" article  \nAre machine learning performance metrics defined for both training and scoring?  \nThe methodology of translating the training metrics to scoring metrics should be well-defined and understood. Depending on the data type and model, the model metrics calculation might differ in production and in training. For example, the training procedure calculated metrics for a long period of time (a year, a decade) with different seasonal characteristics while the scoring procedure will calculate the metrics per a restricted time interval (for example a week, a month, a quarter). Well-defined ML performance metrics are essential in production so that a decrease or increase in model performance can be accurately detected.  \nThings to consider:  \nIn forecasting, if you change the period of assessing the performance, from one month to a year for example, then you might get a different result. For example, if your model is predicting sales of a product per day and the RMSE (Root Mean Squared Error) is very low for the first month the model is in production. As the model is live for longer, the RMSE is increasing, becoming 10x the RMSE for the first year compared to the first month.  \nIn a classification scenario, the overall accuracy is good, but the model is performing poorly for some subgroups. For example, a classifier has an accuracy of 80% overall, but only 55% for the 20-30 age group. If this is a significant age group for the production data, then your accuracy might suffer greatly when in production.  \nIn scene classification scenario, the model is trying to identify a specific scene in a video, and the model has been trained and tested (80-20 split) on 50000 segments where half are segments containing the scene and half of the segments do not contain the scene. The accuracy on the training set is 85% and 84% on the test set. However, when an entire video is scored, scores are obtained on all segments, and we expect few segments to contain the scene. The accuracy for an entire video is not comparable with the training/test set procedure in this case, hence different metrics should be considered.  \nIf sampling techniques (over-sampling, under-sampling) are used to train model when classes are imbalanced, ensure the metrics used during training are comparable with the ones used in scoring.  \nIf the number of samples used for training and testing is small, the performance metrics might change significantly as new data is scored.  \nIs the model benchmarked?  \nThe trained model to be put into production is well benchmarked if machine learning performance metrics (such as accuracy, recall, RMSE or whatever is appropriate) are measured on the train and test set. Furthermore, the train and test set split should be well documented and reproducible.  \nCan ground truth be obtained or inferred in production?  \nWithout a reliable ground truth, the machine learning metrics cannot be calculated. It is important to identify if the ground truth can be obtained as the model is scoring new data by either manual or automatic means. If the ground truth cannot be obtained systematically, other proxies and methodology should be investigated in order to obtain some measure of model performance.  \nOne option is to use humans to manually label samples. One important aspect of human labelling is to take into account the human accuracy. If there are two different individuals labelling an image, the labels will likely be different for some samples. It is important to understand how the labels were obtained to assess the reliability of the ground truth (that is why we talk about human accuracy).  \nFor clarity, let's consider the following examples (by no means an exhaustive list):  \nForecasting: Forecasting scenarios are an example of machine learning problems where the ground truth could be obtained in most cases even though a delay might occur. For example, for a model predicting the sales of ice cream in a local shop, the ground truth will be obtained as the sales are happening, but it might appear in the system at a later time than as the model prediction.  \nRecommender systems: For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.  \nObject detection in images: For an object detection model, as new images are scored, there are no new labels being generated automatically. One option to obtain the ground truth for the new images is to use people to manually label the images. Human labelling is costly, time-consuming and not 100% accurate, so in most cases, only a subset of images can be labelled. These samples can be chosen at random or by using active learning techniques of selecting the most informative unlabeled samples.  \nHas the data distribution of training, testing and validation sets been analyzed?  \nThe data distribution of your training, test and validation (if applicable) dataset (including labels) should be analyzed to ensure they all come from the same distribution. If this is not the case, some options to consider are: re-shuffling,  re-sampling, modifying the data, more samples need to be gathered or features removed from the dataset.  \nSignificant differences in the data distributions of the different datasets can greatly impact the performance of the model. Some potential questions to ask:  \nHow much does the training and test data represent the end result?  \nIs the distribution of each individual feature consistent across all your datasets? (i.e. same representation of age groups, gender, race etc.)  \nIs there any data lineage information? Where did the data come from? How was the data collected? Can collection and labelling be automated?  \nResources:  \n\"Splitting into train, dev and test\" tutorial  \nHave goals and hard limits for performance, speed of prediction and costs been established, so they can be considered if trade-offs need to be made?  \nSome machine learning models achieve high ML performance, but they are costly and time-consuming to run. In those cases, a less performant and cheaper model could be preferred. Hence, it is important to calculate the model performance metrics (accuracy, precision, recall, RMSE etc), but also to gather data on how expensive it will be to run the model and how long it will take to run. Once this data is gathered, an informed decision should be made on what model to productionize.  \nSystem metrics to consider:  \nCPU/GPU/memory usage  \nCost per prediction  \nTime taken to make a prediction  \nHow will the model be integrated into other systems, and what impact will it have?  \nMachine Learning models do not exist in isolation, but rather they are part of a much larger system. These systems could be old, proprietary systems or new systems being developed as a results of the creation a new machine learning model. In both of those cases, it is important to understand where the actual model is going to fit in, what output is expected from the model and how that output is going to be used by the larger system. Additionally, it is essential to decide if the model will be used for batch and/or real-time inference as production paths might differ.  \nPossible questions to assess model impact:  \nIs there a human in the loop?  \nHow is feedback collected through the system? (for example how do we know if a prediction is wrong)  \nIs there a fallback mechanism when things go wrong?  \nIs the system transparent that there is a model making a prediction and what data is used to make this prediction?  \nWhat is the cost of a wrong prediction?  \nHow will incoming data quality be monitored?  \nAs data systems become increasingly complex in the mainstream, it is especially vital to employ data quality monitoring, alerting and rectification protocols. Following data validation best practices can prevent insidious issues from creeping into machine learning models that, at best, reduce the usefulness of the model, and at worst, introduce harm. Data validation, reduces the risk of data downtime (increasing headroom) and technical debt and supports long-term success of machine learning models and other applications that rely on the data.  \nData validation best practices include:  \nEmploying automated data quality testing processes at each stage of the data pipeline  \nRe-routing data that fails quality tests to a separate data store for diagnosis and resolution  \nEmploying end-to-end data observability on data freshness, distribution, volume, schema and lineage  \nNote that data validation is distinct from data drift detection. Data validation detects errors in the data (ex. a datum is outside of the expected range), while data drift detection uncovers legitimate changes in the data that are truly representative of the phenomenon being modeled (ex. user preferences change). Data validation issues should trigger re-routing and rectification, while data drift should trigger adaptation or retraining of a model.  \nResources:  \n\"Data Quality Fundamentals\" by Moses et al.  \nHow will drift in data characteristics be monitored?  \nData drift detection uncovers legitimate changes in incoming data that are truly representative of the phenomenon being modeled,and are not erroneous (ex. user preferences change). It is imperative to understand if the new data in production will be significantly different from the data in the training phase. It is also important to check that the data distribution information can be obtained for any of the new data coming in. Drift monitoring can inform when changes are occurring and what their characteristics are (ex. abrupt vs gradual) and guide effective adaptation or retraining strategies to maintain performance.  \nPossible questions to ask:  \nWhat are some examples of drift, or deviation from the norm, that have been experience in the past or that might be expected?  \nIs there a drift detection strategy in place? Does it align with expected types of changes?  \nAre there warnings when anomalies in input data are occurring?  \nIs there an adaptation strategy in place? Does it align with expected types of changes?  \nResources:  \n\"Learning Under Concept Drift: A Review\" by Lu at al.  \nUnderstanding dataset shift  \nHow will performance be monitored?  \nIt is important to define how the model will be monitored when it is in production and how that data is going to be used to make decisions. For example, when will a model need retraining as the performance has degraded and how to identify what are the underlying causes of this degradation could be part of this monitoring methodology.  \nIdeally, model monitoring should be done automatically. However, if this is not possible, then there should be a manual periodical check of the model performance.  \nModel monitoring should lead to:  \nAbility to identify changes in model performance  \nWarnings when anomalies in model output are occurring  \nRetraining decisions and adaptation strategy  \nHave any ethical concerns been taken into account?  \nEvery ML project goes through the Responsible AI process to ensure that it upholds Microsoft's 6 Responsible AI principles.",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\ml-model-checklist.md"
    },
    {
        "chunkId": "chunk196_0",
        "chunkContent": "Envisioning and Problem Formulation  \nBefore beginning a data science investigation, we need to define a problem statement which the data science team can explore; this problem statement can have a significant influence on whether the project is likely to be successful.  \nEnvisioning goals  \nThe main goals of the envisioning process are:  \nEstablish a clear understanding of the problem domain and the underlying business objective  \nDefine how a potential solution would be used and how its performance should be measured  \nDetermine what data is available to solve the problem  \nUnderstand the capabilities and working practices of the data science team  \nEnsure all parties have the same understanding of the scope and next steps (e.g., onboarding, data exploration workshop)  \nThe envisioning process usually entails a series of 'envisioning' sessions where the data science team work alongside subject-matter experts to formulate the problem in such a way that there is a shared understanding a shared understanding of the problem domain, a clear goal, and a predefined approach to evaluating a potential solution.  \nUnderstanding the problem domain  \nGenerally, before defining a project scope for a data science investigation, we must first understand the problem domain:  \nWhat is the problem?  \nWhy does the problem need to be solved?  \nDoes this problem require a machine learning solution?  \nHow would a potential solution be used?  \nHowever, establishing this understanding can prove difficult, especially for those unfamiliar with the problem domain. To ease this process, we can approach problems in a structured way by taking the following steps:  \nIdentify a measurable problem and define this in business terms. The objective should be clear, and we should have a good understanding of the factors that we can control - that can be used as inputs - and how they affect the objective. Be as specific as possible.  \nDecide how the performance of a solution should be measured and identify whether this is possible within the restrictions of this problem. Make sure it aligns with the business objective and that you have identified the data required to evaluate the solution. Note that the data required to evaluate a solution may differ from the data needed to create a solution.  \nThinking about the solution as a black box, detail the function that a solution to this problem should perform to fulfil the objective and verify that the relevant data is available to solve the problem.  \nOne way of approaching this is by thinking about how a subject-matter expert could solve the problem manually, and the data that would be required; if a human subject-matter expert is unable to solve the problem given the available data, this is indicative that additional information is required and/or more data needs to be collected.  \nBased on the available data, define specific hypothesis statements - which can be proved\nor disproved - to guide the exploration of the data science team. Where possible, each hypothesis statement should have a clearly defined success criteria (e.g., with an accuracy of over 60%), however, this is not always possible - especially for projects where no solution to the problem currently exists. In these cases, the measure of success could be based on a subject-matter expert verifying that the results meet their expectations.  \nDocument all the above information, to ensure alignment between stakeholders and establish a clear understanding of the problem to be solved. Try to ensure that as much relevant domain knowledge is captured as possible, and that the features present in available data - and the way that the data was collected - are clearly explained, such that they can be understood by a non-subject matter expert.  \nOnce an understanding of the problem domain has been established, it may be necessary to break down the overall problem into smaller, meaningful chunks of work to maintain team focus and ensure a realistic project scope within the given time frame.  \nListening to the end user  \nThese problems are complex and require understanding from a variety of perspectives. It is not uncommon for the stakeholders to not be the end user of the solution framework. In these cases, listening to the actual end users is critical to the success of the project.  \nThe following questions can help guide discussion in understanding the stakeholders' perspectives:  \nWho is the end user?  \nWhat is the current practice related to the business problem?  \nWhat's the performance of the current solution?  \nWhat are their pain points?  \nWhat is their toughest problem?  \nWhat is the state of the data used to build the solution?  \nHow does the end user or SME envision the solution?  \nEnvisioning Guidance  \nDuring envisioning sessions, the following may prove useful for guiding the discussion. Many of these points are taken directly, or adapted from, [1] and [2].  \nProblem Framing  \nDefine the objective in business terms.  \nHow will the solution be used?  \nWhat are the current solutions/workarounds (if any)? What work has been done in this area so far? Does this solution need to fit into an existing system?  \nHow should performance be measured?  \nIs the performance measure aligned with the business objective?  \nWhat would be the minimum performance needed to reach the business objective?  \nAre there any known constraints around non-functional requirements that would have to be taken into account? (e.g., computation times)  \nFrame this problem (supervised/unsupervised, online/offline, etc.)  \nIs human expertise available?  \nHow would you solve the problem manually?  \nAre there any restrictions on the type of approaches which can be used? (e.g., does the solution need to be completely explainable?)  \nList the assumptions you or others have made so far. Verify these assumptions if possible.  \nDefine some initial hypothesis statements to be explored.  \nHighlight and discuss any responsible AI concerns if appropriate.  \nWorkflow  \nWhat data science skills exist in the organization?  \nHow many data scientists/engineers would be available to work on this project? In what capacity would these resources be available (full-time, part-time, etc.)?  \nWhat does the team's current workflow practices look like? Do they work on the cloud/on-prem? In notebooks/IDE? Is version control used?  \nHow are data, experiments and models currently tracked?  \nDoes the team employ an Agile methodology? How is work tracked?  \nAre there any ML solutions currently running in production? Who is responsible for maintaining these solutions?  \nWho would be responsible for maintaining a solution produced during this project?  \nAre there any restrictions on tooling that must/cannot be used?  \nExample - a recommendation engine problem  \nTo illustrate how the above process can be applied to a tangible problem domain, as an example, consider that we are looking at implementing a recommendation engine for a clothing retailer. This example was, in part, inspired by [3].  \nOften, the objective may be simply presented, in a form such as \"to improve sales\". However, whilst this is ultimately the main goal, we would benefit from being more specific here. Suppose that we were to deploy a solution in November and then observed a December sales surge; how would we be able to distinguish how much of this was as a result of the new recommendation engine, as opposed to the fact that December is a peak buying season?  \nA better objective, in this case, would be \"to drive additional sales by presenting the customer with items that they would not otherwise have purchased without the recommendation\". Here, the inputs that we can control are the choice of items that are presented to each customer, and the order in which they are displayed; considering factors such as how frequently these should change, seasonality, etc.  \nThe data required to evaluate a potential solution in this case would be which recommendations resulted in new sales, and an estimation of a customer's likeliness to purchase a specific item without a recommendation. Note that, whilst this data could also be used to build a recommendation engine, it is unlikely that this data will be available before a recommendation system has been implemented, so it is likely that we will have to use an alternate data source to build the model.  \nWe can get an initial idea of how to approach a solution to this problem by considering how it would be solved by a subject-matter expert. Thinking of how a personal stylist may provide a recommendation, they are likely to recommend items based on one or more of the following:  \ngenerally popular items  \nitems similar to those liked/purchased by the customer  \nitems that were liked/purchased by similar customers  \nitems which are complementary to those owned by the customer  \nWhilst this list is by no means exhaustive, it provides a good indication of the data that is likely to be useful to us:  \nitem sales data  \ncustomer purchase histories  \ncustomer demographics  \nitem descriptions and tags  \nprevious outfits, or sets, which have been curated by the stylist  \nWe would then be able to use this data to explore:  \na method of measuring similarity between items  \na method of measuring similarity between customers  \na method of measuring how complementary items are relative to one another  \nwhich can be used to create and rank recommendations. Depending on the project scope, and available data, one or more of these areas could be selected to create hypotheses to be explored by the data science team. Some examples of such hypothesis statements could be:  \nFrom the descriptions of each item, we can determine a measure of similarity between different items to a degree of accuracy which is specified by a stylist.  \nBased on the behavior of customers with similar purchasing histories, we are able to predict certain items that a customer is likely to purchase; with a certainty which is greater than random choice.  \nUsing sets of items which have previously been sold together, we can formulate rules around the features which determine whether items are complementary or not which can be verified by a stylist.  \nNext Steps  \nTo ensure clarity and alignment, it is useful to summarize the envisioning stage findings focusing on proposed detailed scenarios, assumptions and agreed decisions as well next steps.  \nWe suggest confirming that you have access to all necessary resources (including data) as a next step before proceeding with data exploration workshops.  \nBelow are the links to the exit document template and to some questions which may be helpful in confirming resource access.  \nSummary of Scope Exit Document Template  \nList of Resource Access Questions  \nList of Data Exploration Workshop Questions  \nReferences  \nMany of the ideas presented here - and much more - were inspired by, and can be found in the following resources; all of which are highly recommended.  \nAur\u00e9lien G\u00e9ron's Machine learning project checklist  \nFast.ai's Data project checklist  \nDesigning great data products. Jeremy Howard, Margit Zwemer and Mike Loukides",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\ml-problem-formulation-envisioning.md"
    },
    {
        "chunkId": "chunk197_0",
        "chunkContent": "Profiling Machine Learning and MLOps Code  \nData Science projects, especially the ones that involve Deep Learning techniques, usually are resource intensive. One model training iteration might be multiple hours long. Although large data volumes processing genuinely takes time, minor bugs and suboptimal implementation of some functional pieces might cause extra resources consumption.  \nProfiling can be used to identify performance bottlenecks and see which functions are the costliest in the application code. Based on the outputs of the profiler, one can focus on largest and easiest-to-resolve inefficiencies and therefore achieve better code performance.\nAlthough profiling follows the same principles of any other software project, the purpose of this document is to provide profiling samples for the most common scenarios in MLOps/Data Science projects.  \nBelow are some common scenarios in MLOps/Data Science projects, along with suggestions on how to profile them.  \nGeneric Python profiling  \nPyTorch model training profiling  \nAzure Machine Learning pipeline profiling  \nGeneric Python profiling  \nUsually an MLOps/Data Science solution contains plain Python code serving different purposes (e.g. data processing) along\nwith specialized model training code. Although many Machine Learning frameworks provide their own profiler,\nsometimes it is also useful to profile the whole solution.  \nThere are two types of profilers: deterministic (all events are tracked, e.g. cProfile) and statistical (sampling with regular intervals, e.g., py-spy). The sample below shows an example of a deterministic profiler.  \nThere are many options of generic deterministic Python code profiling. One of the default options for profiling used to be a built-in\ncProfile profiler. Using cProfile one can easily profile\neither a Python script or just a chunk of code. This profiling tool produces a file that can be either\nvisualized using open source tools or analyzed using stats.Stats class. The latter option requires setting up filtering\nand sorting parameters for better analysis experience.  \nBelow you can find an example of using cProfile to profile a chunk of code.  \n{% raw %}  \n```python\nimport cProfile\n\nStart profiling\n\nprofiler = cProfile.Profile()\nprofiler.enable()\n\n-- YOUR CODE GOES HERE ---\n\nStop profiling\n\nprofiler.disable()\n\nWrite profiler results to an html file\n\nprofiler.dump_stats(\"profiler_results.prof\")\n```  \n{% endraw %}  \nYou can also run cProfile outside of the Python script using the following command:  \n{% raw %}  \nbash\npython -m cProfile [-o output_file] [-s sort_order] (-m module | myscript.py)  \n{% endraw %}  \nNote: one epoch of model training is usually enough for profiling. There's no need to run more epochs and produce\nadditional cost.  \nRefer to The Python Profilers for further details.  \nPyTorch model training profiling  \nPyTorch 1.8 includes an updated PyTorch\nprofiler\nthat is supplied together with the PyTorch distribution and doesn't require any additional installation.\nUsing PyTorch profiler one can record CPU side operations as well as CUDA kernel launches on GPU side.\nThe profiler can visualize analysis results using TensorBoard plugin as well as provide suggestions\non bottlenecks and potential code improvements.  \n{% raw %}  \npython\nwith torch.profiler.profile(",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\ml-profiling.md"
    },
    {
        "chunkId": "chunk197_1",
        "chunkContent": "# Limit number of training steps included in profiling\nschedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\ml-profiling.md"
    },
    {
        "chunkId": "chunk197_2",
        "chunkContent": "# Automatically saves profiling results to disk\non_trace_ready=torch.profiler.tensorboard_trace_handler,\nwith_stack=True\n) as profiler:\nfor step, data in enumerate(trainloader, 0):",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\ml-profiling.md"
    },
    {
        "chunkId": "chunk197_3",
        "chunkContent": "# -- TRAINING STEP CODE GOES HERE ---\nprofiler.step()  \n{% endraw %}  \nThe tensorboard_trace_handler can be used to generate result files for TensorBoard. Those can be visualized by installing TensorBoard.\nplugin and running TensorBoard on your log directory.  \n{% raw %}  \n```bash\npip install torch_tb_profiler\ntensorboard --logdir=\n\nNavigate to http://localhost:6006/#pytorch_profiler\n\n```  \n{% endraw %}  \nNote: make sure to provide the right parameters to the torch.profiler.schedule. Usually you would need several steps of training to be profiled rather than the whole epoch.  \nMore information on PyTorch profiler:  \nPyTorch Profiler Recipe  \nIntroducing PyTorch Profiler - the new and improved performance tool  \nAzure Machine Learning pipeline profiling  \nIn our projects we often use Azure Machine Learning\npipelines to train Machine Learning models. Most of the profilers can also be used in conjunction with Azure Machine Learning.\nFor a profiler to be used with Azure Machine Learning, it should meet the following criteria:  \nTurning the profiler on/off can be achieved by passing a parameter to the script ran by Azure Machine Learning  \nThe profiler produces a file as an output  \nIn general, a recipe for using profilers with Azure Machine Learning is the following:  \n(Optional) If you're using profiling with an Azure Machine Learning pipeline, you might want to add --profile\nBoolean flag as a pipeline parameter  \nUse one of the profilers described above or any other profiler that can produce a file as an output  \nInside of your Python script, create step output folder, e.g.:\n{% raw %}\npython\noutput_dir = \"./outputs/profiler_results\"\nos.makedirs(output_dir, exist_ok=True)\n{% endraw %}  \nRun your training pipeline  \nOnce the pipeline is completed, navigate to Azure ML portal and open details of the step that contains training code.\nThe results can be found in the Outputs+logs tab, under outputs/profiler_results folder.  \nYou might want to download the results and visualize it locally.  \nNote: it's not recommended to run profilers simultaneously. Profiles also consume resources, therefore a simultaneous run\nmight significantly affect the results.",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\ml-profiling.md"
    },
    {
        "chunkId": "chunk198_0",
        "chunkContent": "Agile Development Considerations for ML Projects  \nOverview  \nWhen running ML projects, we follow the Agile methodology for software development with some adaptations, as we acknowledge that research and experimentation are sometimes difficult to plan and estimate.  \nGoals  \nRun and manage ML projects effectively  \nCreate effective collaboration between the ML team and the other teams working on the project  \nTo learn more about how ISE runs the Agile process for software development teams, refer to this doc.  \nWithin this framework, the team follows these Agile ceremonies:  \nBacklog management  \nRetrospectives  \nScrum of Scrums (where applicable)  \nSprint planning  \nStand-ups  \nWorking agreement  \nNotes on Agile process during exploration and experimentation  \nWhile acknowledging the fact that ML user stories and research spikes are less predictable than software development ones, we strive to have a deliverable for every user story in every sprint.  \nUser stories and spikes are usually estimated using T-shirt sizes or similar, and not in actual days/hours. See more here on story estimation.  \nML design sessions should be included in each sprint.  \nExamples of ML deliverables for each sprint  \nWorking code (e.g. models, pipelines, exploratory code)  \nDocumentation of new hypotheses, and the acceptance or rejection of previous hypotheses as part of a Hypothesis Driven Analysis (HDA). For more information see Hypothesis Driven Development on Barry Oreilly's website  \nExploratory Data Analysis (EDA) results and learnings documented  \nNotes on collaboration between ML team and software development team  \nThe ML and Software Development teams work together on the project. The team uses one backlog and attend the same Agile ceremonies. In cases where the project has many participants, we will divide into working groups, but still have the entire team join the Agile ceremonies.  \nIf possible, feasibility study and initial model experimentation takes place before the operationalization work kicks off.  \nThe ML team and dev team both share the accountability for the MLOps solution.  \nThe ML model interface (API) is determined as early as possible, to allow the developers to consider its integration into the production pipeline.  \nMLOps artifacts are developed with a continuous collaboration and review of the ML team, to ensure the appropriate approaches for experimentation and\nproductization are used.  \nRetrospectives and sprint planning are performed on the entire team level, and not the specific work groups level.",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\ml-project-management.md"
    },
    {
        "chunkId": "chunk199_0",
        "chunkContent": "Proposed ML Process  \nIntroduction  \nThe objective of this document is to provide guidance to produce machine learning (ML) applications that are based on code, data and models that can be reproduced and reliably released to production environments.\nWhen developing ML applications, we consider the following approaches:  \nBest practices in ML engineering:  \nThe ML application development should use engineering fundamentals to ensure high quality software deliverables.  \nThe ML application should be reliability released into production, leveraging automation as much as possible.  \nThe ML application can be deployed into production at any time. This makes the decision about when to release it a business decision rather than a technical one.  \nBest practices in ML research:  \nAll artifacts, specifically data, code and ML models, should be versioned and managed using standard tools and workflows, in order to facilitate continuous research and development.  \nWhile the model outputs can be non-deterministic and hard to reproduce, the process of releasing ML software into production should be reproducible.  \nResponsible AI aspects are carefully analyzed and addressed.  \nCross-functional team:  \nA cross-functional team consisting of different skill sets in data science, data engineering, development, operations, and industry domain specialists is required.  \nML process  \nThe proposed ML development process consists of:  \nData and problem understanding  \nResponsible AI assessment  \nFeasibility study  \nBaseline model experimentation  \nModel evaluation and experimentation  \nModel operationalization\nUnit and Integration testing\nDeployment\nMonitoring and Observability  \nVersion control  \nDuring all stages of the process, it is suggested that artifacts should be version-controlled. Typically, the process is iterative and versioned artifacts can assist in traceability and reviewing. See more here.  \nUnderstanding the problem  \nDefine the business problem for the ML project:  \nAgree on the success criteria with the customer.  \nIdentify potential data sources and determine the availability of these sources.  \nDefine performance evaluation metrics on ground truth data  \nConduct a Responsible AI assessment to ensure development and deployment of the ML solution in a responsible manner.  \nConduct a feasibility study to assess whether the business problem is feasible to solve satisfactorily using ML with the available data. The objective of the feasibility study is to mitigate potential over-investment by ensuring sufficient evidence that ML is possible and would be the best solution. The study also provides initial indications of what the ML solution should look like. This ensures quality solutions supported by thorough consideration and evidence. Refer to feasibility study.  \nExploratory data analysis is performed and discussed with the team  \nTypical output:  \nData exploration source code (Jupyter notebooks/scripts) and slides/docs  \nInitial ML model code (Jupyter notebook or scripts)  \nInitial solution architecture with initial data engineering requirements  \nData dictionary (if not yet available)  \nList of assumptions  \nBaseline Model Experimentation  \nData preparation: creating data source connectors, determining storage services to be used and potential versioning of raw datasets.  \nFeature engineering: create new features from raw source data to increase the predictive power of the learning algorithm. The features should capture additional information that is not apparent in the original feature set.  \nSplit data into training, validation and test sets: creating training, validation, and test datasets with ground truth to develop ML models. This would entail joining or merging various feature engineered datasets. The training dataset is used to train the model to find the patterns between its features and labels (ground truth). The validation dataset is used to assess the model architecture, and the test data is used to confirm the prediction quality of the model.  \nInitial code to create access data sources, transform raw data into features and model training as well as scoring.  \nDuring this phase, experiment code (Jupyter notebooks or scripts) and accompanying utility code should be version-controlled using tools such as ADO (Azure DevOps).  \nTypical output: Rough Jupyter notebooks or scripts in Python or R, initial results from baseline model.  \nFor more information on experimentation, refer to the experimentation section.  \nModel Evaluation  \nCompare the effectiveness of different algorithms on the given problem.  \nTypical output:  \nEvaluation flow is fully set up.  \nReproducible experiments for the different approaches experimented with.  \nModel Operationalization  \nTaking \"experimental\" code and preparing it, so it can be deployed. This includes data pre-processing, featurization code, training model code (if required to be trained using CI/CD) and model inference code.  \nTypical output:  \nProduction-grade code (Preferably in the form of a package) for:\nData preprocessing / post processing\nServing a model\nTraining a model  \nCI/CD scripts.  \nReproducibility steps for the model in production.  \nSee more here.  \nUnit and Integration Testing  \nEnsuring that production code behaves in the way we expect it to, and that its results match those we saw during the Model Evaluation and Experimentation phases.  \nRefer to ML testing post for further details.  \nTypical output: Test suite with unit and end-to-end tests is created and completes successfully.  \nDeployment  \nResponsible AI considerations such as bias and fairness analysis. Additionally, explainability/interpretability of the model should also be considered.  \nIt is recommended for a human-in-the-loop to verify the model and manually approve deployment to production.  \nGetting the model into production where it can start adding value by serving predictions. Typical artifacts are APIs for accessing the model and integrating the model to the solution architecture.  \nAdditionally, certain scenarios may require training the model periodically in production.  \nReproducibility steps of the production model are available.  \nTypical output: model readiness checklist is completed.  \nMonitoring and Observability  \nThis is the final phase, where we ensure our model is doing what we expect it to in production.  \nRead more about ML observability.  \nRead more about Azure ML's offerings around ML models production monitoring.  \nIt is recommended to consider incorporating data drift monitoring process in the production solution. This will assist in detecting potential changes in new datasets presented for inference that may significantly impact model performance. For more info on detecting data drift with Azure ML see the Microsoft docs article on how to monitor datasets.  \nTypical output: Logging and monitoring scripts and tools set up, permissions for users to access monitoring tools.",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\ml-proposed-process.md"
    },
    {
        "chunkId": "chunk200_0",
        "chunkContent": "Testing Data Science and MLOps Code  \nThe purpose of this document is to provide samples of tests for the most common operations in MLOps/Data Science projects. Testing the code used for MLOps or data science projects follows the same principles of any other software project.  \nSome scenarios might seem different or more difficult to test. The best way to approach this is to always have a test design session, where the focus is on the input/outputs, exceptions and testing the behavior of data transformations. Designing the tests first makes it easier to test as it forces a more modular style, where each function has one purpose, and extracting common functionality functions and modules.  \nBelow are some common operations in MLOps or Data Science projects, along with suggestions on how to test them.  \nSaving and loading data  \nTransforming data  \nModel load or predict  \nData validation  \nModel testing  \nSaving and loading data  \nReading and writing to csv, reading images or loading audio files are common scenarios encountered in MLOps projects.  \nExample: Verify that a load function calls read_csv if the file exists  \nutils.py  \n{% raw %}  \npython\ndef load_data(filename: str) -> pd.DataFrame:\nif os.path.isfile(filename):\ndf = pd.read_csv(filename, index_col='ID')\nreturn df\nreturn None  \n{% endraw %}  \nThere's no need to test the read_csv function, or the isfile functions, we can leave testing them to the pandas and os developers.  \nThe only thing we need to test here is the logic in this function, i.e. that load_data loads the file if the file exists with the right index column, and doesn't load the file if it doesn't exist, and that it returns the expected results.  \nOne way to do this would be to provide a sample file and call the function, and verify that the output is None or a DataFrame. This requires separate files to be present, or not present, for the tests to run. This can cause the same test to run on one machine and then fail on a build server which is not a desired behavior.  \nA much better way is to mock calls to isfile, and read_csv. Instead of calling the real function, we will return a predefined return value, or call a stub that doesn't have any side effects. This way no files are needed in the repository to execute the test, and the test will always work the same, independent of what machine it runs on.  \nNote: Below we mock the specific os and pd functions referenced in the utils file, any others are left unaffected and would run as normal.  \ntest_utils.py  \n{% raw %}  \n```python\nimport utils\nfrom mock import patch\n\n@patch('utils.os.path.isfile')\n@patch('utils.pd.read_csv')\ndef test_load_data_calls_read_csv_if_exists(mock_isfile, mock_read_csv):\n# arrange\n# always return true for isfile\nutils.os.path.isfile.return_value = True\nfilename = 'file.csv'\n\n```  \n{% endraw %}  \nSimilarly, we can verify that it's called 0 or multiple times. In the example below where we verify that it's not called if the file doesn't exist  \n{% raw %}  \n```python\n@patch('utils.os.path.isfile')\n@patch('utils.pd.read_csv')\ndef test_load_data_does_not_call_read_csv_if_not_exists(mock_isfile, mock_read_csv):\n# arrange\n# file doesn't exist\nutils.os.path.isfile.return_value = False\nfilename = 'file.csv'\n\n```  \n{% endraw %}  \nExample: Using the same sample data for multiple tests  \nIf more than one test will use the same sample data, fixtures are a good way to reuse this sample data. The sample data can be the contents of a json file, or a csv, or a DataFrame, or even an image.  \nNote: The sample data is still hard coded if possible, and does not need to be large. Only add as much sample data as required for the tests to make the tests readable.  \nUse the fixture to return the sample data, and add this as a parameter to the tests where you want to use the sample data.  \n{% raw %}  \n```python\nimport pytest\n\n@pytest.fixture\ndef house_features_json():\nreturn {'area': 25, 'price': 2500, 'rooms': np.nan}\n\ndef test_clean_features_cleans_nan_values(house_features_json):\ncleaned_features = clean_features(house_features_json)\nassert cleaned_features['rooms'] == 0\n\ndef test_extract_features_extracts_price_per_area(house_features_json):\nextracted_features = extract_features(house_features_json)\nassert extracted_features['price_per_area'] == 100\n```  \n{% endraw %}  \nTransforming data  \nFor cleaning and transforming data, test fixed input and output, but try to limit each test to one verification.  \nFor example, create one test to verify the output shape of the data.  \n{% raw %}  \n```python\ndef test_resize_image_generates_the_correct_size():\n# Arrange\noriginal_image = np.ones((10, 5, 2, 3))\n\n# act\nresized_image = utils.resize_image(original_image, 100, 100)\n\n# assert\nresized_image.shape[:2] = (100, 100)\n```  \n{% endraw %}  \nand one to verify that any padding is made appropriately  \n{% raw %}  \n```python\ndef test_resize_image_pads_correctly():\n# Arrange\noriginal_image = np.ones((10, 5, 2, 3))\n\n# Act\nresized_image = utils.resize_image(original_image, 100, 100)\n\n# Assert\nassert resized_image[0][0][0][0] == 0\nassert resized_image[0][0][2][0] == 1\n```  \n{% endraw %}  \nTo test different inputs and expected outputs automatically, use parametrize  \n{% raw %}  \n```python\n@pytest.mark.parametrize('orig_height, orig_width, expected_height, expected_width',\n[\n# smaller than target\n(10, 10, 20, 20),\n# larger than target\n(20, 20, 10, 10),\n# wider than target\n(10, 20, 10, 10)\n])\ndef test_resize_image_generates_the_correct_size(orig_height, orig_width, expected_height, expected_width):\n# Arrange\noriginal_image = np.ones((orig_height, orig_width, 2, 3))\n\n# act\nresized_image = utils.resize_image(original_image, expected_height, expected_width)\n\n# assert\nresized_image.shape[:2] = (expected_height, expected_width)\n```  \n{% endraw %}  \nModel load or predict  \nWhen unit testing we should mock model load and model predictions similarly to mocking file access.  \nThere may be cases when you want to load your model to do smoke tests, or integration tests.  \nSince these will often take a bit longer to run it's important to be able to separate them from unit tests so that the developers on the team can still run unit tests as part of their test driven development.  \nOne way to do this is using marks  \n{% raw %}  \npython\n@pytest.mark.longrunning\ndef test_integration_between_two_systems():",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\ml-testing.md"
    },
    {
        "chunkId": "chunk200_1",
        "chunkContent": "# this might take a while  \n{% endraw %}  \nRun all tests that are not marked longrunning  \n{% raw %}  \nbash\npytest -v -m \"not longrunning\"  \n{% endraw %}  \nBasic Unit Tests for ML Models  \nML unit tests are not intended to check the accuracy or performance of a model. Unit tests for an ML model is for code quality checks - for example:  \nDoes the model accept the correct inputs and produce the correctly shaped outputs?  \nDo the weights of the model update when running fit?  \nTo do this, the ML model tests do not strictly follow best practices of standard Unit tests - not all outside calls are mocked. These tests are much closer to a narrow integration test.\nHowever, the benefits of having simple tests for the ML model help to stop a poorly configured model from spending hours in training, while still producing poor results.  \nExamples of how to implement these tests (for Deep Learning models) include:  \nBuild a model and compare the shape of input layers to that of an example source of data. Then, compare the output layer shape to the expected output.  \nInitialize the model and record the weights of each layer. Then, run a single epoch of training on a dummy data set, and compare the weights of the \"trained model\" - only check if the values have changed.  \nTrain the model on a dummy dataset for a single epoch, and then validate with dummy data - only validate that the prediction is formatted correctly, this model will not be accurate.  \nData Validation  \nAn important part of the unit testing is to include test cases for data validation. For example, no data supplied, images that are not in the expected format, data containing null values or outliers to make sure that the data processing pipeline is robust.  \nModel Testing  \nApart from unit testing code, we can also test, debug and validate our models in different ways during the training process  \nSome options to consider at this stage:  \nAdversarial and Boundary tests to increase robustness  \nVerifying accuracy for under-represented classes",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\ml-testing.md"
    },
    {
        "chunkId": "chunk201_0",
        "chunkContent": "TPM considerations for Machine Learning projects  \nIn this document, we explore some of the Program Management considerations for Machine Learning (ML) projects and suggest recommendations for Technical Program Managers (TPM) to effectively work with Data and Applied Machine Learning engineering teams.  \nDetermine the need for Machine Learning in the project  \nIn Artificial Intelligence (AI) projects, the ML component is generally a part of an overall business problem and NOT the problem itself. Determine the overall business problem first and then evaluate if ML can help address a part of the problem space.\nFew considerations for identifying the right fit for the project:  \nEngage experts in human experience and employ techniques such as Design Thinking and Problem Formulation to understand the customer needs and human behavior first. Identify the right stakeholders from both business and technical leadership and invite them to these workshops. The outcome should be end-user scenarios and personas to determine the real needs of the users.  \nFocus on System Design principles to identify the architectural components, entities, interfaces, constraints. Ask the right questions early and explore design alternatives with the engineering team.  \nThink hard about the costs of ML and whether we are solving a repetitive problem at scale. Many a times, customer problems can be solved with data analytics, dashboards, or rule-based algorithms as the first phase of the project.  \nSet Expectations for high ambiguity in ML components  \nML projects can be plagued with a phenomenon we can call as the \"Death by Unknowns\". Unlike software engineering projects, ML focused projects can result in quick success early (aka sudden decrease in error rate), but this may flatten eventually. Few things to consider:  \nSet clear expectations: Identify the performance metrics and discuss on a \"good enough\" prediction rate that will bring value to the business. An 80% \"good enough\" rate may save business costs and increase productivity but if going from 80 to 95% would require unimaginable cost and effort. Is it worth it? Can it be a progressive road map?  \nCreate a smaller team and undertake a feasibility analysis through techniques like EDA (Exploratory Data Analysis). A feasibility study is much cheaper to evaluate data quality, customer constraints and model feasibility. It allows a TPM to better understand customer use cases and current environment and can act as a fail-fast mechanism. Note that feasibility should be shorter (in weeks) else it misses the point of saving costs.  \nAs in any project, there will be new needs (additional data sources, technical constraints, hiring data labelers, business users time etc.). Incorporate Agile techniques to fail fast and minimize cost and schedule surprises.  \nNotebooks != ML Production  \nNotebooks are a great way to kick start Data Analytics and Applied Machine Learning efforts, however for a production releases, additional constraints should be considered:  \nUnderstand the end-end flow of data management, how data will be made available (ingestion flows), what's the frequency, storage, retention of data. Plan user stories and design spikes around these flows to ensure a robust ML pipeline is developed.  \nEngineering team should follow the same rigor in building ML projects as in any software engineering project. We at ISE (Industry Solutions Engineering) have built a good set of resources from our learnings in our ISE Engineering Playbook.  \nThink about the how the model will be deployed, for example, are there technical constraints due to an edge device, or network constraints that will prevent updating the model. Understanding of the environment is critical, refer to the Model Production Checklist as a reference to determine model deployment choices.  \nML Focussed projects are not a \"one-shot\" release solution, they need to be nurtured, evolved, and improved over time. Plan for a continuous improvement lifecycle, the initial phases can be model feasibility and validation to get the good enough prediction rate, the later phases can be then be scaling and improving the models through feedback loops and fresh data sets.  \nGarbage Data In -> Garbage Model Out  \nData quality is a major factor in affecting model performance and production roll-out, consider the following:  \nConduct a data exploration workshop and generate a report on data quality that includes missing values, duplicates, unlabeled data, expired or not valid data, incomplete data (e.g., only having male representation in a people dataset).  \nIdentify data source reliability to ensure data is coming from a production source. (e.g., are the images from a production or industrial camera or taken from an iPhone/Android phone.)  \nIdentify data acquisition constraints: Determine how the data is being obtained and the constraints around it. Some example may include legal, contractual, Privacy, regulation, ethics constraints. These can significantly slow down production roll out if not captured in the early phases of the project.  \nDetermine data volumes: Identify if we have enough data for sampling the required business use case and how will the data be improved over time. The thumb rule here is that data should be enough for generalization to avoid overfitting.  \nPlan for Unique Roles in AI projects  \nAn ML Project has multiple stages, and each stage may require additional roles. For example, Design Research & Designers for Human Experience, Data Engineer for Data Collection, Feature Engineering, a Data Labeler for labeling structured data, engineers for MLOps and model deployment and the list can go on. As a TPM, factor in having these resources available at the right time to avoid any schedule risks.  \nFeature Engineering and Hyperparameter tuning  \nFeature Engineering enables the transformation of data so that it becomes usable for an algorithm. Creating the right features is an art and may require experimentation as well as domain expertise. Allocate time for domain experts to help with improving and identifying the best features. For example, for a natural language processing engine for text extraction of financial documents, we may involve financial researchers and run a relevance judgment exercise and provide a feedback loop to evaluate model performance.  \nResponsible AI considerations  \nBias in machine learning could be the number one issue of a model not performing to its intended needs. Plan to incorporate Responsible AI principles from Day 1 to ensure fairness, security, privacy and transparency of the models.  For example, for a person recognition algorithm, if the data source is only feeding a specific skin type, then production scenarios may not provide good results.  \nPM Fundamentals  \nCore to a TPM role are the fundamentals that include bringing clarity to the team, design thinking, driving the team to the right technical decisions, managing risk, managing stakeholders, backlog management, project management. These are a TPM superpowers. A TPM can complement the machine learning team by ensuring the problem and customer needs are understood, a wholistic system design is evaluated, the stakeholder expectations and driving customer objectives.\nHere are some references that may help:  \nThe T in a TPM  \nThe TPM Don't M*ck up framework  \nThe mind of a TPM  \nML Learning Journey for a TPM",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\ml-tpm-guidance.md"
    },
    {
        "chunkId": "chunk202_0",
        "chunkContent": "Machine Learning Fundamentals at ISE  \nThis guideline documents the Machine Learning (ML) practices in ISE. ISE works with customers on developing ML models and putting them in production, with an emphasis on engineering and research best practices throughout the project's life cycle.  \nGoals  \nProvide a set of ML practices to follow in an ML project.  \nProvide clarity on ML process and how it fits within a software engineering project.  \nProvide best practices for the different stages of an ML project.  \nHow to use these fundamentals  \nIf you are starting a new ML project, consider reading through the general guidance documents.  \nFor specific aspects of an ML project, refer to the guidelines for different project phases.  \nML Project phases  \nThe diagram below shows different phases in an ideal ML project. Due to practical constraints and requirements, it might not always be possible to have a project structured in such a manner, however best practices should be followed for each individual phase.  \nEnvisioning: Initial problem understanding, customer goals and objectives.  \nFeasibility Study: Assess whether the problem in question is feasible to solve satisfactorily using ML with the available data.  \nModel Milestone: There is a basic model that is achieving the minimum required performance, both in terms of ML performance and system performance. Using the knowledge gathered to this milestone, define the scope, objectives, high-level architecture, definition of done and plan for the entire project.  \nModel(s) experimentation: Tools and best practices for conducting successful model experimentation.  \nModel(s) Operationalization: Model readiness for production checklist.  \nGeneral guidance  \nML Process Guidance  \nML Fundamentals checklist  \nData Exploration  \nAgile ML development  \nTesting Data Science and ML Ops code  \nProfiling Machine Learning and ML Ops code  \nResponsible AI  \nProgram Management for ML projects  \nReferences  \nModel Operationalization",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\README.md"
    },
    {
        "chunkId": "chunk203_0",
        "chunkContent": "Responsible AI in ISE  \nMicrosoft's Responsible AI principles  \nEvery ML project in ISE goes through a Responsible AI (RAI) assessment to ensure that it upholds Microsoft's 6 Responsible AI principles:  \nFairness  \nReliability & Safety  \nPrivacy & Security  \nInclusiveness  \nTransparency  \nAccountability  \nEvery project goes through the RAI process, whether we are building a new ML model from scratch, or putting an existing model in production.  \nISE's Responsible AI process  \nThe process begins as soon as we start a prospective project. We start to complete a Responsible AI review document, and an impact assessment, which provides a structured way to explore topics such as:  \nCan the problem be addressed with a non-technical (e.g. social) solution?  \nCan the problem be solved without AI? Would simpler technology suffice?  \nWill the team have access to domain experts (e.g. doctors, refugees) in the field where the AI is applicable?  \nWho are the stakeholders in this project? Who does the AI impact? Are there any vulnerable groups affected?  \nWhat are the possible benefits and harms to each stakeholder?  \nHow can the technology be misused, and what can go wrong?  \nHas the team analyzed the input data properly to make sure that the training data is suitable for machine learning?  \nIs the training data an accurate representation of data that will be used as input in production?  \nIs there a good representation of all users?  \nIs there a fall-back mechanism (a human in the loop, or a way to revert decisions based on the model)?  \nDoes data used by the model for training or scoring contain PII? What measures have been taken to remove sensitive data?  \nDoes the model impact consequential decisions, like blocking people from getting jobs, loans, health care etc. or in the cases where it may, have appropriate ethical considerations been discussed?  \nHave measures for re-training been considered?  \nHow can we address any concerns that arise, and how can we mitigate risk?  \nAt this point we research available tools and resources, such as InterpretML or Fairlearn, that we may use on the project. We may change the project scope or re-define the ML problem definition if necessary.  \nThe Responsible AI review documents remain living documents that we re-visit and update throughout project development, through the feasibility study, as the model is developed and prepared for production, and new information unfolds. The documents can be used and expanded once the model is deployed, and monitored in production.",
        "source": "..\\data\\docs\\code-with-engineering\\machine-learning\\responsible-ai.md"
    },
    {
        "chunkId": "chunk204_0",
        "chunkContent": "Guidance for Alerting  \nOne of the goals of building highly observable systems is to provide valuable insight into the behavior of the application. Observable systems allow problems to be identified and surfaced through alerts before end users are impacted.  \nBest Practices  \nThe foremost thing to do before creating alerts is to implement observability. Without monitoring systems in place, it becomes next to impossible to know what activities need to be monitored and when to alert the teams.  \nIdentify what the application's minimum viable service quality needs to be. It is not what you intend to deliver, but is acceptable for the customer. These Service Level Objectives(SLOs) are a metric for measurement of the application's performance.  \nSLOs are defined with respect to the end users. The alerts must watch for visible impact to the user. For example, alerting on request rate, latency and errors.  \nUse automated, scriptable tools to mimic end-to-end important code paths relatable to activities in the application. Create alert polices on user impacting events or metric rate of change.  \nAlert fatigue is real. Engineers are recommended to pay attention to their monitoring system so that accurate alerts and thresholds can be defined.  \nEstablish a primary channel for alerts that needs immediate attention and tag the right team/person(s) based on the nature of the incident. Not every single alert needs to be sent to the primary on-call channel.  \nEstablish a secondary channel for items that need to be looked into and does not affect the users, yet. For example, storage that nearing capacity threshold. These items will be what the engineering services will look to regularly to monitor the health of the system.  \nEnsure to set up proper alerting for failures in dependent services like Redis cache, Service Bus etc. For example, if Redis cache is throwing 10 exceptions in last 60 secs, proper alerts are recommended to be created so that these failures are surfaced and action be taken.  \nIt is important to learn from each incident and continually improve the process. After every incident has been triaged, conduct a post mortem of the scenario. Scenarios and situations that were not initially considered will occur, and the post-mortem workflow is a great way to highlight that to improve the monitoring/alerting of the system. Configuring an alert to detect that incident scenario is a good idea to see if the event occurs again.",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\alerting.md"
    },
    {
        "chunkId": "chunk205_0",
        "chunkContent": "Recommended Practices  \nCorrelation Id: Include unique identifier at the start of the interaction to tie down aggregated data from various system components and provide a holistic view. Read more guidelines about using correlation id.  \nEnsure health of the services are monitored and provide insights into system's performance and behavior.  \nEnsure dependent services are monitored properly. Errors and exceptions in dependent services like Redis cache, Service bus, etc. should be logged and alerted. Also, metrics related to dependent services should be captured and logged.  \nAdditionally, failures in dependent services should be propagated up each level of the stack by the health check.  \nFaults, crashes, and failures are logged as discrete events. This helps engineers identify problem area(s) during failures.  \nEnsure logging configuration (eg: setting logging to \"verbose\") can be controlled without code changes.  \nEnsure that metrics around latency and duration are collected and can be aggregated.  \nStart small and add where there is customer impact. Avoiding metric fatigue is very crucial to collecting actionable data.  \nIt is important that every data that is collected contains relevant and rich context.  \nPersonally Identifiable Information or any other customer sensitive information should never be logged. Special attention should be paid to any local privacy data regulations and collected data must adhere to those. (ex: GDPR)  \nHealth checks : Appropriate health checks should added to determine if service is healthy and ready to serve traffic. On a kubernetes platform different types of probes e.g. Liveness, Readiness, Startup etc. can be used to determine health and readiness of the deployed service.  \nRead more here to understand what to watch out for while designing and building an observable system.",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\best-practices.md"
    },
    {
        "chunkId": "chunk206_0",
        "chunkContent": "Correlation IDs  \nThe Need  \nIn a distributed system architecture (microservice architecture), it is highly difficult to understand a single end to end customer transaction flow through the various components.  \nHere are some the general challenges -  \nIt becomes challenging to understand the end-to-end behavior of a client request entering the application.  \nAggregation: Consolidating logs from multiple components and making sense out of these logs is difficult, if not impossible.  \nCyclic dependencies on services, course of events and asynchronous requests are not easily deciphered.  \nWhile troubleshooting a request, the diagnostic context of the logs are very important to get to the root of the problem.  \nSolution  \nA Correlation ID is a unique identifier that is added to the very first interaction (incoming request) to  identify the context and is passed to all components that are involved in the transaction flow. Correlation ID becomes the glue that binds the transaction together and helps to draw an overall picture of events.  \nNote: Before implementing your own Correlation ID, investigate if your telemetry tool of choice provides an auto-generated Correlation ID and that it serves the purposes of your application. For instance, Application Insights offers dependency auto-collection for some application frameworks  \nRecommended Practices  \nAssign each external request a Correlation ID that binds the message to a transaction.  \nThe Correlation ID for a transaction must be assigned as early as you can.  \nPropagate Correlation ID to all downstream components/services.  \nAll components/services of the transaction use this Correlation ID in their logs.  \nFor an HTTP Request, Correlation ID is typically passed in the header.  \nAdd it to an outgoing response where possible.  \nBased on the use case, there can be additional correlation IDs that may be needed. For instance, tracking logs based on both Session ID and User ID may be required. While adding multiple correlation ID, remember to propagate them through the components.  \nConsider using OpenTelemetry as it implements open-source cross-platform context propagation for end-to-end distributed transactions over heterogeneous components out-of-the-box. It takes care of automatically creating and managing the \"Correlation-id\", called TraceId.  \nUse Cases  \nLog Correlation  \nLog correlation is the ability to track disparate events through different parts of the application. Having a Correlation ID provides more context making it easy to build rules for reporting and analysis.  \nSecondary reporting/observer systems  \nUsing Correlation ID helps secondary systems to correlate data without application context. Some examples - generating metrics based on tracing data, integrating runtime/system diagnostics etc. For example, feeding AppInsights data and correlating it to infrastructure issues.  \nTroubleshooting Errors  \nFor troubleshooting an errors, Correlation ID is a great starting point to trace the workflow of a transaction.",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\correlation-id.md"
    },
    {
        "chunkId": "chunk207_0",
        "chunkContent": "Diagnostic tools  \nBesides Logging, Tracing and Metrics, there are additional tools to help diagnose issues when applications do not behave as expected. In some scenarios, analyzing the memory consumption and drilling down into why a specific process takes longer than expected may require additional measures. In these cases, platform or programming language specific diagnostic tools come into play and are useful to debug a memory leak, profile the CPU usage, or the cause of delays in multi-threading.  \nProfilers and Memory Analyzers  \nThere are two types of diagnostics tools you may want to use: profilers and memory analyzers.  \nProfiling  \nProfiling is a technique where you take small snapshots of all the threads in a running application to see the stack trace of each thread for a specified duration. This tool can help you identify where you are spending CPU time during the execution of your application. There are two main techniques to achieve this: CPU-Sampling and Instrumentation.  \nCPU-Sampling is a non-invasive method which takes snapshots of all the stacks at a set interval. It is the most common technique for profiling and doesn't require any modification to your code.  \nInstrumentation is the other technique where you insert a small piece of code at the beginning and end of each function which is going to signal back to the profiler about the time spent in the function, the function name, parameters and others. This way you modify the code of your running application. There are two effects to this: your code may run a little bit more slowly, but on the other hand you have a more accurate view of every function and class that has been executed so far in your application.  \nWhen to use Sampling vs Instrumentation?  \nNot all programming languages support instrumentation. Instrumentation is mostly supported for compiled languages like .NET and Java, and some languages interpreted at runtime like Python and Javascript. Keep in mind that enabling instrumentation can require to modify your build pipeline, i.e. by adding special parameters to the command line argument. You should normally start with Sampling because it doesn't require to modify your binaries, it doesn't affect your process performance, and can be quicker to start with.  \nOnce you have your profiling data, there are multiple ways to visualize this information depending of the format you saved it. As an example for .NET (dotnet-trace), there are three available formats to save these traces: Chromium, NetTrace and SpeedScope. Select the output format depending on the tool you are going to use. SpeedScope is an online web application you can use to visualize and analyze traces, and you only need a modern browser. Be careful with online tools, as dumps/traces might contain confidential information that you don't want to share outside of your organization.  \nMemory analyzers  \nMemory analyzers and memory dumps are another set of diagnostic tools you can use to identify issues in your process.  Normally these types of tools take the whole memory the process is using at a point in time and saves it in a file which  can be analyzed. When using these types of tools, you want to stress your process as much as possible to amplify whatever deficiency you may have in terms of memory management. The memory dump should then be taken when the process is in this stressed state.  \nIn some scenarios we recommend to take more than one memory dump during the reproduction of a problem. For example, if you suspect a memory leak and you are running a test for 30 min, it is useful to take at least 3 dumps at different intervals (i.e. 10, 20 & 30 min) to compare them with each other.  \nThere are multiple ways to take a memory dump depending the operating system you are using. Also, each operating system has it own debugger which is able to load this memory dump, and explore the state of the process at the time the memory dump was taken.  \nThe most common debuggers are:  \nWindows - WinDbg and WinDgbNext (included in the Windows SDK), Visual Studio can also load a memory dump for a .NET Framework and .NET Core process  \nLinux - GDB is the GNU Debugger  \nMac OS - LLDB Debugger  \nThere are a range of developer platform specific diagnostic tools which can be used:  \n.NET Core diagnostic tools, GitHub repository  \nJava diagnostic tools - version specific  \nPython debugging and profiling - version specific  \nNode.js Diagnostics working group  \nEnvironment for profiling  \nTo create an application profile as close to production as possible, the environment in which the application is intended to run in production has to be considered and it might be necessary to perform a snapshot of the application state under load.  \nDiagnostics in containers  \nFor monolithic applications, diagnostics tools can be installed and run on the VM hosting them. Most scalable applications are developed as microservices and have complex interactions which require to install the tools in the containers running the process or to leverage a sidecar container (see sidecar pattern). Some platforms expose endpoints to interact with the application and return a dump.  \nUseful links:  \n.NET Core diagnostics in containers  \nExperimental tool dotnet-monitor, What's new, GItHub repository  \nSpring Boot actuator endpoints",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\diagnostic-tools.md"
    },
    {
        "chunkId": "chunk208_0",
        "chunkContent": "Logs vs Metrics vs Traces  \nOverview  \nMetrics  \nThe purpose of metrics is to inform observers about the health & operations regarding a component or system. A metric represents a point in time measure of a particular source, and data-wise tends to be very small. The compact size allows for efficient collection even at scale in large systems. Metrics also lend themselves very well to pre-aggregation within the component before collection, reducing computation cost for processing & storing large numbers of metric time series in a central system. Due to how efficiently metrics are processed & stored, it lends itself very well for use in automated alerting, as metrics are an excellent source for the health data for all components in the system.  \nLogs  \nLog data inform observers about the discrete events that occurred within a component or a set of components. Just about every software component log information about its activities over time. This rich data tends to be much larger than metric data and can cause processing issues, especially if components are logging too verbosely. Therefore, using log data to understand the health of an extensive system tends to be avoided and depends on metrics for that data. Once metric telemetry highlights potential problem sources, filtered log data for those sources can be used to understand what occurred.  \nTraces  \nWhere logging provides an overview to a discrete, event-triggered log, tracing encompasses a much wider, continuous view of an application. The goal of tracing is to following a program\u2019s flow and data progression.  \nIn many instances, tracing represents a single user\u2019s journey through an entire app stack. Its purpose isn\u2019t reactive, but instead focused on optimization. By tracing through a stack, developers can identify bottlenecks and focus on improving performance.  \nA distributed trace is defined as a collection of spans. A span is the smallest unit in a trace and represents a piece of the workflow in a distributed landscape. It can be an HTTP request, call to a database, or execution of a message from a queue.  \nWhen a problem does occur, tracing allows you to see how you got there:  \nWhich function.  \nThe function\u2019s duration.  \nParameters passed.  \nHow deep into the function the user could get.  \nUsage Guidance  \nWhen to use metric or log data to track a particular piece of telemetry can be summarized with the following points:  \nUse metrics to track the occurrence of an event, counting of items, the time taken to perform an action or to report the current value of a resource (CPU, memory, etc.)  \nUse logs to track detailed information about an event also monitored by a metric, particularly errors, warnings or other exceptional situations.  \nA trace provides visibility into how a request is processed across multiple services in a microservices environment. Every trace needs to have a unique identifier associated with it.",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\log-vs-metric-vs-trace.md"
    },
    {
        "chunkId": "chunk209_0",
        "chunkContent": "Guidance for Privacy  \nOverview  \nTo ensure the privacy of your system users, as well as comply with several regulations like GDPR, some types of data shouldn\u2019t exist in logs.\nThis includes customer's sensitive, Personal Identifiable Information (PII), and any other data that wasn't legally sanctioned.  \nRecommended Practices  \nSeparate components and minimize the parts of the system that log sensitive data.  \nKeep sensitive data out of URLs, since request URLs are typically logged by proxies and web servers.  \nAvoid using PII data for system debugging as much as possible. For example, use ids instead of usernames.  \nUse Structured Logging and include a deny-list for sensitive properties.  \nPut an extra effort on spotting logging statements with sensitive data during code review, as it is common for reviewers to skip reading logging statements. This can be added as an additional checkbox if you're using Pull Request Templates.  \nInclude mechanisms to detect sensitive data in logs, on your organizational pipelines for QA or Automated Testing.  \nTools and Implementation Methods  \nUse these tools and methods for sensitive data de-identification in logs.  \nApplication Insights  \nApplication Insights offers telemetry interception in some of the SDKs, that can be done by implementing the ITelemetryProcessor interface.\nITelemetryProcessor processes the telemetry information before it is sent to Application Insights, and can be useful in many situations, such as filtering and modifications. Below is an example of intercepting 'trace' typed telemetry:  \n{% raw %}  \n```csharp\nusing Microsoft.ApplicationInsights.DataContracts;\n\nnamespace Example\n{\nusing Microsoft.ApplicationInsights.Channel;\nusing Microsoft.ApplicationInsights.Extensibility;\n\n}\n```  \n{% endraw %}  \nElastic Stack  \nElastic Stack (formerly \"ELK stack\") allows logs interception by Logstash's filter-plugins.\nUsing some of the existing plugins, like 'mutate', 'alter' and 'prune' might be sufficient for most cases of deidentifying and redacting PIIs.\nFor a more robust and customized use-case, a 'ruby' plugin can be used, executing arbitrary Ruby code.\nFilter plugins also exists in some Logstash alternatives, like Fluentd and Fluent Bit.  \nPresidio  \nPresidio offers data protection and anonymization API. It provides fast identification and anonymization modules for private entities in text.\nPresidio allows using predefined or custom PII recognizers, leveraging Named Entity Recognition, regular expressions, rule based logic and checksum with relevant context in multiple languages.\nIt can be used alongside the log interception methods mentioned above to help and ensure sensitive data is properly managed and governed.\nPresidio is containerized for REST HTTP API and also can be installed as a python package, to be called from python code.\nInstead of handling the anonymization in the application code, both APIs can be used using external calls.\nElastic Stack, for example, can handle PII redaction using the 'ruby' filter plugin to call Presidio in REST HTTP API, or by calling a python script consuming Presidio as a package:  \nlogstash.conf  \n{% raw %}  \n```ruby\ninput {\n...\n}\n\nfilter {\nruby {\ncode => 'require \"open3\"\nmessage = event.get(\"message\")\n# Call a python script triggering Presidio analyzer and anonymizer, and printing the result.\ncmd =  \"python /path/to/presidio/anonymization/script.py \\\"#{message}\\\"\"\n# Fetch the script's stdout\nstdin, stdout, stderr = Open3.popen3(cmd)\n# Override message with the anonymized text.\nevent.set(\"message\", stdout.read)\nfilter_matched(event)'\n}\n}\n\noutput {\n...\n}\n```  \n{% endraw %}",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\logs-privacy.md"
    },
    {
        "chunkId": "chunk210_0",
        "chunkContent": "Observability in Microservices  \nMicroservices is a very popular software architecture, where the application is arranged as a collection of loosely coupled services. Some of those services can be written in different languages by different teams.  \nMotivations  \nWe need to consider special cases when creating a microservice architecture from the perspective of observability. We want to capture the interactions when making requests between those microservices and correlate them.  \nImagine we have a microservice that accesses a database to retrieve some data as part of a request. This microservice is going to be called by someone else as part of an incoming http request or an internal process being executed. What happens if a problem occurs during the retrieval of the data (or the update of the data)? How can we associate, or correlate, that this particular call failed in the destination microservice?  \nThis is a common issue. When calling other microservices, depending on the technology stack we use, we can accidentally hide errors and exceptions that might happen on the other side. If we are using a simple REST interface, the other microservice can return a 500 HTTP status code and we don't have any idea what happen inside that microservice.  \nMore important, we don't have any way to associate our Correlation Id to whatever happens inside that microservice. Therefore, is so important to have a plan in place to be able to extend your traceability and monitoring efforts, especially when using a microservice architecture.  \nHow to extend your tracing information between microservices  \nThe W3C consortium is working on a Trace Context definition that can be applied when using HTTP as the protocol in a microservice architecture. But let's explain how we can implement this functionality in our software.  \nThe main idea behind this is to propagate the correlation information between HTTP request so other pieces of software can read this information and correctly correlate telemetry across microservices.  \nThe way to propagate this information is to use HTTP Headers for the Correlation Id, parent Correlation Id, etc.  \nWhen you are in the scope of a HTTP Request, your tracing system should already have created four properties that you can use to send across your microservices.  \nRequestId:0HLQV2BC3VP2T:00000001,  \nSpanId:da13aa3c6fd9c146,  \nTraceId:f11a03e3f078414fa7c0a0ce568c8b5c,  \nParentId:5076c17d0a604244  \nThis is an example of the four properties you can find which identify the current request.  \nRequestId is the unique id that represent the current HTTP Request.  \nSpanId is the default automatically generated span. You can have more than one Span that scope different functionality inside your software.  \nTraceId represent the id for current log trace.  \nParentId is the parent span id, that in some case can be the same or something different.  \nExample  \nNow we are going to explore an example with 3 microservices that calls to each other in a row.  \nThis image is the summary of what is needed in each microservice to propagate the trace-id from A to C.  \nThe root caller is A and that is why it doesn't have a parent-id, only have a new trace-id. Next, A calls B using HTTP. To propagate the correlation information as part of the request, we are using two new headers based on the W3C Correlation specification, trace-id and parent-id. In this example because A is the root caller, A only sends its own trace-id to microservice B.  \nWhen microservice B receives the incoming HTTP request, it checks the contents of these two headers. It reads the content of the trace-id header and sets its own parent-id to this trace-id (as shown in the green rectangle inside's B). In addition, it creates a new trace-id to signal that is a new scope for the telemetry. During the execution of microservice B, it also calls microservice C and repeats the pattern. As part of the request it includes the two headers and propagates trace-id and parent-id as well.  \nFinally, microservice C, reads the value for the incoming trace-id and sets as his own parent-id, but also creates a new trace-id that will use to send telemetry about his own operations.  \nSummary  \nA number of Application Monitoring (APM) technology products already supports most of this Correlation Propagation. The most popular is OpenZipkin/B3-Propagation. W3C already proposed a recommendation for the W3C Trace Context, where you can see what SDK and frameworks already support this functionality. It's important to correctly implement the propagation specially when there are different teams that used different technology stacks in the same project.  \nConsider using OpenTelemetry as it implements open-source cross-platform context propagation for end-to-end distributed transactions over heterogeneous components out-of-the-box. It takes care of automatically creating and managing the Trace Context object among a full stack of microservices implemented across different technical stacks.",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\microservices.md"
    },
    {
        "chunkId": "chunk211_0",
        "chunkContent": "Observability in Machine Learning  \nDevelopment process of software system with machine learning component is more complex\nthan traditional software. We need to monitor changes and variations in three dimensions:\nthe code, the model and the data.\nWe can distinguish two stages of such system lifespan: experimentation and production\nthat require  different approaches to observability as discussed below:  \nModel experimentation and tuning  \nExperimentation is a process of finding suitable machine learning model and its parameters via training and evaluating such models with one or more datasets.  \nWhen developing and tuning machine learning models, the data scientists are interested in observing and comparing selected performance metrics for various model parameters.\nThey also need a reliable way to reproduce a training process, such that a given dataset and given parameters produces the same models.  \nThere are many model metric evaluation solutions available, both open source (like MLFlow) and proprietary (like Azure Machine Learning Service), and of which some serve different purposes. To capture model metrics, there are a.o. the following options available:  \nAzure Machine Learning Service SDK\nAzure Machine Learning service provides an SDK for Python, R and C# to capture your evaluation metrics to an Azure Machine Learning service (AML) Experiment. Experiments are viewed in the AML dashboard. Reproducibility is achieved by storing code or notebook snapshot together with viewed metric. You can create versioned Datasets within Azure Machine Learning service.  \nMLFlow (for Databricks)\nMLFlow is open source framework, and can be hosted on Azure Databricks as its remote tracking server (it currently is the only solution that offers first-party integration with Databricks). You can use the MLFlow SDK tracking component to capture your evaluation metrics or any parameter you would like and track it at experimentation board in Azure Databricks. Source code and dataset version are also saved with log snapshot to provide reproducibility.  \nTensorBoard\nTensorBoard is a popular tool amongst data scientist to visualize specific metrics of Deep Learning runs, especially of TensorFlow runs. TensorBoard is not an MLOps tool like AML/MLFlow, and therefore does not offer extensive logging capabilities. It is meant to be transient; and can therefore be used as an addition to an end-to-end MLOps tool like AML, but not as a complete MLOps tool.  \nApplication Insights\nApplication Insights can be used as an alternative sink to capture model metrics, and can therefore offer more extensive options as metrics can be transferred to e.g. a PowerBI dashboard. It also enables log querying. However, this solution means that a custom application needs to be written to send logs to AppInsights (using for example the OpenCensus Python SDK), which would mean extra effort of creating/maintaining custom code.  \nAn extensive comparison of the four tools can be found as follows:  \nAzure ML MLFlow TensorBoard Application Insights Metrics support Values, images, matrices, logs Values, images, matrices and plots as files Metrics relevant to DL research phase Values, images, matrices, logs Customizabile Basic Basic Very basic High Metrics accessible AML portal, AML SDK MLFlow UI, Tracking service API Tensorboard UI, history object Application Insights Logs accessible Rolling logs written to .txt files in blob storage, accessible via blob or AML portal. Not query-able Rolling logs are not stored Rolling logs are not stored Application Insights in Azure Portal. Query-able with KQL Ease of use and set up Very straightforward, only one portal More moving parts due to remote tracking server A bit over process overhead. Also depending on ML framework More moving parts as a custom app needs to be maintained Shareability Across people with access to AML workspace Across people with access to remote tracking server Across people with access to same directory Across people with access to AppInsights  \nModel in production  \nThe trained model can be deployed to production as container. Azure Machine Learning service provides SDK to deploy model as Azure Container Instance and publishes REST endpoint. You can monitor it using microservice observability methods( for more details -refer to Recipes section). MLFLow is an alternative way to deploy ML model as a service.  \nTraining and re-training  \nTo automatically retrain the model you can use AML Pipelines or Azure Databricks.\nWhen re-training with AML Pipelines you can monitor information of each run, including the output, logs, and various metrics in the Azure portal experiment dashboard, or manually extract it using the AML SDK  \nModel performance over time: data drift  \nWe re-train machine learning models to improve their performance and make models better aligned with data changing over time. However, in some cases model performance may degrade. This may happen in case data change dramatically and do not exhibit the patterns we observed during model development anymore. This effect is called data drift. Azure Machine Learning Service has preview feature to observe and report data drift.\nThis article describes it in detail.  \nData versioning  \nIt is recommended practice to add version to all datasets. You can create a versioned Azure ML Dataset for this purpose, or manually version it if using other systems.",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\ml-observability.md"
    },
    {
        "chunkId": "chunk212_0",
        "chunkContent": "Observability as Code  \nAs much as possible, configuration and management of observability assets such as cloud resource provisioning, monitoring alerts and dashboards must be managed as code. Observability as Code is achieved using any one of Terraform / Ansible / ARM Templates  \nExamples of Observability as Code  \nDashboards as Code - Monitoring Dashboards can be created as JSON or XML templates. This template is source control maintained and any changes to the dashboards can be reviewed. Automation can be built for enabling the dashboard. More about how to do this in Azure. Grafana dashboard can also be configured as code which eventually can be source-controlled to be used in automation and pipelines.  \nAlerts as Code - Alerts can be created within Azure by using Terraform or ARM templates. Such alerts can be source-controlled and be deployed as part of pipelines (Azure DevOps pipelines, Jenkins, GitHub Actions etc.). Few references of how to do this are: Terraform Monitor Metric Alert. Alerts can also be created based on log analytics query and can be defined as code using Terraform Monitor Scheduled Query Rules Alert.  \nAutomating Log Analytics Queries - There are several use cases where automation of log analytics queries may be needed. Example, Automatic Report Generation, Running custom queries programmatically for analysis, debugging etc. For these use cases to work, log queries should be source-controlled and automation can be built using log analytics REST or azure cli.  \nWhy  \nIt makes configuration repeatable and automatable. It also avoids manual configuration of monitoring alerts and dashboards from scratch across environments.  \nConfigured dashboards help troubleshoot errors during integration and deployment (CI/CD)  \nWe can audit changes and roll them back if there are any issues.  \nIdentify actionable insights from the generated metrics data across all environments, not just production.  \nConfiguration and management of observability assets like alert threshold, duration, configuration\nvalues using IAC help us in avoiding configuration mistakes, errors or overlooks during deployment.  \nWhen practicing observability as code, the changes can be reviewed by the team similar to other code\ncontributions.",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\observability-as-code.md"
    },
    {
        "chunkId": "chunk213_0",
        "chunkContent": "Observability for Azure Databricks  \nOverview  \nAzure Databricks is an Apache Spark\u2013based analytics service that makes it easy to rapidly develop and deploy big data analytics. Monitoring and troubleshooting performance issues is critical when\noperating production Azure Databricks workloads. It is important to log adequate information from Azure Databricks so that it is helpful to monitor and troubleshoot performance issues.  \nSpark is designed to run on a cluster - a cluster is a set of Virtual Machines (VMs). Spark can horizontally scale with bigger workloads needed more VMs. Azure Databricks can scale in and out as\nneeded.  \nApproaches to Observability  \nAzure Diagnostic Logs  \nAzure Diagnostic Logging is provided out-of-the-box by Azure Databricks, providing\nvisibility into actions performed against DBFS, Clusters, Accounts, Jobs, Notebooks, SSH, Workspace, Secrets, SQL Permissions, and Instance Pools.  \nThese logs are enabled using Azure Portal or CLI and can be configured to be delivered to one of these Azure resources.  \nLog Analytics Workspace  \nBlob Storage  \nEvent Hub  \nCluster Event Logs  \nCluster Event logs provide a quick overview into important Cluster lifecycle events. The\nlog are structured - Timestamp, Event Type and Details. Unfortunately, there is no native way to export logs to Log Analytics. Logs will have to be delivered to Log Analytics either using REST API or polled in the dbfs using Azure Functions.  \nVM Performance Metrics (OMS)  \nLog Analytics Agent provides insights into the performance counters from the Cluster VMs and helps to understand the\nCluster Utilization patters. Leveraging Linux OMX Agent to onboard VMs into Log Analytics, helps provide insights into the VM metrics, performance, inventory and syslog metrics. It is important to\nnote that Linux OMS Agent is not specific to Azure Databricks.  \nApplication Logging  \nOf all the logs collected, this is perhaps the most important one. Spark Monitoring library collects metrics about the driver, executors, JVM, HDFS, cache\nshuffling, DAGs, and much more. This library provides helpful insights to fine-tune Spark jobs. It allows monitoring and tracing each layer within Spark workloads, including performance and resource\nusage on the host and JVM, as well as Spark metrics and application-level logging. The library also includes ready-made Grafana dashboards that is a great starting point for building Azure Databricks\ndashboard.  \nLogs via REST API  \nAzure Databricks also provides REST API support. If there's any specific log data that is required, this data can be collected using the REST API calls.  \nNSG Flow Logs  \nNetwork security group (NSG) flow logs is a feature of Azure Network Watcher that allows you to log\ninformation about IP traffic flowing through an NSG. Flow data is sent to Azure Storage accounts from where you can access it as well as export it to any visualization tool, SIEM, or IDS of your choice.\nThis log information is not specific to NSG Flow logs. This data can be used to identify unknown or undesired traffic and monitor traffic levels and/or bandwidth consumption. This is possible only with\nVNET-injected workspaces.  \nPlatform Logs  \nPlatform logs can be used to review provisioning/de-provisioning operations. This can be used to review activity in Databricks managed resource group. It helps discover operations performed at\nsubscription level (like provisioning of VM, Disk etc.)  \nThese logs can be enabled via Azure Monitor > Activity Logs and shipped to Log Analytics.  \nGanglia metrics  \nGanglia metrics is a Cluster Utilization UI and is available on the Azure Databricks. It is great for viewing live metrics of interactive clusters. Ganglia metrics is available by default and takes\nsnapshot of usage every 15 minutes. Historical metrics are stored as .png files, making it impossible to analyze data.",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\observability-databricks.md"
    },
    {
        "chunkId": "chunk214_0",
        "chunkContent": "Observability of CI/CD Pipelines  \nWith increasing complexity to delivery pipelines, it is very important\nto consider Observability in the context of build and release of\napplications.  \nBenefits  \nHaving proper instrumentation during build time helps gain insights into the various stages of the build and release process.  \nHelps developers understand where the pipeline performance bottlenecks are, based on the data collected. This\nhelps in having data-driven conversations around identifying latency between jobs, performance issues,\nartifact upload/download times providing valuable insights into agents availability and capacity.  \nHelps to identify trends in failures, thus allowing developers to quickly do root cause analysis.  \nHelps to provide an organization-wide view of pipeline health to easily identify trends.  \nPoints to Consider  \nIt is important to identify the Key Performance Indicators (KPIs) for evaluating a successful CI/CD pipeline. Where needed, additional tracing can be added to better record KPI metrics. For example, adding pipeline build tags to identify a 'Release Candidate' vs. 'Non-Release Candidate' helps in evaluating the end-to-end release process timeline.  \nDepending on the tooling used (Azure DevOps, Jenkins etc.,), basic reporting on the pipelines is\navailable out-of-the-box. It is important to evaluate these reports against the KPIs to understand if\na custom reporting solution for their pipelines is needed. If required, custom dashboards can be built using\nthird-party tools like Grafana or Power BI Dashboards.",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\observability-pipelines.md"
    },
    {
        "chunkId": "chunk215_0",
        "chunkContent": "Things to Watch for when Building Observable Systems  \nObservability as an afterthought  \nOne of the design goals when building a system should be to enable monitoring of the system. This helps planning and thinking application availability, logging and metrics at the time of design and development. Observability also acts as a great debugging tool providing developers a bird's eye view of the system. By leaving instrumentation and logging of metrics towards the end, the development teams lose valuable insights during development.  \nMetric Fatigue  \nIt is recommended to collect and measure what you need and not what you can. Don't attempt to monitor everything.  \nIf the data is not actionable, it is useless and becomes noise. On the contrary, it is sometimes very difficult to forecast every possible scenario that could go wrong.  \nThere must be a balance between collecting what is needed vs. logging every single activity in the system. A general rule of thumb is to follow these principles  \nrules that catch incidents must be simple, relevant and reliable  \nany data that is collected but not aggregated or alerted on must be reviewed if it is still required.  \nContext  \nAll data logged must contain rich context, which is useful for getting an overall view of the system and easy to trace back errors/failures during troubleshooting. While logging data, care must also be taken to avoid data silos.  \nPersonally Identifiable Information  \nAs a general rule, do not log any customer sensitive and Personal Identifiable Information (PII). Ensure any pertinent privacy regulations are followed regarding PII (Ex: GDPR etc.)\nRead more here on how to keep sensitive data out of logs.",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\pitfalls.md"
    },
    {
        "chunkId": "chunk216_0",
        "chunkContent": "Profiling  \nOverview  \nProfiling is a form of runtime analysis that measures various components of the runtime such as, memory allocation, garbage collection, threads and locks, call stacks, or frequency and duration of specific functions. It can be used to see which functions are the most costly in your binary, allowing you to focus your effort on removing the largest inefficiencies as quickly as possible. It can help you find deadlocks, memory leaks, or inefficient memory allocation, and help inform decisions around resource allocation (ie: CPU or RAM).  \nHow to Profile your Applications  \nProfiling is somewhat language dependent, so start off by searching for \"profile $language\" (some common tools are listed below). Additionally, Linux Perf is a good fallback, since a lot of languages have bindings in C/C++.  \nProfiling does incur some cost, as it requires inspecting the call stack, and sometimes pausing the application all together (ie: to trigger a full GC in Java). It is recommended to continuously profile your services, say for 10s every 10 minutes. Consider the cost when deciding on tuning these parameters.  \nDifferent tools visualize profiles differently. Common CPU profiles might use a directed graph  or a flame graph.  \nUnfortunately, each profiler tool typically uses its own format for storing profiles, and comes with its own visualization.  \nSpecific tools  \n(Java, Go, Python, Ruby, eBPF) Pyroscope continuous profiling out of the box.  \n(Java and Go) Flame - profiling containers in Kubernetes  \n(Java, Python, Go) Datadog Continuous profiler  \n(Go) profefe, which builds pprof to provide continuous profiling  \n(Java) Eclipse Memory Analyzer",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\profiling.md"
    },
    {
        "chunkId": "chunk217_0",
        "chunkContent": "Observability  \nBuilding observable systems enables development teams at ISE to measure how well the application is behaving. Observability serves the following goals:  \nProvide holistic view of the application health.  \nHelp measure business performance for the customer.  \nMeasure operational performance of the system.  \nIdentify and diagnose failures to get to the problem fast.  \nPillars of Observability  \nLogs  \nMetrics  \nTracing  \nLogs vs Metrics vs Traces  \nInsights  \nDashboards and Reporting  \nTools, Patterns and Recommended Practices  \nTooling and Patterns  \nObservability As Code  \nRecommended Practices  \nDiagnostics tools  \nOpenTelemetry  \nFacets of Observability  \nObservability for Microservices  \nObservability in Machine Learning  \nObservability of CI/CD Pipelines  \nObservability in Azure Databricks  \nRecipes  \nUseful links  \nNon-Functional Requirements Guidance",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\README.md"
    },
    {
        "chunkId": "chunk218_0",
        "chunkContent": "Recipes  \nApplication Insights/ASP.NET  \nGitHub Repo, Article.  \nApplication Insights/ASP.NET Core with distributed Trace Context propagation to Kafka  \nGitHub Repo.  \nExample: OpenTelemetry over a message oriented architecture in Java with Jaeger, Prometheus and Azure Monitor  \nGitHub Repo  \nExample: Setup Azure Monitor dashboards and alerts with Terraform  \nGitHub Repo  \nOn-premises Application Insights  \nOn-premise Application Insights is a service that is compatible with Azure App Insights, but stores the data in an in-house database like PostgreSQL or object storage like Azurite.  \nOn-premises Application Insights is useful as a drop-in replacement for Azure Application Insights in scenarios where a solution must be cloud deployable but must also support on-premises disconnected deployment scenarios.  \nOn-premises Application Insights is also useful for testing telemetry integration. Issues related to telemetry can be hard to catch since often these integrations are excluded from unit-test or integration test flows due to it being non-trivial to use a live Azure Application Insights resource for testing, e.g. managing the lifetime of the resource, having to ignore old telemetry for assertions, if a new resource is used it can take a while for the telemetry to show up, etc. The On-premise Application Insights service can be used to make it easier to integrate with an Azure Application Insights compatible API endpoint during local development or continuous integration without having to spin up a resource in Azure. Additionally, the service simplifies integration testing of asynchronous workflows such as web workers since integration tests can now be written to assert against telemetry logged to the service, e.g. assert that no exceptions were logged, assert that some number of events of a specific type were logged within a certain time-frame, etc.  \nAzure DevOps Pipelines Reporting with Power BI  \nThe Azure DevOps Pipelines Report contains a Power BI template for monitoring project, pipeline, and pipeline run data from an Azure DevOps (AzDO) organization.  \nThis dashboard recipe provides observability for AzDO pipelines by displaying various metrics (i.e. average runtime, run outcome statistics, etc.) in a table. Additionally, the second page of the template visualizes pipeline success and failure trends using Power BI charts. Documentation and setup information can be found in the project README.  \nPython Logger class for Application Insights using OpenCensus  \nThis repository contains \"AppLogger\" class which is a python logger class for Application Insights using Opencensus. It also contains sample code that shows the usage of \"AppLogger\".  \nGitHub Repo  \nJava OpenTelemetry Examples  \nThis GitHub Repo contains a set of fully-functional, working examples of using the OpenTelemetry Java APIs and SDK.",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\recipes-observability.md"
    },
    {
        "chunkId": "chunk219_0",
        "chunkContent": "Dashboard  \nOverview  \nDashboard is a form of data visualization that provides \"at a glance\" view of Key Performance Indicators(KPIs) of observable system. Dashboard connects multiple data sources allowing creation of visual representation of data insights which otherwise are difficult to understand. Dashboard can be used to:  \nshow trends  \nidentify patterns(user, usage, search etc)  \nmeasure efficiency easily  \nidentify data outliers and correlations  \nview health state or performance of the system  \ngive an outlook of the KPI that is important to a business/process  \nBest Practices  \nCommon questions to ask yourself when building dashboard would be:  \nWhere did my user spend most of their time at?  \nWhat is my user searching?  \nHow do I better help my team with alerts and troubleshooting?  \nIs my system healthy for the past one day/week/month/quarter?  \nHere are principles to consider when building dashboards:  \nSeparate a dashboard in multiple sections for simplicity. Adding page jump or anchor(#section) is also a plus if applicable.  \nAdd multiple and simple charts. Build simple chart, have more of them rather than a complicated all in one chart.  \nIdentify goals or KPI measurement. Identifying goals or KPI helps in defining what needs to be achieved. Here are some examples - server downtime, mean time to address error, service level agreement.  \nAsk questions that can help reach the defined goal or KPI. This may sound counter-intuitive, the more questions asked while constructing dashboard the better the outcome will be. Questions like location, internet service provider, time of day the users make requests to server would be a good start.  \nValidate the questions. This is often done with stakeholders, sponsors, leads or project managers.  \nObserve the dashboard that is built. Is the data reflecting what the stakeholders set out to answer?  \nAlways remember this process takes time. Building dashboard is easy, building an observable dashboard to show pattern is hard.  \nRecommended Tools  \nAzure Monitor Workbooks - Supporting markdown, Azure Workbooks is tightly integrated with Azure services making this highly customizable without extra tool.  \nCreate dashboard using log query - Dashboard can be created using log query on Log Analytics data.  \nBuilding dashboards using Application Insights - Dashboards can be created using Application Insights as well.  \nPower Bi - Power Bi is one of the easier tools to create dashboards from data sources and reports.  \nGrafana - Getting started with Grafana. Grafana is a popular open source tool for dashboarding and visualization.  \nAzure Monitor as Grafana data source - This provides a step by step integration of Azure Monitor to Grafana.  \nBrief comparison of various tools  \nDashboard Samples and Recipes  \nAzure Workbooks  \nPerformance analysis - A measurement on how the system performs. Workbook template available in gallery.  \nFailure analysis - A report about system failure with details. Workbook template available in gallery.  \nApplication Performance Index(Apdex) - This is a way to measure user satisfaction. It classifies performance into three zones based on a baseline performance threshold T. The template for Appdex is available in Azure Workbooks gallery as well.  \nApplication Insights  \nUser retention analysis  \nUser navigation patterns analysis  \nUser session analysis  \nFor other tools, these can be used as a reference to recreate if a template is not readily available.  \nGrafana with Azure Monitor as Data Source  \nAzure Kubernetes Service - Cluster & Namespace Metrics - Container Insights metrics for Kubernetes clusters. Cluster utilization, namespace utilization, Node cpu & memory, Node disk usage & disk io, node network & kubelet docker operation metrics  \nAzure Kubernetes Service - Container Level & Pod Metrics - This contains Container level and Pod Metrics like CPU and Memory which are missing in the above dashboard.  \nSummary  \nIn order to build an observable dashboard, the goal is to make use of collected metrics, logs, traces to give an insight on how the system performs, user behaves and identify patterns. There are a lot of tools and templates out there. Whichever the choice is, a good dashboard is always a dashboard that can help you answer questions about the system and user, keep track of the KPI and goal while also allowing informed business decisions to be made.",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\pillars\\dashboard.md"
    },
    {
        "chunkId": "chunk220_0",
        "chunkContent": "Logging  \nOverview  \nLogs are discrete events with the goal of helping engineers identify problem area(s) during failures.  \nCollection Methods  \nWhen it comes to log collection methods, two of the standard techniques are a direct-write, or an agent-based approach.  \nDirectly written log events are handled in-process of the particular component, usually utilizing a provided library. Azure Monitor has direct send capabilities, but it's not recommended for serious/production use. This approach has some advantages:  \nThere is no external process to configure or monitor  \nNo log file management (rolling, expiring) to prevent out of disk space issues.  \nThe potential trade-offs of this approach:  \nPotentially higher memory usage if the particular library is using a memory backed buffer.  \nIn the event of an extended service outage, log data may get dropped or truncated due to buffer constraints.  \nMultiple component process logging will manage & emit logs individually, which can be more complex to manage for the outbound load.  \nAgent-based log collection relies on an external process running on the host machine, with the particular component emitting log data stdout or file. Writing log data to stdout is the preferred practice when running applications within a container environment like Kubernetes. The container runtime redirects the output to files, which can then be processed by an agent. Azure Monitor, Grafana Loki Elastic's Logstash and Fluent Bit are examples of log shipping agents.  \nThere are several advantages when using an agent to collect & ship log files:  \nCentralized configuration.  \nCollecting multiple sources of data with a single process.  \nLocal pre-processing & filtering of log data before sending it to a central service.  \nUtilizing disk space as a data buffer during a service disruption.  \nThis approach isn't without trade-offs:  \nRequired exclusive CPU & memory resources for the processing of log data.  \nPersistent disk space for buffering.  \nBest Practices  \nPay attention to logging levels. Logging too much will increase costs and decrease application throughput.  \nEnsure logging configuration can be modified without code changes. Ideally, make it changeable without application restarts.  \nIf available, take advantage of logging levels per category allowing granular logging configuration.  \nCheck for log levels before logging, thus avoiding allocations and string manipulation costs.  \nEnsure service versions are included in logs to be able to identify problematic releases.  \nLog a raised exception only once. In your handlers, only catch expected exceptions that you can handle gracefully (even with a specific return code). If you want to log and rethrow, leave it to the top level exception handler. Do the minimal amount of cleanup work needed then throw to maintain the original stack trace. Don\u2019t log a warning or stack trace for expected exceptions (eg: properly expected 404, 403 HTTP statuses).  \nFine tune logging levels in production (>= warning for instance). During a new release the verbosity can be increased to facilitate bug identification.  \nIf using sampling, implement this at the service level rather than defining it in the logging system. This way we have control over what gets logged. An additional benefit is reduced number of roundtrips.  \nOnly include failures from health checks and non-business driven requests.  \nEnsure a downstream system malfunction won't cause repetitive logs being stored.  \nDon't reinvent the wheel, use existing tools to collect and analyze the data.  \nEnsure personal identifiable information policies and restrictions are followed.  \nEnsure errors and exceptions in dependent services are captured and logged. For example, if an application uses Redis cache, Service Bus or any other service, any errors/exceptions raised while accessing these services should be captured and logged.  \nIf there's sufficient log data, is there a need for instrumenting metrics?  \nLogs vs Metrics vs Traces covers some high level guidance on when to utilize metric data and when to use log data. Both have a valuable part to play in creating observable systems.  \nHaving problems identifying what to log?  \nAt application startup:  \nUnrecoverable errors from startup.  \nWarnings if application still runnable, but not as expected (i.e. not providing blob connection string, thus resorting to local files. Another example is if there's a need to fail back to a secondary service or a known good state, because it didn\u2019t get an answer from a primary dependency.)  \nInformation about the service\u2019s state at startup (build #, configs loaded, etc.)  \nPer incoming request:  \nBasic information for each incoming request: the url (scrubbed of any personally identifying data, a.k.a. PII), any user/tenant/request dimensions, response code returned, request-to-response latency, payload size, record counts, etc. (whatever you need to learn something from the aggregate data)  \nWarning for any unexpected exceptions, caught only at the top controller/interceptor and logged with or alongside the request info, with stack trace. Return a 500. This code doesn\u2019t know what happened.  \nPer outgoing request:  \nBasic information for each outgoing request: the url (scrubbed of any personally identifying data, a.k.a. PII), any user/tenant/request dimensions, response code returned, request-to-response latency, payload sizes, record counts returned, etc. Report perceived availability and latency of dependencies and including slicing/clustering data that could help with later analysis.  \nRecommended Tools  \nAzure Monitor - Umbrella of services including system metrics, log analytics and more.  \nGrafana Loki - An open source log aggregation platform, built on the learnings from the Prometheus Community for highly efficient collection & storage of log data at scale.  \nThe Elastic Stack - An open source log analytics tech stack utilizing Logstash, Beats, Elastic search and Kibana.  \nGrafana - Open source dashboard & visualization tool. Supports Log, Metrics and Distributed tracing data sources.",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\pillars\\logging.md"
    },
    {
        "chunkId": "chunk221_0",
        "chunkContent": "Metrics  \nOverview  \nMetrics provide a near real-time stream of data, informing operators and stakeholders about the functions the system is performing as well as its health. Unlike logging and tracing, metric data tends to be more efficient to transmit and store.  \nCollection Methods  \nMetric collection approaches fall into two broad categories: push metrics & pull metrics. Push metrics means that the originating component sends the data to a remote service or agent. Azure Monitor and Etsy's statsd are examples of push metrics. Some strengths with push metrics include:  \nOnly require network egress to the remote target.  \nOriginating component controls the frequency of measurement.  \nSimplified configuration as the component only needs to know the destination of where to send data.  \nSome trade-offs with this approach:  \nAt scale, it is much more difficult to control data transmission rates, which can cause service throttling or dropping of values.  \nDetermining if every component, particularly in a dynamic scale environment, is healthy and sending data is difficult.  \nIn the case of pull metrics, each originating component publishes an endpoint for the metric agent to connect to and gather measurements. Prometheus and its ecosystem of tools are an example of pull style metrics. Benefits experienced using a pull metrics setup may involve:  \nSingular configuration for determining what is measured and the frequency of measurement for the local environment.  \nEvery measurement target has a meta metric related to if the collection is successful or not, which can be used as a general health check.  \nSupport for routing, filtering and processing of metrics before sending them onto a globally central metrics store.  \nItems of concern to some may include:  \nConfiguring & managing data sources can lead to a complex configuration. Prometheus has tooling to auto-discover and configure data sources in some environments, such as Kubernetes, but there are always exceptions to this, which lead to configuration complexity.  \nNetwork configuration may add further complexity if firewalls and other ACLs need to be managed to allow connectivity.  \nBest Practices  \nWhen should I use metrics instead of logs?  \nLogs vs Metrics vs Traces covers some high level guidance on when to utilize metric data and when to use log data. Both have a valuable part to play in creating observable systems.  \nWhat should be tracked?  \nSystem critical measurements that relate to the application/machine health, which are usually excellent alert candidates. Work with your engineering and devops peers to identify the metrics, but they may include:  \nCPU and memory utilization.  \nRequest rate.  \nQueue length.  \nUnexpected exception count.  \nDependent service metrics like response time for Redis cache, Sql server or Service bus.  \nImportant business-related measurements, which drive reporting to stakeholders. Consult with the various stakeholders of the component, but some examples may include:  \nJobs performed.  \nUser Session length.  \nGames played.  \nSite visits.  \nDimension Labels  \nModern metric systems today usually define a single time series metric as the combination of the name of the metric and its dictionary of dimension labels. Labels are an excellent way to distinguish one instance of a metric, from another while still allowing for aggregation and other operations to be performed on the set for analysis. Some common labels used in metrics may include:  \nContainer Name  \nHost name  \nCode Version  \nKubernetes cluster name  \nAzure Region  \nNote: Since dimension labels are used for aggregations and grouping operations, do not use unique strings or those with high cardinality as the value of a label. The value of the label is significantly diminished for reporting and in many cases has a negative performance hit on the metric system used to track it.  \nRecommended Tools  \nAzure Monitor - Umbrella of services including system metrics, log analytics and more.  \nPrometheus - A real-time monitoring & alerting application. It's exposition format for exposing time-series is the basis for OpenMetrics's standard format.  \nThanos - Open source, highly available Prometheus setup with long term storage capabilities.  \nCortex - Horizontally scalable, highly available, multi-tenant, long term Prometheus.  \nGrafana - Open source dashboard & visualization tool. Supports Log, Metrics and Distributed tracing data sources.",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\pillars\\metrics.md"
    },
    {
        "chunkId": "chunk222_0",
        "chunkContent": "Pillars  \nLogging  \nMetrics  \nTracing  \nDashboards",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\pillars\\README.md"
    },
    {
        "chunkId": "chunk223_0",
        "chunkContent": "Tracing  \nOverview  \nProduces the information required to observe series of correlated operations in a distributed system. Once collected they show the path, measurements and faults in an end-to-end transaction.  \nBest Practices  \nEnsure that at least key business transactions are traced.  \nInclude in each trace necessary information to identify software releases (i.e. service name, version). This is important to correlate deployments and system degradation.  \nEnsure dependencies are included in trace (databases, I/O).  \nIf costs are a concern use sampling, avoiding throwing away errors, unexpected behavior and critical information.  \nDon't reinvent the wheel, use existing tools to collect and analyze the data.  \nEnsure personal identifiable information policies and restrictions are followed.  \nRecommended Tools  \nAzure Monitor - Umbrella of services including system metrics, log analytics and more.  \nJaeger Tracing - Open source, end-to-end distributed tracing.  \nGrafana - Open source dashboard & visualization tool. Supports Log, Metrics and Distributed tracing data sources.  \nConsider using OpenTelemetry as it implements open-source cross-platform context propagation for end-to-end distributed transactions over heterogeneous components out-of-the-box. It takes care of automatically creating and managing the Trace Context object among a full stack of microservices implemented across different technical stacks.",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\pillars\\tracing.md"
    },
    {
        "chunkId": "chunk224_0",
        "chunkContent": "Kubernetes UI Dashboards  \nThis document covers the options and benefits of various Kubernetes UI Dashboards which are useful tools for monitoring and debugging your application on Kubernetes Clusters. It allows the management of applications running in the cluster, debug them and manage the cluster all through these dashboards.  \nOverview and Background  \nThere are times when not all solutions can be run locally. This limitation could be due to a cloud service which does not offer a robust or efficient way to locally debug the environment. In these cases, it is necessary to use other tools which provide the capabilities to monitor your application with Kubernetes.  \nAdvantages and Use Cases  \nAllows the ability to view, manage and monitor the operational aspects of the Kubernetes Cluster.  \nBenefits of using a UI dashboard includes the following:  \nsee an overview of the cluster  \ndeploy applications onto the cluster  \ntroubleshoot applications running on the cluster  \nview, create, modify, and delete Kubernetes resources  \nview basic resource metrics including resource usage for Kubernetes objects  \nview and access logs  \nlive view of the pods state (e.g. started, terminating, etc)  \nDifferent dashboards may provide different functionalities, and the use case to choose a particular dashboard will depend on the requirements. For example, many dashboards provide a way to only monitor your applications on Kubernetes but do not provide a way to manage them.  \nOpen Source Dashboards  \nThere are currently several UI dashboards available to monitor your applications or manage them with Kubernetes. For example:  \nOctant  \nPrometheus and Grafana  \nKube Prometheus Stack Chart: provides an easy way to operate end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator.  \nK8Dash  \nkube-ops-view: a tool to visualize node occupancy & utilization  \nLens: Client side desktop tool  \nThanos and Cortex: Multi-cluster implementations  \nReferences  \nAlternatives to Kubernetes Dashboard  \nPrometheus and Grafana with Kubernetes",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\tools\\KubernetesDashboards.md"
    },
    {
        "chunkId": "chunk225_0",
        "chunkContent": "Loki  \nLoki is a horizontally-scalable, highly-available, multi-tenant log aggregation system, created by Grafana\nLabs inspired by the learnings from Prometheus. Loki is commonly referred as 'Prometheus, but for logs', which\nmakes total sense. Both tools follow the same architecture, which is an agent collecting metrics in each\nof the components of the software system, a server which stores the logs and also the Grafana dashboard, which\naccess the loki server to build its visualizations and queries. That being said, Loki has three main\ncomponents:  \nPromtail  \nIt is the agent portion of Loki. It can be used to grab logs from several places, like var/log/ for\nexample. The configuration of the Promtail is a yaml file called config-promtail.yml. In this file, its described all the paths and log sources that will be\naggregated on Loki Server.  \nLoki Server  \nLoki Server is responsible for receiving and storing all the logs received from all the different systems. The Loki Server is also\nresponsible for the queries done on Grafana, for example.  \nGrafana Dashboards  \nGrafana Dashboards are responsible for creating the visualizations and performing queries. After all, it will\nbe a web page that people with the right access can log into to see, query and create alerts for the aggregated\nlogs.  \nWhy use Loki  \nThe main reason to use Loki instead of other log aggregation tools, is that Loki optimizes the necessary\nstorage. It does that by following the same pattern as prometheus, which index the labels and make chunks\nof the log itself, using less space than just storing the raw logs.  \nReferences  \nLoki Official Site  \nInserting logs into Loki  \nAdding Loki Source to Grafana  \nLoki Best Practices",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\tools\\loki.md"
    },
    {
        "chunkId": "chunk226_0",
        "chunkContent": "Open Telemetry  \nBuilding observable systems enable one to measure how well or bad the application is behaving and WHY it is behaving either way. Adopting open-source standards related to implementing telemetry and tracing features built on top of the OpenTelemetry framework helps decouple vendor-specific implementations while maintaining an extensible, standard, and portable open-source solution.  \nOpenTelemetry is an open-source observability standard that defines how to generate, collect and describe telemetry in distributed systems. OpenTelemetry also provides a single-point distribution of a set of APIs, SDKs, and instrumentation libraries that implements the open-source standard, which can collect, process, and orchestrate telemetry data (signals) like traces, metrics, and logs. It supports multiple popular languages (Java, .NET, Python, JavaScript, Golang, Erlang, etc.). Open telemetry follows a vendor-agnostic and standards-based approach for collecting and managing telemetry data. An important point to note is that OpenTelemetry does not have its own backend; all telemetry collected by OpenTelemetry Collector must be sent to a backend like Prometheus, Jaeger, Zipkin, Azure Monitor, etc. Open telemetry is also the 2nd most active CNCF project only after Kubernetes.  \nThe main two Problems OpenTelemetry solves are: First, vendor neutrality for tracing, monitoring, and logging APIs and second, out-of-the-box cross-platform context propagation implementation for end-to-end distributed tracing over heterogeneous components.  \nOpen Telemetry Core Concepts  \nOpen Telemetry Implementation Patterns  \nA detailed explanation of OpenTelemetry concepts is out of the scope of this repo. There is plenty of available information about how the SDK and the automatic instrumentation are configured and how the Exporters, Tracers, Context, and Span's hierarchy work. See the Reference section for valuable OpenTelemetry resources.  \nHowever, understanding the core implementation patterns will help you know what approach better fits the scenario you are trying to solve. These are three main patterns as follows:  \nAutomatic telemetry: Support for automatic instrumentation is available for some languages. OpenTelemetry automatic instrumentation (100% codeless) is typically done through library hooks or monkey-patching library code. Automatic instrumentation will intercept all interactions and dependencies and automatically send the telemetry to the configured exporters. More information about this concept can be found in the OpenTelemetry instrumentation doc.  \nManual tracing: This must be done by coding using the OpenTelemetry SDK, managing the tracer objects to obtain Spans, and forming instrumented OpenTelemetry Scopes to identify the code segments to be manually traced. Also, by using the @WithSpan annotations (method decorations in C# and Java) to mark whole methods that will be automatically traced.  \nHybrid approach: Most Production-ready scenarios will require a mix of both techniques, using the automatic instrumentation to collect automatic telemetry and the OpenTelemetry SDK to identify code segments that are important to instrument manually. When considering production-ready scenarios, the hybrid approach is the way to go as it allows for a throughout cover over the whole solution. It provides automatic context propagation and events correlation out of the box.  \nCollector  \nThe collector is a separate process that is designed to be a \u2018sink\u2019 for telemetry data emitted by many processes, which can then export that data to backend systems. The collector has two different deployment strategies \u2013 either running as an agent alongside a service or as a gateway which is a remote application. In general, using both is recommended: the agent would be deployed with your service and run as a separate process or in a sidecar; meanwhile, the collector would be deployed separately, as its own application running in a container or virtual machine. Each agent would forward telemetry data to the collector, which could then export it to a variety of backend systems such as Lightstep, Jaeger, or Prometheus. The agent can be also replaced with the automatic instrumentation if supported. The automatic instrumentation provides the collector capabilities of retrieving, processing and exporting the telemetry.  \nRegardless of how you choose to instrument or deploy OpenTelemetry, exporters provide powerful options for reporting telemetry data. You can directly export from your service, you can proxy through the collector, or you can aggregate into standalone collectors \u2013 or even a mix of these.  \nInstrumentation Libraries  \nA library that enables observability for another library is called an instrumentation library. OpenTelemetry libraries are language specific, currently there is good support for Java, Python, Javascript, dotnet and golang. Support for automatic instrumentation is available for some libraries which make using OpenTelemetry easy and trivial. In case automatic instrumentation is not available, manual instrumentation can be configured by using the OpenTelemetry SDK.  \nIntegration of OpenTelemetry  \nOpenTelemetry can be used to collect, process and export data into multiple backends, some popular integrations supported with OpenTelemetry are:  \nZipkin  \nPrometheus  \nJaeger  \nNew Relic  \nAzure Monitor  \nAWS X-Ray  \nDatadog  \nKafka  \nLightstep  \nSplunk  \nGCP Monitor  \nWhy use OpenTelemetry  \nThe main reason to use OpenTelemetry is that it offers an open-source standard for implementing distributed telemetry (context propagation) over heterogeneous systems. There is no need to reinvent the wheel to implement end-to-end business flow transactions monitoring when using OpenTelemetry.  \nIt enables tracing, metrics, and logging telemetry through a set of single-distribution multi-language libraries and tools that allow for a plug-and-play telemetry architecture that includes the concept of agents and collectors.  \nMoreover, avoiding any proprietary lock down and achieving vendor-agnostic neutrality for tracing, monitoring, and logging APIs AND backends allow maximum portability and extensibility patterns.  \nAnother good reason to use OpenTelemetry would be whether the stack uses OpenCensus or OpenTracing. As OpenCensus and OpenTracing have carved the way for OpenTelemetry, it makes sense to introduce OpenTelemetry where OpenCensus or OpenTracing is used as it still has backward compatibility.  \nApart from adding custom attributes, sampling, collecting data for metrics and traces, OpenTelemetry is governed by specifications and backed up by big players in the Observability landscape like Microsoft, Splunk, AppDynamics, etc. OpenTelemetry will likely become a de-facto open-source standard for enabling metrics and tracing when all features become GA.  \nCurrent Status of OpenTelemetry Project  \nOpenTelemetry is a project which emerged from merging of OpenCensus and OpenTracing in 2019. Although OpenCensus and OpenTracing are frozen and no new features are being developed for them, OpenTelemetry has backward compatibility with OpenCensus and OpenTracing. Some features of OpenTelemetry are still in beta, feature support for different languages is being tracked here: Feature Status of OpenTelemetry. Status of OpenTelemetry project can be tracked here.  \nFrom the website:  \nOur goal is to provide a generally available, production quality release for the tracing data source across most OpenTelemetry components in the first half of 2021. Several components have already reached this milestone! We expect metrics to reach the same status in the second half of 2021 and are targeting logs in 2022.  \nWhat to watch out for  \nAs OpenTelemetry is a very recent project (first GA version of some features released in 2020), many features are still in beta hence due diligence needs to be done before using such features in production. Also, OpenTelemetry supports many popular languages but features in all languages are not at par. Some languages offer more features as compared to other languages. It also needs to be called out as some features are not in GA, there may be some incompatibility issues with the tooling. That being said, OpenTelemetry is one of the most active projects of CNCF, so it is expected that many more features would reach GA soon.  \nJanuary 2022 UPDATE  \nApart from the logging specification and implementation that are still marked as draft or beta, all other specifications and implementations regarding tracing and metrics are marked as stable or feature-freeze. Many libraries are still on active development whatsoever, so thorough analysis has to be made depending on the language on a feature basis.  \nIntegration Options with Azure Monitor  \nUsing the Azure Monitor OpenTelemetry Exporter Library  \nThis scenario uses the OpenTelemetry SDK as the core instrumentation library. Basically this means you will instrument your application using the OpenTelemetry libraries, but you will additionally use the Azure Monitor OpenTelemetry Exporter and then added it as an additional exporter with the OpenTelemetry SDK. In this way, the OpenTelemetry traces your application creates will be pushed to your Azure Monitor Instance.  \nUsing the Application Insights Agent Jar file - Java only  \nJava OpenTelemetry instrumentation provides another way to integrate with Azure Monitor, by using Applications Insights Java Agent jar.  \nWhen configuring this option, the Applications Insights Agent file is added when executing the application. The applicationinsights.json configuration file must be also be added as part of the applications artifacts. Pay close attention to the preview section, where the \"openTelemetryApiSupport\": true, property is set to true, enabling the agent to intercept OpenTelemetry telemetry created in the application code pushing it to Azure Monitor.  \nOpenTelemetry Java Agent instrumentation supports many libraries and frameworks and application servers. Application Insights Java Agent enhances this list.\nTherefore, the main difference between running the OpenTelemetry Java Agent vs. the Application Insights Java Agent is demonstrated in the amount of traces getting logged in Azure Monitor. When running with Application Insights Java agent there's more telemetry getting pushed to Azure Monitor. On the other hand, when running the solution using the Application Insights agent mode, it is essential to highlight that nothing gets logged on Jaeger (or any other OpenTelemetry exporter). All traces will be pushed exclusively to Azure Monitor. However, both manual instrumentation done via the OpenTelemetry SDK and all automatic traces, dependencies, performance counters, and metrics being instrumented by the Application Insights agent are sent to Azure Monitor. Although there is a rich amount of additional data automatically instrumented by the Application Insights agent, it can be deduced that it is not necessarily OpenTelemetry compliant. Only the traces logged by the manual instrumentation using the OpenTelemetry SDK are.  \nOpenTelemetry vs Application Insights agents compared  \nHighlight OpenTelemetry Agent App Insights Agent Automatic Telemetry Y Y Manual OpenTelemetry Y Y Plug and Play Exports Y N Multiple Exports Y N Full Open Telemetry layout (decoupling agents, collectors and exporters) Y N Enriched out of the box telemetry N Y Unified telemetry backend N Y  \nSummary  \nAs you may have guessed, there is no \"one size fits all\" approach when implementing OpenTelemetry with Azure Monitor as a backend. At the time of this writing, if you want to have the flexibility of having different OpenTelemetry backends, you should definitively go with the OpenTelemetry Agent, even though you'd sacrifice all automating tracing flowing to Azure Monitor.\nOn the other hand, if you want to get the best of Azure Monitor and still want to instrument your code with the OpenTelemetry SDK, you should use the Application Insights Agent and manually instrument your code with the OpenTelemetry SDK to get the best of both worlds.\nEither way, instrumenting your code with OpenTelemetry seems the right approach as the ecosystem will only get bigger, better, and more robust.  \nAdvanced topics  \nUse the Azure OpenTelemetry Tracing plugin library for Java to enable distributed tracing across Azure components through OpenTelemetry.  \nManual trace context propagation  \nThe trace context is stored in Thread-local storage. When the application flow involves multiple threads (eg. multithreaded work-queue, asynchronous processing) then the traces won't get combined into one end-to-end trace chain with automatic context propagation.\nTo achieve that you need to manually propagate the trace context (example in Java) by storing the trace headers along with the work-queue item.  \nTelemetry testing  \nMission critical telemetry data should be covered by testing. You can cover telemetry by tests by mocking the telemetry collector web server. In automated testing environment the telemetry instrumentation can be configured to use OTLP exporter and point the OTLP exporter endpoint\nto the collector web server. Using mocking servers libraries (eg. MockServer or WireMock) can help verify the telemetry data pushed to the collector.  \nReferences  \nOpenTelemetry Official Site  \nGetting Started with dotnet and OpenTelemetry  \nUsing OpenTelemetry Collector  \nOpenTelemetry Java SDK  \nManual Instrumentation  \nOpenTelemetry Instrumentation Agent for Java  \nApplication Insights Java Agent  \nAzure Monitor OpenTelemetry Exporter client library for Java  \nAzure OpenTelemetry Tracing plugin library for Java  \nApplication Insights Agent's OpenTelemetry configuration",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\tools\\OpenTelemetry.md"
    },
    {
        "chunkId": "chunk227_0",
        "chunkContent": "Prometheus  \nOverview  \nOriginally built at SoundCloud, Prometheus is an open-source monitoring and alerting toolkit based on time series metrics data. It has become a de facto standard metrics solution in the Cloud Native world and widely used with Kubernetes.  \nclient libraries for programming languages to extend the functionalities of Prometheus beyond the basics.\nThe client libraries offer four  \nmetric types:  \nWhy Prometheus?  \nPrometheus is a time series database and allow for events or measurements to be tracked, monitored, and aggregated over time.  \nPrometheus is a pull-based tool. One of the biggest advantages of Prometheus over other monitoring tools is that Prometheus actively scrapes targets in order to retrieve metrics from them. Prometheus also supports the push model for pushing metrics.  \nPrometheus allows for control over how to scrape, and how often to scrape them. Through the Prometheus server, there can be multiple scrape configurations, allowing for multiple rates for different targets.  \nSimilar to Grafana, visualization for the time series can be directly done through the Prometheus Web UI. The Web UI provides the ability to easily filter and have an overview of what is taking place with your different targets.  \nPrometheus provides a powerful functional query language called PromQL (Prometheus Query Language) that lets the user aggregate time series data in real time.  \nIntegration with Other Tools  \nPrometheus client libraries currently are  \nPrometheus' metrics format is supported by a wide array of tools and services including:  \nAzure Monitor  \nStackdriver  \nDatadog  \nCloudWatch  \nNew Relic  \nFlagger  \nGrafana  \nGitLab  \netc...  \nThere are numerous exporters which are used in exporting existing metrics from third-party databases, hardware, CI/CD tools, messaging systems, APIs and other monitoring systems. In addition to client libraries and exporters, there is a significant number of integration points for service discovery, remote storage, alerts and management.  \nReferences  \nPrometheus Docs  \nPrometheus Best Practices  \nGrafana with Prometheus",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\tools\\Prometheus.md"
    },
    {
        "chunkId": "chunk228_0",
        "chunkContent": "Tools and Patterns  \nThere are a number of modern tools to make systems observable. While identifying and/or creating tools that work for your system, here are a few things to consider to help guide the choices.  \nMust be simple to integrate and easy to use.  \nIt must be possible to aggregate and visualize data.  \nTools must provide real-time data.  \nMust be able to guide users to the problem area with suitable, adequate end-to-end context.  \nChoices  \nLoki  \nOpenTelemetry  \nKubernetes Dashboards  \nPrometheus  \nService Mesh  \nLeveraging a Service Mesh that follows the Sidecar Pattern quickly sets up a go-to set of metrics, and traces (although traces need to be propagated from incoming requests to outgoing requests manually).  \nA sidecar works by intercepting all incoming and outgoing traffic to your image. It then adds trace headers to each request and emits a standard set of logs and metrics. These metrics are extremely powerful for observability, allowing every service, whether client-side or server-side, to leverage a unified set of metrics, including:  \nLatency  \nBytes  \nRequest Rate  \nError Rate  \nIn a microservice architecture, pinpointing the root cause of a spike in 500's can be non-trivial, but with the added observability from a sidecar you can quickly determine which service in your service mesh resulted in the spike in errors.  \nService Mesh's have a large surface area for configuration, and can seem like a daunting undertaking to deploy. However, most services (including Linkerd) offer a sane set of defaults, and can be deployed via the happy path to quickly land these observability wins.",
        "source": "..\\data\\docs\\code-with-engineering\\observability\\tools\\README.md"
    },
    {
        "chunkId": "chunk229_0",
        "chunkContent": "Privacy and Data  \nGoal  \nThe goal of this section is to briefly describe best practices in privacy fundamentals for data heavy projects or portions of a project that may contain data.  \nWhat it is not: This document is not a checklist for how customers or readers should handle data in their environment, and does not override Microsoft's or the customers' policies for data handling, data protection and information security.  \nIntroduction  \nMicrosoft runs on trust. Our customers trust ISE to adhere to the highest standards when handling their data.\nProtecting our customers' data is a joint responsibility between Microsoft and the customers;\nboth have the responsibility to help projects follow the guidelines outlined on this page.  \nDevelopers working on ISE projects should implement best practices and guidance on handling data throughout the project phases. This page is not meant to suggest how customers should handle data in their environment. It does not override:  \nMicrosoft's Information Security Policy  \nLimited Data Protection Addendum  \nProfessional Services Data Protection Addendum  \n5 W's of data handling  \nWhen working on an engagement it is important to address the following 5 W's:  \nWho \u2013 gets access to and with whom will we share the data and/or models developed with the data?  \nWhat \u2013 data is shared with us and under what expectations and understanding.\nCustomers need to be explicit about how the data they share applies to the overarching effort.\nThe understanding shouldn't be vague and we shouldn't have access to broad set of data if not necessary.  \nWhere \u2013 will the data be stored and what legal jurisdiction will preside over that data.\nThis is particularly important in countries like Germany, where different privacy laws apply\nbut also important when it comes to responding to legal subpoenas for the data.  \nWhen \u2013 will the access to data be provided and for how long?\nIt is important to not leave straggling access to data once the engagement is completed, and define a priori the data retention policies.  \nWhy \u2013 have you given access to the data?\nThis is particularly important to clarify the purpose and any restrictions on usage beyond the intended purpose.  \nPlease use the above guidelines to ensure the data is used only for intended purposes and thereby gain trust.\nIt is important to be aware of data handling best practices and ensure the required clarity is provided to adhere to the above 5Ws.  \nHandling data in ISE engagements  \nData should never leave customer-controlled environments and contractors and/or other members in the engagement\nshould never have access to complete customer data sets but use limited customer data sets using the following prioritized approaches:  \nContractors or engagement partners do not work directly with production data, data will be copied before processing per the guidelines below.  \nAlways apply data minimization\nprinciples to minimize the blast radius of errors, only work with the minimal data set required to achieve the goals.  \nGenerate synthetic data to support engagement work. If synthetic data is not possible to achieve project goals,\nrequest anonymized data in which the likelihood that unique individuals can be re-identified is minimal.  \nSelect a suitably diverse, limited data set, again,\nfollow the Principles of Data Minimization and attempt to work with the fewest rows possible to achieve the goals.  \nBefore work begins on data, ensure OS patches are up to date and permissions are properly set with no open internet access.  \nDevelopers working on ISE projects will work with our customers to define the data needed for each engagement.  \nIf there is a need to access production data,\nISE needs to review the need with their lead and work with the customer to put audits in place verifying what data was accessed.  \nProduction data must only be shared with approved members of the engagement team and must not be processed/transferred outside of the customer controlled environment.  \nCustomers should provide ISE with a copy of the requested data in a location managed by the customer.\nThe customer should consider turning any logging capabilities on so they can clearly identify who has access and what they do with that access.\nISE should notify the customer when they are done with the data and suggest the customer destroy copies of the data if they are no longer needed.  \nOur guiding principles when handling data in an engagement  \nNever directly access production data.  \nExplicitly state the intended purpose of data that can be used for engagement.  \nOnly share copies of the production data with the approved members of the engagement team.  \nThe entire team should work together to ensure that there are no dead copies of data. When the data is no longer needed,\nthe team should promptly work to clean up engagement copies of data.  \nDo not send any copies of the production data outside the customer-controlled environment.  \nOnly use the minimal subset of the data needed for the purpose of the engagement.  \nQuestions to consider when working with data  \nWhat data do we need?  \nWhat is the legal basis for processing this data?  \nIf we are the processor based on contract obligation what is our responsibility listed in the contract?  \nDoes the contract need to be amended?  \nHow can we contain data proliferation?  \nWhat security controls are in place to protect this data?  \nWhat is the data breech protocol?  \nHow does this data benefit the data subject?  \nWhat is the lifespan of this data?  \nDo we need to keep this data linked to a data subject?  \nCan we turn this data into Not in a Position to Identify (NPI) data to be used later on?  \nHow is the system architected so data subject rights can be fulfilled? (ex manually, automated)  \nIf personal data is involved have engaged privacy and legal teams for this project?  \nSummary  \nIt is important to only pull in data that is needed for the problem at hand,\nwhen this is put in practice we find that we only maintain data that is adequate,\nrelevant and limited to what is necessary in relation to the purposes for which they are processed.\nThis is particularly important for personal data. Once you have personal data there are many rules and regulations that apply,\nsome examples of these might be HIPPA, GDPR, CCPA.\nThe customer should be aware of and surface any applicable regulations that apply to their data.\nFurthermore the seven principles of privacy by design\nshould be reviewed and considered when handling any type of sensitive data.  \nResources  \nMicrosoft Trust Center  \nTools for responsible AI - Protect  \nData Protection Resources  \nFAQ and White Papers  \nMicrosoft Compliance Offerings  \nAccountability Readiness Checklists  \nPrivacy by Design The 7 Foundational Principles",
        "source": "..\\data\\docs\\code-with-engineering\\privacy\\data-handling.md"
    },
    {
        "chunkId": "chunk230_0",
        "chunkContent": "Privacy related frameworks  \nThe following tools/frameworks could be leveraged when data analysis or model development needs to take place on private data.\nNote that the use of such frameworks still requires the solution to adhere to privacy regulations and others, and additional safeguards should be applied.  \nTypical scenarios for leveraging a Privacy framework  \nSharing data or results while preserving data subjects' privacy  \nPerforming analysis or statistical modeling on private data  \nDeveloping privacy preserving ML models and data pipelines  \nPrivacy frameworks  \nProtecting private data involves the entire data lifecycle, from acquisition, through storage, processing, analysis, modeling and usage in reports or machine learning models. Proper safeguards and restrictions should be applied in each of these phases.  \nIn this section we provide a non-exhaustive list of privacy frameworks which can be leveraged for protecting and preserving privacy.  \nWe focus on four main use cases in the data lifecycle:  \nObtaining non-sensitive data  \nEstablishing trusted research and modeling environments  \nCreating privacy preserving data and ML pipelines  \nData loss prevention  \nObtaining non-sensitive data  \nIn many scenarios, analysts, researchers and data scientists require access to a non-sensitive version or sample of the private data.\nIn this section we focus on two approaches for obtaining non-sensitive data.  \nNote: These two approaches do not guarantee that the outcome would not include private data, and additional measures should be applied.  \nData de-identification  \nDe-identification is the process of applying a set of transformations to a dataset,\nin order to lower the risk of unintended disclosure of personal data.\nDe-identification involves the removal or substitution of both direct identifiers (such as name, or social security number) or quasi-identifiers,\nwhich can be used for re-identification using additional external information.  \nDe-identification can be applied to different types of data, such as structured data, images and text.\nHowever, de-identification of non-structured data often involves statistical approaches which might result in undetected PII (Personal Identifiable Information) or non-private information being redacted or replaced.  \nHere we outline several de-identification solutions available as open source:  \nSolution Notes Presidio Presidio helps to ensure sensitive data is properly managed and governed. It provides fast identification and anonymization modules for private entities in text such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more in unstructured text and images. It's useful when high customization is required, for example to detect custom PII entities or languages. Link to repo , link to docs , link to demo . FHIR tools for anonymization FHIR Tools for Anonymization is an open-source project that helps anonymize healthcare FHIR data (FHIR=Fast Healthcare Interoperability Resources, a standard for exchanging Electric Health Records), on-premises or in the cloud, for secondary usage such as research, public health, and more. Link . Works with FHIR format (Stu3 and R4), allows different strategies for anonymization (date shift, crypto-hash, encrypt, substitute, perturb, generalize) ARX Anonymization using statistical models, specifically k-anonymity, \u2113-diversity, t-closeness and \u03b4-presence. Useful for validating the anonymization of aggregated data. Links: Repo , Website . Written in Java. k-Anonymity GitHub repo with examples on how to produce k-anonymous datasets. k-anonymity protects the privacy of individual persons by pooling their attributes into groups of at least k people. repo  \nSynthetic data generation  \nA synthetic dataset is a repository of data generated from actual data and has the same statistical properties as the real data.\nThe degree to which a synthetic dataset is an accurate proxy for real data is a measure of utility.\nThe potential benefit of such synthetic datasets is for sensitive applications \u2013 medical classifications or financial modelling, where getting hands on a high-quality labelled dataset is often prohibitive.  \nWhen determining the best method for creating synthetic data, it is essential first to consider what type of synthetic data you aim to have. There are two broad categories to choose from, each with different benefits and drawbacks:  \nFully synthetic: This data does not contain any original data, which means that re-identification of any single unit is almost impossible, and all variables are still fully available.  \nPartially synthetic: Only sensitive data is replaced with synthetic data, which requires a heavy dependency on the imputation model. This leads to decreased model dependence but does mean that some disclosure is possible due to the actual values within the dataset.  \nSolution Notes Synthea Synthea was developed with numerous data sources collected on the internet, including US Census Bureau demographics, Centers for Disease Control and Prevention prevalence and incidence rates, and National Institutes of Health reports. The source code and disease models include annotations and citations for all data, statistics, and treatments. These models of diseases and treatments interact appropriately with the health record. PII dataset generator A synthetic data generator developed on top of Fake Name Generator which takes a text file with templates (e.g. my name is PERSON ) and creates a list of Input Samples which contain fake PII entities instead of placeholders. CheckList CheckList provides a framework for perturbation techniques to evaluate specific behavioral capabilities of NLP models systematically Mimesis Mimesis a high-performance fake data generator for Python, which provides data for a variety of purposes in a variety of languages. Faker Faker is a Python package that generates fake data for you. Whether you need to bootstrap your database, create good-looking XML documents, fill-in your persistence to stress test it, or anonymize data taken from a production service, Faker is for you. Plaitpy The idea behind plait.py is that it should be easy to model fake data that has an interesting shape. Currently, many fake data generators model their data as a collection of IID variables; with plait.py we can stitch together those variables into a more coherent model.  \nTrusted research and modeling environments  \nTrusted research environments  \nTrusted Research Environments (TREs) enable organizations to create secure workspaces for analysts,\ndata scientists and researchers who require access to sensitive data.  \nTREs enforce a secure boundary around distinct workspaces to enable information governance controls.\nEach workspace is accessible by a set of authorized users, prevents the exfiltration of sensitive data,\nand has access to one or more datasets provided by the data platform.  \nWe highlight several alternatives for Trusted Research Environments:  \nSolution Notes Azure Trusted Research Environment An Open Source TRE for Azure. Aridhia DRE  \nEyes-off machine learning  \nIn certain situations, Data Scientists may need to train models on data they are not allowed to see. In these cases, an \"eyes-off\" approach is recommended.\nAn eyes-off approach provides a data scientist with an environment in which scripts can be run on the data but direct access to samples is not allowed.\nWhen using Azure ML, tools such as the Identity Based Data Access can enable this scenario,\nalongside proper role assignment for users.  \nDuring the processing within the eyes-off environment, only certain outputs (e.g. logs) are allowed to be extracted back to the user.\nFor example, a user would be able to submit a script which trains a model and inspect the model's performance, but would not be able to see on which samples the model predicted the wrong output.  \nIn addition to the eyes-off environment, this approach usually entails providing access to an \"eyes-on\" dataset, which is a representative, cleansed, sample set of data for model design purposes.\nThe Eyes-on dataset is often a de-identified subset of the private dataset, or a synthetic dataset generated based on the characteristics of the private dataset.  \nPrivate data sharing platforms  \nVarious tools and systems allow different parties to share data with 3rd parties while protecting private entities, and securely process data while reducing the likelihood of data exfiltration.\nThese tools include Secure Multi Party Computation (SMPC) systems,\nHomomorphic Encryption systems, Confidential Computing,\nprivate data analysis frameworks such as PySift among others.  \nPrivacy preserving data pipelines and ML  \nEven when our data is secure, private entities can still be extracted when the data is consumed.\nPrivacy preserving data pipelines and ML models focus on minimizing the risk of private data exfiltration during data querying or model predictions.  \nDifferential Privacy  \nDifferential privacy (DP) is a system that enables one to extract meaningful insights from datasets about subgroups of people, while also providing strong guarantees with regards to protecting any given individual's privacy.\nThis is typically achieved by adding a small statistical noise to every individual's information,\nthereby introducing uncertainty in the data.\nHowever, the insights gleaned still accurately represent what we intend to learn about the population in the aggregate.\nThis approach is known to be robust to re-identification attacks and data reconstruction by adversaries who possess auxiliary information.\nFor a more comprehensive overview,\ncheck out Differential privacy: A primer for a non-technical audience.  \nDP has been widely adopted in various scenarios such as learning from census data, user telemetry data analysis, audience engagement to advertisements, and health data insights where PII protection is of paramount importance. However, DP is less suitable for small datasets.  \nTools that implement DP include SmartNoise, Tensorflow Privacy among some others.  \nHomomorphic Encryption  \nHomomorphic Encryption (HE) is a form of encryption allowing one to perform calculations on encrypted data without decrypting it first.\nThe result of the computation F is in an encrypted form, which on decrypting gives us the same result if computation F was done on raw unencrypted data.\n(source)  \nHomomorphic Encryption frameworks:  \nSolution Notes Microsoft SEAL Secure Cloud Storage and Computation, ML Modeling. A widely used open-source library from Microsoft that supports the BFV and the CKKS schemes. Palisade A widely used open-source library from a consortium of DARPA-funded defense contractors that supports multiple homomorphic encryption schemes such as BGV, BFV, CKKS, TFHE and FHEW, among others, with multiparty support. Link to repo PySift Private deep learning. PySyft decouples private data from model training, using Federated Learning, Differential Privacy, and Encrypted Computation (like Multi-Party Computation (MPC) and Homomorphic Encryption (HE)) within the main Deep Learning frameworks like PyTorch and TensorFlow.  \nA list of additional OSS tools can be found here.  \nFederated learning  \nFederated learning is a Machine Learning technique which allows the training of ML models in a decentralized way without having to share the actual data.\nInstead of sending data to the processing engine of the model, the approach is to distribute the model to the different data owners and perform training in a distributed fashion.  \nFederated learning frameworks:  \nSolution Notes TensorFlow Federated Learning OSS federated learning system built on top of TensorFlow FATE An OSS federated learning system with different options for deployment and different algorithms adapted for federated learning IBM Federated Learning A Python based federated learning framework focused on enterprise environments.  \nData loss prevention  \nOrganizations have sensitive information under their control such as financial data, proprietary data, credit card numbers, health records, or social security numbers.\nTo help protect this sensitive data and reduce risk, they need a way to prevent their users from inappropriately sharing it with people who shouldn't have it.\nThis practice is called data loss prevention (DLP).  \nBelow we focus on two aspects of DLP: Sensitive data classification and Access management.  \nSensitive data classification  \nSensitive data classification is an important aspect of DLP, as it allows organizations to track, monitor, secure and identify sensitive and private data.\nFurthermore, different sensitivity levels can be applied to different data items, facilitating proper governance and cataloging.  \nThere are typically four levels data classification levels:  \nPublic  \nInternal  \nConfidential  \nRestricted / Highly confidential  \nTools for data classification on Azure:  \nSolution Notes Microsoft Information Protection (MIP) A suite for DLP, sensitive data classification, cataloging  and more. Azure Purview A unified data governance service, which includes the classification and cataloging of sensitive data. Azure Purview leverages the MIP technology for data classification and more. Data Discovery & Classification for Azure SQL Database, Azure SQL Managed Instance, and Azure Synapse Basic capabilities for discovering, classifying, labeling, and reporting the sensitive data in Azure SQL and Synapse databases. Data Discovery & Classification for SQL Server Capabilities for discovering, classifying, labeling & reporting the sensitive data in SQL Server databases.  \nOften, tools used for de-identification can also serve as sensitive data classifiers. Refer to de-identification tools for such tools.  \nAdditional resources:  \nExample guidelines for data classification  \nLearn about sensitivity levels  \nAccess management  \nAccess control is an important component of privacy by design and falls into overall data lifecycle protection.\nSuccessful access control will restrict access only to authorized individuals that should have access to data.\nOnce data is secure in an environment, it is important to review who should access this data and for what purpose.\nAccess control may be audited with a comprehensive logging strategy which may include the integration of activity logs that can provide insight into operations performed on resources in a subscription.  \nOWASP Access Control Cheat Sheet",
        "source": "..\\data\\docs\\code-with-engineering\\privacy\\privacy-frameworks.md"
    },
    {
        "chunkId": "chunk231_0",
        "chunkContent": "Privacy fundamentals  \nThis part of the engineering playbook focuses on privacy design guidelines and principles.\nPrivate data handling and protection requires both the proper design of software,\nsystems and databases, as well as the implementation of organizational processes and procedures.  \nIn general, developers working on ISE projects should adhere to Microsoft's recommended standard practices and regulations on Privacy and Data Handling.  \nThe playbook currently contains two main parts:  \nPrivacy and Data: Best practices for properly handling sensitive and private data.  \nPrivacy frameworks: A list of frameworks which could be applied in private data scenarios.",
        "source": "..\\data\\docs\\code-with-engineering\\privacy\\README.md"
    },
    {
        "chunkId": "chunk232_0",
        "chunkContent": "Reliability  \nAll the other ISE Engineering Fundamentals work towards a more reliable infrastructure. Automated integration and deployment ensures code is properly tested, and helps remove human error, while slow releases build confidence in the code. Observability helps more quickly pinpoint errors when they arise to get back to a stable state, and so on.  \nHowever, there are some additional steps we can take, that don't neatly fit into the previous categories, to help ensure a more reliable solution. We'll explore these below.  \nRemove \"Foot-Guns\"  \nPrevent your dev team from shooting themselves in the foot. People make mistakes; any mistake made in production is not the fault of that person, it's the collective fault of the system to not prevent that mistake from happening.  \nCheck out the below list for some common tooling to remove these foot guns:  \nIn Kubernetes, leverage Admission Controllers to prevent \"bad things\" from happening.  \nYou can create custom controllers using the Webhook Admission controller.  \nGatekeeper is a pre-built Webhook Admission controller, leveraging OPA underneath the hood, with support for some out-of-the-box protections  \nIf a user ever makes a mistake, don't ask: \"how could somebody possibly do that?\", do ask: \"how can we prevent this from happening in the future?\"  \nAutoscaling  \nWhenever possible, leverage autoscaling for your deployments. Vertical autoscaling can scale your VMs by tuning parameters like CPU, disk, and RAM, while horizontal autoscaling can tune the number of running images backing your deployments. Autoscaling can help your system respond to inorganic growth in traffic, and prevent failing requests due to resource starvation.  \nNote: In environments like K8s, both horizontal and vertical autoscaling are offered as a native solution. The VMs backing each Pod however, may also need autoscaling to handle an increase in the number of Pods.  \nIt should also be noted that the parameters that affect autoscaling can be difficult to tune. Typical metrics like CPU or RAM utilization, or request rate may not be enough. Sometimes you might want to consider custom metrics, like cache eviction rate.  \nLoad shedding & DOS Protection  \nOften we think of Denial of Service [DOS] attacks as an act from a malicious actor, so we place some load shedding at the gates to our system and call it a day. In reality, many DOS attacks are unintentional, and self-inflicted. A bad deployment that takes down a Cache results in hammering downstream services. Polling from a distributed system synchronizes and results in a thundering herd. A misconfiguration results in an error which triggers clients to retry uncontrollably. Requests append to a stored object until it is so big that future reads crash the server. The list goes on.  \nFollow these steps to protect yourself:  \nAdd a jitter (random) to any action that occurs from a non-user triggered flow (ie: add a random duration to the sleep in a cron, or job that continuously polls a downstream service).  \nImplement exponential backoff retry policies in your client code  \nAdd load shedding to your servers (yes, your internal microservices too).  \nThis can be configured easily when leveraging a sidecar like envoy.  \nBe careful when deserializing user requests, and use buffer limits.  \nie: HTTP/gRPC Servers can set limits on how much data will get read from the socket.  \nSet alerts for utilization, servers restarting, or going offline to detect when your system may be failing.  \nThese types of errors can result in Cascading Failures, where a non-critical portion of your system takes down the entire service. Plan accordingly, and make sure to put extra thought into how your system might degrade during failures.  \nBackup Data  \nData gets lost, corrupted, or accidentally deleted. It happens. Take data backups to help get your system back up online as soon as possible. It can happen in the application stack, with code deleting or corrupting data, or at the storage layer by losing the volumes, or losing encryption keys.  \nConsider things like:  \nHow long will it take to restore data.  \nHow much data loss can you tolerate.  \nHow long will it take you to notice there is data loss.  \nLook into the difference between snapshot and incremental backups. A good policy might be to take incremental backups on a period of N, and a snapshot backup on a period of M (where N < M).  \nTarget Uptime & Failing Gracefully  \nIt's a known fact that systems cannot target 100% uptime. There are too many factors in today's software systems to achieve this, many outside of our control. Even a service that never gets updated and is 100% bug free will fail. Upstream DNS servers have issues all the time. Hardware breaks. Power outages, backup generators fail. The world is chaotic. Good services target some number of \"9's\" of uptime. ie: 99.99% uptime means that the system has a \"budget\" of 4 minutes and 22 seconds of uptime each month. Some months might achieve 100% uptime, which means that budget gets rolled over to the next month. What uptime means is different for everybody, and up to the service to define.  \nA good practice is to use any leftover budget at the end of the period (ie: year, quarter), to intentionally take that service down, and ensure that the rest of your systems fail as expected. Often times other engineers and services come to rely on that additional achieved availability, and it can be healthy to ensure that systems fail gracefully.  \nWe can build graceful failure (or graceful degradation) into our software stack by anticipating failures. Some tactics include:  \nFailover to healthy services  \nLeader Election can be used to keep healthy services on standby in case the leader experiences issues.  \nEntire cluster failover can redirect traffic to another region or availability zone.  \nPropagate downstream failures of dependent services up the stack via health checks, so that your ingress points can re-route to healthy services.  \nCircuit breakers can bail early on requests vs. propagating errors throughout the system.\nConsider using a well-known, tested library such as Polly (.NET) that enables configurable implementations of this and other common resilience and transient fault-handling patterns.  \nPractice  \nNone of the above recommendations will work if they are not tested. Your backups are meaningless if you don't know how to mount them. Your cluster failover and other mitigations will regress over time if they are not tested. Here are some tips to test the above:  \nMaintain Playbooks  \nNo software service is complete without playbooks to navigate the developers through unfamiliar territory. Playbooks should be thorough and cover all known failure scenarios and mitigations.  \nRun maintenance exercises  \nTake the time to fabricate scenarios, and run a D&D style campaign to solve your issues. This can be as elaborate as spinning up a new environment and injecting errors, or as simple as asking the \"players\" to navigate to a dashboard and describing would they would see in the fabricated scenario (small amounts of imagination required). The playbooks should easily navigate the user to the correct solution/mitigation. If not, update your playbooks.  \nChaos Testing  \nLeverage automated chaos testing to see how things break. You can read this playbook's article on fault injection testing for more information on developing a hypothesis-driven suite of automated chaos test. The following list of chaos testing tools as well as this section in the article linked above have more details on available platforms and tooling for this purpose:  \nAzure Chaos Studio - An in-preview tool for orchestrating controlled fault injection experiments on Azure resources.  \nChaos toolkit - A declarative, modular chaos platform with many extensions, including the Azure actions and probes kit.  \nKraken - An Openshift-specific chaos tool, maintained by Redhat.  \nChaos Monkey - The Netflix platform which popularized chaos engineering (doesn't support Azure OOTB).  \nMany services meshes, like Linkerd, offer fault injection tooling through the use of their sidecars.  \nChaos Mesh  \nSimmy - A .NET library for chaos testing and fault injection integrated with the Polly library for resilience engineering.\nThis ISE dev blog post provides code snippets as an example of how to use Polly and Simmy to implement a hypothesis-driven approach to resilience and chaos testing.  \nAnalyze all Failures  \nWriting up a post-mortem is a great way to document the root causes, and action items for your failures. They're also a great way to track recurring issues, and create a strong case for prioritizing fixes.  \nThis can even be tied into your regular Agile restrospectives.",
        "source": "..\\data\\docs\\code-with-engineering\\reliability\\README.md"
    },
    {
        "chunkId": "chunk233_0",
        "chunkContent": "Contributing  \nWe love pull requests from everyone. By participating in this project, you\nagree to abide by the Microsoft Open Source Code of Conduct  \nFork, then clone the repo  \nMake sure the tests pass  \nMake your change. Add tests for your change. Make the tests pass  \nPush to your fork and submit a pull request.  \nAt this point you're waiting on us. We like to at least comment on pull requests\nwithin three business days (and, typically, one business day). We may suggest\nsome changes or improvements or alternatives.  \nSome things that will increase the chance that your pull request is accepted:  \nWrite tests.  \nFollow our engineering playbook and the style guide for this project.  \nWrite a good commit message.",
        "source": "..\\data\\docs\\code-with-engineering\\resources\\templates\\CONTRIBUTING.md"
    },
    {
        "chunkId": "chunk234_0",
        "chunkContent": "project-xyz  \nDescription of the project  \nDeploying to Azure  \nGetting started  \nDependencies  \nRun it locally  \nCode of conduct  \nBy participating in this project, you\nagree to abide by the Microsoft Open Source Code of Conduct",
        "source": "..\\data\\docs\\code-with-engineering\\resources\\templates\\README.md"
    },
    {
        "chunkId": "chunk235_0",
        "chunkContent": "Security  \nDevelopers working on projects should adhere to industry-recommended standard practices for secure design and implementation of code. For the purposes of our customers, this means our engineers should understand the OWASP Top 10 Web Application Security Risks, as well as how to mitigate as many of them as possible, using the resources below.  \nIf you are looking for a fast way to get started evaluating your application or design, check out the \"Secure Coding Practices Quick Reference\" document below, which contains an itemized checklist of high-level concepts you can validate are being done properly. This checklist covers many common errors associated with the OWASP Top 10 list linked above, and should be the minimum amount of effort being put into security.  \nRequesting Security Reviews  \nWhen requesting a security review for your application, please make sure you have familiarized yourself with the Rules of Engagement. This will help you to prepare the application for testing, as well as understand the scope limits of the test.  \nQuick References  \nSecure Coding Practices Quick Reference  \nWeb Application Security Quick Reference  \nSecurity Mindset/Creating a Security Program Quick Start  \nCredential Scanning / Secret Detection  \nThreat Modelling  \nAzure DevOps Security  \nSecurity Engineering DevSecOps Practices  \nAzure DevOps Data Protection Overview  \nSecurity and Identity in Azure DevOps  \nSecurity Code Analysis  \nDevSecOps  \nIntroduce security to your project at early stages. The DevSecOps section covers security practices, automation, tools and frameworks as part of the application CI.  \nOWASP Cheat Sheets  \nNote: OWASP is considered to be the gold-standard in computer security information. OWASP maintains an extensive series of cheat sheets which cover all the OWASP Top 10 and more. Below, many of the more relevant cheat sheets have been summarized. To view all the cheat sheets, check out their Cheat Sheet Index.  \nAccess Control Basics  \nAttack Surface Analysis  \nContent Security Policy (CSP)  \nCross-Site Request Forgery (CSRF) Prevention  \nCross-Site Scripting (XSS) Prevention  \nCryptographic Storage  \nDeserialization  \nDocker/Kubernetes (k8s) Security  \nInput Validation  \nKey Management  \nOS Command Injection Defense  \nQuery Parameterization Examples  \nServer-Side Request Forgery Prevention  \nSQL Injection Prevention  \nUnvalidated Redirects and Forwards  \nWeb Service Security  \nXML Security  \nRecommended Tools  \nCheck out the list of tools to help enable security in your projects.  \nNote: Although some tools are agnostic, the below list is geared towards Cloud Native security, with a focus on Kubernetes.  \nVulnerability Scanning  \nSonarCloud  \nIntegrates with Azure Devops with the click of a button.  \nSnyk  \nTrivy  \nCloudsploit  \nAnchore  \nOther tools from OWASP  \nSee why you should check for vulnerabilities at all layers of the stack, as well as a couple of other useful tips to reduce surface area for attacks.  \nRuntime Security  \nFalco  \nTracee  \nKubelinter  \nMay not fully qualify as runtime security, but helps ensure you're enabling best practices.  \nBinary Authorization  \nBinary authorization can happen both at the docker registry layer, and runtime (ie: via a K8s admission controller).\nThe authorization check ensures that the image is signed by a trusted authority. This can occur for both (pre-approved) 3rd party images,\nand internal images. Taking this a step further the signing should occur only on images where all code has been reviewed and approved.\nBinary authorization can both reduce the impact of damage from a compromised hosting environment, and the damage from malicious insiders.  \nHarbor\nOperator available  \nPortieris  \nNotary\nNote harbor leverages notary internally.  \nTUF  \nOther K8s Security  \nOPA, Gatekeeper, and the Gatekeeper Library  \ncert-manager for easy certificate provisioning and automatic rotation.  \nQuickly enable mTLS between your microservices with Linkerd.  \nUseful links  \nNon-Functional Requirements Guidance",
        "source": "..\\data\\docs\\code-with-engineering\\security\\README.md"
    },
    {
        "chunkId": "chunk236_0",
        "chunkContent": "Rules of Engagement  \nWhen performing application security analysis, it is expected that the tester follow the Rules of Engagement as laid out below. This is to standardize the scope of application testing and provide a concrete awareness of what is considered \"out of scope\" for security analysis.  \nRules of Engagement - For those requesting review  \nWeb Application Firewalls can be up and configured, but do not enable any automatic blocking. This can greatly slow down the person performing the test.  \nSimilarly, if a service is running on a virtual machine, ensure services such as fail2ban are disabled.  \nYou cannot make changes to the running application until the test is complete. This is to prevent accidentally breaking an otherwise valid attack in progress.  \nAny review results are not considered as \"final\". A security review should always be performed by a security team orchestrated by the customer prior to moving an application into production. If a customer requires further assistance, they can engage Premier Support.  \nRules of Engagement - For those performing tests  \nDo not attempt to perform Denial-of-Service attacks or otherwise crash services. Heavy active scanning is tolerated (and is assumed to be somewhat of a load test) but deliberate takedowns are not permitted.  \nDo not interact with human beings. Phishing credentials or other such client-side attacks are off-limits. Detailing XSS and similar attacks is encouraged as a part of the test, but do not leverage these against internal users or customers.  \nAttack from a single point. Especially if the application is currently in the customer's hands, provide the IP address or hostname of the attacking host to avoid setting off alarms.",
        "source": "..\\data\\docs\\code-with-engineering\\security\\rules-of-engagement.md"
    },
    {
        "chunkId": "chunk237_0",
        "chunkContent": "Overview  \nThis document covers the threat models for a sample project which takes video frames from video camera and process these frames on IoTEdge device and send them to Azure Cognitive Service to get the audio output.  \nThese models can be considered as reference template to show how we can construct threat modeling document. Each of the labeled entities in the figures below are accompanied by meta-information which describe the threats, recommended mitigations, and the associated  \nsecurity principle or goal.  \nArchitecture Diagram  \nAssets  \nAsset Entry Point Trust Level Azure Blob Storage Http End point Connection String Azure Monitor Http End Point Connection String Azure Cognitive Service Http End Point Connection String IoTEdge Module: M1 Http End Point Public Access (Local Area Network) IoTEdge Module: M2 Http End Point Public Access (Local Area Network) IoTEdge Module: M3 Http End Point Public Access (Local Area Network) IoTEdge Module: IoTEdgeMetricsCollector Http EndPoint Public Access (Local Area Network) Application Insights Http End Point Connection String  \nData Flow Diagram  \nClient Browser makes requests to the M1 IoTEdge module. Browser and IoTEdge device are on same network, so browser directly hits the webapp URL.  \nM1 IoTEdge module interacts with other two IoTEdge modules to render live stream from video device and display order scanning results via WebSockets.  \nIoTEdge modules interact with Azure Cognitive service to get the translated text via OCR and audio stream via Text to Speech Service.  \nIoTEdge modules send telemetry information to application insights.  \nIoTEdge device is deployed with IoTEdge runtime which interacts with IoTEdge hub for deployments.  \nIoTEdge module also sends some data to Azure storage which is required for debugging purpose.  \nCognitive service, application insights and Azure Storage are authenticated using connection strings which are stored in GitHub secrets and deployed using CI/CD pipelines.  \nThreat List  \nAssumptions  \nSecrets like ACR credentials are stored in GitHub secrets store which are deployed to IoTEdge Device by CI/CD pipelines. However, CI/CD pipelines are out of scope.  \nThreats  \nVector Threat Mitigation (1) Sniff Unencrypted data can be intercepted in transit Not Mitigated (2) Access to M1 IoT Edge Module Unauthorized Access to M1 IoT Edge Module Not Mitigated (3) Access to M2 IoT Edge Module Unauthorized Access to M2 IoT Edge Module Not Mitigated (4) Access to M3 IoT Edge Module Unauthorized Access to M3 IoT Edge Module Not Mitigated (5) Steal Storage Credentials Unauthorized Access to M2 IoTEdge Module where database secrets are used Not Mitigated (6) Denial Of Service Dos attack on all IoTEdge Modules since there is no Authentication Not Mitigated (7) Tampering with Log data Application Insights is connected via Connection String which is stored in .env file on the IoTEdge device. Once user gains access to the device, .env file can be read and attacker can tamper the log data. Not Mitigated (8) Tampering with video camera device. Video camera path is stored in .env file on the IoTEdge device. Once user gains access to the device, .env file can be read and attacker can tamper the video feed or use another video source or fake video stream. Not Mitigated (9) Spoofing Tampering Azure IoT Hub connection string is stored in .env file on IoTEdge Device. Once user gains access to the device, .env file can be read and attacker cause Dos attacks on IoTHub Not Mitigated (10) Denial of Service DDOS attack Azure Cognitive Service connection string is stored in .env file on IoTEdge Device. Once user gains access to the device, .env file can be read and attacker cause DoS attacks on Azure Cognitive Service Not Mitigated (11) Tampering with Storage Storage connection string is stored in .env file on the IoTEdge device. Once user gains access to the device, .env file can be read and attacker can tamper data on storage or read from the storage. Not Mitigated (12) Tampering with Storage Cognitive Service connection string is stored in .env file on the IoTEdge device. Once user gains access to the device, .env file can be read and attacker use cognitive service API's for his own purpose causing increase cost to use. Not Mitigated  \nThreat Model  \nThreat Properties  \nNotable Threats # Principle Threat Mitigation 1 Authenticity Since channel from browser to IoTEdge Module is not authenticated, anyone can spoof it once gains access to WiFi network. Add authentication in all IoTEdge modules. 2 Confidentiality and Integrity As a result of the vulnerability of not encrypting data, plaintext data could be intercepted during transit via a man-in-the-middle (MitM) attack. Sensitive data could be exposed or tampered with to allow further exploits. All products and services must encrypt data in transit using approved cryptographic protocols and algorithms.  Use TLS to encrypt all HTTP-based network traffic. Use other mechanisms, such as IPSec, to encrypt non-HTTP network traffic that contains customer or confidential data. Applies to data flow from browser to IoTEdge modules. 3 Confidentiality Data is a valuable target for most threat actors and attacking the data store directly, as opposed to stealing it during transit, allows data exfiltration at a much larger scale. In our scenario we are storing some data in Azure Blob containers. All customer or confidential data must be encrypted before being written to non-volatile storage media (encrypted at-rest) per the following requirements.  Use approved algorithms. This includes AES-256, AES-192, or AES-128. Encryption must be enabled before writing data to storage. Applies to all data stores on the diagram. Azure Storage encrypt data at rest by default (AES-256). 4 Confidentiality Broken or non-existent authentication mechanisms may allow attackers to gain access to confidential information. All services within the Azure Trust Boundary must authenticate all incoming requests, including requests coming from the same network. Proper authorizations should also be applied to prevent unnecessary privileges.  Whenever available, use Azure Managed Identities to authenticate services. Service Principals may be used if Managed Identities are not supported. External users or services may use UserName + Passwords, Tokens, Certificates or Connection Strings to authenticate, provided these are stored on Key Vault or any other vaulting solution. For authorization, use Azure RBAC to segregate duties and grant only the least amount of access to perform an action at a particular scope. Applies to Azure services like Azure IoTHub, Azure Cognitive Service, Azure Application Insights are authenticated using connection strings. 5 Confidentiality and Integrity A large attack surface, particularly those that are exposed on the internet, will increase the probability of a compromise Minimize the application attack surface by limiting publicly exposed services.  Use strong network controls by using virtual networks, subnets and network security groups to protect against unsolicited traffic. Use Azure Private Endpoint for Azure Storage. Applies to Azure storage. 6 Confidentiality and Integrity Browser and IoTEdge device are connected over in store WIFI network Minimize the attack on WIFI network by using secure algorithm like WPA2. Applies to connection between browser and IoTEdge devices. 7 Integrity Exploitation of insufficient logging and monitoring is the bedrock of nearly every major incident. Attackers rely on the lack of monitoring and timely response to achieve their goals without being detected. Logging of critical application events must be performed to ensure that, should a security incident occur, incident response and root-cause analysis may be done. Steps must also be taken to ensure that logs are available and cannot be overwritten or destroyed through malicious or accidental occurrences. At a minimum, the following events should be logged. Login/logout events Privilege delegation events Security validation failures (e.g. input validation or authorization check failures) Application errors and system events Application and system start-ups and shut-downs, as well as logging initialization 6 Availability Exploitation of the public endpoint by malicious actors who aim to render the service unavailable to its intended users by interrupting the service normal activity, for instance by flooding the target service with requests until normal traffic is unable to be processed (Denial of Service) Application is accessed via web app deployed as one of the IoTEdge modules on the IoTEdge device. This app can be accessed by anyone in the local area network. Hence DDoS attacks are possible if the attacker gained access to local area network. All services deployed as IoTEdge modules must use authentication. Applies to services deployed on IoTEdge device 7 Integrity Tampering with data Data at rest, in Azure Storage must be encrypted on disk. Data at rest, in Azure can be protected further by Azure Advanced Threat Protection. Data at rest, in Azure Storage and Azure monitor workspace will use Azure RBAC to segregate duties and grant only the least amount of access to perform an action at a particular scope. Data in motion between services can be encrypted in TLS 1.2 Applies to data flow between IoTEdge modules and Azure Services.  \nSecurity Principles  \nConfidentiality refers to the objective of keeping data private or secret. In practice, it\u2019s about controlling access to data to prevent unauthorized disclosure.  \nIntegrity is about ensuring that data has not been tampered with and, therefore, can be trusted. It is correct, authentic, and reliable.  \nAvailability means that networks, systems, and applications are up and running. It ensures that authorized users have timely, reliable access to resources when they are needed.",
        "source": "..\\data\\docs\\code-with-engineering\\security\\threat-modelling-example.md"
    },
    {
        "chunkId": "chunk238_0",
        "chunkContent": "Threat Modeling  \nThreat modeling is an effective way to help secure your systems, applications, networks, and services. It's a systematic approach that identifies potential threats and recommendations to help reduce risk and meet security objectives earlier in the development lifecycle.  \nThreat Modeling Phases  \nDiagram\nCapture all requirements for your system and create a data-flow diagram  \nIdentify\nApply a threat-modeling framework to the data-flow diagram and find potential security issues. Here we can use STRIDE framework to identify the threats.  \nMitigate\nDecide how to approach each issue with the appropriate combination of security controls.  \nValidate\nVerify requirements are met, issues are found, and security controls are implemented.  \nExample of these phases is covered in the  \nthreat modelling example.  \nMore details about these phases can be found at  \nThreat Modeling Security Fundamentals.  \nThreat Modeling Example  \nHere is an example of a threat modeling document which talks about the architecture and different phases involved in the threat modeling. This document can be used as reference template for creating threat modeling documents.  \nReferences  \nThreat Modeling  \nMicrosoft Threat Modeling Tool  \nSTRIDE (Threat modeling framework)",
        "source": "..\\data\\docs\\code-with-engineering\\security\\threat-modelling.md"
    },
    {
        "chunkId": "chunk239_0",
        "chunkContent": "Component Versioning  \nGoal  \nLarger applications consist of multiple components that reference each other and rely on compatibility of the interfaces/contracts of the components.  \nTo achieve the goal of loosely coupled applications, each component should be versioned independently hence allowing developers to detect breaking changes or seamless updates just by looking at the version number.  \nVersion Numbers and Versioning schemes  \nFor developers or other components to detect breaking changes the version number of a component is important.  \nThere is different versioning number schemes, e.g.  \nmajor.minor[.build[.revision]]  \nor  \nmajor.minor[.maintenance[.build]].  \nUpon build / CI these version numbers are being generated. During CD / release components are pushed to a component repository such as Nuget, NPM, Docker Hub where a history of different versions is being kept.  \nEach build the version number is incremented at the last digit.  \nUpdating the major / minor version indicates changes of the API / interfaces / contracts:  \nMajor Version: A breaking change  \nMinor Version: A backwards-compatible minor change  \nBuild / Revision: No API change, just a different build.  \nSemantic Versioning  \nSemantic Versioning is a versioning scheme specifying how to interpret the different version numbers. The most common format is major.minor.patch. The version number is incremented based on the following rules:  \nMajor version when you make incompatible API changes,  \nMinor version when you add functionality in a backwards-compatible manner, and  \nPatch version when you make backwards-compatible bug fixes.  \nExamples of semver version numbers:  \n1.0.0-alpha.1: +1 commit after the alpha release of 1.0.0  \n2.1.0-beta: 2.1.0 in beta branch  \n2.4.2: 2.4.2 release  \nA common practice is to determine the version number during the build process. For this the source control repository is utilized to determine the version number automatically based the source code repository.  \nThe GitVersion tool uses the git history to generate repeatable and unique version number based on  \nnumber of commits since last major or minor release  \ncommit messages  \ntags  \nbranch names  \nVersion updates happen through:  \nCommit messages or tags for Major / Minor / Revision updates.\nWhen using commit messages a convention such as Conventional Commits is recommended (see Git Guidance - Commit Message Structure)  \nBranch names (e.g. develop, release/..) for Alpha / Beta / RC  \nOtherwise: Number of commits (+12, ...)  \nSemantic Versioning within a Monorepo  \nA monorepo, short for \"monolithic repository\", is a software development practice where multiple related projects, components, or modules are stored within a single version-controlled repository as opposed to maintaining them in separate repositories.  \nChallenges with Versioning in a monorepo structure  \nVersioning in a monorepo involves making decisions about how to assign version numbers to different projects and components contained within the repository.  \nAssigning a single version number to all projects in a monorepo can lead to frequent version increments if changes in one project don't match the significance of changes in another. This might be excessive if some projects undergo rapid development while others evolve more slowly.  \nIdeally, we would want each project within the monorepo to have its own version number. Changes in one project shouldn't necessarily trigger version changes in others.\nThis strategy allows projects to evolve at their own pace, without forcing all projects to adopt the same version number. It aligns well with the differing release cadences of distinct projects.  \nsemantic-release package for versioning  \nsemantic-release simplifies the entire process of releasing a package, which encompasses tasks such as identifying the upcoming version number, producing release notes, and distributing the package. This process severs the direct link between human sentiments and version identifiers. Instead, it rigorously adheres to the Semantic Versioning standards and effectively conveys the significance of alterations to end users.  \nsemantic-release relies on commit messages to assess how codebase changes impact consumers. By adhering to structured conventions for commit messages, semantic-release autonomously identifies the subsequent semantic version, compiles a changelog, and releases the software.  \nAngular Commit Message Conventions serve as the default for semantic-release. However, the configuration options of the @semantic-release/commit-analyzer and @semantic-release/release-notes-generator plugins, including presets, can be adjusted to modify the commit message format.  \nThe table below shows which commit message gets you which release type when semantic-release runs (using the default configuration):  \nCommit message Release type fix(pencil): stop graphite breaking when too much pressure applied Patch Fix Release feat(pencil): add 'graphiteWidth' option Minor Feature Release perf(pencil): remove graphiteWidth option BREAKING CHANGE: The graphiteWidth option has been removed.  The default graphite width of 10mm is always used for performance reasons. Major Breaking Release (Note that the BREAKING CHANGE:  token must be in the footer of the commit)  \nThe inherent setup of semantic-release presumes a direct correspondence between a GitHub repository and a package. Hence changes anywhere in the project result in a version upgrade for the project.  \nThe semantic-release-monorepo tool permits the utilization of semantic-release within a solitary GitHub repository that encompasses numerous packages.  \nInstead of attributing all commits to a single package, commits are assigned to packages based on the files that a commit touched.  \nIf a commit touches a file in or below a package's root, it will be considered for that package's next release. A single commit can belong to multiple packages and may trigger the release of multiple packages.  \nIn order to avoid version collisions, generated git tags are namespaced using the given package's name: <package-name>-<version>.  \nsemantic-release configurations  \nsemantic-release\u2019s options, mode and plugins can be set via either:  \nA .releaserc file, written in YAML or JSON, with optional extensions: .yaml/.yml/.json/.js/.cjs  \nA release.config.(js|cjs) file that exports an object  \nA release key in the project's package.json file  \nHere is an example .releaserc file which contains the configuration for:  \ngit tags for the releases from different types of branches  \nAny plugins required, list of supported plugins can be found here. In this file semantic-release-monorepo plugin is extended.  \n{% raw %}  \njson\n{\n\"ci\": true,\n\"repositoryUrl\": \"your repository url\",\n\"branches\": [\n\"master\",\n{\n\"name\": \"feature/*\",\n\"prerelease\": \"beta-${name.replace(/\\\\//g, '-').replace(/_/g, '-')}\"\n},\n{\n\"name\": \"[a-zA-Z0-9_]+/[a-zA-Z0-9-_]+\",\n\"prerelease\": \"dev-${name.replace(/\\\\//g, '-').replace(/_/g, '--')}\"\n}\n],\n\"plugins\": [\n\"@semantic-release/commit-analyzer\",\n\"@semantic-release/release-notes-generator\",\n[\n\"@semantic-release/exec\",\n{\n\"verifyReleaseCmd\": \"echo ${nextRelease.name} > .VERSION\"\n}\n],\n\"semantic-release-ado\"\n],\n\"extends\": \"semantic-release-monorepo\"\n}  \n{% endraw %}  \nResources  \nGitVersion  \nSemantic Versioning  \nVersioning in C#  \nsemantic-release  \nsemantic-release-monorepo",
        "source": "..\\data\\docs\\code-with-engineering\\source-control\\component-versioning.md"
    },
    {
        "chunkId": "chunk240_0",
        "chunkContent": "Merge strategies  \nAgree if you want a linear or non-linear commit history. There are pros and cons to both approaches:  \nPro linear: Avoid messy git history, use linear history  \nCon linear: Why you should stop using Git rebase  \nApproach for non-linear commit history  \nMerging topic into main  \n{% raw %}  \n```md\nA---B---C topic\n/         \\\nD---E---F---G---H main\n\ngit fetch origin\ngit checkout main\ngit merge topic\n```  \n{% endraw %}  \nTwo approaches to achieve a linear commit history  \nRebase topic branch before merging into main  \nBefore merging topic into main, we rebase topic with the main branch:  \n{% raw %}  \n```bash\nA---B---C topic\n/         \\\nD---E---F-----------G---H main\n\ngit checkout main\ngit pull\ngit checkout topic\ngit rebase origin/main\n```  \n{% endraw %}  \nCreate a PR topic --> main in Azure DevOps and approve using the squash merge option  \nRebase topic branch before squash merge into main  \nSquash merging is a merge option that allows you to condense the Git history of topic branches when you complete a pull request. Instead of adding each commit on topic to the history of main, a squash merge takes all the file changes and adds them to a single new commit on main.  \n{% raw %}  \nbash\nA---B---C topic\n/\nD---E---F-----------G---H main  \n{% endraw %}  \nCreate a PR topic --> main in Azure DevOps and approve using the squash merge option",
        "source": "..\\data\\docs\\code-with-engineering\\source-control\\merge-strategies.md"
    },
    {
        "chunkId": "chunk241_0",
        "chunkContent": "Naming branches  \nWhen contributing to existing projects, look for and stick with the agreed branch naming convention. In open source projects this information is typically found in the contributing instructions, often in a file named CONTRIBUTING.md.  \nIn the beginning of a new project the team agrees on the project conventions including the branch naming strategy.  \nHere's an example of a branch naming convention:  \n{% raw %}  \nplaintext\n<user alias>/[feature/bug/hotfix]/<work item ID>_<title>  \n{% endraw %}  \nWhich could translate to something as follows:  \n{% raw %}  \nplaintext\ndickinson/feature/271_add_more_cowbell  \n{% endraw %}  \nThe example above is just that - an example. The team can choose to omit or add parts. Choosing a branch convention can depend on the development model (e.g. trunk-based development), versioning model, tools used in managing source control, matter of taste etc. Focus on simplicity and reducing ambiguity; a good branch naming strategy allows the team to understand the purpose and ownership of each branch in the repository.",
        "source": "..\\data\\docs\\code-with-engineering\\source-control\\naming-branches.md"
    },
    {
        "chunkId": "chunk242_0",
        "chunkContent": "Source Control  \nThere are many options when working with Source Control. In ISE we use AzureDevOps for private repositories and GitHub for public repositories.  \nSections within Source Control  \nMerge Strategies  \nBranch Naming  \nVersioning  \nWorking with Secrets  \nGit Guidance  \nGoal  \nFollowing industry best practice to work in geo-distributed teams which encourage contributions from all across ISE as well as the broader OSS community  \nImprove code quality by enforcing reviews before merging into main branches  \nImprove traceability of features and fixes through a clean commit history  \nGeneral Guidance  \nConsistency is important, so agree to the approach as a team before starting to code. Treat this as a design decision, so include a design proposal and review, in the same way as you would document all design decisions (see Working Agreements and Design Reviews).  \nCreating a new repository  \nWhen creating a new repository, the team should at least do the following  \nAgree on the branch, release and merge strategy  \nDefine the merge strategy (linear or non-linear)  \nLock the default branch and merge using pull requests (PRs)  \nAgree on branch naming (e.g. user/your_alias/feature_name)  \nEstablish branch/PR policies  \nFor public repositories the default branch should contain the following files:  \nLICENSE  \nREADME.md  \nCONTRIBUTING.md  \nContributing to an existing repository  \nWhen working on an existing project, git clone the repository and ensure you understand the team's branch, merge and release strategy (e.g. through the projects CONTRIBUTING.md file).  \nMixed DevOps Environments  \nFor most engagements having a single hosted DevOps environment (i.e. Azure DevOps) is the preferred path but there are times when a mixed DevOps environment (i.e. Azure DevOps for Agile/Work item tracking & GitHub for Source Control) is needed due to customer requirements. When working in a mixed environment:  \nManually tag PR's in work items  \nEnsure that the scope of work items / tasks align with PR's  \nResources  \nGit --local-branching-on-the-cheap  \nAzure DevOps  \nISE Git detailsdetails on how to use Git as part of a ISE project.  \nGitHub - Removing sensitive data from a repository  \nHow Git Works Pluralsight course  \nMastering Git Pluralsight course",
        "source": "..\\data\\docs\\code-with-engineering\\source-control\\README.md"
    },
    {
        "chunkId": "chunk243_0",
        "chunkContent": "Working with Secrets in Source Control  \nThe best way to avoid leaking secrets is to store them in local/private files and exclude these from git tracking with a .gitignore file.\nE.g. the following pattern will exclude all files with the extension .private.config:  \n{% raw %}  \n```bash\n\nremove private configuration\n\n.private.config\n```  \n{% endraw %}  \nFor more details on proper management of credentials and secrets in source control, and handling an accidental commit of secrets to source control, please refer to the Secrets Management document which has further information, split by language as well.  \nAs an extra security measure, apply credential scanning in your CI/CD pipeline.",
        "source": "..\\data\\docs\\code-with-engineering\\source-control\\secrets-management.md"
    },
    {
        "chunkId": "chunk244_0",
        "chunkContent": "Using Git LFS and VFS for Git introduction  \nGit LFS and VFS for Git are solutions for using Git with (large) binary files and large source trees.  \nGit LFS  \nGit is very good and keeping track of changes in text-based files like code, but it is not that good at tracking binary files. For instance, if you store a Photoshop image file (PSD) in a repository, with every change, the complete file is stored again in the history. This can make the history of the Git repo very large, which makes a clone of the repository more and more time-consuming.  \nA solution to work with binary files is using Git LFS (or Git Large File System). This is an extension to Git and must be installed separately, and it can only be used with a repository platform that supports LFS. GitHub.com and Azure DevOps for instance are platforms that have support for LFS.  \nThe way it works in short, is that a placeholder file is stored in the repo with information for the LFS system. It looks something like this:  \n{% raw %}  \nshell\nversion https://git-lfs.github.com/spec/v1\noid a747cfbbef63fc0a3f5ffca332ae486ee7bf77c1d1b9b2de02e261ef97d085fe\nsize 4923023  \n{% endraw %}  \nThe actual file is stored in a separate storage. This way Git will track changes in this placeholder file, not the large file. The combination of using Git and Git LFS will hide this from the developer though. You will just work with the repository and files as before.  \nWhen working with these large files yourself, you'll still see the Git history grown on your own machine, as Git will still start tracking these large files locally, but when you clone the repo, the history is actually pretty small. So it's beneficial for others not working directly on the large files.  \nPros of Git LFS  \nUses the end to end Git workflow for all files  \nGit LFS supports file locking to avoid conflicts for undiffable assets  \nGit LFS is fully supported in Azure DevOps Services  \nCons of Git LFS  \nEveryone who contributes to the repository needs to install Git LFS  \nIf not set up properly:  \nBinary files committed through Git LFS are not visible as Git will only download the data describing the large file  \nCommitting large binaries will push the full binary to the repository  \nGit cannot merge the changes from two different versions of a binary file; file locking mitigates this  \nAzure Repos do not support using SSH for repositories with Git LFS tracked files - for more information see the Git LFS authentication documentation  \nInstallation and use of Git LFS  \nGo to https://git-lfs.github.com and download and install the setup from there.  \nFor every repository you want to use LFS, you have to go through these steps:  \nSetup LFS for the repo:  \n{% raw %}  \nshell\ngit lfs install  \n{% endraw %}  \nIndicate which files have to be considered as large files (or binary files). As an example, to consider all Photoshop files to be large:  \n{% raw %}  \nshell\ngit lfs track \"*.psd\"  \n{% endraw %}  \nThere are more fine-grained ways to indicate files in a folder and more. See the Git LFS Documentation.  \nWith these commands a .gitattribute file is created which contains these settings and must be part of the repository.  \nFrom here on you just use the standard Git commands to work in the repository. The rest will be handled by Git and Git LFS.  \nCommon LFS commands  \nInstall Git LFS  \n{% raw %}  \nbash\ngit lfs install       # windows\nsudo apt-get git-lfs  # linux  \n{% endraw %}  \nSee the Git LFS installation instructions for installation on other systems  \nTrack .mp4 files with Git LFS  \n{% raw %}  \nbash\ngit lfs track '*.mp4'  \n{% endraw %}  \nUpdate the .gitattributes file listing the files and patterns to track  \n{% raw %}  \nbash\n*.mp4 filter=lfs diff=lfs merge=lfs -text\ndocs/images/* filter=lfs diff=lfs merge=lfs -text  \n{% endraw %}  \nList all patterns tracked  \n{% raw %}  \nbash\ngit lfs track  \n{% endraw %}  \nList all files tracked  \n{% raw %}  \nbash\ngit lfs ls-files  \n{% endraw %}  \nDownload files to your working directory  \n{% raw %}  \nbash\ngit lfs pull\ngit lfs pull --include=\"path/to/file\"  \n{% endraw %}  \nVFS for Git  \nImagine a large repository containing multiple projects, ex. one per feature. As a developer you may only be working on some features, and thus you don't want to download all the projects in the repo. By default, with Git however, cloning the repository means you will download all files/projects.  \nVFS for Git (or Virtual File System for Git) solves this problem, as it will only download what you need to your local machine, but if you look in the file system, e.g. with Windows Explorer, it will show all the folders and files including the correct file sizes.  \nThe Git platform must support GVFS to make this work. GitHub.com and Azure DevOps both support this out of the box.  \nInstallation and use of VFS for Git  \nMicrosoft create VFS for Git and made it open source. It can be found at https://github.com/microsoft/VFSForGit. It's only available for Windows.  \nThe necessary installers can be found at https://github.com/Microsoft/VFSForGit/releases  \nOn the releases page you'll find two important downloads:  \nGit 2.28.0.0 installer, which is a requirement for running VFS for Git. This is not the same as the standard Git for Windows install!  \nSetupGVFS installer.  \nDownload those files and install them on your machine.  \nTo be able to use VFS for Git for a repository, a .gitattributes file needs to be added to the repo with this line in it:  \n{% raw %}  \nshell\n* -text  \n{% endraw %}  \nTo clone a repository to your machine using VFS for Git you use gvfs instead of git like so:  \n{% raw %}  \nshell\ngvfs clone [URL] [dir]  \n{% endraw %}  \nOnce this is done, you have a folder which contains a src folder which contains the contents of the repository. This is done because of a practice to put all outputs of build systems outside this tree. This makes it easier to manage .gitignore files and to keep Git performant with lots of files.  \nFor working with the repository you just use Git commands as before.  \nTo remove a VFS for Git repository from your machine, make sure the VFS process is stopped and execute this command from the main folder:  \n{% raw %}  \nshell\ngvfs unmount  \n{% endraw %}  \nThis will stop the process and unregister it, after that you can safely remove the folder.  \nReferences  \nGit LFS getting started  \nGit LFS manual  \nGit LFS on Azure Repos",
        "source": "..\\data\\docs\\code-with-engineering\\source-control\\git-guidance\\git-lfs-and-vfs.md"
    },
    {
        "chunkId": "chunk245_0",
        "chunkContent": "Git Guidance  \nWhat is Git?  \nGit is a distributed version control system. This means that - unlike SVN or CVS - it doesn't use a central server to synchronize. Instead, every participant has a local copy of the source-code, and the attached history that is kept in sync by comparing commit hashes (SHA hashes of changes between each git commit command) making up the latest version (called HEAD).  \nFor example:  \n{% raw %}  \nplain\nrepo 1: A -> B -> C -> D -> HEAD\nrepo 2: A -> B -> HEAD\nrepo 3: X -> Y -> Z -> HEAD\nrepo 4: A -> J -> HEAD  \n{% endraw %}  \nSince they share a common history, repo 1 and repo 2 can be synchronized fairly easily, repo 4 may be able to synchronize as well, but it's going to have to add a commit (J, and maybe a merge commit) to repo 1. Repo 3 cannot be easily synchronized with the others. Everything related to these commits is stored in a local .git directory in the root of the repository.  \nIn other words, by using Git you are simply creating immutable file histories that uniquely identify the current state and therefore allow sharing whatever comes after. It's a Merkle tree.  \nBe sure to run git help after Git installation to find really in-depth explanations of everything.  \nInstallation  \nGit is a tool set that must be installed. Install Git and follow the First-Time Git Setup.  \nA recommended installation is the Git Lens extension for Visual Studio Code. Visualize code authorship at a glance via Git blame annotations and code lens, seamlessly navigate and explore Git repositories, gain valuable insights via powerful comparison commands, and so much more.  \nYou can use these commands as well to configure your Git for Visual Studio Code as an editor for merge conflicts and diff tool.  \n{% raw %}  \n```cmd\ngit config --global user.name [YOUR FIRST AND LAST NAME]\ngit config --global user.email [YOUR E-MAIL ADDRESS]\n\ngit config --global merge.tool vscode\ngit config --global mergetool.vscode.cmd \"code --wait $MERGED\"\n\ngit config --global diff.tool vscode\ngit config --global difftool.vscode.cmd \"code --wait --diff $LOCAL $REMOTE\"\n```  \n{% endraw %}  \nBasic workflow  \nA basic Git workflow is as follows; you can find more information on the specific steps below.  \n{% raw %}  \n```cmd\n\npull the latest changes\n\ngit pull\n\nstart a new feature branch based on the develop branch\n\ngit checkout -b feature/123-add-git-instructions develop\n\nedit some files\n\nadd and commit the files\n\ngit add\ngit commit -m \"add basic instructions\"\n\nedit some files\n\nadd and commit the files\n\ngit add\ngit commit -m \"add more advanced instructions\"\n\ncheck your changes\n\ngit status\n\npush the branch to the remote repository\n\ngit push --set-upstream origin feature/123-add-git-instructions\n```  \n{% endraw %}  \nCloning  \nWhenever you want to make a change to a repository, you need to first clone it. Cloning a repository pulls down a full copy of all the repository data, so that you can work on it locally. This copy includes all versions of every file and folder for the project.  \n{% raw %}  \ncmd\ngit clone https://github.com/username/repo-name  \n{% endraw %}  \nYou only need to clone the repository the first time. Before any subsequent branches you can sync any changes from the remote repository using git pull.  \nBranching  \nTo avoid adding code that has not been peer reviewed to the main branch (ex. develop) we typically work in feature branches, and merge these back to the main trunk with a Pull Request. It's even the case that often the main or develop branch of a repository are locked so that you can't make changes without a Pull Request. Therefore, it is useful to create a separate branch for your local/feature work, so that you can work and track your changes in this branch.  \nPull the latest changes and create a new branch for your work based on the trunk (in this case develop).  \n{% raw %}  \ncmd\ngit pull\ngit checkout -b feature/feature-name develop  \n{% endraw %}  \nAt any point, you can move between the branches with git checkout <branch> as long as you have committed or stashed your work. If you forget the name of your branch use git branch --all to list all branches.  \nCommitting  \nTo avoid losing work, it is good to commit often in small chunks. This allows you to revert only the last changes if you discover a problem and also neatly explains exactly what changes were made and why.  \nMake changes to your branch  \nCheck what files were changed\n{% raw %}\n```cmd\n\ngit status\nOn branch feature/271-basic-commit-info\nChanges not staged for commit:\n(use \"git add ...\" to update what will be committed)\n(use \"git restore ...\" to discard changes in working directory)\nmodified:   source-control/git-guidance/README.md\n```  \n{% endraw %}  \nTrack the files you wish to include in the commit. To track all modified files:\n{% raw %}\ncmd\ngit add --all\n{% endraw %}  \nOr to track only specific files:  \nCommit the changes to your local branch with a descriptive commit message\n{% raw %}\ncmd\ngit commit -m \"add basic git instructions\"\n{% endraw %}  \nPushing  \nWhen you are done working, push your changes to a branch in the remote repository using:  \n{% raw %}  \ncmd\ngit push  \n{% endraw %}  \nThe first time you push, you first need to set an upstream branch as follows. After the first push, the --set-upstream parameter and branch name are not needed anymore.  \n{% raw %}  \ncmd\ngit push --set-upstream origin feature/feature-name  \n{% endraw %}  \nOnce the feature branch is pushed to the remote repository, it is visible to anyone with access to the code.  \nMerging  \nWe encourage the use of Pull Request to merge code to the main repository to make sure that all code in the final product is code reviewed  \nThe Pull Request (PR) process in Azure DevOps, GitHub and other similar tools make it easy both to start a PR, review a PR and merge a PR.  \nMerge Conflicts  \nIf multiple people make changes to the same files, you may need to resolve any conflicts that have occurred before you can merge.  \n{% raw %}  \n```cmd\n\ncheck out the develop branch and get the latest changes\n\ngit checkout develop\ngit pull\n\ncheck out your branch\n\ngit checkout\n\nmerge the develop branch into your branch\n\ngit merge develop\n\nif merge conflicts occur, above command will fail with a message telling you that there are conflicts to be solved\n\nfind which files need to be resolved\n\ngit status\n```  \n{% endraw %}  \nYou can start an interactive process that will show which files have conflicts. Sometimes you removed a file, where it was changed in dev. Or you made changes to some lines in a file where another developer made changes as well. If you went through the installation steps mentioned before, Visual Studio Code is set up as merge tool. You can also use a merge tool like kdiff3. When editing conflicts occur, the process will automatically open Visual Studio Code where the conflicting parts are highlighted in green and blue, and you have make a choice:  \nAccept your changes (current)  \nAccept the changes from dev branch (incoming)  \nAccept them both and fix the code (probably needed)  \n{% raw %}  \n```text\nHere are lines that are either unchanged from the common\nancestor, or cleanly resolved because only one side changed.\n<<<<<<< yours:sample.txt\nConflict resolution is hard;\nlet's go shopping.\n=======\nGit makes conflict resolution easy.\n\ntheirs:sample.txt\nAnd here is another line that is cleanly resolved or unmodified\n```  \n{% endraw %}  \nWhen this process is completed, make sure you test the result by executing build, checks, test to validate this merged result.  \n{% raw %}  \n```cmd\n\nconclude the merge\n\ngit merge --continue\n\nverify that everything went ok\n\ngit log\n\npush the changes to the remote branch\n\ngit push\n```  \n{% endraw %}  \nIf no other conflicts appear, the PR can now be merged, and your branch deleted. Use squash to reduce your changes into a single commit, so the commit history can be within an acceptable size.  \nStashing changes  \ngit stash is super handy if you have un-committed changes in your working directory, but you want to work on a different branch. You can run git stash, save the un-committed work, and revert to the HEAD commit. You can retrieve the saved changes by running git stash pop:  \n{% raw %}  \ncmd\ngit stash\n\u2026\ngit stash pop  \n{% endraw %}  \nOr you can move the current state into a new branch:  \n{% raw %}  \ncmd\ngit stash branch <new_branch_to_save_changes>  \n{% endraw %}  \nRecovering lost commits  \nIf you \"lost\" a commit that you want to return to, for example to revert a git rebase where your commits got squashed, you can use git reflog to find the commit:  \n{% raw %}  \ncmd\ngit reflog  \n{% endraw %}  \nThen you can use the reflog reference (HEAD@{}) to reset to a specific commit before the rebase:  \n{% raw %}  \ncmd\ngit reset HEAD@{2}  \n{% endraw %}  \nCommit Best Practices  \nA commit combines changes into a logical unit. Adding a descriptive commit message can aid in comprehending the code changes and understanding the rationale behind the modifications. Consider the following when making your commits:  \nMake small commits. This makes changes easier to review, and if we need to revert a commit, we lose less work. Consider splitting the commit into separate commits with git add -p if it includes more than one logical change or bug fix.  \nDon't mix whitespace changes with functional code changes. It is hard to determine if the line has a functional change or only removes a whitespace, so functional changes may go unnoticed.  \nCommit complete and well tested code. Never commit incomplete code, get in the habit of testing your code before committing.  \nWrite good commit messages.  \nWhy is it necessary? It may fix a bug, add a feature, improve performance, or just be a change for the sake of correctness  \nWhat effects does this change have? In addition to the obvious ones, this may include benchmarks, side effects etc.  \nYou can specify the default git editor, which allows you to write your commit messages using your favorite editor. The following command makes Visual Studio Code your default git editor:  \n{% raw %}  \nbash\ngit config --global core.editor \"code --wait\"  \n{% endraw %}  \nCommit Message Structure  \nThe essential parts of a commit message are:  \nsubject line: a short description of the commit, maximum 50 characters long  \nbody (optional): a longer description of the commit, wrapped at 72 characters, separated from the subject line by a blank line  \nYou are free to structure commit messages; however, git commands like git log utilize above structure.\nTherefore, it can be helpful to follow a convention within your team and to utilize git best.  \nFor example, Conventional Commits is a lightweight convention that complements SemVer, by describing the features, fixes, and breaking changes made in commit messages. See Component Versioning for more information on versioning.  \nFor more information on commit message conventions, see:  \nA Note About Git Commit Messages  \nConventional Commits  \nGit commit best practices  \nHow to Write a Git Commit Message  \nHow to Write Better Git Commit Messages  \nInformation in commit messages  \nOn commit messages  \nManaging remotes  \nA local git repository can have one or more backing remote repositories. You can list the remote repositories using git remote - by default, the remote repository you cloned from will be called origin  \n{% raw %}  \n```cmd\n\ngit remote -v\norigin  https://github.com/microsoft/code-with-engineering-playbook.git (fetch)\norigin  https://github.com/microsoft/code-with-engineering-playbook.git (push)\n```  \n{% endraw %}  \nWorking with forks  \nYou can set multiple remotes. This is useful for example if you want to work with a forked version of the repository.\nFor more info on how to set upstream remotes and syncing repositories when working with forks see GitHub's Working with forks documentation.  \nUpdating the remote if a repository changes names  \nIf the repository is changed in some way, for example a name change, or if you want to switch between HTTPS and SSH you need to update the remote  \n{% raw %}  \n```cmd\n\nlist the existing remotes\n\ngit remote -v\norigin  https://hostname/username/repository-name.git (fetch)\norigin  https://hostname/username/repository-name.git (push)\n\nchange the remote url\n\ngit remote set-url origin https://hostname/username/new-repository-name.git\n\nverify that the remote URL has changed\n\ngit remote -v\norigin  https://hostname/username/new-repository-name.git (fetch)\norigin  https://hostname/username/new-repository-name.git (push)\n```  \n{% endraw %}  \nRolling back changes  \nReverting and deleting commits  \nTo \"undo\" a commit, run the following two commands: git revert and git reset. git revert creates a new commit that undoes commits while git reset allows deleting commits entirely from the commit history.  \nIf you have committed secrets/keys, git reset will remove them from the commit history!  \nTo delete the latest commit use HEAD~:  \n{% raw %}  \nbash\ngit reset --hard HEAD~1  \n{% endraw %}  \nTo delete commits back to a specific commit, use the respective commit id:  \n{% raw %}  \nbash\ngit reset --hard <sha1-commit-id>  \n{% endraw %}  \nafter you deleted the unwanted commits, push using force:  \n{% raw %}  \nbash\ngit push origin HEAD --force  \n{% endraw %}  \nInteractive rebase for undoing commits:  \n{% raw %}  \nbash\ngit rebase -i HEAD~N  \n{% endraw %}  \nThe above command will open an interactive session in an editor (for example vim) with the last N commits sorted from oldest to newest. To undo a commit, delete the corresponding line of the commit and save the file. Git will rewrite the commits in the order listed in the file and because one (or many) commits were deleted, the commit will no longer be part of the history.  \nRunning rebase will locally modify the history, after this one can use force to push the changes to remote without the deleted commit.  \nUsing submodules  \nSubmodules can be useful in more complex deployment and/or development scenarios  \nAdding a submodule to your repo  \n{% raw %}  \nbash\ngit submodule add -b master <your_submodule>  \n{% endraw %}  \nInitialize and pull a repo with submodules:  \n{% raw %}  \nbash\ngit submodule init\ngit submodule update --init --remote\ngit submodule foreach git checkout master\ngit submodule foreach git pull origin  \n{% endraw %}  \nWorking with images, video and other binary content  \nAvoid committing frequently changed binary files, such as large images, video or compiled code to your git repository. Binary content is not diffed like text content, so cloning or pulling from the repository may pull each revision of the binary file.  \nOne solution to this problem is Git LFS (Git Large File Storage) - an open source Git extension for versioning large files. You can find more information on Git LFS in the Git LFS and VFS document.  \nWorking with large repositories  \nWhen working with a very large repository of which you don't require all the files, you can use VFS for Git - an open source Git extension that virtualize the file system beneath your Git repository, so that you seem to work in a regular working directory but while VFS for Git only downloads objects as they are needed. You can find more information on VFS for Git in the Git LFS and VFS document.  \nTools  \nVisual Studio Code is a cross-platform powerful source code editor with built in git commands. Within Visual Studio Code editor you can review diffs, stage changes, make commits, pull and push to your git repositories.\nYou can refer to Visual Studio Code Git Support for documentation.  \nUse a shell/terminal to work with Git commands instead of relying on GUI clients.  \nIf you're working on Windows, posh-git is a great PowerShell environment for Git. Another option is to use Git bash for Windows. On Linux/Mac, install git and use your favorite shell/terminal.",
        "source": "..\\data\\docs\\code-with-engineering\\source-control\\git-guidance\\README.md"
    },
    {
        "chunkId": "chunk246_0",
        "chunkContent": "Maintainability  \nComing soon!",
        "source": "..\\data\\docs\\code-with-engineering\\user-interface-engineering\\maintainability.md"
    },
    {
        "chunkId": "chunk247_0",
        "chunkContent": "User Interface and User Experience Engineering  \nAlso known as UI/UX, Front End Development, or Web Development, user interface and user experience engineering is a broad topic and encompasses many different aspects of modern application development. When a user interface is required, ISE primarily develops a web application. Web apps can be built in a variety of ways with many different tools.  \nGoal  \nThe goal of the User Interface section is to provide guidance on developing web applications. Everyone should begin by reading the General Guidance for a quick introduction to the four main aspects of every web application project. From there, readers are encouraged to dive deeper into each topic, or begin reviewing technical guidance that pertains to their engagement. All UI/UX projects should begin with a detailed design document. Review the Design Process section for more details, and a template to get started.  \nKeep in mind that like all software, there is no \"right way\" to build a user interface application. Leverage and trust your team's or your customer's experience and expertise for the best development experience.  \nGeneral Guidance  \nThe state of web platform engineering is fast moving. There is no one-size-fits-all solution. For any team to be successful in building a UI, they need to have an understanding of the higher-level aspects of all UI project.  \nAccessibility - ensuring your application is usable and enjoyed by as many people as possible is at the heart of accessibility and inclusive design.  \nUsability - how effortless should it be for any given user to use the application? Do they need special training or a document to understand how to use it, or will it be intuitive?  \nMaintainability - is the application just a proof of concept to showcase an idea for future work, or will it be an MVP and act as the starting point for a larger, production-ready application? Sometimes you don't need React or any other framework. Sometimes you need React, but not all the bells and whistles from create-react-app. Understanding project maintainability requirements can simplify an engagement\u2019s tooling needs significantly and let folks iterate without headaches.  \nStability - what is the cost of adding a dependency? Is it actively stable/updated/maintained? If not, can you afford the tech debt (sometimes the answer can be yes!)? Could you get 90% of the way there without adding another dependency?  \nMore information is available for each general guidance section in the corresponding pages.  \nDesign Process  \nAll user interface applications begin with the design process. The true definition for \"the design process\" is ever changing and highly opinion based as well. This sections aims to deliver a general overview of a design process any engineering team could conduct when starting an UI application engagement.  \nWhen committing to a UI/UX project, be certain to not over-promise on the web application requirements. Delivering a production-ready application involves a large number of engineering complexities resulting in a very long timeline. Always start with a proof-of-concept or minimum-viable-product first. These projects can easily be achieved within a couple month timeline (and sometimes even less).  \nThe first step in the design process is to understand the problem at hand and outline what the solution should achieve. Commonly referred to as Desired Outcomes, the output of this first step should be a generalized list of outcomes that the solution will accomplish. Consider the following example:  \nA public library has a set of data containing information about its collection. The data stores text, images, and the status of a book (borrowed, available, reserved). The library librarian wants to share this data with its users.  \nAs the librarian, I want to notify users before they receive late penalties for overdue books  \nAs the librarian, I want to notify users when a book they have reserved becomes available  \nWith the desired outcomes in mind, the next step in the design process is to define user personas. Regardless of the solution for a given problem, understanding the user needs leads to a better understanding of feature development and technological choices. Personas are written as prose-like paragraphs that describe different types of users. Considering the previous example, the various user personas could be:  \nAn individual with no disabilities, but is unfamiliar with using software interfaces  \nAn individual with no disabilities, and is familiar with using software interfaces  \nAn individual with disabilities, and is unfamiliar with using software interfaces (with or without the use of accessibility tooling)  \nAn individual with disabilities, but familiar with using software interfaces through the use of accessibility tooling  \nAfter defining these personas it is clear that whatever the solution is, it requires a lot of accessibility and user experience design work. Sometimes personas can be simpler than this, but always include disabled users. Even when a user set is predefined as a group of individuals without disabilities, there is no guarantee that the user set will remain that way.  \nAfter defining the desired outcomes as well as the personas, the next step in the design process is to begin conducting Trade Studies for potential solutions. The first trade study should be high-level and solution oriented. It will utilize the results of previous steps and propose multiple solutions for achieving the desired outcomes with the listed personas in mind. Continuing with the library example, this first trade study may compare various application solutions such as automated emails or text messages, an RSS feed, or an user interface application. There are pros and cons for each solution both from an user experience and a developer experience perspective, but at this stage it is important to focus on the users. After arriving on the best solution, the next trade study can dive into different implementation methods. It is in this subsequent trade studies that developer experience becomes more important.  \nThe benefit of building software applications is that there are truly infinite ways to build something. A team can use the latest shiny tools, or they can utilize the tried-and-tested ones. It is for this reason that focussing completely on the user until a solution is defined is better than obsessing over technology choices. Within ISE, we often reach for tools such as the React framework. React is a great tool when wielded by an experienced team. Otherwise, it can create more hurdles than it is worth. Keep in mind that even if you feel capable with React, the rest of your team and your customer's dev team needs to as well. Some other great options to consider when building a proof-of-concept or minimum-viable-product are:  \nHTML/CSS/JavaScript  \nBack to the basics! Start with a single index.html, include a popular CSS framework such as Bootstrap using their CDN link, and start prototyping!  \nRarely will you have to support legacy browsers; thus, you can rely on modern JavaScript language features! No need for build tools or even TypeScript (did you know you can type check JavaScript).  \nWeb Component frameworks  \nWeb Components are now standardized in all modern browsers  \nMicrosoft has their own, stable & actively-maintained framework, Fast  \nFor more information of choosing the right implementation tool, read the Recommended Technologies document.  \nContinue reading the Trade Study section of this site for more information on completing this step in the design process.  \nAfter iterating through multiple trade study documents, this design process can be considered complete! With an agreed upon solution and implementation in mind, it is now time to begin development. A natural continuation of the design process is to get users (or stakeholders) involved as early as possible. Constantly look for design and usability feedback, and utilize this to improve the application as it is being developed.  \nExample  \nComing soon!",
        "source": "..\\data\\docs\\code-with-engineering\\user-interface-engineering\\README.md"
    },
    {
        "chunkId": "chunk248_0",
        "chunkContent": "Recommended Technologies  \nThe purpose of this page is to review the commonly selected technology options when developing user interface applications. To reiterate from the general guidance section:  \nKeep in mind that like all software, there is no \"right way\" to build a user interface application. Leverage and trust your team's or your customer's experience and expertise for the best development experience.  \nAdditionally, while some of these technologies are presented as alternate options, many can be combined together. For example, you can use React in a basic HTML/CSS/JS workflow by inline-importing React along with Babel. See the Add React to a Website for more details. Similarly, any Fast web component can be integrated into any existing React application. And of course, every JavaScript technology can also be used with TypeScript!  \nTypeScript  \nTypeScript is JavaScript with syntax for types. TypeScript is a strongly typed programming language that builds on JavaScript, giving you better tooling at any scale.\ntypescriptlang.org  \nTypeScript is highly recommended for all new web application projects. The stability it provides for teams is unmatched, and can make it easier for folks with C# backgrounds to work with web technologies.  \nThere are many ways to integrate TypeScript into a web application. The easiest way to get started is by reviewing the TypeScript Tooling in 5 Minutes guide from the official TypeScript docs. The other sections on this page contain information regarding integration with TypeScript.  \nReact  \nReact is a framework developed and maintained by Facebook. React is used throughout Microsoft and has a vast open source community.  \nDocumentation & Recommended Resources  \nOne can expect to find a multitude of guides, answers, and posts on how to work with React; don't take everything at face value. The best place to review React concepts is the React documentation. From there, you can review articles from various sources such as React Community Articles, Kent C Dodd's Blog, CSS Tricks Articles, and Awesome React.  \nThe React API has changed dramatically over time. Older resources may contain solutions or patterns that have since been changed and improved upon. Modern React development uses the React Hooks pattern. Rarely will you have to implement something using React Class pattern. If you're reading an article/answer/docs that instruct you to use the class pattern you may be looking at an out-of-date resource.  \nBootstrapping  \nThere are many different ways to bootstrap a React application. Two great tool sets to use are create-react-app and vite.  \ncreate-react-app  \nFrom Adding TypeScript  \n{% raw %}  \nsh\nnpx create-react-app my-app --template typescript  \n{% endraw %}  \nVite  \nFrom Scaffolding your First Vite Project  \n{% raw %}  \n```sh\n\nnpm 6.x\n\nnpm init vite@latest my-app --template react-ts\n\nnpm 7.x\n\nnpm init vite@latest my-app -- --template react-ts\n```  \n{% endraw %}  \nHTML/CSS/JS  \nComing soon!  \nWeb Components  \nComing soon!",
        "source": "..\\data\\docs\\code-with-engineering\\user-interface-engineering\\recommended-technologies.md"
    },
    {
        "chunkId": "chunk249_0",
        "chunkContent": "Stability  \nComing soon!",
        "source": "..\\data\\docs\\code-with-engineering\\user-interface-engineering\\stability.md"
    },
    {
        "chunkId": "chunk250_0",
        "chunkContent": "Usability  \nComing soon!",
        "source": "..\\data\\docs\\code-with-engineering\\user-interface-engineering\\usability.md"
    },
    {
        "chunkId": "chunk251_0",
        "chunkContent": "author: shanepeckham\ntitle: Data discovery: Understanding data for AI and beyond\nms.author: shanepec\nms.service: playbook\ndescription: The data discovery process can be enhanced by using ML. By using ML techniques, data discovery can become smart, can discover relationships between data and accelerate an organization's understanding of their data.\nrings:\n- public  \nData discovery: Understanding data for AI and beyond  \nThis article discusses common data discovery challenges related to finding data for use in AI projects.  \nDiscovery challenges for data consumers  \nTraditionally, discovering enterprise data sources has been an organic process based on communal knowledge. For companies that want the most value from their information assets, this approach presents many challenges:  \nBecause there's no central location to register data sources, users might be unaware of a data source unless they come into contact with it as part of another process.  \nUnless users know the location of a data source, they can't connect to the data by using a client application. Data-consumption experiences require users to know the connection string or path.  \nThe intended use of the data is hidden to users unless they know the location of a data source's documentation. Data sources and documentation might live in several places and be consumed through different kinds of experiences.  \nIf users have questions about an information asset, they must locate the expert or team responsible for that data and engage them offline. There's no explicit connection between the data and the experts that understand the data's context.  \nUnless users understand the process for requesting access to the data source, discovering the data source and its documentation won't help them access the data.  \nDiscovery challenges for data producers  \nAlthough data consumers face the previously mentioned challenges, users who are responsible for producing and maintaining information assets face challenges of their own:  \nAnnotating data sources with descriptive metadata is often a lost effort. Client applications typically ignore descriptions that are stored in the data source.  \nCreating documentation for data sources can be difficult and it's an ongoing responsibility to keep documentation in sync with data sources. Users might not trust documentation that's perceived as being out of date.  \nCreating and maintaining documentation for data sources is complex and time-consuming. - Making that documentation readily available to everyone who uses the data source can be even more so.  \nRestricting access to data sources and ensuring that data consumers know how to request access is an ongoing challenge.  \nAll these challenges present a significant barrier for companies that want to encourage and promote the use of enterprise data.  \nDiscovery challenges for security administrators  \nUsers who are responsible for ensuring the security of their organization's data may have any of the challenges listed above as data consumers and producers, and the following extra challenges:  \nAn organization's data is constantly growing and being stored and shared in new directions. The task of discovering, protecting, and governing your sensitive data is one that never ends. You need to ensure that your organization's content is being shared with the correct people, applications, and with the correct permissions.  \nUnderstanding the risk levels in your organization's data requires diving deep into it, looking for keywords, RegEx patterns, and sensitive data types. For example, sensitive data types might include Credit Card numbers, Social Security numbers or Bank Account numbers. You must constantly monitor all data sources for sensitive content, as even the smallest amount of data loss can be critical to your organization.  \nEnsuring compliance with corporate security policies can be a challenge for organizations. As their content grows and the policies are updated to address evolving digital realities. Security administrators need to ensure data security in the quickest time possible.  \nMicrosoft Purview provides capabilities to help address these challenges.  \nData discovery from an MLOps perspective  \nThe data discovery process can be enhanced by using ML. By using ML techniques, data discovery can become smart, can discover relationships between data and accelerate an organization's understanding of their data.  \nCoupled with visualizations, data analysts and business domain experts can quickly derive insights from previously unexplored data.  \nBelow are some examples of how ML can be used to address the typical challenges during the data discovery process:  \nPropose data preparation steps such as normalization and handling of missing data  \nInfer relationships between unstructured data types such as documents, video and images, which are difficult to work with  \nDetect Personal Identifiable Information (PII) and other types of sensitive data  \nEnrich and index data meaningfully for users to easily search and discover on their own.  \nPerform automatic data translation so that it is accessible to users in different languages  \nIdentify outliers and patterns in the data  \nDetect anomalies in the data  \nImprove understanding of behavioral data - users and customers. Generate subsequential recommendations  \nDiscover more data sources, which may be useful to ML practitioners  \nSome useful resources for data discovery:  \nResource Description Data Discovery Toolkit - Unstructured data A  repository containing guidance and code assets that use various Machine Learning techniques to discover insights in unstructured data such as documents, images and videos AI Enrichment Pipeline tutorial Complete sample for processing text, image and video files through a full enrichment pipeline with event grid, service bus, functions, logic apps, cognitive services and video indexer Azure Cognitive Search - An AI-first approach to content understanding This project demonstrates how you can use both the built-in and custom AI in Cognitive Search. Cognitive Search ingests your data from almost any datasource and enriches it using a set of cognitive skills that extract knowledge and then lets you explore the data using Search. Azure Cognitive Search Powerskills Power Skills are a collection of useful functions to be deployed as custom skills for Azure Cognitive Search. Microsoft Presidio Presidio can help identify sensitive/PII data in un/structured text. PII Detection Cognitive Skill The PII Detection skill extracts personal information from an input text and gives you the option of masking it. End to end Knowledge Mining for Video A video discovery pipeline that includes Azure Search and user feedback  \n\u2139\ufe0f Refer to the DataOps: Data Discovery section for more information nuanced to a Data Engineer/Governance role.",
        "source": "..\\data\\docs\\code-with-mlops\\data-discovery.md"
    },
    {
        "chunkId": "chunk252_0",
        "chunkContent": "author: day-jeff\ntitle: Artificial Intelligence Playbook\nms.topic: landing-page\nms.author: jeffday\nms.service: azure\ndescription: The Artificial Intelligence (AI) Playbook provides enterprise software engineers with solutions, capabilities, and code developed to solve real-world AI problems. Everything in the playbook is developed with, and validated by, some of Microsoft's largest and most influential customers and partners.\nrings:\n- public  \nSolving Artificial Intelligence problems  \nThe Artificial Intelligence (AI) Playbook provides enterprise software engineers with solutions, capabilities, and code developed to solve real-world AI problems. Everything in the playbook is validated.  \nThis playbook provides AI solutions focused in the following areas:  \nPre-trained ML models, such as Large Language Models (LLM), and Azure Cognitive Services (at times referred to as Model as a Service or MaaS).  \nCustom Machine Learning Models - that are developed with Azure tools such as Azure Machine Learning (AML), Databricks, Synapse and Fabric.  \nSoftware as a Service (SaaS) solutions, such as Microsoft's PowerVirtualAgents.  \nSome good places to start:  \nTech Topic Getting Started Solution OpenAI Working with LLMs Azure ML Getting Started with Azure ML Automated and Monitored Training Pipelines  \nAbout the AI Playbook  \nThe AI Playbook aims to accelerate real-world application development by following good engineering practices, such as:  \nImproving application design and developer productivity by sharing code and knowledge developed by experts for Microsoft customers.  \nUsing automation to make repetitive tasks faster, more reliable, and auditable  \nMaking application deployments and operations secure and observable.  \nSecuring applications by following security best practices at all stages of the engineering lifecycle.  \nWriting portable solutions that run in multiple locations, including edge devices, on-premises data centers, the Microsoft cloud, and competitor's clouds.  \nIntegrated solutions  \nPlaybook solutions span multiple Microsoft products and services and focus on creating integrated end-to-end solutions often using a range of open-source software libraries.  \nProven with real customers  \nThe guidance, documentation, and code in this playbook are based on real-world engineering projects Microsoft has completed with customers. While the documentation and code has been generalized to remove confidential information, it is real code that has been deployed by Microsoft customers. The playbook shares real-world approaches and learnings that have worked for real projects.",
        "source": "..\\data\\docs\\code-with-mlops\\index.md"
    },
    {
        "chunkId": "chunk253_0",
        "chunkContent": "MLOps content styling guide  \nSolutions README  \nThe index.md file for a solution should contain the following sections:  \nCustomer Pattern: Problem statement the accurately covers what the solution addresses  \nChallenges: Challenges addressed by the solution  \nSolution: Link to the code related to the solution with a description",
        "source": "..\\data\\docs\\code-with-mlops\\STYLE_GUIDE.md"
    },
    {
        "chunkId": "chunk254_0",
        "chunkContent": "author: bjcmit\ntitle: AI/ML Capabilities Map\nms.topic: conceptual\nms.author: brysmith\nms.service: azure\ndescription: The AI/ML Capabilities Map shows the main capabilities within a typical AI/ML lifecycle.\nrings:\n- public  \nUnderstanding production AI building blocks  \nDeveloping, deploying, and operating an AI solution in a real production environment is extremely complex. This section describes the building blocks, or capabilities, that are needed to successfully build and operate AI solutions.  \nThe AI capabilities map shows the building blocks needed for a production AI solution. Each capability needs to be addressed at some point in the lifecycle of AI projects, but it's important to remember that they are often performed multiple times and in parallel rather than as discrete, linear phases of a project. AI/ML projects tend to be iterative, and teams need to iterate on each capability multiple times to develop, deploy, and operate high quality AI solutions.  \nThis part of the AI playbook breaks down required AI capabilities into a manageable set of topics, including:  \nData curation: Data curation capabilities include governance, privacy, discovery, enrichment, labeling, and versioning.  \nExperimentation: Experimentation capabilities include exploratory data analysis, feature engineering, and algorithm exploration.  \nModel development: Model development includes model engineering, feature management, pipeline workflows, model validation, testing, and performance evaluation.  \nModel deployment: Packaging and deploying models comes after model development. Capabilities include model deployment, packaging options, and deployment strategies.  \nML lifecycle management: ML lifecycle management includes production data collection, data drift, and model retraining.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\index.md"
    },
    {
        "chunkId": "chunk255_0",
        "chunkContent": "author: shanepeckham\ntitle: Data discovery: Finding data for AI projects\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: The data discovery process can be enhanced by using ML. By using ML techniques, data discovery can become smart, can discover relationships between data and accelerate an organization's understanding of their data.\nrings:\n- public  \nData discovery: Finding data for AI projects  \nThis article discusses common data discovery challenges related to finding data for use in AI projects.  \nDiscovery challenges for data consumers  \nTraditionally, discovering enterprise data sources has been an organic process based on communal knowledge. For companies that want the most value from their information assets, this approach presents many challenges:  \nBecause there's no central location to register data sources, users might be unaware of a data source unless they come into contact with it as part of another process.  \nUnless users know the location of a data source, they can't connect to the data by using a client application. Data-consumption experiences require users to know the connection string or path.  \nThe intended use of the data is hidden to users unless they know the location of a data source's documentation. Data sources and documentation might live in several places and be consumed through different kinds of experiences.  \nIf users have questions about an information asset, they must locate the expert or team responsible for that data and engage them offline. There's no explicit connection between the data and the experts that understand the data's context.  \nUnless users understand the process for requesting access to the data source, discovering the data source and its documentation won't help them access the data.  \nDiscovery challenges for data producers  \nAlthough data consumers face the previously mentioned challenges, users who are responsible for producing and maintaining information assets face challenges of their own:  \nAnnotating data sources with descriptive metadata is often a lost effort. Client applications typically ignore descriptions that are stored in the data source.  \nCreating documentation for data sources can be difficult and it's an ongoing responsibility to keep documentation in sync with data sources. Users might not trust documentation that's perceived as being out of date.  \nCreating and maintaining documentation for data sources is complex and time-consuming. Making that documentation readily available to everyone who uses the data source can be even more so.  \nRestricting access to data sources and ensuring that data consumers know how to request access is an ongoing challenge.  \nAll these challenges present a significant barrier for companies that want to encourage and promote the use of enterprise data.  \nDiscovery challenges for security administrators  \nUsers who are responsible for ensuring the security of their organization's data may have any of the challenges listed above as data consumers and producers, and the following extra challenges:  \nAn organization's data is constantly growing and being stored and shared in new directions. The task of discovering, protecting, and governing your sensitive data is one that never ends. You need to ensure that your organization's content is being shared with the correct people, applications, and with the correct permissions.  \nUnderstanding the risk levels in your organization's data requires diving deep into it, looking for keywords, RegEx patterns, and sensitive data types. For example, sensitive data types might include Credit Card numbers, Social Security numbers or Bank Account numbers. You must constantly monitor all data sources for sensitive content, as even the smallest amount of data loss can be critical to your organization.  \nEnsuring compliance with corporate security policies is a challenge for organizations. As their content grows and the policies are updated to address evolving digital realities, Security administrators need to ensure data security as quickly as possible.  \nEnsuring compliance with corporate security policies can be a challenge for organizations. As their content grows and the policies are updated to address evolving digital realities. Security administrators need to ensure data security in the quickest time possible.  \nMicrosoft Purview provides capabilities to help address these challenges.  \nData discovery from an MLOps perspective  \nThe data discovery process can be enhanced by using ML. By using ML techniques, data discovery can become smart, can discover relationships between data and accelerate an organization's understanding of their data.  \nCoupled with visualizations, data analysts and business domain experts can quickly derive insights from previously unexplored data.  \nBelow are some examples of how ML can be used to address the typical challenges during the data discovery process:  \nPropose data preparation steps such as normalization and handling of missing data  \nInfer relationships between unstructured data types such as documents, video and images, which are difficult to work with  \nDetect Personal Identifiable Information (PII) and other types of sensitive data  \nEnrich and index data meaningfully for users to easily search and discover on their own.  \nPerform automatic data translation so that it is accessible to users in different languages  \nIdentify outliers and patterns in the data  \nDetect anomalies in the data  \nImprove understanding of behavioral data - users and customers. Generate subsequential recommendations  \nDiscover more data sources, which may be useful to ML practitioners  \nSome useful resources for data discovery:  \nResource Description Data Discovery Toolkit - Unstructured data A  repository containing guidance and code assets that use various Machine Learning techniques to discover insights in unstructured data such as documents, images and videos. AI Enrichment Pipeline tutorial A complete sample for processing text, image and video files through a full enrichment pipeline with event grid, service bus, functions, logic apps, cognitive services and video indexer. Azure Cognitive Search - An AI-first approach to content understanding This project demonstrates how you can use both the built-in and custom AI in Cognitive Search. Cognitive Search ingests your data from almost any data source. Then enriches it using a set of cognitive skills that extract knowledge and then lets you explore the data using Search. Azure Cognitive Search Powerskills Power Skills are a collection of useful functions to be deployed as custom skills for Azure Cognitive Search. Microsoft Presidio Presidio can help identify sensitive/PII data in un/structured text. PII Detection Cognitive Skill The PII Detection skill extracts personal information from an input text and gives you the option of masking it. End to end Knowledge Mining for Video A video discovery pipeline that includes Azure Search and user feedback  \n\u2139\ufe0f Refer to the DataOps: Data Discovery section for more information nuanced to a Data Engineer/Governance role.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\data-curation\\data-discovery.md"
    },
    {
        "chunkId": "chunk256_0",
        "chunkContent": "author: shanepeckham\ntitle: Data engineering for AI projects\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Data must be carefully engineered if an AI project is to be successful. Data engineering includes data normalization, pre-processing, enrichment and other forms of data preparation.\nrings:\n- public  \nData engineering for AI projects  \nData must be carefully engineered if an AI project is to be successful. Data engineering includes data normalization, pre-processing, enrichment and other forms of data preparation.  \nData ingestion  \nData ingestion is the process of extracting data from one or multiple sources and then preparing it for training an ML model. This process can be quite time intensive if done manually and/or if dealing with substantial amounts of data. It includes both structured and unstructured data in different formats from varying source types.  \n\u2139\ufe0f Refer to the DataOps: Data Ingestion section for more information nuanced to a Data Engineer/Governance role.  \nData discovery  \nData discovery may be defined as the collection and evaluation of data from disparate sources with the goal of deriving business value from that data.  \nFor more information, see Data discovery: Finding data sources for AI projects.  \nData enrichment  \nData enrichment is a general term that refers to processes used to enhance, refine or otherwise improve raw data. Within the context of MLOps, we will refer to the process of enriching data using ML models and techniques.  \nCase studies: Aggregating data for AutoML Image Object Detection  \nIn order to train a computer vision model, such as AutoML Image Object Detection, you require labeled training data. The images needs to be uploaded to the Azure Blob Storage and label annotations (of each image) need to be in a JSONL format. Once all images are labeled, you can perform data aggregation which will transform\nthese multiple label annotations into a single JSON file, which you can use to create an MLTable that will serve as data input of this Azure ML model.  \nDuring this step, data aggregation helps to transform raw data into meaningful and useful information that can be used to train, test, and evaluate\nthe machine learning model.  \nRefer to the AML v2 P&ID symbol detection train sample project as a sample implementation of an AML workflow to train a symbol detection model. This\nincludes a data aggregation step for transforming the stored image and label datasets into a format that the AutoML training job can consume.  \nCase studies: Normalizing data for Form Recognizer  \nIn the following example, scanned images require enrichment to handle noise or poor quality scans that impact the usability of an OCR model. Quality of the images will also clearly impact the accuracy of the extraction of data from the form. Data normalization is part of the process of preparing data for training an ML model. This process transforms raw ingested data into a consistent format that is readily consumable by downstream steps/tasks and processes.  \nDuring this step, the data is normalized, de-noised for improved results, and evaluated for techniques on how the data can be segmented.  \nRefer to the Pre-Processing section of the Playbook for Knowledge Extraction For Forms Accelerators & Examples  \n{% if extra.ring == 'internal' %}  \nCase Studies: Normalizing data for Custom Translator  \nIn order to be able to train Custom Translator well on custom language data, clean and well-aligned datasets are required. Custom Translation memory files may include incorrect formatting and encoding, depending on how they were generated. For guidance and examples of cleaning translation memory files and generating datasets for translation using Custom Translator, refer to the Creating datasets section for the Custom Translator recipes.\n{% endif %}  \nCase studies: Enriching data  \nThe following repository showcases a collection of small and discrete data enrichment functions, using various infrastructures. The functions are built for Azure Cognitive Search, but they can be used in any data enrichment pipeline. These PowerSkills contain standard API interfaces so they can be consistently consumed.  \nRefer to the Azure Search PowerSkills for assets, guidance and examples.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\data-curation\\data-engineering.md"
    },
    {
        "chunkId": "chunk257_0",
        "chunkContent": "author: shanepeckham\ntitle: Data Governance\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: The governance process of the data for enterprise systems. It also covers the security and compliance aspects so data is reliable and meets the standards, regulations, and policies of the organization.\nrings:\n- public  \nComplying with enterprise data governance policies  \nEnterprises define data governance policies for how data is managed and can be accessed. AI engineering teams need to respect and support data security and compliance policies when working with enterprise data.  \nThis content discusses the data governance capabilities underlined in the diagram below. For more information about data governance, see Data in the Microsoft Solutions Playbook.  \nData catalog  \nA data catalog is an in-depth inventory of all data assets in an organization. Its main purpose is to make it easy for users to find the right data for analytical or business purpose.  \nThese data assets can include, amongst others:  \nStructured data  \nUnstructured data such as documents, images, audio, and video  \nReports and query results  \nData visualizations and dashboards  \nML models  \nFeatures extracted from ML models  \nConnections between databases  \nMicrosoft provides the following data catalog services:  \nAzure Data Catalog helps your organization find, understand, and consume data sources.  \nAzure Purview Data Catalog finds trusted data sources by browsing and searching your data assets. The data catalog aligns your assets with friendly business terms and data classification to identify data sources.  \n\u2139\ufe0f See DataOps: data catalog for more information nuanced to a Data Engineer/Governance role.  \nData Catalog from an MLOps perspective  \nAI practitioners need to easily find, access, and search organizational data that can be valuable for solving problems with machine learning. Considerations such as data privacy and access need to be enforced as well.  \nThe following are levels of data catalogs that relate to ML practitioners beyond a standard data catalog, namely:  \nFinding pre-processed datasets  \nThe bulk of development time in an ML project typically is spent finding, pre-processing and preparing data. Thus, significant savings can result by ensuring that ML practitioners are able to easily discover existing datasets.  \nThe data versioning guide provides more detailed information on how versioning may be achieved.  \nFinding pre-trained ML models  \nSimilarly, being able to find a model that has already been trained can avoid the costs associated with duplicated effort. A model registry allows an organization to store and version models and also helps to organize and keep track of trained models.  \nFor more information, see Azure Machine Learning Model Registry.  \nFinding pre-computed/pre-extracted ML features  \nBeing able to store and re-use pre-computed or extracted features from a trained ML model saves an organization both development time and compute costs. It is important for organizations dealing with a large number of machine learning models to have an effective way to discover and select these features. These needs are typically now met with what is known as a Feature Store.  \nA Feature Store is a data system designed to manage raw data transformations into features - data inputs for a model. Feature Stores often include metadata management tools to register, share, and track features as well.  \nThe feature store adoption guide dives deeper into whether a Feature Store may be right for your project.  \nFinding previous experiments  \nTracking experiments in ML is essential to ensure repeatability and transparency. It allows for a clear record of work already done, making it easier to understand and replicate experiments. Keeping a complete audit record of experiments allows organizations to avoid duplicating efforts and incurring unnecessary costs. Even if not all models make it to production, the recorded experiments enable the organization to resume the work in the future.  \nFor more information, see AI experimentation for more detailed guidance.  \nSome useful resources for data catalogs:  \nMedical Imaging Server for DICOM: Catalog and discover medical imagery for a holistic view of patient data  \nData Classification  \nData Classification is the process of defining and categorizing data and other critical business information. Data Classification typically relates to the sensitivity of the data and who can access it. Data classification enables administrators to identify sensitive data and determine how it should be accessed and shared. Data Classification forms an essential first step toward data regulation compliancy.  \n\u2139\ufe0f Refer to the DataOps: data classification section for more information nuanced to a Data Engineer/Governance role.\n\u2139\ufe0f For information on creating a Data Classification Framework, refer to the Create a well-designed data classification framework article on Microsoft Learn.  \nData Classification from an MLOps perspective  \nData classification within ML relates to using ML algorithms and models to tag data by some means of an inferred relationship. At this foundational data stage, this classification may relate to identifying PII and other sensitive data, or as part of the Data Discovery process, logically grouping or clustering data that has not been explored or understood.  \nData lineage and versioning  \nData lineage is the process of tracking the flow of data over time from its provenance, transformations applied, final output and consumption within a Data Pipeline.  \nUnderstanding this lineage is crucial for many reasons, for example, to facilitate troubleshooting, to validate accuracy and for consistency.  \n\u2139\ufe0f Refer to the DataOps: Data Lineage section for more information nuanced to a Data Engineer/Governance role.  \nData lineage and versioning from an MLOps perspective  \nData lineage in machine learning requires tracking the origin and transformations applied to the data and includes both the ML-specific transformations and any existing transformations from the DataOps data pipeline.  \nSee Versioning and Tracking ML Datasets for more information on implementing this in Azure Machine Learning through its dataset registration and versioning capabilities. The data versioning guide in this playbook provides more general guidance on the patterns and concepts used.  \nBy registering a dataset, you have a clear and complete record of the specific data used to train an ML model. This is a vital component of MLOps because it enables reproducibility and comprehensive interpretation of the model\u2019s predictions. Data versions are useful to tag the addition of new training data or the application of data engineering/feature transform techniques. Note that in Azure ML pipelines, it's possible to register and version both the input and output datasets passed through the pipeline steps. This is crucial for interpretability and reproducibility across a machine learning workflow, as emphasized above.  \nData Quality  \nData Quality may be defined as its suitability to solve the underlying business problem. In general, the following characteristics may be applied to determine the quality of data, amongst others, namely:  \nValidity: The data is stored in the same format.  \nCompleteness: All data attributes are present and consistently collected and stored.  \nConsistency: The same rules are applied consistently to all of the data.  \nAccuracy: The data represents exactly what it should.  \nTimeliness: The data is relevant to the time frame, which needs to be addressed within the business context.  \n\u2139\ufe0f Refer to the Data: Data Quality section for more information nuanced to a Data Engineer/Governance role.  \nGreat Expectations is a useful tool for assessing data quality.  \nData quality from an MLOps perspective  \nFor an ML practitioner, data quality can include characteristics such as:  \nIts distribution, outliers and completeness.  \nIts representativeness to solve a specific business problem using ML.  \nWhether it contains PII data and whether redaction or obfuscation is required.  \nRefer to Exploratory data analysis for information.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\data-curation\\data-governance.md"
    },
    {
        "chunkId": "chunk258_0",
        "chunkContent": "author: shanepeckham\ntitle: Data Labeling\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Data Labeling is the process of adding metadata and information to existing data. It helps in enriching existing data and is useful for downstream processes to act on them. Although it is not necessary for every project, a data catalog is helpful for projects involving unstructured blob data such as images, videos, documents. However, it can also be useful for structured data i.e. JSON, CSV files.\nrings:\n- public  \nLabeling data to enable accurate AI model evaluation  \nData Labeling is the process of adding metadata and information to existing data. It helps to enrich existing data and is useful for downstream processes to act on. Although it is not necessary for every project, a data catalog is helpful for projects involving unstructured blob data such as images, videos, and documents. However, it can also be useful for structured data, such as, JSON, and CSV files.  \nWhy do you need to label data?  \nLabeling data is primarily done for the following reasons:  \nTo train a supervised model to enable classification of unseen new data.  \nTo index the labels for a search solution such as Azure Cognitive Search, to enable users to search meaningful terms.  \nFor more information on how to label images in Azure ML, look at .  \nWhat options are available for labeling data?  \nData labeling can be approached in various ways, including manual labeling within the organization, outsourcing it, or using machine learning to automate it. It can be a very time consuming and expensive exercise and if not done well, will lead to poor performance and accuracy.  \nApproach Pros Cons Within the organization Users know the data and terminology. Full control Data does not leave the organization Team needs to be dedicated to labeling Time consuming Expensive Requires training External Trained professionals Can be done quickly Data needs to leave the organization or external access granted Loss of control Crowdsourced Fast Cost efficient Data needs to leave the organization or external access granted Loss of control Quality not guaranteed Automated via ML Can be cost efficient Full control Data does not need to leave the organization Requires human validation Requires a Data Scientist for best results Synthetic data Protects private data Full control Data does not need to leave the organization Requires human validation Requires a Data Scientist for best results Can be time consuming to generate realistic and representative data Programmatic labeling Full control Data does not need to leave the organization Requires a Data Scientist for best results Requires development effort  \nWithin the organization  \nThis approach allows an organization to have full control of the labeling process. Sensitive data need not to leave the organization but it requires resources to be trained and dedicated to this task.  \nSome data labeling tools  \nTool/Service Modality Options Azure Machine Learning Data Labeling Images Text AutoML and automatic labeling Label Studio Images Text Audio Time-Series Custom UI Sloth Images Video Doccano Text Label Box Text Images Audio Medical Imagery Geospatial Video Industry-specific solutions Playment Text Video 3d Sensor Audio Geospatial Synthetic data Light Tag Text SuperAnnotate Text Video Images Full end to end pipelines CVAT Video Images DataTurks Text Video Images spaCy Explosion Text v7 Images Supervisely Images Universal Data Tool Images Text Audio Video No installation DataLoop Images Video 3D Sensor Yolo Mark Images Pixel Annotation Tool Images Open Labeling Images Video Med Tagger Images Medical Imagery Semi auto-image annotation tool Images  \nExternal  \nWith this option, labeling can be outsourced to an external company. It can be cost effective and quick, but external access must be granted to the data.  \nSome External and Crowdsourced labeling companies  \nService Modality Azure Machine Learning Vendor Labeling Images Text v7 Images Amazon Mechanical Turk Images Text ClickWorker Images Video Audio Appen Images Video Audio  \nAutomated via ML  \nThis approach uses unsupervised ML approaches to cluster the data together and then requires human input to assess the clusters. After organizing the data into meaningful clusters, labeling the clusters with relevant information becomes easy, and the labels can be extended to all the records within the clusters.  \nEnriching the data with meaningful labels has the advantage of quickly indexing the information using Azure Cognitive Search or training a supervised model. For more detail, refer to the Exploratory Data Analysis phase  \nThe Data Discovery solution uses unsupervised ML techniques to cluster large amounts of unstructured data automatically so that a domain expert can quickly label the cluster and the underlying records.  \nSynthetic data generation  \nSynthetic data is artificially generated data that is representative of real data. It can be beneficial for many reasons, for example PII data and other sensitive data can be removed or obfuscated to share more broadly. Synthetic data generation requires development efforts but offers much flexibility.  \nProgrammatic labeling  \nThis approach entails taking ground truth values and programmatically labeling the data as opposed to manual labeling. Thus it can be a way to label a large amount of data quickly.  \nAn example scenario would be if a customer wants to train a Form Recognizer model to extract values from a form. Form Recognizer utilizes transfer learning to train the underlying model, requiring only a few labeled forms per form type. However, when dealing with thousands of form types, a considerable amount of manual labeling is still necessary.  \nWith programmatic labeling, the extracted ground truth values can be mapped to the existing forms and thus the labeling process can be automated.  \nThe image below illustrates the overall process:  \nFor assets and guidance, refer to the Auto-Labelling section of the Playbook for Knowledge Extraction For Forms Accelerators & Examples",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\data-curation\\data-labeling.md"
    },
    {
        "chunkId": "chunk259_0",
        "chunkContent": "author: shanepeckham\ntitle: Data privacy and security for AI (and other projects that use sensitive data)\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: As the amount of data that an organization collects and uses for analyses increases, so do concerns of privacy and security. Analyses require data. Typically, the more data used to train Machine Learning models, the more accurate they are likely to be. When personal information is used for analysis, it's especially important that the data remains private throughout its use.\nrings:\n- public\ntags:\n- Financial Services\n- Healthcare  \nData privacy and security for AI (and other projects that use sensitive data)  \n{% set thisPage = self._TemplateReference__context.page.url %}  \nAs the amount of data that an organization collects and uses for analyses increases, so do concerns of privacy and security. Analyses require data. Typically, the more data used to train Machine Learning models, the more accurate they are likely to be. When personal information is used for analysis, it's especially important that the data remains private throughout its use.  \n\u2139\ufe0f Refer to the Data: Data Governance and Protection content for more information nuanced to a Data Engineer/Governance role. For best practices, refer to the Data Security Best Practices in the DataOps content.  \nData Privacy from an MLOps perspective  \nData privacy in MLOps can relate to the following areas:  \nProtecting sensitive data: Anonymizing, obfuscating and synthesizing PII data for model training.  \nFederated Learning or Multi-Party Computation: Building ML models across multiple parties where no single party is allowed access to all of the data.  \nHomomorphic Encryption: Using encrypted data exclusively during the entire MLOps process.  \nDifferential Privacy: A set of systems and practices that help keep the data of individuals safe and private.  \nTrusted Research Environments: Enforce a secure boundary around distinct workspaces to enable information governance controls to be enforced.  \nRemote Execution: Executing ML on machines that ML practitioners do not have access to.  \nPII data redaction, anonymization and synthetic data  \nData anonymization is the process to alter the data so that a data subject can no longer be identified directly or indirectly.  \nData obfuscation or pseudonymization, means to replace any information, which could be used to identify an individual with a pseudonym. It can still allow for some form of re-identification of the data.  \nRefer to the Pseudonymization definition link for more information.  \nRefer to the Anonymization definition link for more information.  \n\u2139\ufe0f Refer to the DataOps: Data Privacy for masking and obfuscation from a Data Engineering perspective.  \nBelow are some PII data detection and masking resources for Azure:  \nMicrosoft Presidio: Microsoft Presidio helps to ensure sensitive data is properly managed and governed. It provides identification and anonymization modules for private entities in text and images such as: credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data.  \nPII Detection Azure Cognitive Service: the PII detection feature can identify, categorize, and redact sensitive information in unstructured text. For example: phone numbers, email addresses, and forms of identification.  \nMasking PII data in Azure Cognitive Search: The PII Detection skill extracts personal information from an input text and gives you the option of masking it.  \nBelow are some synthetic data resources in Azure:  \n{% set link = '../../solutions/redacting-and-obfuscating-scanned-forms-and-images/index.md' %}\n{% if relative_path_exists(thisPage, link) %}  \nThe Data Obfuscation Pipeline Solution - An end to end solution to mask, obfuscate PII data or generate synthetic values in images.\n{% endif %}  \nMicrosoft DSynth: DSynth is a flexible template-driven data generator.  \nMicrosoft Synthetic data showcase: Generates synthetic data and user interfaces for privacy-preserving data sharing and analysis.  \nFederated Learning or Multi-Party Computation  \nAccording to the Wikipedia definition for Federated Learning, it is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging them. This approach differs from traditional centralized machine learning methods, where all local datasets are uploaded to a single server. It also differs from classical decentralized approaches that assume local data samples have identical distributions.  \nBelow are some Federated Learning resources for Azure:  \nMicrosoft Flute: Federated Learning Utilities and Tools for Experimentation  \nHomomorphic Encryption  \nHomomorphic encryption allows computation directly on encrypted data, making it easier to apply the potential of the cloud for privacy-critical data.  \nMicrosoft SEAL: Microsoft SEAL is an easy-to-use open-source (MIT licensed) homomorphic encryption library developed by the Cryptography and Privacy Research Group at Microsoft.  \nRefer to the SEAL documentation and Microsoft Research SEAL for detailed guidance.  \nDifferential Privacy  \nDifferential privacy is a set of systems and practices that help keep the data of individuals safe and private. In machine learning solutions, differential privacy might be required for regulatory compliance.  \nRefer to this document for more information on Differential privacy in machine learning.  \nSome resources include:  \nMicrosoft Synthetic data showcase: Generates synthetic data and user interfaces for privacy-preserving data sharing and analysis.  \nAzure CounterFit: a CLI that provides a generic automation layer for assessing the security of ML models.  \nSmartNoise: Differential privacy validator and runtime.  \nTrusted Research Environments  \nTrusted Research Environments (TREs) enforce a secure boundary around distinct workspaces to enable information governance controls to be enforced. Each workspace is accessible by a set of authorized users, prevents the exfiltration of sensitive data, and has access to one or more datasets provided by the data platform.  \nSome resources include:  \nAzure Trusted Research Environment: on Azure  \nAzure confidential ledger: Tamperproof, unstructured data store hosted in trusted execution environments (TEEs) and backed by cryptographically verifiable evidence:  \nSecure MLOps solutions with Azure network security: How to protect ML solutions using Azure network security capabilities such as: Azure Virtual Network, network peering, Azure Private Link, and Azure DNS.  \nRemote Execution  \nIt is the ability to remotely and securely execute ML.  \nSome resources include:  \nCloud analytics options  \nMicrosoft Azure Attestation: A unified solution to remotely verifying the trustworthiness of a platform and integrity of the binaries running on it.  \nSecurity management in Azure: A comprehensive security resource for Azure.  \nPySyft: PySyft is an open-source library that provides secure and private Deep Learning in Python.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\data-curation\\data-privacy.md"
    },
    {
        "chunkId": "chunk260_0",
        "chunkContent": "author: shanepeckham\ntitle: Understanding data curation and management for AI projects\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: The role of data is vital to the success of an AI project, as unlike traditional software development, production level AI solutions couple engineering practices with data-based models. Data is essential for an ML solution, as the quality of the model depends on the quality of the training data.\nrings:\n- public  \nUnderstanding data curation and management for AI projects  \nThe role of data is vital to the success of an AI project solution and the quality of the model depends on the quality of the training data. Unlike traditional software development, production-level AI solutions couple software engineering with data-based models.  \nBefore an AI-based solution can be implemented and operationalized for production, specific data fundamentals, known as DataOps, have to be in place. DataOps is a lifecycle approach to data management. It uses agile practices to orchestrate tools, code, and infrastructure to quickly deliver high-quality data with improved security.  \nPragmatically stated, DataOps aims to reduce the lifecycle cost of data management while increasing and improving the value data generates for an organization. Similar to MLOPs, DataOps is concerned with the automation and testing of both code and data. Both share the goal of delivering business value from data.  \nThere are large areas of overlap between DataOps and MLOps, but they are nuanced by role. For example, both data and AI engineers deal with synthesizing data, but their focuses differ. A data engineer may emphasize testing data privacy controls, while an AI engineer concentrates on training AI models that accurately represent real data and do not reveal personally identifiable information (PII).  \n\ud83d\udca1Key outcomes of data curation:  \nThe sources of data have been identified and the data can be ingested for curation.  \nData sensitivity, security and privacy controls are in place to protect data, and project engineers can only access data for which they have permission.  \nNormalization, transformation, and other required pre-processing have been at least identified, and potentially applied, to the data.  \nWe will cover a few key areas of DataOps and their significance to MLOps.  \nData governance  \nData governance is a set of capabilities that ensure the availability, reliability, and security of high-quality data throughout its lifecycle. It involves implementing data controls to support business objectives. It also encompasses the people, processes and technologies required to discover, manage and protect internal data assets.  \nFor more detail, refer to AI data governance  \nDevOps for data  \nDevOps can be defined as the union of people, process, and products to enable continuous delivery of value to the business.  \nDevOps for data is concerned with the orchestration and automation of data pipelines that convert raw data to data of value to an organization.  \n\u2139\ufe0f For more detail, refer to the Data: DevOps for Data section.  \nData requirements for an AI project  \nBefore an AI project can be undertaken, and any fundamental data requirements are implemented, affirmative answers are required for each of the following questions:  \nCan the data be accessed both for training and inference purposes?  \nIs the data siloed or centralized?  \nIs only the correct data accessible? Are data sensitivity and privacy controls in place?  \nIs the data representative of the problem to be solved?  \nIs a Subject Matter Expert (SME) who understands the data available throughout the project?  \nDoes the project team understand the data?  \nData engineering  \nData engineering relates to all aspects of data normalization, pre-processing and other forms of data preparation. It is a prerequisite for an ML project to be successful.  \nFor more detail, refer to Data engineering for AI projects  \nData labeling  \nData labeling is the process of adding metadata and information to existing data.  \nFor more detail, see Labeling AI data  \nData versioning  \nData versioning tracks datasets as they evolve over time, allowing us to identify what version of a dataset was used to train a given model and thus enabling reproducibility.  \nFor more detail, see Exploratory data analysis: Data versioning  \nData privacy  \nData privacy ensures that sensitive data is properly managed and governed.  \nFor more detail, refer to AI data privacy  \nData security  \nData Security Governance (DSG) contains all policies and processes, which deal specifically with protecting corporate data (in both structured database and unstructured file-based forms). It greatly affects the ability to access the data on which the model will be trained.  \nFor more detail, see Data privacy and security for AI  \nCustomer readiness: Data curation  \nData curation is an important responsibility for data engineers, who must clean raw data ingested from multiple sources. Typical data problems include duplicates, invalid data, inconsistency, non-compliance with standard formats, and the presence of PII data.  \nHaving high confidence in data quality and compliance with data access rules is crucial for the success of AI projects.  \nData engineers find that different customers have different levels of maturity when it comes to data curation. Engineers should assess each customer's capability to identify and classify data, to assess data quality, and to protect data security. If a customer understand these aspects of data that has been ingested, or if they are still developing this capability, data engineers should work with the customer to verify data quality and security before using it for the AI project. If a customer has low confidence on the available data, there is risk that AI models will not deliver expected or usable results.  \nStandardizing data  \nData from multiple sources can be in different formats. For example, dates can be in dd/mm/yyyy format from one source and mm/dd/yy from another. It is important that customer and their engineers are able to provide insights that enable data to be standardized.  \nIdentifying PII  \nSome data may contain personally identifiable information (PII) about consumers, customers, or employees. This data should be obfuscated, masked and/or even encrypted. Customers should be able to identify their PII data and obfuscate/mask it before the source data are ingested for use as part of the AI project. Customers might be able to generate synthetic data to replace PII data.  \nFor for more information, see Obfuscation and Masking.  \nData integrity and uniformity  \nRules for data uniformity and correctness are important aspects of customer readiness. Customers should be able to determine rules for uniformity. For example, 'United states' from one source can be mapped to 'US' from another source and vice-versa.  \nThe customer should be aware of data domain integrity rules to ensure that the data adheres to data ranges, uniqueness and relational integrity while performing the data curation exercise.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\data-curation\\index.md"
    },
    {
        "chunkId": "chunk261_0",
        "chunkContent": "author: shanepeckham\ntitle: Data discovery: Understanding data for AI and beyond\nms.author: shanepec\nms.service: playbook\ndescription: The data discovery process can be enhanced by using ML. By using ML techniques, data discovery can become smart, can discover relationships between data and accelerate an organization's understanding of their data.\nrings:\n- public  \nData discovery: Understanding data for AI and beyond  \nThis article discusses common data discovery challenges related to finding data for use in AI projects.  \nDiscovery challenges for data consumers  \nTraditionally, discovering enterprise data sources has been an organic process based on communal knowledge. For companies that want the most value from their information assets, this approach presents many challenges:  \nBecause there's no central location to register data sources, users might be unaware of a data source unless they come into contact with it as part of another process.  \nUnless users know the location of a data source, they can't connect to the data by using a client application. Data-consumption experiences require users to know the connection string or path.  \nThe intended use of the data is hidden to users unless they know the location of a data source's documentation. Data sources and documentation might live in several places and be consumed through different kinds of experiences.  \nIf users have questions about an information asset, they must locate the expert or team responsible for that data and engage them offline. There's no explicit connection between the data and the experts that understand the data's context.  \nUnless users understand the process for requesting access to the data source, discovering the data source and its documentation won't help them access the data.  \nDiscovery challenges for data producers  \nAlthough data consumers face the previously mentioned challenges, users who are responsible for producing and maintaining information assets face challenges of their own:  \nAnnotating data sources with descriptive metadata is often a lost effort. Client applications typically ignore descriptions that are stored in the data source.  \nCreating documentation for data sources can be difficult and it's an ongoing responsibility to keep documentation in sync with data sources. Users might not trust documentation that's perceived as being out of date.  \nCreating and maintaining documentation for data sources is complex and time-consuming. - Making that documentation readily available to everyone who uses the data source can be even more so.  \nRestricting access to data sources and ensuring that data consumers know how to request access is an ongoing challenge.  \nAll these challenges present a significant barrier for companies that want to encourage and promote the use of enterprise data.  \nDiscovery challenges for security administrators  \nUsers who are responsible for ensuring the security of their organization's data may have any of the challenges listed above as data consumers and producers, and the following extra challenges:  \nAn organization's data is constantly growing and being stored and shared in new directions. The task of discovering, protecting, and governing your sensitive data is one that never ends. You need to ensure that your organization's content is being shared with the correct people, applications, and with the correct permissions.  \nUnderstanding the risk levels in your organization's data requires diving deep into it, looking for keywords, RegEx patterns, and sensitive data types. For example, sensitive data types might include Credit Card numbers, Social Security numbers or Bank Account numbers. You must constantly monitor all data sources for sensitive content, as even the smallest amount of data loss can be critical to your organization.  \nEnsuring compliance with corporate security policies can be a challenge for organizations. As their content grows and the policies are updated to address evolving digital realities. Security administrators need to ensure data security in the quickest time possible.  \nMicrosoft Purview provides capabilities to help address these challenges.  \nData discovery from an MLOps perspective  \nThe data discovery process can be enhanced by using ML. By using ML techniques, data discovery can become smart, can discover relationships between data and accelerate an organization's understanding of their data.  \nCoupled with visualizations, data analysts and business domain experts can quickly derive insights from previously unexplored data.  \nBelow are some examples of how ML can be used to address the typical challenges during the data discovery process:  \nPropose data preparation steps such as normalization and handling of missing data  \nInfer relationships between unstructured data types such as documents, video and images, which are difficult to work with  \nDetect Personal Identifiable Information (PII) and other types of sensitive data  \nEnrich and index data meaningfully for users to easily search and discover on their own.  \nPerform automatic data translation so that it is accessible to users in different languages  \nIdentify outliers and patterns in the data  \nDetect anomalies in the data  \nImprove understanding of behavioral data - users and customers. Generate subsequential recommendations  \nDiscover more data sources, which may be useful to ML practitioners  \nSome useful resources for data discovery:  \nResource Description Data Discovery Toolkit - Unstructured data A  repository containing guidance and code assets that use various Machine Learning techniques to discover insights in unstructured data such as documents, images and videos AI Enrichment Pipeline tutorial Complete sample for processing text, image and video files through a full enrichment pipeline with event grid, service bus, functions, logic apps, cognitive services and video indexer Azure Cognitive Search - An AI-first approach to content understanding This project demonstrates how you can use both the built-in and custom AI in Cognitive Search. Cognitive Search ingests your data from almost any datasource and enriches it using a set of cognitive skills that extract knowledge and then lets you explore the data using Search. Azure Cognitive Search Powerskills Power Skills are a collection of useful functions to be deployed as custom skills for Azure Cognitive Search. Microsoft Presidio Presidio can help identify sensitive/PII data in un/structured text. PII Detection Cognitive Skill The PII Detection skill extracts personal information from an input text and gives you the option of masking it. End to end Knowledge Mining for Video A video discovery pipeline that includes Azure Search and user feedback  \n\u2139\ufe0f Refer to the DataOps: Data Discovery section for more information nuanced to a Data Engineer/Governance role.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\data-curation\\data-engineering\\data-discovery.md"
    },
    {
        "chunkId": "chunk262_0",
        "chunkContent": "author: shanepeckham\ntitle: Data Engineering\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Data must be carefully engineered if an AI project is to be successful. Data engineering includes data normalization, pre-processing, enrichment and other forms of data preparation.\nrings:\n- public  \nEngineering usable data for AI projects  \nData must be carefully engineered if an AI project is to be successful. Data engineering includes data normalization, pre-processing, enrichment and other forms of data preparation.  \nData ingestion  \nData ingestion is the process of extracting data from one or multiple sources and then preparing it for training an ML model. This process can be quite time intensive if done manually and/or if dealing with substantial amounts of data. It includes both structured and unstructured data in different formats from varying source types.  \n\u2139\ufe0f Refer to the DataOps: Data Ingestion section for more information nuanced to a Data Engineer/Governance role.  \nData discovery  \nData discovery may be defined as the collection and evaluation of data from disparate sources with the goal of deriving business value from that data.  \nFor more information, see Data discovery: Finding data sources for AI projects.  \nData enrichment  \nData enrichment is a general term that refers to processes used to enhance, refine or otherwise improve raw data. Within the context of MLOps, we will refer to the process of enriching data using ML models and techniques.  \nCase studies: Aggregating data for AutoML Image Object Detection  \nIn order to train a computer vision model, such as AutoML Image Object Detection, you require labeled training data. The images needs to be uploaded to the Azure Blob Storage and label annotations (of each image) need to be in a JSONL format. Once all images are labeled, you can perform data aggregation which will transform\nthese multiple label annotations into a single JSON file, which you can use to create an MLTable that will serve as data input of this Azure ML model.  \nDuring this step, data aggregation helps to transform raw data into meaningful and useful information that can be used to train, test, and evaluate\nthe machine learning model.  \nRefer to the AML v2 P&ID symbol detection train sample project as a sample implementation of an AML workflow to train a symbol detection model. This\nincludes a data aggregation step for transforming the stored image and label datasets into a format that the AutoML training job can consume.  \nCase studies: Normalizing data for Form Recognizer  \nIn the following example, scanned images require enrichment to handle noise or poor quality scans that impact the usability of an OCR model. Quality of the images will also clearly impact the accuracy of the extraction of data from the form. Data normalization is part of the process of preparing data for training an ML model. This process transforms raw ingested data into a consistent format that is readily consumable by downstream steps/tasks and processes.  \nDuring this step, the data is normalized, de-noised for improved results, and evaluated for techniques on how the data can be segmented.  \nRefer to the Pre-Processing section of the Playbook for Knowledge Extraction For Forms Accelerators & Examples  \nCase studies: Enriching data  \nThe following repository showcases a collection of small and discrete data enrichment functions, using various infrastructures. The functions are built for Azure Cognitive Search, but they can be used in any data enrichment pipeline. These PowerSkills contain standard API interfaces so they can be consistently consumed.  \nRefer to the Azure Search PowerSkills for assets, guidance and examples.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\data-curation\\data-engineering\\index.md"
    },
    {
        "chunkId": "chunk263_0",
        "chunkContent": "author: shanepeckham\ntitle: AI Model Deployment\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Model deployment involves taking a trained ML model, packaging it in a reusable asset (be it a container image or a pipeline), and deploying that asset to be used for inference. It also includes packaging data preparation code alongside the model to manipulate incoming data to match the expected inputs for the trained model.\nrings:\n- public  \nDeployment  \nModel deployment involves taking a trained ML model, packaging it in a reusable asset (be it a container image or a pipeline), and deploying that asset to be used for inference. It also includes packaging data preparation code alongside the model to manipulate incoming data to match the expected inputs for the trained model.  \n\ud83d\udca1Key Outcomes of Deployment:  \nPerformance and parallel processing improvements are implemented against the optimum compute environment.  \nMonitoring and observability consistent with the entire solution have been implemented.  \nThe target deployment endpoint is available to other services within the solution. Security, authentication and access controls are implemented.  \nIntegration and system tests are in place.  \nA release pipeline per environment, bound to source control is implemented.  \nBatch or runtime scoring monitoring is in place to track the model\u2019s performance over time.  \nUser interactions with the model are logged for future improvement  \nModel Flighting  \nDetails on Model Flighting can be found in the Azure ML official docs  \nModel Release  \nDetails on Model Release can be found in Model Release  \nDeployment Infrastructure  \nThere are several ways to deploy ML solutions in the cloud or on the edge.  \nCloud-based deployment  \nAzure offers many different options for hosting ML solutions in the cloud.  \nAzure Kubernetes Service  \nAzure Kubernetes Service (AKS) makes it simple to deploy a managed Kubernetes cluster in Azure. AKS reduces the complexity and operational overhead of managing Kubernetes by offloading much of that responsibility to Azure. As a hosted Kubernetes service, Azure handles critical tasks like health monitoring and maintenance for you.  \nAzure Machine Learning  \nAzure Machine Learning service is a cloud\nservice used to train, deploy, automate, and manage machine learning models, all at a broad scale.  \nAzure Logic Apps  \nConnect your business-critical apps and services with Azure Logic Apps, automating your workflows without writing a single line of code.  \nAzure Functions  \nAzure Functions is a computing service that allows you to run your code in response to events happening in Azure, third-party, or on-premises systems. It enhances the Azure application platform by providing an event-driven, compute-on-demand experience.  \nAzure Cognitive Search  \nAzure Cognitive Search is a managed service that provides an enrichment pipeline. This service can extract information, invoke custom APIs along with Azure Cognitive Services and store the output to a datastore.  \nEdge-based deployment  \nIn addition to the cloud deployment, the models created in Azure ML can also be deployed to \"the edge\".  \nIn this case, the model is used on computes that reside outside the cloud - and either on premise, or on an isolated device.  \nThe most common reasons for edge deployment are:  \nPerformance the time taken to have the inference data travel up and down the cloud can be eliminated, reducing the total inference time  \nSecurity the user may be concerned about sending inference data to a cloud service. This may be due either to some regulations preventing them from doing so, or for some safety or security concerns.  \nThe most common mechanism for edge-deployment is normally done by creating a docker container image with the ML model, and deploying this image to an on-premise or disconnected compute.  \n{% if extra.ring == 'internal' %}\nMore information on the mechanisms used can be found on this link: Edge Inference\n{% endif %}  \nWhen that is the case, some mechanisms are used to deploy the container(s).  \nAzure IoT Edge Modules  \nAzure IoT Edge is a mechanism to deploy custom business logic to edge devices with centralized management via Azure IoT Hub.  \nMore information on it can be found in What is IoT Edge?.  \nKubernetes clusters and Azure ARC  \nAzure Arc simplifies governance and management of on-premise and multi-cloud infrastructure by delivering a consistent multi-cloud and on-premises management platform.  \nFor instance, using Azure ARC the user can deploy containers directly to on-premise infrastructure (such as edge-devices).  \nMore information on Azure ARC can be found in Azure ARC.  \nOthers  \nWhilst there are other mechanisms to deploy Docker containers to edge devices, we will focus on the two mechanisms mentioned above.  \nAdditional Deployment Information  \nFor an in-depth exploration of using Azure Machine Learning to deploy models, see the Azure ML Deployment Trade Study document in this repository.  \nOfficial Azure ML docs on deployment:  \nOnline Endpoints using CLI v2  \nBatch Endpoints using CLI v2",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\deployment\\index.md"
    },
    {
        "chunkId": "chunk264_0",
        "chunkContent": "author: shanepeckham\ntitle: AI Model Release\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Model Release involves the process of packaging the latest (and ideally best performing) model from the training pipeline, and promoting it through to the Production environment.\nrings:\n- public  \nModel Release  \nModel Release involves the process of packaging the latest (and ideally best performing) model from the training pipeline,\nand promoting it through to the Production environment.  \nModel Packaging Options  \n{% if extra.ring == 'internal' %}  \nModel processing pipeline for generating docker image\n{% endif %}  \nModel deployment options in Azure ML  \nDeploying an MLFlow model using Azure ML SDK 2  \nConverting a Custom Model into an MLFlow Model  \nModel processing pipeline for generating a docker image  \nAfter a model is determined through experimentation by a data scientist, it's time to deploy to an environment. Although it is possible to deploy a model along with its artifacts directly to an environment, a better practice is to configure a docker image with model artifacts. Then, run containers based on the docker image. Docker helps in providing more flexibility to test the model including security scanning, smoke test and publishing it to a container registry. {% if extra.ring == 'internal' %}The details about this asset is available at Model processing pipeline for generating docker image.{% endif %}  \nModel promotion using Shared Registries  \nCurrently in preview, the Machine Learning Registries for MLOps in Azure ML allows you to register a model once, and easily retrieve it across multiple workspaces (including in different subscriptions). Instead of copying a model to workspaces in each deployment environment, each of those workspaces could refer to the same registry. Then, as a model is tested, its tags are updated to reflect its status. Models ready for production can be deployed directly from the shared registry to the production environment.  \nModel Release Example: Finding the best performing AutoML model during the build process  \nThe following example illustrates how to deploy a solution that uses Azure ML's AutoML functionality to:  \nlook for the best performing model  \ndetermine if more data has been labeled and a training run has initiated  \ndownload it for serving.  \nPerforming unit and smoke tests are recommended to ensure any new models perform as expected.  \nThis example is part of a larger end-to-end solution: \"Building a custom video search experience using Azure Video Indexer for Media, Azure Machine Learning and Azure Cognitive Search\".  \nRefer to the Azure ML AutoML containerized API for an example of automated model release.  \nModel Release Example: How to manage a Form Recognizer (FR) model  \nWhen submitting a request to train a custom model, FR generates a unique model id to refer to the model training attempt. FR expects the user to track this model id and pass it whenever they need to use or manage the trained custom model. In addition to the model id, FR tracks when the model was trained, training duration, and the status of the model. Status examples include ready for inference, pending training, etc.  \nAs a user, you need to create a custom solution to track your model ids. The model id is a UUID, thus it would be beneficial to track additional information as part of your solution. For example, a description of the model, readable labels, an audit trail of how the model came to exist, and previous versions of the model. One thing to note, FR does not support retraining an existing model, instead it would generate a new model with a new model id for each training request. Each version of your model would have a different model id to be tracked.  \nFR has a max limit on how many models it can persist at any given time. Training attempts contribute towards this limit, regardless of whether the training was successful or not. You can identify your FR instance limit via the management APIs. Depending on your use case, you may need to manage which models to keep in your FR instance vs archive or remove. It might be beneficial to persist recent models to enable model rollback if needed. For archived models, the audit trail can persist the necessary information to re-train an archived model. On the other hand, if your use case justifiably requires persisting a high number of models, the FR models limit can be increased by reaching out to Azure customer support. It would be beneficial to evaluate the needed capacity before engaging with customer support.  \nFor assets and guidance for Form Recognizer model management and more, refer to the Playbook for Knowledge Extraction For Forms Accelerators & Examples",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\deployment\\model-release.md"
    },
    {
        "chunkId": "chunk265_0",
        "chunkContent": "author: shanepeckham\ntitle: Release Pipelines\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: In general, there are 2 paths to deploying ML code into Production environments. 1) Implement CI/CD pipelines on your development branch to deploy QA and Production environments; and 2) Use several branches to move code from development to QA and Production environments  \nrings:\n- public  \nRelease Pipelines  \nIn general, there are 2 paths to deploying ML code into Production environments:  \nImplement CI/CD pipelines on your development branch to deploy QA and Production environments  \nUse several branches to move code from development to QA and Production environments  \nAny of the deployment options can fit the proposed deployment workflow.  \nCI/CD on the development branch  \nThis approach is the most common, where we prepare and store the required artifacts (models and libraries) in advance. To indicate their readiness for production, we assign a special tag to the approved artifacts.  \nFor example, if we execute training from our development branch on the full dataset and we are generating a model, the model should be evaluated and approved. Once the model is ready, we can label it with a special attribute, like a \"production tag.\" Then, during the continuous deployment (CD) phase, we use the latest model version with the production tag for deployment. We don't care much about our code in the development branch because we are not planning to use it in production. We only use it to do training in our development environment.  \nRollback strategy can be implemented with no challenges: we just need to remove tag from the latest model and execute the CI/CD again to pick up the previous version that was ready for production.  \nIn above implementation, it's OK to use just one branch (main, for example) as the primary source for development and executing CI/CD.  \nThe following image demonstrates the final version of the CI/CD process:  \nAn example implementation of a CI pipeline for training in Azure ML can be found here in this basic template. This example pipeline executes several steps when a PR is merged into the development branch:  \nThe build_validation_pipeline.yml pipeline executes, running unit tests and code validation (such as flake8).  \nThe second stage executes several steps to kick off an Azure ML Pipeline\nconfigure_azureml_agent.yml installs pip requirements and uses the Azure CLI extension to install the Azure ML CLI\nconnect_to_workspace.yml uses pipeline variables to connect to the correct Azure ML workspace\ncreate_compute.yml ensures that there is available Azure ML compute resources to execute the training pipeline\nexecute_no_wait_job.yml uses the Azure ML CLI to deploy and trigger the Azure ML pipeline defined by the amlJobExecutionScript variable. In this case, it is ./mlops/nyc-taxi/pipeline.yml.\nThis step does not wait for the Azure ML pipeline to complete, as long running pipelines would hold the DevOps build agent. However, the execute_and_wait_job.yml step is available instead for scenarios where training may be quick, and quickly identifying failure is critical. In the pr_to_dev_pipeline.yml, the wait job is used for this reason  \nThe full template repo is available in the following repo.  \nThere are more templates based on a similar pattern available at:  \nMLOps Template for Azure ML SDK v2  \nMLOps Model Factory Template  \nMulti-branch strategy  \nYou can use several branches to move your code from development to production. This strategy works fine if you deploy the code itself, and don\u2019t update the production environment often. Also you have several versions of the production environment for different partners or departments.  \nWe cannot treat the development branch as a production ready branch. The branch is stable but moving it to production \"as is\" might cause some issues. For example, we could have updated training pipelines and no model exists based on it yet. Another example, somebody could activate a wrong model to be used in scoring service that is not critical for development branch, but critical for production.  \nHence, we propose having a flow moving development branch to production using two stages:  \nOnce we, data scientists, believe that the current version of the development branch is ready for production, we move code alongside the current model to a special QA branch. This branch is going to be used for testing to validate that our code and the models work fine. Additionally, we can test our deployment scripts there.  \nOnce all tests on the QA branch have been completed successfully, we can move code and the models from the QA branch to the production one.  \nWe use branches rather than tags because it allows us to execute some more DevOps pipelines prior to commit code from one branch to another. In these pipelines, we can implement an approval process and move ML pipelines between environments.  \nThe PR Build on the movement from the development to the QA branch should include the following tasks:  \nDeploy needed infrastructure for production. In most cases, you don\u2019t need to deploy your pipelines since you are not doing training in the production environment. So, you can deploy just scoring infrastructure.  \nCopy all latest approved models (approved models can be tagged).  \nImplementing this process, we are working across several branches:  \ndev: the development branch as the default stable branch for developers  \nQA: the QA branch to test scoring pipeline and the model  \nmain: the source branch for production environment  \nOnce we finish testing for our QA environment, we can create another PR and start a process to move everything to main. The PR should be approved, and it will trigger the deployment Build. The Build has to update scoring infrastructure in the production environment and clone our model again.  \nDeployment Artifacts  \nDeployment artifacts in CI/CD process depend on the model usage in the production environment. The most common usage scenarios are batch scoring and runtime scoring:  \nBatch scoring: In this case, we use the model together with another ML pipeline to run it under the Machine Learning Pipeline engine umbrella (like Azure ML) that has been used for training.  \nRuntime scoring: In this case, we need a service to serve our model (e.g., Azure Functions, Azure Kubernetes Service, Azure ML Online Endpoints, locally run Docker container), pack the model to an image alongside with all needed components and make the deployment.  \nYou may also register artifacts such as Azure ML Environments, Pipeline Components, and models, in a Registry in an Azure ML Workspace.  \nEach scenario defines what components you need in the QA and Production environments.  \nBatch Scoring  \nFor batch scoring, model registry and compute pipeline resources must be deployed to infrastructure, such as an Azure ML Workspace, or a Databricks instance. These resources will be used to trigger scoring pipelines (which need to be managed as outlined in ML Pipelines. Implementing a CD pipeline for the production environment generally consists of two core steps: copy the latest ready for production model to QA/Prod environment, and publish the scoring service to the appropriate scoring pipeline (for example, Azure ML Pipeline, or Databricks pipeline).  \nAlternatively, using the Azure ML registries preview, multiple Workspaces are able to access shared models, components, and environments from a single registry - removing the need to copy the model between each Workspace. These shared registries work even across multiple Azure Subscriptions.  \nRuntime Scoring  \nFor a runtime service, a replicated model registry service is an optional component in the QA/Prod environment, and it depends on how you are planning to use the model. Depending on the scenario, several methods have been successful:  \nUsing the model as a separate entity in any kind of external application: copy the model to a known location in QA/Prod environment to make it available for all consumers. A separate QA/Prod model registry is not needed in this scenario  \nDeploying a model as a part of a custom image to serve in any service that is not connected to your model registry (or Azure ML): create a Docker image and deploy it to the desired service. If you are using Azure ML, you can re-use your Azure Container Registry (ACR) instance in the Dev environment, or a separate container registry.  \nAzure Kubernetes Service (AKS) that is managed by an Azure ML service: replicate Azure ML Workspace and AKS in each environment to follow the best security practices making sure that the AKS instances in each environment are isolated from one another.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\deployment\\release-pipelines.md"
    },
    {
        "chunkId": "chunk266_0",
        "chunkContent": "author: shanepeckham\ntitle: Algorithm exploration for AI projects\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Training an AI model is an iterative process. This article explores approaches for automatically finding an appropriate algorithm for use in AI model development\nrings:\n- public  \nAlgorithm exploration for AI projects  \nTraining an AI model is an iterative process. At the beginning of an AI project, we don't know which AI algorithm will produce the best performing model. Based on domain expertise, there is usually a small set of AI algorithms that perform well in a given domain. But, each of these algorithms must be tried and evaluated. This article explores approaches for automatically finding an appropriate algorithm for use in AI model development  \nAutoML  \nAutomated machine learning (automated ML or AutoML) is the process of partially automating algorithm and hyperparameter exploration during model development. It allows data scientists, analysts, and developers to build ML models at high scale, while sustaining model quality.  \nTraditional machine learning model development is resource-intensive, requiring significant domain knowledge and time to produce and compare dozens of models. Automated machine learning accelerates the time it takes to get production-ready ML models in cases with standard problem formulations, sufficient data quality, and sufficient data quantity.  \nRefer to the Ways to use AutoML in Azure Machine Learning for how to get started in Azure ML.  \nBelow are some AutoML resources for Azure:  \nAzure Machine Learning examples  \nMicrosoft NNI  \nAn open source toolkit for automating the ML lifecycle. The toolkit includes feature engineering, neural architecture search, model compression and hyperparameter tuning.  \nSet up AutoML training with the Azure ML Python SDK v2  \nSample project for an AML v2 pipeline + AutoML object detection  \nProvides a sample implementation of an AML pipeline using the v2 Python SDK to use AutoML on image training data for object detection for AutoML object detection; dataset registration and versioning for consistent data lineage records; and stratified splitting techniques for consistent feature representation across the training and validation sets.  \nAutoML Vision Classifier  \nIllustrates how to deploy an AutoML generated model to a Docker container, using FastAPI and Uvicorn to check for best performing model.  \nPrevent overfitting and imbalanced data with AutoML  \nOther useful resources:  \nTPOT: TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\experimentation\\algorithm-exploration.md"
    },
    {
        "chunkId": "chunk267_0",
        "chunkContent": "author: shanepeckham\ntitle: Using MLOps to assist with experimentation\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: How can experimentation and model development code be shared? Ignoring this issue at beginning of the project can lead to a situation where data scientists and software engineers are working on two different code bases and spending significant time to sync code between them. The good news is that MLOps provides a way to solve the problem.\nrings:\n- public  \nUsing MLOps to assist with experimentation  \nIn the introduction, we provided some possible outcomes and aspects of the experimentation phase. At the same time, the primary goal of the phase is to find a clear way/solution/algorithm to solve an ML-related problem. Obviously, until we have an understanding of how to solve the problem, we can't start the model development phase, nor build an end-to-end ML flow.  \nThe Importance of MLOps  \nSome engineers believe that it's a good idea to wait until the experimentation phase is done prior to setting up MLOps. There are two challenges to this thinking:  \nThe experimentation phase has various outcomes that can be reused in the model development and even in the inferencing phases. Almost all the code, starting from data augmentation up to model training, can be migrated to the end to end ML flow.  \nThe experimentation phase can be overlapped with the model development phase. The phase begins once we know how to solve the ML problem and now want to tune parameters and components.  \nThese challenges can be summarized into a common problem statement: How can we share experimentation and model development code?  \nIgnoring this issue at beginning of the project can lead to a situation where data scientists and software engineers are working on two different code bases and spending significant time to sync code between them. The good news is that MLOps provides a way to solve the problem.  \nMLOps practices to implement  \nThe following sections summarize good MLOps practices that can help data scientists and software engineers collaborate and share code effectively.  \nStart working with DevOps engineers as soon as possible  \nPrior to starting any experiment, it's worthwhile to make sure that these conditions have been met:  \nDevelopment environment is in place: it can be a compute instance (Azure ML) or an interactive cluster (Databricks) or any other compute (including local one) that allows you to run experiments.  \nAll data sources should be created: it's a good idea to use an ML platform capability to build technology-specific entities like data assets in Azure ML. It allows you to work with some abstract entities rather than with specific data sources (like Azure Blob).  \nData should be uploaded: it's important to have access to different kinds of data sets to do experiments under different conditions. For example, data might be used on local devices, some may represent a toy data set for validation, and some may represent a full data set for training.  \nAuthentication should be in place: data scientists should have access to the right data with the ability to avoid entering credentials in their code.  \nAll the conditions are critical and must be managed via collaboration of ML Engineers and DevOps Engineers.  \nAs soon as data scientists have a stable idea, it's important to collaborate with DevOps engineers to make sure all needed code artifacts are in place to start migrating notebooks into pure Python code. The following steps must take place:  \nNumber of ML flows should be identified: each ML pipeline has its own operationalization aspects and DevOps engineers have to find a way to add all the aspects into CI/CD.  \nPlaceholders for ML pipelines should be created: each ML pipeline has its own folder structure and ability to use some common code in the repo. A placeholder is a folder with some code and configuration files that represents an empty ML pipeline. Placeholders can be added to CI/CD right away, even prior to migration having been started.  \nThe following diagram shows the stages discussed above:  \nStart with methods  \nIt doesn't take much effort to wrap your code into methods (even classes) once it's possible. Later, it will be easy to move the methods from the experimentation notebook into pure Python files.  \nWhen implementing methods in the experimentation notebook, it's important to make sure that there are no global variables, instead rely on parameters. Later, relying on parameters helps to port code outside of the notebook as is.  \nMove stable code outside notebooks as soon as possible  \nIf there are some methods/classes to share between notebooks or methods that are not going to be affected much in future iterations, it's worthwhile to move the methods into Python files and import them into the notebook.  \nAt this point, it's important to work with a DevOps engineer to make sure that linting and other checks are in place. If you begin moving code from notebooks to scripts inside the experimentation folder, it is okay to only apply linting. However, if you move your code into a main code folder (like src), it's important to follow all other rules like unit testing code coverage.  \nClean notebooks  \nCleaning up notebooks before a commit is also good practice. Output information might contain personal information data or non-relevant information that might complicate review. Work with a DevOps engineer to apply some rules and pre-commit hooks. For example, you can use nb-clean.  \nExecuting all notebook cells in sequence should work  \nBroken notebooks are a common situation when software engineers help data scientists to migrate their notebooks into a pipeline using a selected framework. It's especially true for new ML platforms (Azure ML v2) or for complex frameworks that require much coding to implement a basic pipeline, like Kubeflow. These errors are why it's important that all experimentation notebooks can be executed by a software engineer in order, including all cells, or including clear guidance on why a given cell is optional.  \nBenefits of using selected ML platform  \nIf you start using Azure ML or a similar technology during the experimentation, the migration will be smooth. For example, the speed benefits gained from using parallel compute in Azure ML may be worth the small amount of additional code required to configure it. Collaboration with Software Engineers can help find some killer feature in the selected ML framework and use it from the beginning.  \nNext steps  \nExperimentation in Azure ML: Different ways to conduct experiments in Azure ML and best practices about how to set up the development environment.\n{% if extra.ring == 'internal' %}  \nExperimentation in Databricks: How to share code from experiments on interactive clusters in Databricks.\n{% endif %}",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\experimentation\\experimentation-in-mlops.md"
    },
    {
        "chunkId": "chunk268_0",
        "chunkContent": "author: shanepeckham\ntitle: Exploratory Data Analysis\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Every machine learning project requires a deep understanding of the data to understand whether the data is representative of the problem. It is also important to determine the approaches to be undertaken, and for the project to be successful.\nrings:\n- public  \nUse Exploratory Data Analysis (EDA) to explore and understand your data  \nEvery machine learning project requires a deep understanding of the data to understand whether the data is representative of the problem. A systematic approach to understanding the data should be undertaken to ensure project success. Every machine learning project demands a deep understanding of the data to be used to be sure it represents the problem.  \nUnderstanding of the data typically takes place during the Exploratory Data Analysis (EDA) phase. It is a complex part of an AI project where data cleansing takes place, and outliers are identified. In this phase, suitability of the data is assessed to inform hypothesis generation and experimentation. EDA is typically a creative process used to find answers to questions about the data.  \nEDA relates to using AI to analyze and structure data, aimed at better understanding the data and includes:  \nIts distribution, outliers, and completeness  \nIts relevance to solving a specific business problem using ML  \nDoes it contain PII data and will redaction or obfuscation be required  \nHow it can be used to generate synthetic data if necessary  \nAlso as part of this phase data suitability is assessed for hypothesis generation and experimentation.  \nThe following image illustrates the various phases, their respective complexity and roles during a typical machine learning project:  \nHypothesis driven development and experiment tracking \ud83e\uddea  \nCode written during EDA may not make it to production, but treating it as production code is a best practice. It provides an audit and represents the investment made to determine the correct ML solution as part of a hypothesis-driven development approach.  \nIt allows teams to not only reproduce the experiments but also to learn from past lessons, saving time and associated development costs.  \nData versioning  \nIn an ML project it is vital to track the data that experiments are run on and models are trained on. Tracking this relationship or lineage between the ML solution and experiment ensures repeatability, improves interpretability, and is an engineering best practice.  \nDuring iterative labeling, data ingestion, and experimentation it is critical that experiments can be recreated. It can be for simple reasons, such as re-running demos, or for critical reasons like auditing for bias in a model, years after deployment. To replicate the creation of a given model, it is not enough to reuse the same hyperparameters, the same training data must be used as well. In some cases, it is where Feature Management Systems come in. In other cases, it may make more sense to implement data versioning in some other capacity (on its own, or with a Feature Management System).  \n\u2139\ufe0f Refer to the Data: Data Lineage section for more information nuanced to a Data Engineer/Governance role.  \nBasic implementation in Azure ML  \nA common pattern adopted in customer engagements follows these given guidelines:  \nData is stored in blob storage, organized by ingestion date  \nRaw data is treated as immutable  \nAzure ML Data assets are used as training data inputs  \nAll Data assets are tagged with date ranges related to their source.  \nDifferent versions of Data assets point to different date folders.  \nWith these guidelines in place, the guidance in the Azure ML Docs for Versioning and Tracking ML Datasets can be followed. The current docs reference the v1 SDK, but the concept translates to v2 as well.  \nSome useful resources for data registration and versioning:  \nResource Description Azure ML Python SDK v2 to train a symbol detection using P&ID synthetic dataset This project provides a sample implementation of an AML workflow to train a symbol detection model. It includes a \u2018data registration\u2019 step that demonstrates registering a data asset for later use in AML, including stratified splitting techniques for consistent feature representation across the training and validation sets.  \nThird-party tools  \nThird-party data versioning tools, such as DVC, also provide full data versioning and tracking with the overhead of using their tool set. They may work well for customers who are in a greenfield state, looking to build a robust MLOps pipeline. But it requires more work to integrate with existing data pipelines.  \nConsiderations for small datasets  \nRelatively small datasets, a few hundred MB or less, that change slowly are candidates for versioning within the project repository via Git Large File Storage (LFS). The commit of the repo at the point of model training will thus also specify the version of the training data used. Advantages include simplicity of setup and cross compatibility with different environments, such as, local, Databricks, and Azure ML compute resources. The disadvantages include large repository size and the necessity of encoding the repository commit at which training occurs.  \nDrift  \nData evolves over time and the key statistic values of data, used for training an ML Model, might undergo changes as compared to the real data used for prediction. It degrades the performance of a model over time. Periodic comparison and evaluation of both the datasets, training and inference data, is performed to find any drift in them. With enough historical data, this activity can be integrated into the Exploratory Data Analysis phase to understand expected drift characteristics.  \nRefer to the following page for a checklist on Data Drift questions to ask a customer\n{% set link = '../../technology-guidance/working-with-azure-ml/datadrift_operationalization.md' %}\n{% if relative_path_exists(thisPage, link) %}  \nRefer to the following section for a detailed overview of Data Drift Operationalization in Azure ML\n{% endif %}  \nRefer to the following page for detailed overview of Data Drift and Adaption  \nExperiment tracking  \nWhen tracking experiments, the goal is to not only be able to formulate a hypothesis and track the results of the experiment, but to also be able to fully reproduce the experiment if necessary, whether during this project or in a future related project.  \nIn order to be able to reproduce an experiment, below are details illustrating what needs to be tracked at a minimum:  \nCode and scripts used for the experiment  \nEnvironment configuration files (docker is a great way to reliably and easily reproduce an environment)  \nData versions used for training and evaluation  \nHyperParameters and other parameters  \nEvaluation metrics  \nModel weights  \nVisualizations and reports such as confusion matrices  \nCase studies  \nHere are some case studies that help illustrate how EDA is performed on real-world projects  \nSetting up a Development Environment  \nRefer to Experimentation in Azure ML for guidance on conducting experiments in Azure ML and best practices to set up the development environment.  \nWorking with unstructured data  \nThe Playbook for Data Discovery - Unstructured data aims to quickly provide structured views on your text, images and videos. All at scale using Synapse and unsupervised ML techniques that exploit state-of-the-art deep learning models.  \nThe goal is to present this data to and facilitate discussion with a business user/data owner quickly via Power BI visualization. Then the customer and team can decide the next best action with the data, identify outliers, or generate a training data set for a supervised model.  \nAnother goal is to help simplify and accelerate the complex Exploratory Data Analysis phase of the project by democratizing common data science functions.It also accelerate your project so that you can focus more on the business problem you are trying to solve.  \nWorking with structured data  \nForms: for working with forms, refer to the Playbook for Knowledge Extraction For Forms Accelerators & Examples  \nAzure Custom Translator: for analyzing unstructured data and translation memory files for translation using Custom Translator  \nGood places to start for EDA  \nRepository Modality Best Practices, code samples, and documentation for Computer Vision Images Natural Language Processing Best Practices & Examples Text Knowledge Extraction For Forms Accelerators & Examples Forms A data discovery and manipulation toolkit for unstructured data Images, Text Best Practices on Recommendation Systems Tabular Pandas Profiling Tabular Great Expectations: Always know what to expect from your data Tabular",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\experimentation\\exploratory-data-analysis.md"
    },
    {
        "chunkId": "chunk269_0",
        "chunkContent": "author: shanepeckham\ntitle: Feature Engineering\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Data has to be formulated and consumed in an input format that is expected by the underlying model. These inputs are generated by transforming, engineering and enriching the data and called features.\nrings:\n- public  \nFeature engineering  \nFeature engineering is a machine learning approach that creates new variables by analyzing the available data.  \n{% if extra.ring == 'internal' %}\nData has to be formulated and consumed in an input format that is expected by the underlying model. These inputs are generated by transforming, engineering and enriching the data and called features.  \nFeature Development  \nFeature engineering is the process of using domain knowledge to supplement, cull or create new features to aid the machine learning process with the goal of increasing the underlying model's predictive power.\n{% endif %}  \nEmbeddings  \nML models can learn how to represent data. For example, a deep learning model can learn embeddings during training, a lower dimensional representation of the data passing through the model.  \nFor more information on embeddings, refer to Understanding Embeddings in Azure OpenAI Service  \nFeature Extraction  \nA deep learning model can automatically learn an embedding or automatically engineer features from input data. This process can be applied to any input data (in the correct format) to get the associated embedding or data representation from the trained deep learning model. This process is known as feature extraction and can be useful to enrich or augment existing data, or to cluster the data to group together similar records.  \nHere are some tools that offer feature extraction capabilities.  \nTool Description Keras feature extraction notebook Shows how an existing model can be used to extract images features for clustering. For more information, see the Data Discovery Playbook InceptionV3 model pre-trained on imagenet data This model is used to extract data representations or features for images the model has never seen before, which may then be used to cluster the data. The output features are taken from the second last layer of the model where the last layer would typically represent the ImageNet class prediction.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\experimentation\\feature-engineering.md"
    },
    {
        "chunkId": "chunk270_0",
        "chunkContent": "author: shanepeckham\ntitle: AI Experimentation\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: In the experimentation phase, data scientists and ML engineers collaborate. They work on exploratory data analysis, prototyping ML approaches, feature engineering, and testing hypothesis.\nrings:\n- public  \nGetting started with AI experimentation  \nData scientists and AI engineers collaborate during the experimentation phase of an AI project. They work on exploratory data analysis, prototyping ML approaches, feature engineering, and testing hypotheses.  \nWhile hypothesis driven development and agile approaches are encouraged for ML project management, the focus here is on the engineering aspects of experimentation.  \nFor the data science aspects of experimentation, see Model Experimentation.  \n\ud83d\udca1Key outcomes of engineering experimentation:  \nNormalization, transformation, and other required pre-processing has been applied to the data (or a subset of the data). Pre-processed data is evaluated for feasibility and suitability for solving the business problem.  \nThe data has been enriched or augmented to improve its suitability and may even be partially or fully synthetic.  \nA Hypothesis driven approach with centrally tracked experiments and potentially compute has been applied.  \nExperiments are documented, shared, and can be easily reproduced.  \nML approaches, libraries, and algorithms are tested and evaluated and the best performing approach is selected. The best performing approach may not necessarily be the most accurate model. The best approach could be a trade-off in terms of ease of implementation coupled with accuracy. For example, using AutoML for rapid prototyping and development, or exporting to an ONNX model for deployment to an Edge device.  \nThe data distribution is recorded and stored as a reference to measure future drift as the data changes.  \nAn automated pipeline has been designed and potentially partially applied to the Experiments.  \nExperimentation topics  \nExperimentation guidance is provided in the following articles  \nAutomatically finding an AI algorithm: Prototyping approaches and use of Automated Machine Learning (AutoML)  \nUsing MLOps during experimentation: How to design experiments to be ready for the Model Development phase.  \nExploratory Data Analysis (EDA): The process of understanding what data we have  \nFeature engineering: Transforming and enriching data to forms that better support models that address the business problem  \nResponsible AI: Ensuring ML solutions follow responsible AI best practices  \nSynthetic data generation: Creating training data with similar statistical properties as the real data while securing privacy  \nOther resources  \nThe Data Science Toolkit is an open-source collection of proven ML and AI implementation accelerators. Accelerators enable the automation of commonly repeated development processes to allow data science practitioners to focus on delivering complex business value and spend less time on basic setup.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\experimentation\\index.md"
    },
    {
        "chunkId": "chunk271_0",
        "chunkContent": "author: tarockey\ntitle: Synthetic data generation\nms.topic: conceptual\nms.author: tarockey\nms.service: azure\ndescription: Synthetic data serves two purposes -- protecting sensitive data and providing more data in data-poor scenarios. Sensitive data is often necessary to develop ML solutions, but can put vulnerable data at risk of disclosure. In other scenarios, there is insufficient data to explore modeling approaches and acquiring more data is cost or time prohibitive. In both instances, synthetic data can provide a safe and cost-effective resource for model training, evaluation, and testing.\nrings:\n- public  \n{% set thisPage = self._TemplateReference__context.page.url %}  \nSynthetic data generation  \nSynthetic data serves two purposes: protecting sensitive data and providing more data in data-poor scenarios. Sensitive data is often necessary to develop ML solutions, but can put vulnerable data at risk of disclosure. In other scenarios, there is insufficient data to explore modeling approaches and acquiring more data is cost or time prohibitive. In both instances, synthetic data can provide a safe and cost-effective resource for model training, evaluation, and testing.  \nGenerating synthetic data can conserve and multiply the utility of the original data without compromising privacy. This process involves multiple scenarios - for images, it means generating new images and for tabular data it means generating scalar values of multiple types. The synthetic data ideally has similar statistical properties to the real data in ways relevant to the model, while excluding sensitive aspects.  \nSome synthetic data generators  \nDSynth - DSynth is a template driven data generator. DSynth accepts templates of various formats, generates templated data and outputs the generated data into a configured data sink.  \nMicrosoft's Synthetic Data Generator  \nMostly AI  \nGenRocket  \nYdata  \nHazy  \nEdgeCase  \nStatice  \n{% set link = '../../solutions/redacting-and-obfuscating-scanned-forms-and-images/index.md' %}\n{% if relative_path_exists(thisPage, link) %}  \nSynthetic Data Generation for Forms  \nThe Data Obfuscation Pipeline Solution provides data redaction and data obfuscation capabilities for unstructured data. For example, forms scanned as images and PDFs and can also be used to create a synthetic dataset.\n{% endif %}",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\experimentation\\synthetic-data-generation.md"
    },
    {
        "chunkId": "chunk272_0",
        "chunkContent": "Cost Optimization  \n1. Effective Utilization of PTU's  \nThe Azure OpenAI Sizing tool helps the enterprises to plan their Azure OpenAI (AOAI) capacity based on their requirements. They can get more predictable performance on AOAI by procuring Provisioned Throughput Units (PTUs) which necessitate advance payment and reservation of AOAI quotas. However, if this reserved capacity remains underutilized, it can lead to inefficient allocation of resources and financial overhead.  \nTo mitigate this, the following approaches can be leveraged:  \nUsing spillover strategy to control costs: Implementing a spillover strategy allows enterprises to first utilize their pre-purchased PTUs before routing excess traffic to Pay-As-You-Go (PAYG) endpoints. With this approach, the PTU capacity can be lower than the peak capacity required enabling a lower PTU capacity to be used. This technique is elaborated here.  \nEffective consumption of PTUs around the clock: By separating the\nconsumers into real-time and batch (scheduled/on-demand) and leveraging\nthe monitoring approaches discussed above, PTU utilization can be\norchestrated in manner where the batch consumers consume PTUs only when\nthe PTU endpoint are underutilized. A detailed approach on how to\nachieve this is mentioned here.  \n2. Tracking Resource Consumption at Consumer Level  \nIn a large enterprise setup, operational costs are generally shared\namong different business units through a charge-back model. For GenAI\nresources, this involves measuring consumption per consumer for both at\nPTU (Reserved capacity) and TPMs (Pay-as-you-go) quota, enabling the\nBusiness Units (BU) with transparent cost reporting, quota allocation vs consumed\nreporting and cost attribution functionalities.  \nIn the realm of AOAI, the approach for consumption tracking depends on\nmode of interaction with AOAI services.  \nBatch Processing Mode:  \nThe batch processing mode involves sending a set of inputs all at once\nand receiving the outputs after the model has processed the entire\nbatch. In this mode, the usage information returned as part of the\nresponse body contains the total number of tokens consumed in processing\nthat request.  \nAn example of the usage payload from Azure OpenAI completions endpoint:  \njson\n\"usage\": {\n\"prompt_tokens\": 14,\n\"completion_tokens\": 436,\n\"total_tokens\": 450\n}  \nLeveraging the techniques as discussed in the\nMonitoring section, the GenAI gateway\ncan be configured to parse and record this payload at a consumer level.\nThis information can then be aggregated to build a view of token\nconsumption over a specific time interval for each consumer.  \nStreaming Mode:  \nIn the streaming mode, the AOAI will not return the usage statistics\nas part of the response block. If we need to count the tokens, then the\nfollowing approach can be leveraged.  \nMeasure prompt tokens: The number of prompt tokens has to be\ncalculated from the request by using a library like\ntiktoken.  \nMeasure the completion tokens: The number of the events in\nthe stream should represent the number of the tokens in the\nresponse, so just count them while iterating and streaming the\nresponse.  \nTotal tokens is the sum of the prompt and completion tokens. Note that,\nthis still will be an approximation because there is no guarantee that\neach chunk of the response will be only one token.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\genai-gateway\\cost-optimization.md"
    },
    {
        "chunkId": "chunk273_0",
        "chunkContent": "Building GenAI Gateway  \n1. Purpose  \nThis document serves as an essential guide for engineering teams tasked\nwith designing and implementing a gateway solution involving Azure\nOpenAI resources. It aims to equip teams with the essential guidance and\nreference designs required to build a Generative AI (GenAI) gateway that\ncan efficiently handle and optimize GenAI resources utilization,\nfacilitate seamless integration and distribution of workloads across\nvarious deployments, as well as enable a fine-grained monitoring and\nbilling  \nIt's important to note that this document does not delve into\nimplementation-level details, but instead provides guidance on how teams\ncan achieve the end goal of building a GenAI Gateway with key features\nsuch as authentication, observability, compliance, controls, and more.  \n2. Definition & Problem Statement  \n2.1. Problem Statement  \nIn the tech landscape of applications consuming Large Language Models\n(LLMs), organizations face the challenge of efficiently federating and\nmanaging the GenAI resources. As the demand for diverse and specialized\nLLMs grow, the need for a centralized solution that can seamlessly\nintegrate, optimize, and distribute the workloads across a federated\nnetwork of GenAI resources becomes more necessary. Existing traditional\ngateway solutions often lack a unified approach for facilitating the\nfederation of GenAI resources. This deficiency can result in suboptimal\nresource utilization, increased latency, and challenges in managing a\ncollection of AI models.  \nWhat is a GenAI Gateway?  \nA \"GenAI gateway\" serves as an intelligent interface/middleware that\ndynamically balances incoming traffic across backend resources to\nachieve optimizing resource utilization. In addition to this, GenAI\nGateway can be equipped with additional capabilities to address the\nchallenges around billing, monitoring etc.  \nSome key benefits that can be achieved using GenAI gateway:  \nFigure 1: Key Benefits of GenAI Gateway  \n2.2. Conceptual architecture of a GenAI gateway  \nBelow is the conceptual architecture depicting high-level components of\na GenAI gateway.  \nFigure 2: Conceptual Architecture  \n3. Recommended Pre-Reading  \nIt is recommended that readers familiarize themselves with certain key\nconcepts and terminologies essential for establishing a foundational\nunderstanding of Azure OpenAI.  \nUnderstanding rate\nlimits  \nUnderstanding TPMs, RPMs and\nPTUs  \n4. Complexity in Building a GenAI Gateway  \nLarge Language Models (LLMs) are usually exposed through a REST\ninterface, allowing users to easily call their endpoints. In most large\nenterprises, the REST resources are typically hidden from the consumers\nwith a Gateway component. By routing requests through a Gateway,\nenterprises get centralized control over access and usage, enabling\neffective implementation of policies such as rate limiting,\nauthentication, and data privacy controls. In a traditional API Gateway,\nhandling rate limiting and load balancing for resources typically\ninvolves regulating the number of requests over time by employing\ntechniques like throttling or balancing the load across multiple\nbackends.  \nHowever, when using Azure OpenAI resources as a backend, the added\ndimension of TPMs (Tokens Per Minute) introduces an additional layer of\ncomplexity in ensuring consistent and even load distribution across\nbackends. Therefore, apart from the GenAI gateway needing to ensure the\nregulation of the quantity of requests, it also must account for the\ntotal tokens processed across multiple requests.  \n5. Key Considerations while Building GenAI Gateway  \nThis additional dimension of Tokens per minute constraint forces some\nchanges to the traditional gateway and the inherent nature of these AI\nendpoints introduces some challenges that need to be addressed.  \nHere are the key considerations while building a GenAI gateway in alignment with Azure Well Architected Framework.  \nScalability  \nLoad Balancing for Multiple Pay-As-You-Go AOAI Instances  \nManaging Spikes on PTUs with PAYG Endpoints  \nPerformance Efficiency  \nImproving Consumer Latency  \nQuota Management  \nConsumer-Based Request Prioritization  \nSecurity and Data Integrity\nAuthentication\nPII and Data Masking\nData Sovereignty\nContent Moderation  \nOperational Excellence  \nContext Length and Modality  \nMonitoring and Observability  \nLeveraging Hybrid LLMs  \nCost Optimization  \nEffective Utilization of PTU's  \nTracking Resource Consumption at Consumer Level  \n6. Summary  \nThis document not only provides a foundational understanding of key\nconcepts but also offers practical insights and strategies for\nimplementing a GenAI Gateway. It addresses the challenge of efficiently\nfederating and managing GenAI resources, essential for applications\nutilizing AOAI (Azure OpenAI) and Custom Large Language Models (LLMs). This document\nleverages the industry standard well architected framework to\ncategorizes and address complexities of building a GenAI Gateway and\nprovides comprehensive approaches/reference designs that are not only\ntechnically sound but also adheres to best practices.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\genai-gateway\\index.md"
    },
    {
        "chunkId": "chunk274_0",
        "chunkContent": "Operational Excellence  \n1. Context Length and Modality  \nContext length is the number of input tokens that the model can handle.\nThe field of LLMs is rapidly developing with models that can support\nlonger and longer context length. Longer context length means a bigger\nrequest body. Besides increasing context length, some models have\nabilities to work with different modes of data. Other models can also\nproduce varied data types likes images and videos.  \nThe design of the GenAI Gateway must account for these advancements. It\nshould efficiently manage large, mixed-content requests and support\ndiverse output types, ensuring versatility and robustness in handling\ncomplex LLM functionalities.  \n2. Monitoring and Observability  \nMonitoring and Observability are essential for creating robust and\nfault-tolerant systems. When building a GenAI gateway, it is key to\nmeasure and monitor the overall performance which includes tracking a\nvariety of facets such as error rates, total time for requests and\nresponses, latency introduced by the gateway layer, latency introduced\ndue to cross region calls between gateway and AOAI instances etc.  \nBefore designing for monitoring and observability, some of the crucial\naspects that needs to be taken care are:  \nWhat type of information should be recorded (e.g. request, response\nbody/header info etc.)?  \nCan we log only a sampled set, or do we have to log for all\nrequests/responses?  \nWhat is the time lag introduced by the metric/event collector? This\nis the time lag between key events occurring and the processor\nacquiring them.  \nHow do the downstream system components depend on this data?  \nWhat level of data freshness is required? Is near-real-time\ninformation necessary, or can some latency be tolerated?  \nWhat actions will be taken using this information, such as\nscaling, throttling or is it for reporting purposes?  \nWhat is the response mode? Streaming or Batch?  \nThis section lists different possibilities of measuring the metrics\nwhile interacting with the GenAI resources.  \nAzure OpenAI Metrics via Azure monitor: Leveraging the Azure OpenAI\nservice default metrics available via Azure Monitor allows the\ndownstream systems (e.g. GenAI gateway) to access these metrics for\nperforming custom operations, building dashboards, setting up alerts\netc. However, it's important to consider the latency involved with\nAzure\nMonitor\n-- typically ranging from 30 seconds to 15 minutes -- for the ingestion\nand availability of monitoring data to its consumers. This latency\nfactor is a crucial aspect to account for in real-time monitoring and\ndecision-making processes.  \nGenerating Custom Metrics and Logs via GenAI Gateway: There can be\nscenarios where the enterprises need more information beyond what's\nexposed via the AOAI metrics, for example capturing gateway induced\nlatency and custom business metrics. Additionally, this information may\nbe needed on a real-time or near-real-time basis by downstream systems\nto perform critical operations like scaling, optimization, alerting etc.  \nHere are some suggested approaches on how this can be achieved using\nGenAI gateway:  \nEmitting Custom Events to Real-Time Messaging System: The GenAI\ngateway can intercept requests or responses and extract relevant\ninformation to create events and push them asynchronously into\nreal-time messaging systems like Kafka or Azure EventHub. These\nevents can be consumed by a streaming event aggregator (e.g., Azure\nStream Analytics) on a near-real-time basis to populate a data store\nfor dashboards or triggering actions based on certain rules.  \nEmitting Custom Metrics to a Metrics Collector: Alternatively,\nthe GenAI gateway can emit custom metrics to support specific\nbusiness needs to a metrics collector (with time-series database) to\npower dashboards, alerts, and other custom functionality etc. Azure\nMonitor offers mechanisms for emitting and collecting custom metrics\nas well as open-source alternatives like Prometheus can also be\nimplemented, as described in this\npost.  \nIt's essential to understand that these custom metrics differ\nsignificantly from those generated by the AOAI service hence a careful\nassessment on when to leverage what would be crucial.  \nFor a high-level overview for the design, please refer to this\nsection.  \n3. Leveraging Hybrid LLMs  \nThe GenAI gateway in an enterprise acts as a frontend for all GenAI\ndeployments, covering both Azure OpenAI and custom LLM deployments\neither on On-Premises Datacenters or on other cloud providers.  \nAccessing these differently hosted LLMs may vary in multiple aspects:  \nConsumer authentication  \nEmitted metrics  \nQuota management  \nLatency requirements  \nContent moderation approaches  \nHence, while designing the GenAI gateway, it's crucial to understand\nthe organization's hybrid strategy considering the above-mentioned\naspects. This understanding will dictate how the gateway interfaces with\nvarious LLMs and other hybrid services, ensuring efficient and secure\naccess while meeting specific operational requirements.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\genai-gateway\\operational-excellence.md"
    },
    {
        "chunkId": "chunk275_0",
        "chunkContent": "Performance Efficiency  \n1. Improving Consumer Latency  \nConsumer Latency acts as a critical factor when designing a GenAI\ngateway solution. This latency refers to the time taken for a user's\nrequest to travel from the client to the gateway, then to Azure OpenAI (AOAI)\nservices, and back. Minimizing this latency ensures a responsive and\nefficient user experience.  \nBesides the common factors such as Network optimization, Geographical\nproximity, Scalability, Request pre/post processing, and Caching\nstrategies, one key way to reduce consumer latency is choosing AOAI\nStreaming endpoints. Streaming allows quicker responses to consumers as\nyou can 'stream' the completions and process them in parts before the\nfull completion is finished. Both OpenAI and AOAI use Server\nSent Events(SSE)for streaming.  \nHaving said that, here are some of the\ndownsides\nof using streaming which should be considered before making this choice.\nThis list comes from OpenAI, but it also applies to AOAI.  \nHandling streaming responses: The GenAI gateway should have the\ncapability to handle the streaming from the SSE. The gateway must read\neach chunk of the server sent event from AOAI and only process\nthe \"content\" portion of it. It needs to stream \"content\" back to the\napplication and close the connection on stream termination.  \n2. Quota Management  \nAOAI's quota feature enables assignment of rate limits to\ndeployments, up to a global limit called \"quota.\" It uses Tokens Per\nMinute (TPM) and Requests Per Minute (RPM) as units for this consumption\nmanagement. Read more about the quota management provided by AOAI\nhere.  \nIn large enterprises hosting multiple business applications that access\nGenAI resources, it's crucial to manage quota distribution to ensure\nfair usage and optimized resource allocation. Prior to integration with\nthe GenAI gateway, each consumer application should conduct a benchmark\nassessment of their TPM and RPM requirements. Based on this assessment,\nthe GenAI gateway can then allocate consumers to appropriate backend\nAOAI resources.  \nBenchmarking Token Consumption (PTU and PAYG): Benchmarking consumers for TPM and RPM consumption is an important\nactivity as it helps assess and optimize prompt complexity, anticipate\nlatency in request processing, accurately estimate token usage, and\nensure adherence to service limits.  \nThe following steps outline how to conduct a benchmark assessment for\nTPM and RPM requirements.  \nEstimate the frequency and volume of requests for each use case and\nscenario. For example, how many requests per minute, how many tokens\nper request, how many concurrent users, etc.  \nUse a load testing tool or script to simulate the expected traffic\npattern and measure the actual TPM and RPM consumption. For example,\nAzure Load Testing service, Apache JMeter, Locust, Artillery, etc.  \nAnalyze the results and identify the maximum and average TPM and RPM\nvalues for each use case and scenario. Also, note any errors or\nfailures that occurred during the test.  \nCompare the results with the available quota and the expected\nService Level Agreement (SLA) for the consumer application. Based on\nthese benchmark results, Provisioned Throughput Units (PTUs) can be procured if needed.  \nMicrosoft is developing this tool to help in benchmarking:\nhttps://github.com/Azure/azure-openai-benchmark/  \nHere are some suggestions for the approaches for managing quota at a\nconsumer level.  \nSetting up dedicated endpoints for consumers\nFor scenarios with a limited number of consumers, it's recommended to\nassign dedicated endpoints to individual consumers or groups with\nsimilar requirements. The GenAI Gateway should be configured to route\ntraffic to these designated endpoints based on the consumer's identity.\nThis approach is particularly effective for managing a smaller consumer\nbase.\nIn this model, quota distribution is determined at the time of endpoint\ncreation, which requires continuous monitoring to ensure efficient\nutilization of quotas. It's common in such setups for some consumers to\nunderutilize their allocated resources while others may experience a\nshortage, leading to an overall inefficient consumption of GenAI\nresources. Therefore, regular assessment and reallocation of quotas may\nbe necessary to maintain optimal balance and effectiveness in resource\nusage.\nIn this scenario, it is worth taking into the best practices of setting\nup Multi-tenancy for Azure\nOpenAI\nfor the deployment configuration.  \nAssign rate limits at the consumer level\nAn alternative approach is to apply rate limits at the consumer level in\nthe GenAI Gateway. If a consumer surpasses their limit, then the GenAI\ngateway can restrict access to the GenAI resource until the quota is\nreplenished or degrade the consumer experience based on the defined\ncontract with the consumer.\nThis eliminates the need for deployment separation at the Azure OpenAI\nlevel. The consumer then can implement retry logic at their end to have\nbetter resiliency.\nAlong with these approaches a GenAI gateway can also be used to\nenforce rate limiting best\npractices\nat the consumer level. These best practices ensures that the consumers\nadhere to the conventions of setting max_tokens and a small best_of\nvalues to avoid draining the tokens.  \n3. Consumer-Based Request Prioritization  \nThere could be multiple consumers with varying priorities trying to\naccess the AOAI deployments. Since AOAI imposes hard constraints on the\nnumber of tokens that can be consumed per second, request prioritization\nmust allow consumers with critical workloads access the GenAI resources\nfirst when compared to low priority use cases.  \nEach request can be categorized into different priorities and low\npriority requests can be deferred to a queue until the capacity becomes\navailable. The AOAI resource should be continuously monitored to track the\navailable capacity. As capacity becomes available, an automated process\ncan start executing the requests from the queue. Different approaches to\nmonitor PTU capacity are discussed here.  \nRefer to this section for a high-level\ndesign overview.  \nLeveraging Circuit-Breaker technique to prioritize requests:  \nThe Circuit-Breaker\ntechnique\nin an API gateway can be employed to prioritize requests among consumers\nduring peak loads. By designating certain consumers as prioritized and\ndefaulting others as non-prioritized, the gateway monitors the backends'\nresponse code. When the backend returns specific response codes such as\n429, implying the backend quota has reached, the circuit-breaker\ntriggers a longer break for non-prioritized consumers, temporarily\nhalting their requests to reduce stress on the backend. For prioritized\nconsumers, the break will be shorter, ensuring a quicker resumption of\nservice to maintain responsiveness for critical processes.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\genai-gateway\\performance-efficiency.md"
    },
    {
        "chunkId": "chunk276_0",
        "chunkContent": "Scalability  \nScaling Consumers through Request Load Balancing: One of the unique problems that enterprises encounter when they create a\nGenAI gateway is increasing the number of consumers with this limit on\nTPMs as well as RPMs. Here are some of the situations that could occur\nand some possible solutions that could be applied at the GenAI gateway.  \n1. Load Balancing for Multiple Pay-As-You-Go AOAI Instances  \nSupporting High Consumer Concurrency: To accommodate numerous consumers making LLM requests, it is advisable\nto segregate these consumers into distinct regions. Since Azure OpenAI\nquotas are enforced at the regional level, deploying in multiple regions\nallows these consumers to operate concurrently. The GenAI gateway can\nfacilitate load balancing by distributing their requests across various\nregions. However, supporting cross-region deployments might introduce\nlatency issues for consumers. This can be partially mitigated by\nimplementing region affinity, whereby the GenAI gateway routes consumer\nrequests to the nearest regional deployment to the requestor or the\nregions can be identified by performing benchmarking. In benchmarking,\nit is ideal to mimic high, normal loads from requestor and can evaluate\nwhich of the OpenAI instances are working well.  \nFor example, consider two scenarios, the first with a single deployment region and the second with deployments in two regions. Since quota is per-region, the overall maximum RPM is higher in the second scenario, as shown below.  \nDescription Single region deployment Multi region deployment Total TPM limit 240,000 RegionA: 240,000 RegionB: 240,000 RPM enforced per 1000 TPM 6 RegionA: 6 RegionB: 6 Total RPM 1,440 RegionA: 1,440 RegionB: 1,440 Total RPM across all deployments 1,440 2,880  \nIn a multi-region deployment scenario, one can get a potentially higher\nthroughput, thus able to process more concurrent requests. Additionally,\nAzure OpenAI evaluates the requests in a small period (1 sec or 10 sec)\nand based on the values for these periods, it extrapolates the RPM and TPM and\nthrottles the overflow requests. By leveraging multiple deployments,\ndistributing the load across two or more resources is achievable and\nthereby have a reduced probability of hitting the enforced limits on the\ndeployments.  \n2. Managing Spikes on PTUs with PAYG Endpoints  \nEnterprises often opt for Provisioned Throughput Units (PTUs) with Azure\nOpenAI (AOAI) for more stable and predictable performance compared to\nPay-As-You-Go (PAYG). To handle sudden surges in consumer demand, a\n'spillover' strategy can be effective. This involves initially routing\ntraffic to PTU-enabled deployments, but in cases where PTU limits are\nreached, the overflow is redirected to TPM (Tokens Per Minute)-enabled\nAOAI endpoints, ensuring all requests are processed.  \nWhether the PTU limits have been reached can be identified if the PTU\nendpoint starts responding with 429 as a response code, or through a\npro-active monitoring of the PTU utilization as mentioned in this\nsection.  \nStrategies for Load Balancing across Multiple Azure OpenAI\nDeployments  \nWhen there are multiple deployments identified as a potential target\nbackend by the gateway, it is possible to apply any of the following\napproaches to perform load balancing of the consumer requests.  \nRound Robin/Random: The GenAI Gateway can be configured to use\nthe Round Robin algorithm or Random assignments to load balance\nthe requests across the multiple AOAI deployments. This\napproach is recommended if the TPM limit for each deployment is the\nsame.  \nWeighted Round Robin: Load balancing the requests can be done across the multiple AOAI deployments based on the number of\nTPM limit for each deployment. The GenAI Gateway can be configured to assign a weight to each AOAI deployment, and then route\nthe requests based on the weight. In this case, the weight could be\nthe TPMs allocated for the specific deployment. For example, if\nthere are 2 deployments within a PTU with 80% and 20% of token\nallocation, then only 1 in 5 calls should be routed to the\ndeployment with 20% token allocation.  \nDynamic routing based on AOAI Utilization: AOAI\nmetrics can be used to continuously monitor the utilization of the\nAOAI resource and the gateway can use this data to\ndynamically route the request to least utilized resource.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\genai-gateway\\scalability.md"
    },
    {
        "chunkId": "chunk277_0",
        "chunkContent": "Security and Data Integrity  \n1. Authentication  \nAzure OpenAI (AOAI) supports two forms of authentication.  \nMicrosoft Entra ID: AOAI supports Microsoft Entra\nauthentication with managed identities or Entra\napplication.\nUsing a managed identity, it is possible to achieve a keyless authentication between the consumer and the AOAI service. In\ncase of Entra application based authentication, each consumer will\nneed to maintain client id and secret for the Entra app having\naccess to AOAI resource.  \nAPI Key: A secret key can be passed by the consumer to\nauthenticate itself with AOAI. API Keys provided by AOAI are secret\nby nature. If the consumer wants to use API key-based\nauthentication, they need to have access to this secret key.\nHowever, when a consumer needs to distribute requests across\nmultiple AOAI endpoints, they are tasked with managing each\nAPI keys independently. Entrusting consumers with API keys not only\nimposes the responsibility of key management upon them but also\nheightens the risk of security breaches if a key is compromised at\nthe consumer's end. Furthermore, this approach complicates the\nimplementation of other security best practices, such as key\nrotation and the ability to block requests from specific consumers.  \nIt's possible that the Gateway might interface with endpoints that are\nnot AOAI. In those situations, different endpoints could have different\nways of authenticating.  \nOne of the suggested approaches here is to offload authentication to\nAOAI (or other GenAI endpoints) to the GenAI gateway and terminate the\nconsumer authentication at the Gateway level. Decoupling the GenAI\nendpoints auth from the end consumers allow them to employ uniform\nenterprise-wide authentication mechanism like OAuth while authenticating\nagainst the GenAI Gateway and mitigating the above stated risks. In case\nof AOAI endpoints, GenAI gateway can use managed identity to\nauthenticate as well. When authentication is offloaded, the GenAI\nresource cannot recognize the consumers, because the Gateway will use\nits own credentials for all the requests.  \n2. Personally Identifiable Information (PII) and Data Masking  \nAs the GenAI gateway acts as a broker between the consumer and backend\nAOAI services, leveraging the GenAI gateway for PII\ndetection and data masking becomes a critical part of the architecture.\nThis setup allows for centralized handling of sensitive data, ensuring\nthat any personal information is identified and appropriately managed\nbefore being processed by AOAI. A centralized approach also presents an\nopportunity to standardize PII handling practices across multiple\nconsumer applications, leading to more consistent and maintainable data\nprivacy protocols.  \nAutomated processes can be implemented at the Gateway level to intercept\nrequests and detect PII information before it's processed by Azure\nOpenAI services. Once detected, PII data can either be redacted or\nreplaced with generic placeholders.  \nDetecting PII  \nServices such as Azure Text Analytics API can be leveraged for identifying and\ncategorizing PII information (e.g. names, addresses, and phone\nnumbers etc.) in text data. Certain Azure services such as Azure\nPurview can also help in detecting and surfacing PII\ninformation.  \nFor more specific or customized PII detection, a custom domain\nspecific ML model can be trained using Azure Machine Learning\nservices and exposed as a REST endpoint.  \nOrchestration: The GenAI gateway can employ an orchestration layer\nthat takes advantage of Azure services such as Azure Functions or\nLogic Apps to automate workflows, which can include steps for PII\ndetection and data masking operations.  \nHowever, one key consideration is that integrating an additional layer\nfor PII detection and masking can increase the overall response latency\nfor the consumers. This factor must be balanced against the need for\ndata privacy and compliance when designing the system.  \n3. Data Sovereignty  \nData sovereignty in the context of AOAI refers to the legal and\nregulatory requirements related to the storage and processing of data\nwithin the geographic boundaries of a specific country or region.\nPlanning for data sovereignty is critical for a business to avoid\nnon-compliance with local data protection laws resulting in hefty fines.  \nThe GenAI gateway can play a crucial role in adhering to data\nsovereignty requirements by utilizing region affinity based on the\nconsumer's location. By doing so, it can intelligently redirect traffic\nto backend AOAI instances, other cloud services for processing requests\nlocated in regions that comply with the relevant data residency and\nsovereignty laws. In a hybrid setup that combines on-premises custom\nLarge Language Models (LLMs) with AOAI, it is essential to ensure that\nthe hybrid system also adheres to multi-region availability requirements\nto support consumer affinity.  \n4. Content Moderation  \nWith the rise of LLM-based chat applications, organizations must prevent\nusers from disclosing sensitive data to externally hosted LLMs.\nSimilarly, the response data from LLMs must be screened to exclude any\nprofanity.  \nWith the GenAI gateway design, enterprises can implement a centralized\ncontent moderation strategy for their GenAI applications. For Azure\nOpenAI, default content filtering occurs within Azure, and enterprises\ncan configure the level of content moderation within\nAzure.  \nFor additional content moderation needs in custom LLM endpoint responses\nor other GenAI services lacking built-in moderation, it's recommended\nto integrate the GenAI gateway with a content moderation service for\nvalidation before sending the response to the application. Here are some\nsuggestions on an external content moderation service that can be\nintegrated.  \nAzure Content Moderator\nService\nThe Content Moderator service is an API that is powered by artificial\nintelligence and runs on Azure. The service is capable of scanning text,\nimage, and video content for potential risky, offensive, or undesirable\naspects.  \nAI Content\nSafety\nAzure AI Content Safety detects harmful user-generated and AI-generated\ncontent in applications and services. Azure AI Content Safety includes\ntext and image APIs that allow you to detect material that is harmful.\nIt offers advanced AI features and enhanced performance. AI content\nsafety is enabled by default when OpenAI is model is deployed.  \nTo decide on the service, refer to this\ndocument\nfor details. Integrating an additional content moderation service will\nincrease overall response latency.  \nRefer to this section for a high-level design\noverview of the solution.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\genai-gateway\\security-and-data-integrity.md"
    },
    {
        "chunkId": "chunk278_0",
        "chunkContent": "rings:\n- internal  \nInference and Feedback  \n{% set thisPage = self._TemplateReference__context.page.url %}  \nServing models on production to users and collecting feedback are essential pillars for implementing MLOps. Inference refers to the ability to accept user inputs, generate predictions (outputs) from ML model and returning it back to the caller. The input can be a batch of records or real-time data. They can be both structured (tabular) or unstructured(video, image, document) data. The model can be served from Cloud, On-Premises or Edge.  \nMonitoring and implementing feedback loops are essential to ensure that the performance of a model can be monitored. With such tools in place, degradation can be addressed proactively using different techniques like retraining and redeployment of models.  \nData Drift, serving models on the edge are essential general concepts and using Cognitive services to improve the model scoring are specialized cases discussed further in this section.  \n\ud83d\udca1Key Outcomes of Inference and Feedback:  \nThe model\u2019s performance is observed over time to determine whether any drift has taken place.  \nUser interactions with the system or model(s) are observed over time. Metrics determine whether the solution can be optimized or if performance is deteriorating.  \nData distribution changes or drift are identified and fed back to the Experimentation phase.  \nDrift  \nData Drift is the change in data over time that causes a deviation between the current distribution, and the distribution of the data used to create the underlying model. This drift can result in models no longer accurately predicting on real world data.  \n{% if extra.ring == 'internal' %}  \nRefer to the following section for a checklist on Data Drift questions to ask a customer\n{% endif %}  \n{% set link = '../../technology-guidance/working-with-azure-ml/datadrift_operationalization.md' %}\n{% if relative_path_exists(thisPage, link) %}  \nRefer to the following section for a detailed overview of Data Drift Operationalization in Azure ML\n{% endif %}  \nRefer to the following section for detailed overview of Data Drift and Adaption  \nServing models on the Edge (Inferencing)  \nThere are cases where the ML model is deployed on Edge devices. The ML models deployed on the Edge are deployed and managed differently compared to other deployment targets to AKS or ACI. This section primarily deals with ML models that are deployed on Edge devices. This section is the starting place to explore content related to edge inferencing and deployment.  \nUsing Cognitive Services to evaluate prediction scores  \nA process to check and evaluate the scoring output along with its confidence level is essential to monitor a model on production. The \"Custom Translator\" and \"Structured Forms\" assets are two examples that use Cognitive services to monitor the model output values alongside corresponding threshold values. The prediction pipeline in a production environment assesses the confidence of the predictions and evaluates against a pre-determined threshold to determine the performance.  \nCustom Translator  \nThis code asset is a scripted basic pipeline to translate and evaluate a document into a target language and\nevaluate it against the human translation. It generates numerous outputs including automated evaluation scores and\nprepares formats of the translation for human evaluation.  \nThis script will perform the following steps:  \nConvert source and target pdf documents to UTF-8 encoded text documents  \nRun sentence alignment on both documents and generated aligned text documents  \nTranslate each sentence against multiple models and evaluate BLEU scores against each model translation  \nGenerates output reports for analysis  \nFor Custom Translator, refer to the Evaluation section for the Custom Translator recipes\n** Microsoft org access is required on github  \nStructured forms  \nThere are available code accelerators with a simple approach to Scoring a prediction from a Forms Recognizer output. Note, this accelerator is highly specific to the format of your data, the business rules that need to be applied, and the format the data needs to be transformed to. Refer to the Extraction section for the Playbook for Knowledge Extraction For Forms Accelerators & Examples",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\inference-and-feedback\\index.md"
    },
    {
        "chunkId": "chunk279_0",
        "chunkContent": "author: shanepeckham\ntitle: ML Lifecycle Management\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: The MLOps process is not a linear, one-time operation. The ML Lifecycle encapsulates the cyclical nature of this process. Various components also take place across multiple stages of the MLOps process, such as monitoring, data collection, or retraining.\nrings:\n- public  \nUsing MLOps to manage the AI lifecycle  \nMLOps helps teams manage the lifecycle of their AI projects.  \nAI projects do not follow a linear flow where each step happens only once. The AI project lifecycle is iterative in nature, and MLOps automation assures critical steps are performed consistently as the team iterates.  \nIn addition, not all aspects of the AI lifecycle fit into a single part of the MLOps flow. For example, the AI solution likely depends on both external data sources, as well as data generated by the solution itself (for example, feedback loop, data drift, etc.).  \nThis article examines different aspects of how MLOps is used to support the AI project lifecycle.  \nMonitoring  \nDuring training and deployment of ML models, it is imperative that our customers are able to monitor the status of their various ML tasks. Recommended options for monitoring include:  \nThe OpenCensus library is one useful monitoring tool.  \nAzure Machine Learning (Azure ML) has built-in tools for deploying a model, and it provides multiple metrics by default that can be queried from Application Insights. These metrics are primarily performance related such as response time, resource utilization, and failure rates.  \nOnline Endpoints (real-time)  \nWeb Service Endpoints on AKS or ACI (using the v1 SDK deployment tools)  \nDrift  \nDrift is the change in data over time that causes a deviation between the current distribution, and the distribution of the data used to create the underlying model. This drift can result in models no longer accurately predicting on real-world data.  \nRefer to the following section for detailed overview of drift and adaption  \nModel Adaptation  \nModels must be adapted (retrained or updated), over time to remain representative and continue to bring business value. For more information on how models can be adapted appropriately for the circumstances, see the \"How can we adapt to drift?\" section of the drift and adaptation overview.  \n{% if extra.ring == 'internal' %}  \nRefer to the following section for a checklist on drift questions to inform a drift and adaptation strategy  \nRefer to the Data Science Toolkit documentation for details on Data Drift Detection in Images\n{% endif %}\n{% set link = '../../technology-guidance/working-with-azure-ml/datadrift_operationalization.md' %}\n{% if relative_path_exists(thisPage, link) %}  \nRefer to the following section for a detailed overview of Data Drift Operationalization in Azure ML\n{% endif %}  \nData Collection  \nOnce a model has been deployed to production, it may need periodic updates to ensure it continues to appropriately handle new data. This includes monitoring for things such as drift, along with retraining a model on new data. In both cases, it is important to collect input data (barring any privacy/security concerns) to evaluate over time.  \nFurther reading  \nFor more information, please visit the following resources:  \nAzure ML Data Collection  \nOnline Endpoints (real-time)  \nWeb Service Endpoints on AKS or ACI (using the v1 SDK deployment tools)  \nAzure ML SDK v2 Data Drift Deployment Sample",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\ml-lifecycle-management\\index.md"
    },
    {
        "chunkId": "chunk280_0",
        "chunkContent": "author: shanepeckham\ntitle: AI drift checklist\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: This checklist will help you get started in understanding your potential drift context in order to make the best decisions with respect to planning drift detection and adaptation strategies, and drift response.\nrings:\n- public  \nChecklist to help detect and adapt to drift  \nThis checklist helps you get started with understanding your potential drift context in order to make the best decisions about planning drift detection and adaptation strategies, and drift response.  \nDrift detection  \nAre you already performing data validation and data quality analysis?  \nDo these checks ensure data quality issues are caught and managed so that they will not poison drift detection?  \nWhat would the consequences be if a drift was undetected and unaccounted for?  \nWhat are some examples of drift, or deviation from the norm, that you've experienced in the past?  \nHow have you seen your incoming data change over time?\nIn feature value distribution? (For example, the most common customer age range used to be 30-40 and is now 50-60)\nIn velocity?\nIn confounding factors?  \nGive some examples of drift scenarios that you could imagine occurring in the future that would require the system to update/adapt.  \nAre future drifts expected to happen gradually, abruptly or both?  \nAre any future drifts expected to be of high magnitude?  \nHow frequently might you expect future drifts to occur?  \nHow might future drifts manifest (for ex. incremental, dispersed)?  \nIs there currently a drift detection strategy? If so, describe it.  \nIs this implementation currently done via manual investigation as required?  \nIs there currently a monitoring system in place?  \nWhat drift characteristics does this strategy provide to the user (for ex. drift magnitude)?  \nWhat has worked well about this strategy?  \nAre there any pain points with this strategy?  \nWhat is the data velocity (update frequency and batch size)?  \nAdaptation  \nIs there currently an adaptation strategy? If so, describe it.  \nIf there is regular update, what is the frequency?  \nWhat size is the training set for retrain (for ex. all historical data, or just the last 3 years)?  \nWhat's the evaluation strategy for the updated version?  \nWhat has worked well about this strategy?  \nAre there any pain points with this strategy?  \nWhat would be the ideal method of adaptation from the user perspective?  \nIf adaptation is not automatic, what information would be useful for the user in performing adaptation?  \nIs a playbook in place and accountable teams assigned to drift response?",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\ml-lifecycle-management\\drift-and-adaptation\\drift-checklist.md"
    },
    {
        "chunkId": "chunk281_0",
        "chunkContent": "author: shanepeckham\ntitle: AI Drift\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Drift broadly refers to changes in data and the concepts and mechanisms that they represent over time.\nrings:\n- public\ntags:\n- Retail\n- Healthcare  \nUnderstanding and adapting to data drift  \nDrift broadly refers to changes in data and the concepts and mechanisms that they represent over time. It is common to see drift divided into two categories:  \nData drift: Data drift occurs when there is a change in input distribution that may (real drift) or may not (virtual drift) affect the class boundaries.  \nModel drift: Model drift (real drift) occurs when an actual change in the class boundary occurs.  \nIt's important to understand that model drift does not occur if there is no data drift. Both need to be detected, and both require adaptation. For this reason, \"drift\" can be thought of as a single phenomenon that impacts both data and models.  \nDrift occurs when something legitimate changes about the data generating process. A relatable example of drift is a user's preference evolution on a video streaming app. For example, a user might be interested in teen dramas during their teenage years, but gradually become more interested in documentaries. These types of changes should trigger an adaptation in recommender models. Adaptation in this example allows the streaming service's recommender system to remain relevant by suggesting progressively fewer teen dramas, and progressively more documentaries. So, the major benefit of drift detection is informing model adaptation. So, obviously, all deployed models require some form of model management and maintenance. Drift detection makes model management possible in an empirically supported and informed manner.  \nDrift needs to be detected early in the data pipeline and before any data reaches the learning system. If a data validation problem is identified, it should immediately stop the flow of poor-quality data and take steps to rectify the issue. Validated data should continue to flow though.  \nDrift vs. data validation  \nNow that we have a concept of what drift is (see above), let's focus on what drift is not. Drift detection and data validation are commonly mistaken as being one and the same. However, they have different implications and mitigation methods. Data validation issues are caused by illegitimate changes in the data generating process. Examples include:  \nA data pipeline issue causing a feature to no longer be populated or to be populated differently \u2013 perhaps where NAN values were expected, there are now values of \u20131.  \nFeature value ranges may be outside of reasonable expectations \u2013 blood pressure readings report a value outside of humanly possible ranges.  \nChanges in schema or feature drift.  \nWhereas drift detection is as discussed above.  \nReal-world impact  \nThe greatest benefit of drift detection\u00a0is that it informs model adaptation. Most enterprise scenarios have some sort of temporal component, and thus require adaptation and maintenance. Drift detection informs the system about when a drift has occurred and about drift characteristics like magnitude and duration. This information is then used to adapt the learning system appropriately. Instead of following a fixed retrain schedule (that may not align with the actual drift) and adapt to the current concept effectively, adaptive solutions:  \nEnsure that models remain accurate and representative through time  \nReduce manual analysis during model update  \nIncrease the longevity of solutions  \nProvide insights as to why a change has occurred that may have otherwise been overlooked  \nCharacterizing drift  \nDifferent drifts can vary wildly in terms of their characteristics. Below we discuss several relevant categories of drift characteristics.  \nVirtual vs real  \nAs mentioned in the above discussion of data drift vs. model drift, drifts can be  characterized as real or virtual. Virtual drifts occur when there is a change in class priors or class distributions that do not affect the class boundaries. Real drifts occur when an actual change in a class boundary occurs.  \nIn the first column in the image above, there are more cats in our data after the drift. We\u2019ve changed the class balance and thus priors but aren\u2019t sampling from a new space.  \nIn the second column, some kittens are recorded in the data after drift. There has been no change in the cat/dog boundary, but some feature value distributions (like age and size) have changed in the cat class.  \nSince the boundary between cats and dogs doesn't really change, even a mellow Pomeranian is still a dog, we will use happiness with a service as our example. After drift, some new features were rolled out that broke some users\u2019 workflows. The result was a formerly happy area is now frustrated. It changes the classification boundary.  \nThe first and second columns represent virtual drift where the class boundary does not change, but the performance of our model may still change. The third column represents real drift.  \nDrift magnitude  \nA major drift characteristic is magnitude. Drifts can range from low to high magnitude. Take this simplified example: a low magnitude drift might occur if the age ranges of people most frequently purchasing overalls moves from 20-25 to 30-35. Conversely, a high magnitude drift might occur if the age range changes from 20-25 to 80-85.  \nDrift duration  \nDrifts may also range in terms of duration, between abrupt and gradual. For example, cell phone usage patterns drifted gradually over time from mostly audio calls, to mostly mobile internet usage. An abrupt change may occur in buying habits of retail customers, if, for example, there was a popular boycott.  \nDrift manifestation patterns  \nDrift can occur incrementally or dispersed and can also be reoccurring. An intuitive way to conceptualize incremental drift is: suppose the concept white is changing to the concept blue. During an incremental drift, white would slowly become darker and darker versions of blue, until the blue concept was fully integrated and the white concept was fully segregated out. A practical example is aging. Humans age incrementally from young to old. In contrast, we do not age in a dispersed manner when we are a baby for 10 years, then an elderly person for two months, a baby again for 7 years, an elderly person for 6 months and so on.  Recurring concepts can occur in, for example, retail sales data where holiday season concepts would be expected to reoccur annually.  \nAs one might imagine, different types of drifts can be more easily detected by certain types of methods. The best drift detection strategies are problem-specific. These strategies are often functional on varying different types of drift, since it is not known which drift characteristics are expected. Also many types of drifts can occur in one system. In the following section, we compare and contrast several of the most common drift detection strategies.  \nHow can we detect drift?  \nBefore discussing the details of drift detection methods, it is important to understand the mechanics of managing new data from a stream. We define a stream as any temporally arriving data, including temporally ordered batches. Most drift detection methods employ sliding windows for this purpose. The main idea of sliding windows is to maintain a buffer of part of the stream, typically the most recent part. Often, model adaptation is paired with the sliding window, remembering only the concepts contained within it. The window itself can store actual data points, or more commonly, incremental summary statistics on the data points. Most often, a current window of most recent data is compared to a historical window of data, and if there is a significant difference between the two, a drift is flagged. This is most often based on null hypothesis tests and concentration inequalities. While sliding windows can have many complexities, we will focus on this simple paradigm.  \nIt is clear that window sizes will have a significant effect on the functionality of window-based methods. Window sizes can be either fixed or dynamic. Fixed windows are simple, but they can be problematic. Small sizes are favorable in times of drift for swift adaptation, while large sizes are favorable in times of stability to maximize information. Dynamic windowing permits windows to grow and shrink on the fly based on the current environment, allowing them to be efficient in both periods of drift and stability.  \nMany drift detection methods employ sliding windows. The following are high-level categories of methods which are not necessarily mutually exclusive:  \nLearning-based methods  \nDistribution-monitoring methods  \nStatistical process control  \nSequential analysis  \nHeuristic analysis  \nLearning-based methods essentially monitor model performance over time, and when model performance degrades, drift is flagged. Some may use multiple models, for example, one trained on a longer window and another trained on a shorter window, and monitor performance differences between them. While these methods have been reasonably successful and give insight into drift, they do have several key drawbacks. The most glaring drawback in the supervised scenario is that, in order to monitor model performance, ground truth labels are required. Obtaining these labels for the whole stream is downright unreasonable (why have a model at all?), and obtaining labels on even parts of the stream can be difficult and costly. Secondly, fluctuations in model performance cannot be attributed to drift alone, so drift might be flagged due to the model not being generalizable enough to one concept. It removes the benefit of understanding the drift characteristics.  \nContrarily, distribution monitoring methods do not require labeling, and are more reliable in detecting true drift and providing insight into that drift. These methods are better at describing contributing factors. This family of methods monitors the distribution of the input data, and sometimes the output as well. For example, red shirts typically sell in medium size, then for some reason at some point sales decrease in favor of small red shirts. At that point, distribution monitoring methods determine that drift has occurred and offer explainability about the contributing factors. While distribution monitoring approaches have many advantages, they do not consider the learning system. Thus it is possible to detect drifts that don\u2019t have much of an impact on the system, generating more alerts than necessary. Methods monitoring distributions on output data monitor either the concentration of some uncertainty margin of probabilistic models, or the distribution of output predictions.  \n{% if extra.ring == 'internal' %}\nFor an example of distribution monitoring for images, refer to the example in the Data Science Toolkit for Defect Detection.\n{% endif %}  \nSequential analysis functions under the assumption that drift occurs when the probability of observing subsequences under the historical distribution compared to the recent distribution is beyond some threshold. This threshold may or may not be adaptive. For example, if users frequently perform A, B then C in an app in a historical window, but in a current window, they frequently perform A, B then Z, a drift would be flagged. A major benefit of sequential analysis is that it retains intra-window temporal integrity. However, it is not always directly applicable to all data streaming scenarios.  \nStatistical process control, or control charts, are commonly applied with the aforementioned methods. Rather than the binary drift or no drift states, control charts additionally employ a transitionary state. Typical system states include:  \nIn control \u2013 stable distribution/stable performance  \nWarning \u2013 increasing distribution difference/increasing error  \nOut of control \u2013 significant distribution difference/significant error  \nThe warning period in this process is valuable. It provides a backup model to easily substitute for the current model if there is any drift. Additionally, statistical process control offers an inherent measure of the rate of change of the system: time between warning and out of control states. A drawback of determining thresholds for each state, even dynamically, can be challenging and does have a significant effect on the success of the system.  \nHeuristic analysis is the least complex of the methods discussed, and is typically not used in isolation. They are rules-based drift flags that are domain-specific. An example is to flag change if the temperature of an IOT device reaches a particular value. This method should not be confused with data validation procedures. In this example, the rising temperature should indicate a true drift, like a changing season, not a defective device. Heuristics analysis is a simple way to capture domain knowledge of drifts, but if rules become too complex, it is reasonable to use other approaches.  \nHow can we adapt to drift?  \nModels account for drift by adapting. The most fundamental elements of model adaptation are remembering and forgetting. The most basic way of solving the drift problem is: at a regular specified time interval, discard the model and replace it with a new one, trained on data collected within a specified time frame. It is referred to as blind global replacement. Blind adaptations do not use adaptation triggers and update on recent data, which may or may not be relevant. This is in contrast with informed adaptation strategies. Global replacement, as opposed to local replacement, refers to the entire model being discarded and reconstructed from scratch. Do note that for supervised learning tasks, the new data used to train the replacement model is required to be labeled. This approach is currently the most typical for drift in industry today.  \nBlind global replacement strategies come with a host of issues. The first challenge is determining a relevant retrain cadence. In some applications, this decision may be more informed than others. For example, it might make sense to replace a tax model once per year. But it is challenging to determine how frequently to replace a model that makes hourly predictions. It highlights another challenge: global replacement is expensive. It takes time and resources to build an entirely new model from scratch. When we are ready to build a new model from scratch, how do we know which data to train it on? The assumption that recent data is most relevant is correct, but how should we decide which recent data to train on and how far back should be remembered?  \nFor example, suppose you have a daily retrain cadence and you've replaced a model. Suppose a drift occurs one hour after retrain. It means that your model will likely be making erroneous predictions for a full 23 hours before you retrain again. Another challenge arises when deciding which data window to use for retraining to ensure that the new model accurately represents the current data. Perhaps, as a rule, you always retrain on the last 3 days' worth of data. Will your new model know enough about the current concept to be performant, or will it still be too diluted with the old concept from the previous 2 days? These issues are all important and difficult to resolve in the context of a blind global replacement, which may contribute to sub-optimal performance.  \nDrift detection permits informed adaptation, as opposed to blind adaptation, where the model can adapt at the right time and on data that reflects the current concept. While this informed adaptation can guide decisions around which data to retrain on, there is an even better solution: train on all of the data and forget what\u2019s irrelevant on the fly.  \nOnline incremental learning algorithms update the model upon arrival of each data point (or sometimes mini-batches \u2013 let's focus on the data-point-by-data-point architecture for now). These algorithms may adapt implicitly just by learning on increasingly more recent data. Or they may use blind forgetting with a specified periodicity. They could also adapt in an informed manner using drift detection triggers. Some learners\u2019 mechanics only permit them to forget globally, like Na\u00efve Bayes. Others permit local replacement, like decision trees or ensembles. Local replacement involves discarding irrelevant subsections of models to replace them with relevant subsections. If there are re-occurring concepts, inactive models may be stored and re-activated upon re-emergence of the concept that they reflect.  \n{% if extra.ring == 'internal' %}\n\ud83d\udca1 For more info information and examples with sample data, refer to the Drift and Adaptation repository:  \nDrift Checklist  \nData Drift Operationalization in Azure ML  \nData Drift in Images: An implementation of drift monitoring with code\n{% endif %}",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\ml-lifecycle-management\\drift-and-adaptation\\drift-overview.md"
    },
    {
        "chunkId": "chunk282_0",
        "chunkContent": "author: shanepeckham\ntitle: AI/ML Systems Adversarial Threat Modeling\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Running AI/ML systems in production expose companies to a brand-new class of attacks.\nrings:\n- public  \nAI/ML Systems Adversarial Threat Modeling  \nRunning AI/ML systems in production expose companies to a brand-new class of attacks. The following attacks should be considered:  \nModel stealing (extraction): Adversaries can recreate a model that could be used offline to craft evasion attacks or just as it is.  \nModel tricking (evasion): Adversaries can modify model queries to get a desired response.  \nTraining data recovery (inversion): Private training data can be recovered from the model.  \nModel contamination to cause targeted or indiscriminate failures (poisoning): Adversaries can corrupt training data to misclassify specific examples (targeted) or to make the system unavailable (indiscriminate).  \nAttacking ML supply chain: Adversaries can poison third-party data and models.  \nThese attacks should be carefully considered. For information on adversarial attacks and how to include them in the threat modeling process, follow the links below:  \nFailure Modes in ML | Microsoft Docs  \nThreat Modeling AI/ML Documentation | Microsoft Docs",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\model-development\\adversarial-ml-threat-modeling.md"
    },
    {
        "chunkId": "chunk283_0",
        "chunkContent": "author: shanepeckham\ntitle: AI Model Development\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: \ud83d\udca1Key Outcomes of Model Development Datasets are versioned, centrally available and discoverable. Also full lineage of features and engineering is captured. Models trained are versioned, centrally available and discoverable. Unit and integration tests have been applied to the training and inference pipeline. Also static seeds applied where relevant, to ensure near identical and reproducible results. A consistent scoring and evaluation approach has been implemented. Performance and parallel processing improvements are designed, tested, monitored and partially implemented and tested. Compute environments are explored for optimum results.\nrings:\n- public  \nChecklist for production-ready AI models  \nCreating AI models is much more than training and validating a model. This article summarizes key requirements for production-ready models.  \n\ud83d\udca1Model development checklist:  \nDatasets are versioned, centrally available and discoverable. Also full lineage of features and engineering is captured.  \nModels trained are versioned, centrally available and discoverable.  \nUnit and integration tests have been applied to the training and inference pipeline. Also static seeds applied where relevant, to ensure near identical and reproducible results.  \nA consistent scoring and evaluation approach has been implemented.  \nPerformance and parallel processing improvements are designed, tested, monitored and partially implemented and tested. Compute environments are explored for optimum results.  \nThe following sections describe additional elements of developing high quality production models.  \nML pipelines  \nML pipelines are automated workflows, which consist of ingesting training data or features, and training, validating a model prior to packaging. These pipelines are designed to be repeatable and modular to facilitate agile iteration and development.  \nFor an in-depth look, refer to Machine Learning Pipelines  \nFeature management  \nMuch of the complexity in developing ML systems is in data preparation and data normalization for feature engineering workflows. Feature management enables accessing features by name, model version, lineage, etc., in a uniform way across environments. Using feature management provides consistency to help improve developer productivity. It also promotes discoverability and reusability of features across teams.  \n{% if extra.ring == 'internal' %}\nCode examples:  \nFeature Store using Feast: An end-to-end example codebase using Feast, which has been used at customer engagement.  \nFeature Store using Feathr: Codebase with notebooks using Feathr\n{% endif %}  \nFor more information, see Feature Management  \nModel engineering  \nModel engineering consists of the process of iterating on a model, tuning hyperparameters, and preparing the preprocessing/training code for inclusion in an ML training pipeline. This process also includes details on how to integrate standard software engineering practices into model engineering, such as unit testing.  \nTesting your ML code  \nModel validation  \nModel validation is the process of testing a newly trained model to ensure it works (such as, properly accepting inputs and creating expected outputs), and it performs the same or better than any previous model. Validation can include comparing metrics such as accuracy, ROC, etc.  \nFor more information  \nMLOps Model Factory Template based on SDK (v2): A machine learning (ML) model factory is a system for automatically building, training, and deploying ML models at scale. It includes various features that make it easier to create and manage large numbers of ML models. It can also automate the model building process.  \nMLOps Template for Azure ML CLI (v2): An MLOps implementation for projects that are using the Azure ML CLI v2.  \nMLOps Template for Azure ML SDK (v2): An MLOps implementation for projects that are using the Azure ML SDK v2.  \nAzure MLOps (v2) solution accelerator: This project can be a starting point for MLOps implementation in Azure ML CLI or SDK v2.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\model-development\\index.md"
    },
    {
        "chunkId": "chunk284_0",
        "chunkContent": "author: shanepeckham\ntitle: Feature Store Adoption Guide\nms.topic: product-comparison\nms.author: shanepec\nms.service: azure\ndescription: Data Platform teams are looking for ways to provide most of the functionality needed by the data science team. The goal is to provide it in a way that is easy to manage and control via self-service, if possible. Data scientists are looking for a platform where they can find and share features, and abstract feature access during training and inference.\nrings:\n- public  \nDeciding whether to adopt a feature store  \nWho should decide whether it's worth the investment for a team and/or company to build a feature store?  Feature stores offer benefits for data science and infrastructure/platform teams, so it makes sense to include both groups in the discussion Data Science Factors.  \nPlatform engineers on large teams or organizations often recommend adoption of a feature store. In contrast, data scientists are likely to first see the benefits in smaller teams and organizations. Buy-in from both groups is critical to successfully implementing a solution like this one.  \nData platform teams are looking for ways to provide most of the functionality needed by the data science team. The goal is to provide it in an easy to manage and control using self-service, if possible. Data scientists are looking for a platform where they can find and share features and abstract feature access during training and inference.  \nThe following content highlights decision points most teams will reach while considering implementing a feature store. We'll start with the definition of some terms that are frequently used in feature stores.  \nFeature Store Terminology  \nFeature transformation: refers to the process of converting raw data into features. Transformation generally requires building data pipelines to ingest both historical and real-time data.  \nFeature registry: refers to a location where all features used are defined and registered. By using a registry, data scientists can search, find, and reuse features in their models. Feature definitions include information like type, source, and other relevant metadata.  \nFeature serving: refers to being able to serve feature values for both batch operations like training (high latency) and low latency for inference. It abstracts the complexity away when querying the feature values while providing functionality like point-in-time joins.  \nObservation data: refers to the raw input for the data being queried in the Feature Serving layer. Observation data is, at the least, composed of the IDs of the entities of interest and timestamps; both exist as join keys. This concept is called Entity data frame in other feature stores.  \nPoint-in-Time Joins (PITJ): for time-series-driven data. It is important to make sure the data used for training is not mixed with the latest data ingested. Doing so creates feature leakage (also known as label leakage). PITJ ensures that data served corresponds to the closest observation times.  \nData science factors  \nData scientists should review the following questions to help decide if it is worth the cost to invest in a feature store.  \nDecision point Without feature store With feature store Do your data scientists have problems finding available features for reuse? Without a centralized repository for features, data scientists often jump directly to creating feature transformation pipelines. These pipelines increase the complexity of the platform as the use cases supported grow and reduce the value of previously acquired domain knowledge. A key component in a feature store is the feature registry. A feature registry is a module that works as a centralized repository for all features created by and within an organization. It makes discovery and management of features easier. A feature registry contains information about feature definitions and their source. Depending on the feature store, it might include information about the transformation code and lineage information. This component is, ideally, searchable, easy to understand, and accessible from a centralized endpoint. Do you want to share your features with business users? Information about features is scattered throughout docs and code and is not easily shareable with business users. These users provide domain knowledge about which features to use or might become outdated. Feature Stores are a single source of truth with a standardized and structured way of viewing information about features. Do many of your features need to be served/computed in real time? You have clients requesting predictions that do not have the feature values, without a way to inject them into the requests. These features need to be computed in near real-time. A good use case is a real-time recommendation engine where you aggregate streamed events and generate new recommendations on-demand. Feature stores provide a component called an Online Store. The online store contains the latest version of a feature value (often called materialization). The values persist in a low latency data store, ensuring that features are served in near real-time to your model. The feature store abstracts this materialization process. Are many of your features time-dependent? Do your data scientists spend much time handling complex point-in-time joins? Data scientists need to spend time learning how to do point-in-time correct joins. Constructing point-in-time correct data is time-consuming and error-prone. A feature store has built-in point-in-time joining capabilities, abstracting this complexity away from the data scientists. Do your data scientists spend time writing complex queries or code to access the feature data? During feature value retrieval, data scientists must write code to access the data according to the data source of choice. The lack of abstraction can require writing complex queries or spending time writing code that has little direct value to their work. Sometimes, the time is spent debugging infrastructure issues instead of higher value activities like feature engineering/building the ML model itself. Feature stores provide a Serving layer that works as an abstraction away from the infrastructure. Data scientists can minimize the time spent dealing with the infrastructure and specific syntax and focus on the needed features. This layer combines the feature registry and the point-in-time joins, providing a powerful mechanism for data scientists to access data without knowing the underlying infrastructure.  \nPlatform factors  \nInfrastructure and data platform teams should consider the following questions when evaluating the pros and cons of building a feature store.  \nDecision point Without feature store With feature store Do you maintain many duplicated feature transformation code/pipelines? When data scientists are unaware of existing features, there will be a propensity to create and manage duplicate pipelines to perform feature transformations. Managing all these pipelines is expensive and demands a lot of attention from a platform team when making changes or upgrades. Given the predilection for shareability in a feature store, the number of duplicated feature transformation pipelines should be reduced in favor of reusing existing features. Do you have to serve features for training (batch or high-latency) and inference (low-latency)? Processing historical (for training) and streaming (for inference) data is done differently and requires separate pipelines. These pipelines might process the data using different methods and technologies that are specific to how the data is ingested (batch vs. streaming) and store the results in various data stores according to the latency requirements. All these factors increase the complexity of maintaining these pipelines. Most feature stores provide a module for feature computation that takes care of storing data in a suitable data store according to the requirements of the feature. A module like this enables the processing of batch data from ETL processes or historical data in a data warehouse and of streaming data from low latency message bus systems. To make this process consistent, a feature store would provide a domain-specific language (DSL) to perform transformations that deliver consistent results no matter how the data is ingested. The DSL allows features to be computed once for both training (batch) and inference (real-time) and reused across models. An example of such a case is Feathr. Do you need to keep your data systems compliant? Maintaining control of each training dataset used by your data science team might be daunting, especially as the number of use cases grows. Some feature stores provide the governance tools required by an enterprise to exercise control over the feature data. Access control, data quality, policies, auditing, etc., enable the platform team to maintain control over the data ingested and transformed in the feature store from one centralized place.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\model-development\\feature-management\\feature-store-adoption-guide.md"
    },
    {
        "chunkId": "chunk285_0",
        "chunkContent": "author: shanepeckham\ntitle: Comparing popular feature stores\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: This article provides an overview and side-by-side comparison of some better-known feature store solutions, including FeaSt, Databrick FS, Feathr, and Azure's Managed Feature Store.\nrings:\n- public  \nComparing popular feature stores  \nThis article provides an overview and side-by-side comparison of some better-known feature store solutions, including FeaSt, Databricks FS, Feathr, and Azure's Managed Feature Store.  \nFeature store summaries  \nHere are brief summaries of each feature store solution.  \nFeaSt  \nFeaSt is an open-source Feature Store created by GoJek.  \nIt focuses on providing a Feature Registry for sharing features and a Feature Serving layer to provide point-in-time joins support and abstract the queries to access the features from the datastore. FeaSt expects that you bring your own data warehouse and doesn\u2019t provide support for feature transformation. It also expects that your data transformation has been completed and persisted beforehand.  \nDatabricks FS  \nDatabricks FS is a proprietary Feature Store solution provided within the Databricks environment.\nIt makes use of the Delta Lake file system that Databricks uses to provide a solution that works with different data versions. It only works within Databricks and integration with other data sources is not available.  \nFeathr  \nFeathr is an open-source Feature Store created by LinkedIn and Microsoft.  \nIn terms of functionality, Feathr provides a Feature Registry, support for Feature Transformation through its built-in functions, and the functionality to share features across teams.  \nFeathr runs the feature computation on Spark against incoming data from multiple sources. It supports different storage systems to persist that data after it has been processed for consumption at training or inference time.  \nAzure Managed Feature Store  \nThe Managed Feature Store redefines the ML experience and enables ML professionals to develop and produce features from prototyping to operationalizing. Has capability for monitoring features, supports network isolation Private Link and managed Vnet, and can be used as part of Azure AML or other custom ML platforms.  \nComparison  \nProvided below is a comparison of some of these feature store solutions.  \nOnboarding  \nCategory FeaSt Databricks FS Feathr Managed Feature Store Installation FeaSt is used as an SDK from the client\u2019s code by installing a package and running some commands locally: Quickstart - Feast . When deploying in a cloud environment, please refer to: https://docs.feast.dev/how-to-guides/feast-snowflake-gcp-aws Databricks has a feature store initialized on the creation of a Databricks cluster. From there, you need to create a Feature Store Object. You can use the Python API to create it. From here, you can use the Feature Store functionalities. Note: Azure DB docs Feathr documentation on how to install Feathr. feathr/quickstart.md at main \u00b7 linkedin/feathr (github.com) . Has documentation on how to provision an Azure resource - feathr/azure-deployment.md at main \u00b7 linkedin/feathr (github.com) The Feature Store is part of the Azure Machine Learning workspace. Check the Prerequisites on what you need to get started with Managed Feature Store Engineering Onboarding Several tutorials are provided: Quickstart provided Quickstart - Feast . Tutorials provided such as: ( Overview - Feast ) 1. Fraud detection on GCP Fraud detection on GCP - Feast 2. Driver ranking Driver ranking - Feast 3. Real-time credit scoring on AWS Real-time credit scoring on AWS - Feast 4. Driver stats on Snowflake Driver stats on Snowflake - Feast 5. Validating historical features with Great Expectations Validating historical features with Great Expectations - Feast Quickstart notebook - Feature Store Taxi example notebook - Databricks (microsoft.com) Quickstart needs to provision Azure resources 1. QuickStart - feathr/quickstart.md at main \u00b7 linkedin/feathr (github.com) Azure provisioning - feathr/azure-deployment.md at main \u00b7 linkedin/feathr (github.com) Demo notebook - feathr/nyc_driver_demo.ipynb at main \u00b7 linkedin/feathr (github.com) Documentation page - Feathr \u2013 An Enterprise-Grade, High Performance Feature Store Managed Feature Store Documentation Azure support The Azure ML Product Group has created a feast-azure provider plugin Azure/feast-azure: Azure plugins for Feast (FEAture STore) (github.com) Support via Azure Databricks Azure Databricks LinkedIn and Microsoft have open-sourced Feathr and facilitated a quick start guide for Azure . The Managed Feature Store has full Azure support.  \nCapabilities  \nCategory FeaSt Databricks FS Feathr Managed Feature Store Supported Data Sources Currently supported offline stores - S3 - Snowflake - BigQuery - Redshift - Azure SQL DB and/or Synapse SQL (Azure/feast-azure: Azure plugins for Feast (FEAture STore) (github.com)) Databricks Delta tables. No other supported offline stores. - ADLS (Azure Data Lake Storage) - Azure Blob Storage - Azure SQL DB - Azure Synapse Dedicated SQL Pools - Azure SQL in VM - Amazon S3 - SnowFlake - Kafka streaming ADLS (Azure Data Lake Storage) Supported Offline Store For Feature Transformation Not supported. Databricks Delta tables - ADLS (Azure Data Lake Storage) - Azure Storage Blob - Amazon S3 - Delta Lake ADLS (Azure Data Lake Storage) Supported Online Stores Currently supported online stores (Online stores - Feast) - SQLite (locally) - Redis - Datastore - DynamoDB Managed SQL services from Azure/AWS are options: - Azure SQL DB - Azure Database for MySQL - Amazon Aurora (MySQL-compatible) - Amazon RDS MySQL As of April 2022, Redis is not supported. - Redis - CosmosDB - SQL Redis Read from Multiple Data Sources FeaSt only supports one data source per namespace. For one feature store definition you might have multiple namespaces, but can access one namespace at a time per FeaSt SDK instance. You can mix multiple instances and merge data in your code, but there might be some associated performance tax. A data source refers to the DB File System location (if any) being used. Thus, multiple data sources can be maintained, however, they will have to exist in the Databricks Filesystem. When working on the Feature Transformation code, you can define multiple sources and combine the data for processing. At this point multiple data sources are not supported. Multiple data sources are planned for a later release. Feature Definition Versioning No Feature Definition Versioning support \u2013 there is no way to update Features without deleting them. Feature definition versioning is not implemented yet. The only feature versioning that it provides is related to the feature value, using the built-in capability of Delta Lake to track versions. However, if you would like to change a particular feature definition in a feature table, then you need to create a new feature table. Not yet supported, but it is on the roadmap Supports versioning. Feature sets are versioned and immutable. Feature Metadata Storage and Querying All metadata for feature definition is stored in a binary registry file and accessible via the command-line interface or SDK. - Metadata is just a dictionary. All metadata is immutable for Feature Views/Features. - Can list all Entities. Can list all Feature Views (feature groups). Can query by entity + feature view + feature. - Metadata on Feature Views (feature groups), Entities, and on Features. - Note: Can query the FeaSt registered concepts of the above. For example if I have a feature set where the driver's ID is my entity, FeaSt will tell me that the driver_id column is the entity. It will not return the values of the entities that are needed to query the data The feature definition metadata is available in the Feature Store UI. - Versions produced for feature. - Notebook and schedule for which feature data is produced. - Raw data used. - Online stores that a feature is linked to The feature definition metadata is persisted in Purview and accessible using Feathr\u2019s SDK or Purview\u2019s directly. One advanced feature that Feathr provides by using Purview is the ability to get lineage information for features. The Purview lineage shows what is involved in generating the feature. Shows the lineage on which Feathr project, the process to execute it. What anchors and anchor processes are executed to produce the feature. Feature Dataset Lifecycle FeaSt expects that all your features\u2019 values have been created and stored in the offline store beforehand. The only step left for using FeaSt is for the user to create and apply the feature views and services (as code) for their usage. All data is first ingested into the Databricks File System. From here, you can pull the data and apply data transformations. Finally, they are put into a delta table to be used as the offline store. Feature tables will keep track of a list of related features. There could be multiple notebooks, or data transformation logic pipelines pushing to the same feature table if necessary. Feature Engineering: Feathr allows you to explore your data, create transformations, and persist your data in new tables in your offline datastore. Raw data stored in ADLS can be transformed and stored in offline stores. Supported formats are parquet and csv. Time Travel Support Can retrieve features with a specified feature value data timestamp. Can retrieve features with a specified feature value data timestamp. Includes support for point-in-time joins. Supports feature retrieval with specified feature value data timestamp. Lineage tracing - Provides audit functionality. Audit logs are low level and getting data level lineage will require extra work. - FeaSt expects already processed data to be added to the feature store. All tracking from raw data to feature store must be built separately - Deleting individual ingested records is not supported. Lineage is provided through a feature store UI on the Databricks platform Lineage can be seen in Azure Purview Purview is a metadata catalogue where you can see the relationships between a model and feature sets. There is an integration with Purview coming in a future release. At this point, lineage information can be accessed from the Feature Store UI for every model. APIs and Protocol supported Python client library lags behind REST API. Also offers Java and Go client libraries Python API with examples of every function is presented on the Databricks docs . Since the feature store is created on the Databricks cluster infrastructure, our python code would have to have access to the Databricks cluster. The recommended way is to use notebooks on the DB cluster to ensure we have access to the resources. Online stores are designed for native access. As of April 2022, Databricks feature store is primarily geared towards batch inference. FeathrCli and FeathrClient (Python) Serving API users can look up features for training batch inference and online inference. Serving api can pull the data out of a source directly or materialized storage like an offline/online store. Data validation Not native: Validating historical features with Great Expectations Validating historical features with Great Expectations - Feast Not native: Validating historical features with Great Expectations. Great Expectations tutorial Not native: Great expectations would be used to connect to Azure Synapse Spark. See this link on how to connect to data on Azure Blob Storage using Spark Management/Storage of feature data - Feature data is all stored in Databricks Delta tables. These delta tables are created by the user and are used as an offline store. - Feature data can be accessed/seen in Purview - Feathr API has some functionality to retrieve this information No visual capabilities for that, however, the feature store API SDK allows you to retrieve the feature data. Feature Definition Discovery capabilities An interface for access to the feature definitions and metadata has been added to the roadmap. For now, accessing the metadata information is possible using a CLI command. Can filter and search through feature store UI. Text search for features, feature tables, tags is currently available. Can create and edit tags to be assigned to different feature tables. These tags can then be used to search for a subset of features. For example, having a tag called \u2018dev\u2019 would show someone all dev feature tables. Feathr provides an optional UI web application for search and discovery of features from the registry. Feature Catalogue to search and reuse features. Azure ML integration Integration of FeaSt within Azure ML is possible through some work as documented here by the Azure ML PG. No in-built functionality; need to pull from online store for inference. Databricks has an ML platform of its own, so it has links to that rather than to Azure ML now. The Azure ML notebook (run Feathr on notebook) is supported for now. In the future, integrate with Azure ML compute. Available to use from Azure ML and custom ML platforms.  \nObservability  \nCategory FeaSt Databricks FS Feathr Managed Feature Store Metrics No built-in metrics capability, can use Prometheus to scrape metrics from FeaSt Core and Serving. Databricks Machine Learning has some metrics on the experiments and models being run but MLFlow is used by them as well to track the machine learning model itself. No built-in capability: - Can use Prometheus to scrape metrics. synapse/metrics-howto.md at master \u00b7 matrix-org/synapse (github.com) - Can configure to send the diagnostics to Azure Log Analytics. Monitor Azure Synapse Analytics Using Log Analytics (c-sharpcorner.com) Tracing - Python SDK uses python logging libraries. - The FeaSt services themselves do not log anything in their pods running in kubernetes Databricks has integrations with MLFlow that allow us to log models and track deployments, etc. Databricks Feature Store uses MLFlow as a tool to enhance the ML Workflow in the example notebook linked above as well. Can view logs on Synapse When it comes to the materialization process, there is logging for the scheduled process. The job record created can be visualized in the UI. The user can query on that. Logs will be available as part of the job. Whenever a user does CRUD operations, creates assets, there will be information logged.  \nOther categories  \nCategory FeaSt Databricks FS Feathr Managed Feature Store Transformations In FeaSt, any data transformation is expected to happen before the features get created. Transformations can be done in a notebook as is the standard SDLC. Enabled via built-in transformations: - feathr/feature-definition.md at main \u00b7 linkedin/feathr (github.com) Feature transformation capabilities: local development testing, support for Pyspark/Spark sql based transformationsFeature transformation capabilities: fully local development testing, support for Pyspark/Spark sql based transformations Access control and authorization Not implemented. Permissions are enabled that can give access to only certain Databricks feature tables depending on role. Note, these roles in Databricks are different from Azure RBAC policies currently and cannot be synched. A project level role-based access control (RBAC) plugin is available to help you manage who has access to the Feathr Registry. It provides a simple authorization system built on OAuth tokens, along with an SQL database as backend storage, for user role records. More information about Feathr Registry Access Control Azure role based access control is used to manage the access to the resources. Access control for managed feature store. Open source/manage Open-source Proprietary technology maintained by Databricks Open source \u2013 maintained by LinkedIn Not open source at the moment. Other Limitations 1. Databricks Feature Store APIs support batch scoring of models packaged with Feature Store. Online inference is not supported. - 2. Databricks Feature Store does not support deleting individual features from a feature table. A new feature table would have to be created instead. Currently in public preview and not recommended for production workloads since certain features might not be supported.",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\model-development\\feature-management\\feature-store-comparison.md"
    },
    {
        "chunkId": "chunk286_0",
        "chunkContent": "author: shanepeckham\ntitle: AI Feature Management\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Features are descriptive attributes about a dataset that help in model prediction, forecasting and more. Raw data is rarely in a format that can be consumed by an ML model directly, so it needs to be transformed into features. This process is called feature engineering.\nrings:\n- public  \nFeature Management  \nWhat is a Feature?  \nFeatures are descriptive attributes about a dataset that help in model prediction, forecasting, and more.  \nRaw data is rarely in a format that can be consumed by an ML model directly, so it needs to be transformed into features. This process is called feature engineering.  \nWhat is a Feature Store?  \nA Feature Store is a software toolkit designed to manage raw data transformations into features. Feature Stores often include metadata management tools to register, share, and track features. Feature stores also handle the complexity of doing correct point-in-time joins. So, the resulting data frame can be ingested by any model training libraries.  \nThe Feature Store Adoption Guide dives deeper into whether a Feature Store is right for your project.\nThe Feature Store Comparison Guide makes comparisons between the popular feature stores to help you pick the best fit for your use case.  \nHow Feature Stores simplify the ML Model Workflow?  \nA typical ML Model workflow (for example, without Feature Stores) generally means one has to build multiple models using hundreds of features. The maintenance of the feature transformation pipelines alone becomes tedious, slowing down the productivity and efficiency of model development. Also, the design isn't easily reuseable and doesn't promote sharing of features across teams.  \nThe following drawing illustrates workflow change with the introduction of Feature Stores.  \n{% if extra.ring == 'internal' %}  \nCode examples  \nThe following code examples show how to use a Feature Store for data using the Home Credit Default Risk data from Kaggle.  \nFeature Store using Feast: An end-to-end example codebase using Feast, which has been used at customer engagement.  \nFeature Store using Feathr: Sample notebooks using Feathr  \nFeature Store using Databricks: End to end example using Databricks' Feature Store\n{% endif %}  \nPopular Feature Stores  \nFeaSt  \nFeathr  \nTecton",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\model-development\\feature-management\\index.md"
    },
    {
        "chunkId": "chunk287_0",
        "chunkContent": "author: shanepeckham\ntitle: Machine Learning Pipelines\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Automation of machine learning pipelines, training and scoring datasets, models\nrings:\n- public  \nUnderstanding machine learning pipelines  \nA machine learning pipeline is an automated process that generates an AI model. In general, it can be any process that uses machine learning patterns and practices or a part of bigger ML process. It\u2019s common to see data preprocessing pipelines, scoring pipelines for batch scenarios, and even pipelines that orchestrate training based on cognitive services. Therefore, an ML pipeline should be understood as a set of steps that is sequential or in parallel, where each step is executed on a single or multiple nodes (to speed-up some tasks). The pipeline itself defines the steps and configures the compute resources needed to execute the steps and access related datasets and data stores. To keep things simple, a pipeline typically is built with a well-defined set of technologies and/or services.  \nIf a pipeline is properly configured, regenerating a model is simple and reliable. If the same training dataset is used each time the pipeline runs, the quality of the model should be similar. The importance of a given model artifact is thus reduced, as it can be regenerated. (We are not yet discussing model deployment.)  \nDatasets are important from many different perspectives, including traceability, data labeling, and data drift detection. While each of these topics is complex, we'll make some simplified assumptions for this article.  \nA single, immutable dataset in a fixed location.  \nDataset and hyperparameters can be passed to the ML pipeline as a parameter.  \nConsider the following example: We need to train a model to find some bad pixels in raw video frames. In most real-world scenarios, raw data must be preprocessed before they can be used to train models. In this example, raw video files first need to be decomposed into raw video frames, and additional preprocessing is likely to be required for each frame. Other processing includes selecting statistically relevant training data, extract features, and so on. Only when all data curation/preparation steps are finished can model training begin.  \nIt is not uncommon for pre-processing to take more time than training itself. We might mitigate this by running pre-processing on several compute nodes at the same time. This requires that the pipeline be updated to split processing into multiple steps that execute in different environments. For example, using multi-node clusters or single-node clusters, or GPU nodes versus CPU nodes. If we use a diagram to design our pipeline, it might look like this:  \nIn this pipeline, there are four sequential steps:  \nThe first step is executed in parallel on several nodes to convert all available video files into images.  \nThe second step also runs in parallel, but it might use a different number of nodes because we have more images to pre-process than we had raw video files.  \nOne more step is training itself, which can also use a distributed approach (for example, Horovod or Parameter Server).  \nFinally, we have a single node step to log the model into storage.  \nThis example illustrates why a robust ML pipeline is required rather than a simple build script or manually run process. Let\u2019s discuss technologies.  \nBuilding a pipeline with the Azure Machine Learning service  \nThere are many different technologies to implement a pipeline. For example, it's possible to use Kubeflow as a framework for pipeline development and Kubernetes as a compute cluster. Databricks and MLFlow can be utilized as well. At the same time, Microsoft offers a service to develop and manage machine learning pipelines (jobs): Azure Machine Learning service. Let's take Azure ML as an example of a technology to implement our generalized pipeline.  \nAzure ML has several important components that are useful for pipeline implementations:  \nAzure ML SDK for Python and Azure ML CLI: To create all Azure ML components and communicate to Azure ML service.  \nAzure ML Compute: The ability to create and manage clusters to execute pipeline steps there. It can be just one cluster for all steps or different clusters per step. There is a way to mount storage to compute cluster instances, and we are going to use this feature implementing our pipeline steps.  \nAzure ML Model Registry: Azure Machine Learning provides a managed API to register and manage models. A model is a single file or a folder with a set of files with included version tracking.  \n**CommandJob Class: It's possible to implement single instance jobs, batch workloads or distribute training jobs.  \nLet\u2019s show how our generalized pipeline will look from a technologies perspective:  \nWhen to implement an ML pipeline  \nData scientists generally prefer using Jupyter Notebooks for Exploratory Data Analysis and initial experiments.  \nRequiring the use of pipelines at this stage may be premature, as it could limit the data scientists technology choices and overall creativity. Not every Jupyter notebook is going to create a need for a production pipeline, as the data scientists need to test many different approaches and ideas before converging on an approach. Until that work is done, it is hard to design a pipeline.  \nWhile the initial experimentation can be done in a notebook, there are some good signals that tell when it is time to start moving experiments from a notebook to an ML pipeline:  \nThe approach and code are stabilizing, meaning it's reasonable to begin transitioning from research to development.  \nData scientists are ready to share the same code and experiment with different parameters.  \nData scientists need more compute resources since a single VM or a local computer is no longer sufficient.  \nWhen these things begin happening, it\u2019s time to wrap up experimentation and implement a pipeline with technology such as the Azure ML Pipeline SDK.  \nWe recommend following a three-step approach:  \nStep 1. Refactor the notebook into clean Python code. The primary goal being to move all methods/classes to separate Python files to make them independent from the execution environment.  \nStep 2. Convert the existing notebook to a single step pipeline. You can use the following guideline to create a single-step pipeline to execute on a single instance only. The only difference is it is your own code in train.py. At this stage, you will see how the pipeline works and be able to define all datasets/datastores and parameters as needed. Starting from this stage you are able to run several experiments of the pipeline using different parameters.  \nStep 3. Identify critical blocks in the pipeline and move them to different steps. The primary reason for this step is to make your pipeline faster. You need to identify your clusters (GPU or non-GPUs and VM sizes) and the kinds of steps to use to wrap components from your code (single, parallel or distributed TF/PyTorch etc.).  \nHere is an example of an Azure ML pipeline definition: https://github.com/microsoft/mlops-basic-template-for-azureml-cli-v2/blob/main/mlops/nyc-taxi/pipeline.yml. This YAML file defines a series of steps (called \"jobs\" in the YAML) consisting of the Python scripts which execute a training pipeline. The examples follow this sequence of steps:  \nprep_job, which processes and cleans the raw data  \ntransform_job, which shapes the cleansed data to match the training job's expected inputs  \ntrain_job, which trains the model  \npredict_job, which uses a test set of data to run a set of predictions through the trained model  \nscore_job, which takes the predictions, and scores them against the ground truth labels.  \nEach of these steps references a Python script, which was likely once a set of cells in a notebook. Now, they consist of modular components that can be reused by multiple pipelines.  \nThe full template is available in this GitHub repo: MLOps Basic Template for Azure ML CLI v2\n.  \nHere are additional similar templates:  \nMLOps Template for Azure ML SDK v2  \nMLOps Model Factory Template  \nJenkins MLOps Template  \nDevelopment process implementation  \nFrom a DevOps perspective the development process can be divided into three different stages:  \nPublish and Execute a pipeline from a local computer (or VM) using the full dataset or just a subset of data. Usually, it\u2019s happening from a local feature branch where a developer or a data scientist is tuning the pipeline. Note, we are executing the pipeline from a local computer rather than on a local computer.  \nCreate and validate Pull Request for changes from a feature branch towards to the development branch.  \nPublish and execute the pipeline in the development environment based on the full dataset.  \nThere are a few pieces of advice that we can provide:  \nHost all different parameters that are required for pipeline publishing as environment variables. In this case, you will be able to initialize your local environment and publish the pipeline from a local computer. Or, you can use a variable group in your favorite DevOps system to publish the pipeline from there.  \nMake sure the pipeline includes needed files only rather than everything from the repository. For example, Azure ML supports .amlignore file.  \nUse branch names and build IDs to enable developers modifying the pipeline simultaneously (see below).  \nBranch name utilization  \nThe biggest challenge of the ML pipeline development process is how to modify and test the same pipeline from different branches. If we use fixed names for all experiments, models, and pipeline names, it will be hard to differentiate your own artifacts working in a large team. To make sure we can locate all feature branch-related experiments, we recommend using the feature branch name to mark all pipelines. Also, use the branch name in experiments and artifact names. This way, it will be possible to differentiate pipelines from different branches and help data scientists to log various feature-branch runs under the same name.  \nThe designed scripts can be utilized to publish pipelines and execute them from a local computer or from a DevOps system. Below is an example of how you can define Python variables based on initial environment variables and on a branch name:  \n```py\npipeline_type = os.environ.get('PIPELINE_TYPE')\nsource_branch = os.environ.get('BUILD_SOURCEBRANCHNAME')\n\nmodel_name = f\"{pipeline_type}{os.environ.get('MODEL_BASE_NAME')}{source_branch}\"\npipeline_name = f\"{pipeline_type}{os.environ.get('PIPELINE_BASE_NAME')}{source_branch}\"\nexperiment_name = f\"{pipeline_type}{os.environ.get('EXPERIMENT_BASE_NAME')}{source_branch}\"\n```  \nYou can see that in the code above we are using branch name and pipeline type. The second one is useful if you have several pipelines. To get a branch name from a local computer, you can use the following code:  \npy\ngit_branch = subprocess.check_output(\"git rev-parse --abbrev-ref HEAD\",\nshell=True,\nuniversal_newlines=True)  \nPull request builds  \nLet\u2019s start with introduction of two branch types that we are going to use in the process:  \nFeature branch: any branch owned by a developer to develop a feature. Since we are not planning to preserve and manage this branch, the developer has to decide how to name the branch.  \nDevelopment branch: it\u2019s our primary source for all feature branches and we treat it as a stable development environment that we are using to run our experiments on the full dataset. Potentially, the development branch and a current model can be a good candidate for production at any point ine time.  \nSo, we need to start with a feature branch. It's important to guarantee that all code that is going to a development branch can represent working ML training pipelines. The only way to do that is to publish the pipeline and execute the pipelines on a toy data set. The toy dataset should be prepared in advance. It should be small enough to guarantee the pipelines won't spend much time on execution. Therefore, if we have changes in our ML training pipeline, we have to publish it and execute it on the toy dataset.  \nThe diagram above demonstrates the process that we explained in the section. Furthermore, you can see that there is a Build for Linting and Unit Testing. This Build can be an another policy to guarantee code quality.  \nPay attention so that if you have more than one pipeline in the project, you might need to implement several PR Builds. Ideally, one Build per pipeline, if it's available in your DevOps system. Our general recommendation is to have as many Builds for as many pipelines as we have. It allows us to speed up the development process since developers should'nt wait for all experiments to be done. In some cases, it\u2019s not possible.  \nDevelopment branch Builds  \nOnce we have code in our development branch, we need to publish modified pipelines to our stable environment, and we can execute it to produce all required artifacts (models, update scoring infrastructure, etc.)  \nPay attention that our output artifact on this stage is not always a model. It can be a library or even our pipeline itself if we are planning to run it in production (scoring one, for example).  \nLook at the Model Release section to get details about next steps.  \nVideo processing pipeline example based on Cognitive Services  \nAs we mentioned above, there are many technologies to use to implement a generalized pipeline. In this example we show a pipeline implementation based on Microsoft Cognitive Services.  \nThis example contains an AI enrichment pipeline, the pipeline is triggered by the upload of a binary file to an Azure Storage account. The pipeline will \"enrich\" the file with insights from various Azure Cognitive Services, custom code, and Video Indexer before submitting it to a Service Bus queue for further processing.  \nThe entire end-to-end pipeline illustrates the flow data: From ingestion, to enrichment, to training, to inference for use with unstructured data such as videos, documents, and images.  \nThe image below illustrates the high-level system architecture:  \nFor more information, see  AI Enrichment tutorial",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\model-development\\ml-pipelines\\index.md"
    },
    {
        "chunkId": "chunk288_0",
        "chunkContent": "author: shanepeckham\ntitle: Testing for MLOps Scenarios\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Provide samples of tests for the most common operations in MLOps/Data Science projects. Testing the code used for MLOps or data projects follows the same principles of any other software project.\nrings:\n- public  \nTesting for MLOps Scenarios  \nThe purpose of this document is to provide samples of tests for the most common operations in MLOps/Data Science projects. Testing the code used for MLOps or data projects follows the same principles as any other software project.  \nSome scenarios might seem different or more difficult to test. Best practice is to always have a test design session, where the focus is on the input/outputs, exceptions alongside testing the behavior of data transformations. Designing the tests first simplifies the testing process as it forces a more modular style. Each function has one purpose, and extracting common functionality functions and modules.  \nBelow are some common operations in MLOps or Data Science projects, along with suggestions on how to test them.  \nSaving and loading data  \nReading and writing to csv, reading images or loading audio files are common operations scenarios encountered in MLOps projects.  \nExample: Verify that a load function calls read_csv if the file exists  \nutils.py  \npython\ndef load_data(filename: str) -> pd.DataFrame:\nif os.path.isfile(filename):\ndf = pd.read_csv(filename, index_col='ID')\nreturn df\nreturn None  \nThere's no need to test the read_csv function, or the isfile functions, we can leave testing them to the pandas and os developers.  \nThe only thing we need to test here is the logic in this function, such as, that load_data loads the file if the file exists with the right index column, and doesn't load the file if it doesn't exist, and that it returns the expected results.  \nOne approach would be to provide a sample file and call the function, and verify that the output is None or a DataFrame. Separate files are required to exist for the tests to run. The same test may end up running on one machine and then fail on a build server, which is not a desired behavior.  \nA better way is to mock calls to isfile, and read_csv. Instead of calling the real function, we will return a predefined return value, or call a stub that doesn't have any side effects. No files are needed in the repository to execute the test, and the test will always work the same, independent of what machine it runs on.  \nNote: Below we mock the specific os and pd functions referenced in the utils file, any others are left unaffected and would run as normal.  \ntest_utils.py  \n```python\nimport utils\nfrom mock import patch\n\n@patch('utils.os.path.isfile')\n@patch('utils.pd.read_csv')\ndef test_load_data_calls_read_csv_if_exists(mock_isfile, mock_read_csv):\n# arrange\n# always return true for isfile\nutils.os.path.isfile.return_value = True\nfilename = 'file.csv'\n\n```  \nSimilarly, we can verify that it's called 0 or multiple times. In the example below where we verify that it's not called if the file doesn't exist.  \n```python\n@patch('utils.os.path.isfile')\n@patch('utils.pd.read_csv')\ndef test_load_data_doesnt_call_read_csv_if_not_exists(mock_isfile, mock_read_csv):\n# arrange\n# file doesnt exist\nutils.os.path.isfile.return_value = False\nfilename = 'file.csv'\n\n```  \nExample: Using the same Sample data for multiple tests  \nIf more than one test uses the same sample data, fixtures are a good way to reuse this sample data. The sample data can be the contents of a json file, or a csv, or a DataFrame, or even an image.  \nNote: The sample data is still hard coded if possible, and does not need to be large. Only add as much sample data as required for the tests to make the tests readable.  \nUse the fixture to return the sample data and add as a parameter to the tests where you want to use the sample data.  \n```python\nimport pytest\n\n@pytest.fixture\ndef house_features_json():\nreturn {'area': 25, 'price': 2500, 'rooms': np.nan}\n\ndef test_clean_features_cleans_nan_values(house_features_json):\ncleaned_features = clean_features(house_features_json)\nassert cleaned_features['rooms'] == 0\n\ndef test_extract_features_extracts_price_per_area(house_features_json):\nextracted_features = extract_features(house_features_json)\nassert extracted_features['price_per_area'] = 100\n```  \nTransforming data  \nFor cleaning and transforming data, test fixed input and output, but try to limit each test to one verification.  \nFor example, create one test to verify the output shape of the data:  \n```python\ndef test_resize_image_generates_the_correct_size():\n# Arrange\noriginal_image = np.ones((10, 5, 2, 3))\n\n# act\nresized_image = utils.resize_image(original_image, 100, 100)\n\n# assert\nresized_image.shape[:2] = (100, 100)\n```  \nand one to verify that any padding is made appropriately.  \n```python\ndef test_resize_image_pads_correctly():\n# Arrange\noriginal_image = np.ones((10, 5, 2, 3))\n\n# Act\nresized_image = utils.resize_image(original_image, 100, 100)\n\n# Assert\nassert resized_image[0][0][0][0] == 0\nassert resized_image[0][0][2][0] == 1\n```  \nTo test different inputs and expected outputs automatically, use parametrize:  \n```python\n@pytest.mark.parametrize('orig_height, orig_width, expected_height, expected_width',\n[\n# smaller than target\n(10, 10, 20, 20),\n# larger than target\n(20, 20, 10, 10),\n# wider than target\n(10, 20, 10, 10)\n])\ndef test_resize_image_generates_the_correct_size(orig_height, orig_width, expected_height, expected_width):\n# Arrange\noriginal_image = np.ones((orig_height, orig_width, 2, 3))\n\n# act\nresized_image = utils.resize_image(original_image, expected_height, expected_width)\n\n# assert\nresized_image.shape[:2] = (expected_height, expected_width)\n```  \nModel load or predict  \nWhen unit testing we should mock model load and model predictions similarly to mocking file access.  \nThere may be cases when you want to load your model to do smoke tests, or integration tests.  \nNote, it will often take a bit longer to run it's important to be able to separate them from unit tests. The developers on the team will still be able to run unit tests as part of their test-driven development.  \nOne approach is using marks  \npython\n@pytest.mark.longrunning\ndef test_integration_between_two_systems():",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\model-development\\model-engineering\\mlops-testing-scenarios.md"
    },
    {
        "chunkId": "chunk288_1",
        "chunkContent": "# this might take a while  \nRun all tests that are not marked long-running  \nbash\npytest -v -m \"not longrunning\"  \nBasic Unit Tests for ML Models  \nML unit tests are not intended to check the accuracy or performance of a model. Unit tests for an ML model are for code quality checks - for example:  \nDoes the model accept the correct inputs and produce the correctly shaped outputs?  \nDo the weights of the model update when running fit?  \nThe ML model tests do not strictly follow all of the best practices of standard Unit tests; not all outside calls are mocked. These tests are much closer to a narrow integration test.\nHowever, the benefits of having simple tests for the ML model help to stop a poorly configured model from spending hours in training, while still producing poor results.  \nExamples of how to implement these tests (for Deep Learning models) include:  \nBuild a model and compare the shape of input layers to that of an example source of data. Then, compare the output layer shape to the expected output.  \nInitialize the model and record the weights of each layer. Then, run a single epoch of training on a placeholder data set, and compare the weights of the \"trained model\"; only check if the values have changed.  \nTrain the model on a placeholder dataset for a single epoch, and then validate with placeholder data. Only validate that the prediction is formatted correctly, this model will not be accurate.  \nData Validation  \nAn important part of the unit test is to include test cases for data validation.  \nFor example:  \nNo data is supplied.  \nImages are not in the expected format.  \nData containing null values or outliers to make sure that the data processing pipeline is robust.  \nModel Testing  \nApart from unit testing code, we can also test, debug, and validate our models in different ways during the training process.  \nSome options to consider at this stage:  \nAdversarial and Boundary tests to increase robustness  \nVerifying accuracy for under-represented classes",
        "source": "..\\data\\docs\\code-with-mlops\\capabilities\\model-development\\model-engineering\\mlops-testing-scenarios.md"
    },
    {
        "chunkId": "chunk289_0",
        "chunkContent": "What is the AI Platform Team currently working on?  \nBelow are the strategic areas and code assets of investment on which the AI Platform team is working. If you would like to help with, contribute to, or engage an SME from our team, please contact:  \nShane Peckham - EMEA/APAC,\nMario Inchiosa - Americas  \nAreas of investment  \nInvestment Status Asset Contact Data Chunking, Vector Storage In Progress RAG Experiment Accelerator Ritesh Modi, Vadim Kirilin Synthetic data generation for usage with LLMs In Progress RAG Experiment Accelerator , Synthlume Ritesh Modi, Vadim Kirilin Search Evaluation In Progress RAG Experiment Accelerator , Azure AI Search index strategies guidance including vector search Shane Peckham, Taylor Rockey Azure OpenAI Fine Tuning pipelines Planned Azure OpenAI latency and observability In Progress Tooling and guidance on AOAI latency minimization Ren Silva LLM Application Security In Progress Guidance on securing LLM applications Bryan Smith, Ren Silva GenerativeAI Gateway Planned Performance, cost and OpenAI management gateway RAG implementation methodology In Progress How to be successful in RAG Ren Silva  \nRecent code assets  \nRetrieval Augmented Generation (RAG) solutions and accelerators  \nRAG Experiment Accelerator: This is a versatile tool designed to expedite and facilitate the process of conducting experiments and evaluations using Azure Cognitive Search and Azure OpenAI. The primary objective is to streamline and simplify the intricate task of running experiments and evaluating search queries and quality of responses from Azure OpenAI models.  \nLLM scaffolding template with Semantic Kernel: A code repository that demonstrates the process to tune and engineer prompts, log and evaluate results, and publish a service to serve users' requests using Semantic Kernel.  \nPlease note that this repo will be significantly updated/enhanced as soon as we have our next material customer opportunity.  \nLLMOps  \nLLMOps basic template: This code asset help implement MLOps process for Azure Open AI workloads using Prompt Flow SDK and code-first approach. The repository contains all needed components to help in developing and deploying one or more Prompt Flow pipelines starting from local execution and up to online endpoint deployment.  \nLLMOps advanced template: LLMOps with Prompt Flow is a \"LLMOps template and guidance\" to help you build LLM-infused apps using Prompt Flow. It offers a range of features including Centralized Code Hosting, Lifecycle Management, Variant and Hyperparameter Experimentation, A/B Deployment, reporting for all runs and experiments and so on.  \nPlease note that this one is an advanced version of the above basic template with more features. We will soon merge them together but for now we want to keep the basic version in parallel as there are some customers who need only basic. But for broader awareness and socialization, we will use the advanced template.  \nSynthetic data generation  \nSynthlume: This code asset aims to take a few human queries and will use them as the basis to generate more similar synthetic examples. Additional work is in progress to generate more evenly distributed synthetic data with Joana S. Santos and Dimitrios Mavroeidis.",
        "source": "..\\data\\docs\\code-with-mlops\\current\\index.md"
    },
    {
        "chunkId": "chunk290_0",
        "chunkContent": "rings:  \npublic  \nContributors \u2728  \nAshton Mickey Herman Wu Prasanna Muralidharan Dave Williams Swetha Sundar Sarah D'Ettorre Jon Malsan Francisco Beltrao Linda M Thomas Arjen Everaert Ian Ivan Ramirez Faiza Ghazanfar Malvina Matlis Smita Vemulapalli Cheng Chen Dax Ren Silva Telly Cooper Shane Peckham Mikhail Chatillon Oph\u00e9lie Le Mentec Nora Abi Akar J\u00fcrg Staub Laura Damian Oscar Fimbres Maitreyi Nair Joana Santos Drew Robbins",
        "source": "..\\data\\docs\\code-with-mlops\\references\\contributors.md"
    },
    {
        "chunkId": "chunk291_0",
        "chunkContent": "Documentation  \nLinks to official product documentation that may be helpful:  \nMicrosoft resources  \nAzure Machine Learning  \nAzure Databricks  \nAzure Cognitive Services  \nAzure Cognitive Search  \nAzure Synapse Analytics  \nMicrosoft Purview  \nAzure Data Catalog  \nMicrosoft Presidio  \nMicrosoft's Synthetic Data Generator  \nSecurity management in Azure  \nTeam Data Science Process  \nMicrosoft Responsible AI (RAI)  \nThird-party resources  \nData Version Control  \nMLflow  \nFeast Feature Store",
        "source": "..\\data\\docs\\code-with-mlops\\references\\documentation.md"
    },
    {
        "chunkId": "chunk292_0",
        "chunkContent": "Examples  \nAzure ML examples  \nThe AzureML-Examples repository includes the latest (v2) Azure Machine Learning Python CLI and SDK samples. For information on the various example types, see the readme.  \n{% if extra.ring == 'internal' %}  \nVector database examples  \nFacebook AI Similarity Search (FAISS) is a vector store library developed by Meta that allows for on-disk dense vector index storage and efficient similarity search. The faiss_vector_index repository includes examples for creating, serializing, deserializing, and searching over a FAISS vector store as well as walking through a sample usage in the provided notebook.  \n{% endif %}",
        "source": "..\\data\\docs\\code-with-mlops\\references\\examples.md"
    },
    {
        "chunkId": "chunk293_0",
        "chunkContent": "References  \nIn this section, you can find links to official product documentation pages, useful examples as well as templates.",
        "source": "..\\data\\docs\\code-with-mlops\\references\\index.md"
    },
    {
        "chunkId": "chunk294_0",
        "chunkContent": "Templates  \nList to useful templates that we would recommend to use in your projects:  \nMLOps Model Factory Template based on SDK (v2) - A machine learning (ML) model factory is a system for automatically building, training, and deploying ML models at scale. It includes a variety of features that make it easier to create and manage large numbers of ML models, as well as automate the model building process.  \nMLOps Template for Azure ML CLI (v2) - An MLOps implementation for projects that are using the Azure ML CLI v2 and Azure Pipelines. The template implements the model development stage only.  \nMLOps Template for Azure ML SDK (v2) - An MLOps implementation for projects that are using the Azure ML SDK v2 and Azure Pipelines. The template implements the model development stage only.  \nAzure MLOps (v2) solution accelerator - This project is intended to serve as the starting point for MLOps implementation in Azure ML CLI/SDK v2  \nDatabricks MLOps Template: An MLOps example based on Databricks which has been used at multiple customer engagements  \nJenkins MLOps Template: An MLOps example for Jenkins showcasing both training and inference",
        "source": "..\\data\\docs\\code-with-mlops\\references\\templates.md"
    },
    {
        "chunkId": "chunk295_0",
        "chunkContent": "author: shanepeckham\ntitle: AI Solutions\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: A solution is an opinionated engineering approach that brings together a set of capabilities to solve a business problem. It provides guidance, insights, best practices on how to develop a complete functional solution to address an end-to-end business scenario along with code. All solutions listed have been successfully applied and validated by multiple customers.\nrings:\n- public  \nAI solutions  \nA solution is an opinionated engineering approach that brings together a set of capabilities to solve a business problem. It provides guidance, insights, best practices on how to develop a complete functional solution to address an end-to-end business scenario along with code. All solutions listed have been successfully applied and validated by multiple customers.  \nHere is a list of available solutions:  \nAutomating and Monitoring Model Training: How CSE implements ML training pipelines with our customers, including code templates, CI/CD pipelines, and guidance documentation  \nData discovery and classification for unstructured data: Provides guidance and code assets that use various machine learning techniques to discover insights in unstructured data. There are examples for documents, images and videos.  \nMLOps Model Factory: An extension of the standard MLOps template used in the Automating and Monitoring Model Training solution. This accelerator scales to many models, deploying to varying targets.\n{% if extra.ring == 'internal' %}  \nMLOps accelerator on edge: Tools for implementing MLOps with edge deployment and inference.  \nImage classification experimentation with Azure Custom Vision: How to use Azure Custom Vision for Image Classification in a repeatable and traceable way.  \nData enrichment for Azure Cognitive Search: A collection of end-to-end pipelines using various Azure services and standalone ML models. They can be invoked in isolation or as part of a data enrichment pipeline, including the Azure Cognitive Search pipeline as they adhere to the Azure Cognitive Search PowerSkills format.  \nAutomated information extraction from forms: A set of guidance, tools, examples, and documentation that illustrate some known techniques for information extraction. All of these techniques have been applied in real customer solutions.  \nAutomation and optimization of Custom Translator Service: This solution contains guidance, CI/CD pipelines, and scripts to optimize the selection of data and training of the Custom Translator service.  \nAzure Arc and data management in Azure ML: This solution contains guidance on how to set up a hybrid training scenario in Azure ML between cloud compute and an on-premises Kubernetes cluster. It includes a solution for managing training data transfer from the cloud to the on-premises cluster.  \nReal Time Object Detection and Tracking on the Edge: A specific implementation of - Serving Models on Edge, optimized for real time inference.\n{% endif %}",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\index.md"
    },
    {
        "chunkId": "chunk296_0",
        "chunkContent": "Azure Arc and data management in Azure ML  \nBusiness Problem  \nAzure ML provides multiple compute options for training and deployment both in-cloud and on-premises, the latter made possible by using Azure Arc. Azure Arc enables customers to leverage investments they made in on-premises, Kubernetes-enabled compute clusters, while still benefiting from the services provided by Azure ML.  \nIn this scenario, we have a customer who stores their training data in Azure and has an on-prem Kubernetes cluster they want to use for running machine learning training jobs. However, as their teams grow and their machine learning problems become more complex, their on-prem cluster becomes less and less available. This of course affects the productivity of the team and for that reason they want the option of running training jobs on Azure ML compute in the cloud. This hybrid-training method allows them to leverage their on-prem cluster and scale beyond it when necessary. The challenge is in guaranteeing seamless execution of training jobs both on-prem and in-cloud, including a solution for transferring the training data from the cloud to the cluster efficiently and making that data available to the jobs.  \nChallenges  \nSetting up the Azure ML infrastructure, including enabling Azure Arc on the on-prem cluster.  \nTransferring the data from the cloud to the Azure Arc cluster and keeping the data on-prem synchronized with the data in-cloud without re-downloading the full dataset for every training job.  \nSimplifying the execution of jobs regardless of compute target.  \nSolution  \nThe Quick-starter for Azure Machine Learning and Azure Arc Kubernetes provides a solution for the data-transfer challenge and a step-by-step guide for:  \nSetting up Azure Arc on a Kubernetes cluster and adding it as a compute in an Azure ML workspace using a combination of Terraform and a bash script.  \nRunning a simple training job using Azure Arc as a compute.  \nManaging data transfer between the cloud and the Azure Arc cluster using an Azure ML pipeline and bash script for data download.  \nWrapping the training and data transfer logic in a Make-based CLI tool that enables users to run training jobs either on-prem or in-cloud with a simple one-line command.",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\azure-arc-and-data-management\\index.md"
    },
    {
        "chunkId": "chunk297_0",
        "chunkContent": "tags:\n- Manufacturing\n- Media\n- Transportation\n- Retail  \nReal Time Object Detection and Tracking on the Edge  \nThis is a special case of Serving Models on Edge.  \nOverview  \nAI-enabled object detection and tracking is a powerful solution that utilizes object detection models such as YOLO, RCNN, FastRCNN or FasterRCNN, combined with OpenCV-based object tracking algorithms. This approach enables real-time detection and tracking of objects of interest in a video feed, making it an ideal solution for a variety of applications. While traditional machine learning models require significant amounts of expertise to build, there are several platforms available that can simplify the process. These include Azure AutoML for images, Custom Vision, or Custom Object Detection Models.  \nOne of the most common scenarios for AI-enabled object detection and tracking is people counting in real-time. This application is useful in a variety of settings, including retail stores, airports, and public spaces. By detecting and tracking individuals as they move through a space, businesses and organizations can gain valuable insights into foot traffic patterns and make informed decisions about staffing, product placement, and more. In addition to people counting, object detection and tracking can also be used to track vehicles, machinery, materials, and other objects on a shop floor or in a production environment.  \nAnother key application of AI-enabled object detection and tracking is safety and security monitoring. By detecting and tracking objects in real-time, businesses can quickly identify potential safety or security breaches and take immediate action to prevent accidents, theft, or other incidents. This technology can also be used to detect defects in a production line, which can help companies identify quality control issues and improve product consistency. Finally, AI-enabled object detection and tracking can be used to manage traffic, such as in smart city applications, by detecting and tracking vehicles in real-time and optimizing traffic flow to reduce congestion and improve safety.  \nThis solution supports some common scenarios:  \nCounting people in real-time  \nTracking objects, such as cars, machinery, materials on the shop-floor  \nDetecting safety or security breaches  \nDetecting defects in a production line  \nManaging traffic.  \nThe Technical Solution in a Nutshell  \nIn its simplest form, the solution involves the artifacts depicted below:  \nIts components are:  \nA camera, running on the user's premises, produces a video stream, usually using rtsp or rtmp protocol  \nThat video stream is sent through to an AI model to detect a specific object (for instance, persons, or some other moving objects)  \nFurther processing is performed on the detected objects (for instance: counting the number of people, or obfuscating the detected image for privacy, or detecting an unsafe situation)  \nThe original video plus the result of AI and further processing are merged and sent to another video stream.  \nBoth the original and the processed video-streams are then available for viewing in real-time.  \nChallenges  \nOne challenge that the Real Time Object Detection and Tracking on the Edge solution helps address is the need for real-time detection and tracking of objects in a video feed. Traditional machine learning models can be time-consuming to develop, making it difficult to achieve real-time results. However, this solution utilizes object detection models like YOLO, RCNN, FastRCNN, or FasterRCNN, combined with OpenCV-based object tracking algorithms, to enable real-time object detection and tracking on the edge. By detecting and tracking objects in real-time, businesses can gain valuable insights and take immediate action to prevent accidents, theft, or other incidents.  \nAnother challenge that this solution helps address is the need for automated people counting in real-time. By detecting and tracking individuals as they move through a space, businesses and organizations can gain valuable insights into foot traffic patterns and make informed decisions about staffing, product placement, and more. The Real Time Object Detection and Tracking on the Edge solution supports counting people in real-time and can provide businesses with valuable data to optimize their operations.  \nFurthermore, the Real Time Object Detection and Tracking on the Edge solution helps address the need for privacy in video feeds. The solution offers the ability to obfuscate the detected image for privacy while still providing real-time object detection and tracking. This ensures that the privacy of individuals captured on the video feed is protected while still providing businesses with valuable data to optimize their operations.  \nDeployment on the Edge  \nMost of the time, processing needs to be done on a compute closer to action. The AI model needs to run as often as every 300~400ms - and it is impractical to serve the model in the cloud. The video stream needs to be available on-premise, closer to action.  \nFor those reasons, this solution is ofter deployed \"on the edge\", i.e. on a compute that lives outside the cloud.  \nEdge MLOps  \nA series of containers must be deployed to the edge to make up this solution:  \nA video streaming solution - that relays the original video stream to one or more formats  \nOne or more AI \"enrichers\" - that collects the original stream, pipes it through the model, processes it further and relays it to the streaming solution  \nOne or more AI models - these can be open source models, or custom models produced with Azure ML, or other solutions such as Azure Custom Vision  \nA Video Player - a front-end solution that can be used to visualize the produced streams.  \nWe follow similar pattern defined for MLOps for Edge to generate, package, and deploying those containers to the one or more edge devices.  \nA Sample Repo  \nThe github repository Video Broker solution demonstrates how to deploy the building blocks of a solution to Detect and Track people.  \nIt is deployed in an automated fashion to your Azure Account.  \nThe workflows are built primarily with Azure CLI - so converting them to Azure Devops Pipelines can be done easily.  \nThe deployment of the containers uses Azure Azure Arc technology - so the containers are deployed to on-premise kubernetes clusters.  \nTechnology  \nThe repository contains the following technology:  \nThe video streaming solution (2 instances) uses the open-source rtsp-simple-server  \nThe AI \"enrichers\" (2 instances) were built in Python, using OpenCV object tracking. One simply blurs faces, the other one shows if the person is wearing a face-mask  \nThe AI model - (1 instance) is built in Python - and was created with Azure Machine Learning AutoML for Images  \nThe Video Player - (1 instance) was built with React and javascript  \nDeployment uses GitOps FluxV2, deploying the solution to an Arc-enabled kubernetes cluster. (It uses an Azure VM running KIND - Kubernetes in Docker)  \nThe workflow runs in Github Actions  \nInstead of a real RTSP camera, it uses a video running indefinitely, streamed to RTSP. Replacing it with a real RTSP feed is mere question of changing one Environmental Variable.  \nAll building blocks can be easily replaced so that the developers can use them to build a real-life solution.  \nReferences  \nhttps://github.com/microsoft/video-broker  \nArc-enabled kubernetes cluster  \nKIND - Kubernetes in Docker  \nGitOps FluxV2",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\real-time-object-detection-and-tracking-on-edge\\index.md"
    },
    {
        "chunkId": "chunk298_0",
        "chunkContent": "Edge Device Provisioning - Azure Arc  \nIntroduction  \nIn this document, we will describe the pattern to provision the Edge device as an Arc-enabled kubernetes cluster.  \nWith Azure Arc, you can manage your on-premise device from within Azure Kubernetes Service portal. This can help you deploy and manage your edge device from a central location, and enables deployment and management of many devices at once:  \nWith arc-enabled Kubernetes clusters, you can manage and configure Kubernetes clusters across multiple environments, including at the edge and in public clouds.  \nThis makes it easier to keep an eye on everything and make sure it's running smoothly from one place. Plus, with Arc, you can use other Azure services like Azure Policy and Azure Security Center to make sure everything is secure and following the rules. This makes it easier to deploy and manage your edge workloads with more control and consistency, no matter how much you're scaling up.  \nAfter provisioning, you will be able to take full advantage of Azure services for Machine Learning inference on the edge, robotic development, configuration and control scenarios, etc.  \nWe will explore two different provisioning patterns:  \nUsing Continuous Deployment via Github Actions (we will deploy to a Virtual Machina, that acts as the edge device)  \nStep-by-step (manual), using Azure CLI directly on the Edge compute (we will deploy to your local PC)  \nProvisioning Solution Overview  \nThere are 3 stages involved in this process.  \nStage 1: allocating the edge compute device - this can be a local PC/Mac/Linux machine, or a virtual machine (emulating and edge device)  \nStage 2: Create a kubernetes cluster on the edge device  \nStage 3: Installing Arc components and linking the edge device to Azure Kubernetes Services  \nOnce these steps are completed, you can connect your Arc-enabled cluster with GitOps configuration for automated deployment of your solution.  \nThe links below contain examples on provisioning Arc-enabled Kubernetes clusters as edge devices:  \nVirtual machine, in Azure, in automated fashion, using Github Action Workflows: refer to this link: https://github.com/microsoft/video-broker#2-automated-deployment  \nLocal PC (Windows, Linux or Mac), as Arc-enabled kubernetes, step by step (manual): https://github.com/microsoft/video-broker/blob/main/STEP-BY-STEP.md#4-option---deploy-solution-with-azure-arc  \nIn fact, when you follow all the steps, you will completely deploy a solution that runs a video continuously and applies AI inference on that video.  \nThe description of the solution can be found here: https://github.com/microsoft/video-broker#readme  \nFurther Reading  \nArc-enabled kubernetes clusters  \nAzure Arc  \nAzure Container Registry  \nAzure Kubernetes Services  \nKIND - Kubernetes in Docker  \nImplement CI/CD with GitOps using Azure Arc-enabled Kubernetes clusters",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\serving-models-on-edge\\arc-provisioning.md"
    },
    {
        "chunkId": "chunk299_0",
        "chunkContent": "Model Deployment on IoT Edge  \nIntroduction  \nAfter a model is generated, the next step in process is to deploy it to a target platform. Models can be deployed to a variety of targets including Azure Kubernetes Services (AKS), Azure App Services, Azure functions, Azure Container Instances(ACI) and Azure IoT Edge.  \nThe general process for Inferencing on the Edge includes the following steps:  \nStep 1: Model generation.\nStep 2: Provision and configure Azure resources for IoT Edge.\nStep 3: Deploy IoT Edge module (module encapsulate Docker image and ML Model).\nStep 4: Smoke Test and Health check of IoT Edge module.  \nStep 1: Model generation  \nModel generation step consists of all activities related to training, testing, evaluation, registration, and storage of the model. After a Model is registered, it is included and configured within a Docker image and pushed to an Azure Container Registry (ACR). The automation of all activities related to the configuration of a Docker image with an ML model, testing and pushing to ACR is documented here. It provides guidance and sample code for building a Docker Image with an ML Model and storing the same in the ACR.  \nStep 2: Provision and Configure resources for IoT Edge  \nBy this time, the Machine Learning Workspace and Azure Container Registry should already be available as provisioned resources in a resource group. For deployment of ML Models on an IoT Edge device, an IoT Hub service and an IoT Edge device should also be provisioned. The IoT hub Edge device can be a physical device or an Ubuntu based virtual machine.  \nThis step involves:  \nProvisioning of an IoT Hub resource on Azure  \nProvisioning of an Edge device within an IoT Hub service  \nProvisioning of an Edge device (Ubuntu VM)  \nInstall IoT Edge runtime to edge device to turn it to be an IoT Edge device.  \nRegister device ID in IoT Hub with your preferred authentication for your device, you may choose either connection string, or you may use X.509 certificates.  \nConfig the device authentication and provision the IoT Edge device to IoT Hub  \nRefer to the link Provision and Configure IoT Edge related resources.  \nStep 3: Deploy IoT Edge module  \nCreate a new module deployment file (deployment.json) with the following code. The system modules edgeAgent and edgeHub are compulsory. SklearnModule and SimulatedTemperatureSensor are the custom modules you want to deploy.  \nNote: It is important to change the name of the path of the image in this file. It should reflect the path as it appears in the ACR. Also, at the time of writing the latest version for Edge runtime is 1.4 and that the user should check if there is a later version available.  \n```json\n\n{\n\"modulesContent\": {\n\"$edgeAgent\": {\n\"properties.desired\": {\n\"schemaVersion\": \"1.1\",\n\"runtime\": {\n\"type\": \"docker\",\n\"settings\": {\n\"minDockerVersion\": \"v1.25\",\n\"loggingOptions\": \"\",\n\"registryCredentials\": {\n\"<>\": {\n\"username\": \"$CONTAINER_REGISTRY_USERNAME\",\n\"password\": \"$CONTAINER_REGISTRY_PASSWORD\",\n\"address\": \"$CONTAINER_REGISTRY_SERVER\"\n}\n}\n}\n},\n\"systemModules\": {\n\"edgeAgent\": {\n\"type\": \"docker\",\n\"settings\": {\n\"image\": \"mcr.microsoft.com/azureiotedge-agent:1.4\",\n\"createOptions\": \"{}\"\n}\n},\n\"edgeHub\": {\n\"type\": \"docker\",\n\"status\": \"running\",\n\"restartPolicy\": \"always\",\n\"settings\": {\n\"image\": \"mcr.microsoft.com/azureiotedge-hub:1.4\",\n\"createOptions\": \"{\\\"HostConfig\\\":{\\\"PortBindings\\\":{\\\"5671/tcp\\\":[{\\\"HostPort\\\":\\\"5671\\\"}],\\\"8883/tcp\\\":[{\\\"HostPort\\\":\\\"8883\\\"}],\\\"443/tcp\\\":[{\\\"HostPort\\\":\\\"443\\\"}]}}}\"\n}\n}\n},\n\"modules\": {\n\"SklearnModule\": {\n\"version\": \"1.0\",\n\"type\": \"docker\",\n\"status\": \"running\",\n\"restartPolicy\": \"always\",\n\"settings\": {\n\"image\": \"<>\",\n\"createOptions\": \"{}\"\n}\n},\n\"SimulatedTemperatureSensor\": {\n\"version\": \"1.0\",\n\"type\": \"docker\",\n\"status\": \"running\",\n\"restartPolicy\": \"always\",\n\"settings\": {\n\"image\": \"mcr.microsoft.com/azureiotedge-simulated-temperature-sensor:1.0\",\n\"createOptions\": \"{}\"\n}\n}\n}\n}\n},\n\"$edgeHub\": {\n\"properties.desired\": {\n\"schemaVersion\": \"1.1\",\n\"routes\": {\n\"simpletestToIoTHub\": \"FROM /messages/modules/SklearnModule/outputs/* INTO $upstream\",\n\"sensorTosimpletest\": \"FROM /messages/modules/SimulatedTemperatureSensor/outputs/temperatureOutput INTO BrokeredEndpoint(\\\"/modules/SklearnModule/inputs/input1\\\")\"\n},\n\"storeAndForwardConfiguration\": {\n\"timeToLiveSecs\": 7200\n}\n}\n}\n}\n}\n\n```  \nThe deployment.json file contains all the configuration information required to deploy and configure custom modules on IoT Edge device. It can be used to deploy and configure modules using az cli command as shown in following code snippet. Replace the values for variables (device_id and iot_hub_name) with actual values.  \n```bash\n\naz iot edge deployment create --deployment-id $(echo $RANDOM | md5sum | head -c 10; echo;) \\\n--hub-name $(hub_name) \\\n--content ./deployment.json \\\n--target-condition \"deviceId='$(device_id)'\"\n\n```  \nThe command will read the content of the deployment.json file and apply the configuration to device_id device registered with IoT Hub service.  \nStep 4: Smoke Test and Health check of Iot Edge module  \nIt is important to test the ML Model after it is deployed to an IoT Edge device, to ensure its availability and to validate that it can accept and respond to requests with appropriate Model inference values. This Smoke Test can be performed on an IoT Edge device using IoT Edge module deployed alongside the ML Model.\n{% if extra.ring == 'internal' %}\nMore information on Smoke Testing ML Models on an IoT Edge device is available here.\n{% endif %}\nSmoke Tests should be triggered from within the Edge device itself as the device generally does not have inbound internet connectivity. This is one of the important reasons to execute Smote Tests as an IoT module rather than initiating tests from external sources.  \nReference  \nAzure IoT Hub  \nAzure IoT Edge documentation  \nAzure Container Registry  \nAzure Machine learning  \nAzure DevOps pipelines  \nDockerfile  \nAzure Machine learning CLI V2  \nAzure AD Service Principal",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\serving-models-on-edge\\edge-deployment.md"
    },
    {
        "chunkId": "chunk300_0",
        "chunkContent": "Uploading Large Files from IoT Edge to Azure  \n1. Problem Statement  \nWhen running Azure IoT Edge module workloads, messages can be sent to the cloud through the IoT Edge runtime. The maximum message size is 256kb, and we may have a scenario where we need to send a file which is too large to send as a message.  \nFrom our past experience customers, we see a number of scenarios for pushing larger files from the edge, such as;  \nSending image and video data for ML model training and/or inference in the cloud.  \nSending depth images and 3D point clouds from edge sensors in the robotics industry to visualize the 3D environment for use in fleet management.  \nThis document outlines the considerations and options for transferring these types of larger files from an IoT Edge module into Azure.  \n2. Considerations  \nWhen deciding which solution is most suitable for your scenario, there are a number of considerations which can affect your decision.  \nFile size - How large are the files you want to send to the cloud?  \nAuthentication - How do you want to authenticate connections to the cloud?  \nTiming - What are the requirements for the timing or latency of the file delivery? What delivery guarantees are required?  \n3. Solutions  \n1. IoT Edge Blob module  \nThe Azure Blob Storage on IoT Edge module is an IoT Edge module which provides Azure Blob storage on the edge. It can be configured to automatically synchronize with a storage account in the cloud.  \nImplementation  \nThe Azure Blob Storage module is deployed in IoT Edge.  \nThe IoT Edge module containing custom code can write to this blob using the standard Azure client libraries.  \nUsing the deviceToCloudUpload feature the data is synchronized with a cloud blob.  \nConsiderations  \nThe Azure Blob Storage module and any of application modules which write to the blob will need to be provided with a SAS token or a storage AccountKey. Consider the guidelines for secret management.  \nThe token/key can be provided at deployment time or configured using module twin desired properties.  \nIf a SAS token is used, a process is required to update the token when it expires.  \nThe cloud upload will be abstracted from the IoT Edge module, the Blob Storage module will handle asynchronous upload and retries.  \n2. IoT Hub file upload feature  \nFile Upload is a feature of Azure IoT Hub which facilitates uploading files to the cloud.  \nCurrently this feature is not supported for IoT Edge modules. However, it is possible to use this feature by registering a virtual device in our IoT Edge module. This virtual device will appear as a separate device to IoT Hub and have its own device id.  \nImplementation  \nA virtual device needs to be registered in IoT Hub  \nThe IoT Edge module application implements file upload using one of the IoT Hub SDKs  \nA blob storage account must be associated with the IoT Hub.  \nConsiderations  \nThe IoT Edge module which performs the upload will identify itself to IoT Hub as a device (not a module). Therefore it needs to be deployed with a device ID and corresponding authentication (X.509 certificate or Symmetric Key). For these additional credentials please consider best practices for secrets management.  \nThe File Upload feature will provide a SAS Token which will be used to authenticate to the blob storage.  \nThe file upload will be synchronous, but error handling and retry policy will need to be implemented in the IoT Edge module code.  \n3. Azure Blob API/Client Library  \nSince an IoT Edge module is running custom code, the Azure Storage APIs or client libraries can be used to write the files directly to an Azure Blob.  \nImplementation  \nThe IoT Edge module will need to implement the calls to upload to the blob using one of the client libraries (available for many different languages).  \nConsiderations  \nAuthentication can be a Storage Account Key or SAS Key, which will need to be provided to our IoT Edge module.  \nThe file upload will be synchronous, but error handling and retry policy will need to be implemented in the IoT Edge module code.  \n4. Pull using Direct Method  \nThe last method can be used when it is desirable to pull data from the edge rather than pushing to the cloud.  \nImplementation  \nWhen a file on the edge is required, a backend application will call a direct method in IoT Hub to retrieve the file from the IoT Edge module.  \nThe backend application can use either the Invoke REST API or the IoT Hub SDKs to call the direct method.  \nA direct method handler will be implemented in the IoT Edge module.  \nIf the file needs to be persisted the backend application can save it to a blob.  \nConsiderations  \nThe authentication between cloud and device will use the already established connection from IoT Edge to IoT Hub, so no additional credentials are required.  \nThe backend application will need to generate a SAS token to invoke the direct method on IoT Hub.  \nThe response of a direct method call is limited to 128kb, which is actually smaller than the maximum size for a device to cloud message so it may not be suitable if file size is your main concern. Alternatively, it would be possible to send larger files by chunking the files.  \nThe backend application dictates when the file upload will occur.  \nThe direct method will be synchronous, so any error handling or retry policy will need to be implemented in the backend application.  \nAn additional consideration for this method is that the data will be send thru IoT Hub and not directly to the Azure Blob storage. So please consider the cost and scale implications of this when planning your IoT Hub solution.",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\serving-models-on-edge\\edge-to-cloud-file-upload.md"
    },
    {
        "chunkId": "chunk301_0",
        "chunkContent": "MLOps Accelerator for Edge  \nWhat is the MLOps for Edge Solution  \nMLOps for Edge refers to the process of automated generation, packaging, testing of ML Models and deployment to Edge devices. Apart from general MLOps stages, there are additional stages of packaging ML Model suitable for Edge deployment and Deployment steps unique to Edge.  \nMLOps accelerator for Edge is an end to end workflow that supports generating and deploying many models on multiple IoT Edge devices.  \nEdge workflow starts with generation of ML Model and includes deployment of Models on Edge devices. It also includes Model management and observability on the Edge. Once an ML Model is generated as a result of experimentation, and the data scientists are satisfied with its performance, the next step is to package and deploy the ML Model.  \nAn ML Model can be deployed to many different types of targets, both in the cloud and on-premises. One of the most important on-premises targets is the Edge device. A ML Model deployed on the Edge helps in decentralizing the Model's availability to many local machines and devices.  \nThe Edge can be a compute (virtual machine or physical machine) or a physical device itself.  \nA device, e.g., camera, barcode reader, etc. captures the inputs and sends them to the ML Model for inferencing. The Model generates an inference output based on inputs. The inference outputs are logged, returned back to caller and may also be sent to other services for consumption and reporting.  \nMLOps for Edge, in short, refers to the process of generating, packaging, and deploying ML Model on Edge devices.  \nThe assets related to the Edge inferencing, fall under these buckets:  \nTraining and Testing of the ML Model  \nPackaging of ML Model  \nEdge Environment provisioning  \nDeployment of the Model and the Docker image  \nThe Customer Pattern  \nMore and more enterprise customers are looking for edge-based solutions and advance their ML capabilities on the Edge. There are many use-cases that are addresses by deploying ML Models on the Edge. These include anomaly and defect detection, fraud detection, predictive maintenance and more. These customers want to develop, package ML Models on Cloud and deploy on Edge to improve their operational efficiency, user experience, save cost and increase reliability.  \nTypical Challenges faced  \nBuilding and managing ML Models is a complex initiative due to its inherent reliance on ever changing availability of new data, business environment and statutory conditions. These ML Models should be generated and deployed in an efficient and automated manner through automation and orchestration. Deploying Models on edge involves additional complexities on top on general ML Model deployment and management process. Edge devices generally have their ports blocking for incoming requests and work on a low-bandwidth network perimeter. It also has a problem of multiple devices and multiple types of devices. Enterprise customers are looking for a solution that can help automate the entire process of building ML Models, package and deploy them on edge devices in an automated manner to increase predictability and consistency in their process.  \nThe MLOps on Edge Solution  \nMLOps accelerator provides an end-to-end solution comprising of automated modular pipelines for  \nMLOps practices, process and lifecycle through a generic orchestrator that has placeholders for data processing and transformation, training, testing, evaluation, score and registration of ML Models  \nGenerating Packages with ML Model and all its dependencies into a Docker Image to be able to run independently on Edge.  \nProvisioning Edge environment and infrastructure  \nConfiguring and executing ML Model deployment  \nTesting ML Models on the Edge  \nThis solution can be used for ML Models related to multiple different type of problems that needs Images, Videos, documents, tabular data as its input and results in a well-tested deployed functional ML Models on the Edge.  \nHow does the MLOps on the Edge Solution help?  \nSupports generation of multiple ML Models  \nMLOps pipeline for Data preparation, transformation, Model Training, evaluation, scoring and registration  \nEach ML Model is packaged in a independent Docker Image  \nModel verification before storing the Docker image  \nAll Docker images are stored in Azure Container Registry  \nGated approval before moving to deployment phase  \nSupports deployment of Edge Modules to multiple and dynamically selected IoT Edge devices based on conditions  \nAutomated and incremental layered deployment of ML Models on Edge devices  \nBuilds and deploys Smoke Test module on Edge device  \nSmoke test of ML Model on each Edge device after deployment  \nSupports storing inference input and outputs on Edge device  \nEnable transparent and reproducible ML Model training.  \nSimplify the path from experimentation to deployment of a new Model.  \nScale the impact of Data Scientists who define and develop ML Models.  \nApply standards for governance and security of ML Models.  \nPackage both CPU as well as GPU based ML Models.  \nAbility to security test the generated packages.  \nTest the ML Model both while building the package and after deployment to Edge.  \nFail early fail fast. Packages will not be made available unless confirmed by tests.  \nML Model traceability and lineage.  \nThe MLOps on the Edge Process  \n```mermaid\n\nflowchart LR\nsubgraph Infrastructure\ndirection TB\nBuildCloudInfrastructure([\"Build Cloud Infrastructure\"]) --- BuildEdgeInfrastructure([\"Build Edge Infrastructure\"])\nend\nsubgraph Experimentation\ndirection TB\nExperimentationInMLOps([\"Experimentation in MLOps\"]) --- ExperimentationInAzureML([\"Experimentation in Azure ML\"])\nend\nsubgraph ModelDevelopment[\"Model Development\"]\ndirection TB\nMLPipelines([\"ML Pipelines\"]) --- MLTestingScenarios([\"ML Testing Scenarios\"])\nend\nsubgraph ModelPackaging[\"Model Packaging\"]\ndirection TB\nPackagePipeline([\"Package Pipeline\"]) --- BuildBundle([\"Build Bundle Docker Image\"]) --- TestBundle([\"Test Bundle\"]) --- StoreBundle([\"Store Bundle\"])\nend\nsubgraph Deployment\ndirection TB\nReleasePipelines([\"Release Pipelines\"]) --- ModelRelease([\"Model Release\"]) --- DeploymentEdge([\"Model Deployment on Edge ML\"]) --- SmokeTest([\"Model Test on Edge\"])\nend\nsubgraph InferenceAndFeedback[\"Inference and Feedback\"]\ndirection TB\nInf([\"Inference and Feedback\"])\nend\nsubgraph MLLM[\"ML Lifecycle Management\"]\ndirection TB\nDriftMonitoring([\"Drift Monitoring\"])\nend\nInfrastructure o==o Experimentation o==o ModelDevelopment o==o ModelPackaging o==o Deployment o==o InferenceAndFeedback\n```  \nBuilding Blocks  \nThis solution comprises of multiple components. They are:  \n{% if extra.ring == 'internal' %}  \nEdge Infrastructure provisioning  \nThe first step in an industrial scenario operation with Azure services is to provision the Edge device to the Azure Cloud. In this document, we will describe the pattern to provision the Edge device.  \nAfter provisioning, the Edge device on Azure will be able to take full advantages of Azure services for the ML Edge inference scenario, robotic control and configuration scenario, and more.  \nThere are two main ways to deploy ML workloads to the Edge:  \nAzure Arc (the device is configured as an Arc-enabled kubernetes cluster, and connects to a cloud Azure Kubernetes Service - AKS)  \nIoT Edge (the device is configured as an IoT Edge device, and connects to IoT Hub)  \nThe documents below demonstrate provisioning of an Edge device:  \nFor Azure Arc provisioning, refer to Edge Device Provisioning - Azure Arc  \nFor IoT Edge provisioning, refer to Edge Device Provisioning - IoT Edge  \nSomething to consider is scalability of deployment with not only the models but also the rest of the components to interact with the factory floor, the cameras. The Computer Vision Toolkit contains patterns to help solving those challenges.  \n{% endif %}  \n{% if extra.ring == 'internal' %}  \nML Development and Operations  \nOne of the important steps in Edge journey is to develop and generate a ML Model using well-defined MLOps process and principles. This process includes steps for processing data, feature transformation, training, testing, evaluation, and registration of ML Models. There are set of MLOps templates available for Azure Machine Learning and Databricks.  \nRefer to the ML Development and Operations templates page for details on this topic.\n{% endif %}  \nPackaging ML Model into reusable Docker Image  \nIt is a best practice to deploy an ML Model to an Edge device as a Docker container. This Docker image contains the ML Model file, the score code and all of its software dependencies. The Docker image can then be used to run a container on almost any compute target, for example Azure Kubernetes Services(AKS), Azure Container Instance (ACI), or on the Edge. The process of this Docker image generation can be manual, scripted, or automated using an orchestrator like Azure DevOps.  \nThe Azure DevOps pipeline automating the process of Docker image generation has the following steps:  \nDownload the trained ML Model and scoring file from the Azure Machine Learning workspace or Azure Storage account.  \nGenerate a well configured Docker image with the ML Model and its dependencies.  \nRun a container from the newly generated image.  \nTest the ML Model running in container by sending request with sample data.  \nValidate the results of the inference result.  \nTag the Docker image.  \nPush the Docker image to the ACR.  \nThis asset has following features:  \nAbility to deal with both GPU and CPU based images on an out-of-box Ubuntu based build agent.  \nFlexible to include steps related to security scanning of the newly generated image.  \nAuto-load the ML Model in event of restart of Edge device.  \nRefer to the Model Processing pipeline page for details on this topic.  \n{% if extra.ring == 'internal' %}  \nDeployment and Smoke Test ML Model on Edge device  \nAn ML Model deployed as a module on an Edge device should be quickly tested for its availability and validation should take place to ensure that it is able to infer the input to generate output. This is especially necessary on a production environment. Generally, the Edge devices do not have internet connectivity. In case, they have internet connectivity, the connectivity is outbound and almost all inbound requests are blocked.  \nIn such scenarios where initiating requests to Edge devices are constrained or limited from external sources, it is not practical to test the availability of an ML Model deployed on an Edge device directly from external sources. A solution for such cases is to build another  module whose purpose is to test the ML Model and its inferencing. This Edge module (responsible for testing the ML Model) should also be deployed on the Edge device and it can send the results of the test to the caller (Azure DevOps pipeline) or other services like Azure Event Hub for further downstream consumption.  \nThis asset has the following features:  \nAbility to invoke the test on-demand from the Azure DevOps pipeline.  \nAbility to execute tests with sample data.  \nTests implemented as a separate Edge module.  \nLightweight with minimal footprint and load on the Edge device.  \nRefer to the Smoke Testing ML Models on Edge devices page for details on this topic.\n{% endif %}  \nAsset location  \nThe asset can be found at MLOps accelerator for edge  \nDeployment to Edge  \n{% if extra.ring == 'public' %}  \nSmoke Testing Model Availability on Edge Device  \n{% endif %}",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\serving-models-on-edge\\index.md"
    },
    {
        "chunkId": "chunk302_0",
        "chunkContent": "Custom metrics creation for IoT Edge module  \nIntroduction  \nAfter creating an Edge ML inference module or a general IoT Edge module, adding a fine-grained instrumentation is often necessary in order to monitor our custom module beyond the built-in metrics.  \nIn this document, we will describe the pattern to instrument an Edge module by using custom metrics.  \nAfter these steps, we will be able to monitor and visualize our own metrics.  \nThe metrics collector module, provided by Microsoft, will also be introduced as it is a convenient and standard method to send metrics to Azure Monitor or Azure IoT Hub.  \nIoT Edge module instrumentation Overview  \nThere are 3 stages involved in the process.  \nStage 1: Deploy and configure the metrics collector module.  \nStage 2: Choose and create the appropriate metric for your use case.  \nStage 3: Publish your metric.  \nStage 1 - Deploy and configure the metrics collector module  \nIn this stage, we will deploy the metrics collector module and configure it to collect and send our metrics to Azure Monitor.  \nThis module can be added to your deployment either by using the publicly available Docker image at http://mcr.microsoft.com/azureiotedge-metrics-collector or via the IoT Edge Module Marketplace.  \nTransport Options  \nThere are two options for sending metrics to the Cloud:  \nOption 1: Sending to Log Analytics  \nThe metrics will be ingested into the specified Log Analytics workspace using a fixed, native table called InsightsMetrics.  \nRequirements:  \nAccess to the Log Analytics workspace on outbound port 443.  \nThe Log Analytics workspace ID and key must be specified as part of the module configuration.  \nEnable in restricted network access scenarios  \nTo enable this option in restricted networks, allow outbound access to the following URLs:  \nhttps://<LOG_ANALYTICS_WORKSPACE_ID>.ods.opinsights.azure.com/*  \nhttps://<LOG_ANALYTICS_WORKSPACE_ID>.oms.opinsights.azure.com/*  \nOption 2: Sending to IoT Hub  \nWith this option, the metrics are sent as UTF-8 encoded JSON device-to-cloud messages via the edgeHub module.  \nUse cases:  \nMonitoring of locked-down IoT Edge devices that are allowed external access only via the IoT Hub endpoint.  \nMonitoring of child IoT Edge devices in a nested configuration where child devices can only access their parent device.  \nCloud Workflow set up  \nWhen metrics are routed via IoT Hub, a one-time cloud workflow needs to be set up.\nThis workflow processes the messages arriving from the metrics collector module and sends them to the Log Analytics workspace, enabling the curated visualizations and alerts functionality for the metrics arriving through this path.  \nTo send metrics as device-to-cloud (D2C) messages via the edgeHub module, please set the UploadTarget environment variable to IoTMessage.  \nAdd a route from your module's edgeHub to IoT Hub: FROM /messages/modules/[metrics-collector-module-name]/* INTO $upstream.  \nSet up the Cloud Workflow.  \nCurrently, Option 1 is the preferred method, as it requires minimal setup.  \nFor more information, please refer to the metrics collector module's architecture section of the documentation.  \nConfiguration  \nIn this section, we will focus on the configuration variables relevant for our scenario.  \nEnvironment variable name Description ResourceId Resource ID of the IoT hub that the device communicates with. For more information, see the Resource ID section of the documentation. Required Default value: none UploadTarget Controls whether metrics are sent directly to Azure Monitor over HTTPS or to IoT Hub as D2C messages. For more information, see upload target . Can be either AzureMonitor or IoTMessage Not required Default value: AzureMonitor LogAnalyticsWorkspaceId Log Analytics workspace ID . Required only if UploadTarget is AzureMonitor Default value: none LogAnalyticsSharedKey Log Analytics workspace key . Required only if UploadTarget is AzureMonitor Default value: none MetricsEndpointsCSV Comma-separated list of endpoints to collect Prometheus metrics from. All module endpoints to collect metrics from must appear in this list. Example: <http://edgeAgent:9600/metrics> , <http://edgeHub:9600/metrics> , <http://EdgeProcessor:9600/metrics> If our custom module is called EdgeProcessor , the metrics will be exposed at http://EdgeProcessor:9600/metrics . Required Default value: <http://edgeHub:9600/metrics> , <http://edgeAgent:9600/metrics>  \nPlease visit the metrics collector configuration page for an exhaustive list of the configuration variables.  \nStage 2 - Selection and creation of the custom metric  \nAll metrics sent to the metrics collector should use the Prometheus data model.\nTo create our metrics, we leverage the Prometheus Client Library.  \nMetric Types  \nThere are four metric types:  \nCounter: a cumulative metric that represents a single monotonically increasing counter whose value can only increase or be reset to zero on restart.  \nGauge: represents a single numerical value that can arbitrarily go up and down.  \nHistogram: samples observations and counts them in configurable buckets. It also provides a sum of all observed values.  \nSummary: as the histogram it, samples observations. While it also provides a total count of observations and a sum of all observed values, it calculates configurable quantiles over a sliding time window.  \nCode samples for each implementation are available here.\nMore information about how to instrument with Prometheus library here.  \nLabels  \nLabels allow the grouping of related time series.  \nBest practices of metric design is to minimize the number of different label values.  \nFor example:  \nHTTP request method is a good choice for labeling - there are a limited number of values.  \nURL is a bad choice for labeling - it has many possible values and would lead to significant data processing inefficiency.  \nExemplars for distributed tracing  \nDistributed tracing is a diagnostic technique that helps engineers localize failures and performance issues within applications, especially those that may be distributed across multiple machines or processes.  \nBy attaching related trace IDs to metrics, we can can cross-reference traces to understand the origin of a metric's value.  \nPlease find implementation examples of exemplars here.  \nBest practices  \nThe Metrics class is the main entry point to the API of the Prometheus library.  \nThe most common practice in C# code is to have a static readonly field for each metric that you wish to export from a given class.  \nStage 3 - Publishing your metric  \nMetrics without labels are published immediately after the Metrics.CreateX() call.\nThe ones that use labels are published when you provide the label values for the first time.  \nIt is possible to bypass the publishing of the metric's initial value by setting the SuppressInitialValue configuration variable to true during the metric creation.  \nTo mark a metric as ready for publishing, use the .Publish() method.\nTo hide a metric temporarily, use the .Unpublish() method. It's value will still be saved in memory.  \nPlease refer to the code samples of IoT Edge modules exposing Prometheus metrics for implementation details.  \nFurther Reading  \nCollect and transport metrics  \nSamples of IoT Edge modules exposing Prometheus metrics  \nAdd custom metrics  \nPrometheus client library  \nPrometheus - Best Practices  \nTutorial: Monitor IoT Edge devices  \nObservability on Edge - PlatformOps  \n.NET distributed tracing concepts",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\serving-models-on-edge\\iot-edge-custom-metrics.md"
    },
    {
        "chunkId": "chunk303_0",
        "chunkContent": "Edge Device Provisioning - IoT Edge  \nIntroduction  \nThe first step in an industrial IoT scenario operation with Azure IoT services is to provision the Edge device to the Azure Cloud IoT platform. In this document, we will describe the pattern to provision the Edge device to Azure IoT Hub in an automated manner. After provisioning, Edge device on Azure IoT platform will be able to take full advantages of Azure services for an ML edge inference scenario, robotic configuration and control scenario, etc.  \nWe have seen the need to automate the process of IoT Edge installation on Edge devices, including provisioning of devices to IoT Hub, in a time-sensitive fashion within few minutes.  \nThis provisioning pattern is to use an Azure Resource Manager (ARM) template to create and configure an Azure VM as an simulated Edge device, prepare configuration files and packages for the Azure VM, provision the Edge device to Azure IoT Hub as an IoT Edge device.  \nProvisioning Solution Overview  \nThere are 2 stages involved in the process.  \nStage 1: Create an Ubuntu OS based VM as a simulated Edge device and prepare the configuration files.  \nStage 2: Bootstrap and provision the Edge device to IoT Hub and deploy the IoT Edge modules.  \nStage 1 includes the step to create an Azure Virtual Machine(VM) to simulate an Edge device for a proof-of-concept solution, otherwise a physical device could be used directly. Stage 2 bootstraps the Edge device with the configuration files from Stage 1, and provision it to Azure Cloud.  \nThe image below illustrates the stage 2 provisioning architecture.  \nStage 1 - Prepare the Edge device and configuration files  \nIn this stage the operator will need to create an Azure VM as a simulated Edge device if a physical device is not available. The operator will prepare the environment with necessary files for the next stage of  provisioning. The process of VM creation is automated.  \nThe steps are listed below:  \nStep 1: Create Resources  \nCreate an Azure subscription. Try a free Azure subscription.  \nCreate an Azure IoT Hub Instance. Understand more on IoT Hub here.  \nEnsure that the host machine that runs the script has a Linux OS and internet access.  \nStep 2: Secrets and Files Preparation  \nCreate an SSH key. Use your own ssh key or create an SSH key.  \nPrepare an X.509 certificate and upload to Azure Key Vault.  \nPrepare deployment manifest files.  \nMake sure OpenSSL is installed in the host machine that runs the scripts for Step 3-1.  \nStep 3-1: Create an Edge Device VM (if no physical device exists)  \nClone the code sample repository to the local host machine, and follow README-create-edge-VM.md in the provisioning-sample-stg1-create-edge-device folder.The code sample for the implementation details can be found here.  \nThe output configuration file will be generated and contains the private IP and FQDN of the created VM, which is the required input for the provisioning process at the next stage.\njson\n{\n\"edge_private_IP\": < edge-device-private-IP >,\n\"edge_FQDN\": < edge-FQDN >\n}  \nStep 3-2: Prepare the physical device (skip step 3-1)  \nManually prepare the output configuration file based on the template in the code sample from step 3-1. Because if we are using a physical device instead of VM, the output configuration file needs to be prepared manually, as it won't be generated automatically by script as in step 3-1.  \nThe manually prepared output configure file should contain Edge device's private IP and FQDN of the physical device.  \nStep 4: Check the configuration files  \nCheck that the folder contents in file folder \"./preparation-files\" includes the following files.  \n.ssh folder which contains SSH key  \n.env-edge.sh for environment configuration  \ndeployment-manifest.json  \nedgeVMfile.json output configuration file generated from Step 3.  \nThe folder will be used as the input for next stage provisioning.  \nStage 2 - Provision the Edge Device to the Cloud  \nStage 2 of the solution is to configure an Edge device to be an IoT Edge device, authenticate X.509 certificates, provision it to IoT Hub with the software tool iotedge-config tool and the manually prepared output configuration file, which contains the Edge device's private IP and FQDN.  \nIf a physical edge machine is used as an IoT Edge device, the operator needs to manually prepare the output files in the \"preparation-files\" folder by providing the IP address and FQDN in the configuration file, as described in the stage 1 Step 3-2 of the solution.  \nThe provisioning solution includes the following actions:  \nHost machine environment initialization and Azure account login.  \nDownload x.509 certificates from Azure Key Vault.  \nLoad private IP and FQDN of the Edge device.  \nConfigure Edge device with Edge Config Tool, including  provisioning configurations, CA root certificates, deployment manifest, etc.  \nInstall IoT Edge Runtime on the Edge device.  \nConfigure X.509 root certificates on the IoT Edge device.  \nProvision an X.509 certificate authenticated IoT Edge device to IoT Hub.  \nDeploy IoT Edge modules.  \nThe procedures are showed as below:  \nStep 1: Preparation  \nAccess to the physical device or the Edge Device VM with SSH key from preparation files.  \nClone the code sample repository here to local host machine.  \nPlace \"preparation-files\" folder into the script root directory.  \nStep 2: Run Script for Provisioning  \nFor the full features of the script, please refer to the code sample repo in step 1.  \nStep 3: Check  \nCheck the IoT Hub portal. You should have an IoT Edge Device VM or physical device in your IoT Hub running and reporting a connected status of IoT Edge modules.  \nProvisioning Code Sample  \nThe code sample for the provisioning solution can be found here.  \nFurther Reading  \nAzure IoT Hub  \nAzure Key Vault basic concepts  \nAzure Container Registry  \nUnderstand the Azure IoT Edge runtime  \nAzure ARM Templates  \niotedge-config tool for auto-provisioning  \nManually create an IoT Edge device  \nManually create and provision an IoT Edge device on Linux using X.509 certificates",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\serving-models-on-edge\\iot-edge-provisioning.md"
    },
    {
        "chunkId": "chunk304_0",
        "chunkContent": "Model Processing Pipeline  \nIntroduction  \nAfter a model is generated, the next step in process is to deploy it to an environment. The environment could be any environment including production environment.  \nWhile it is possible to deploy newly generated model directly to an environment, it has drawbacks and is prone to errors. There can be a several types of possible errors including missing dependencies, scoring endpoints not available, model could not be loaded etc.  \nIt is important to take the model through a pipeline that can execute process with important steps like baking the model within docker images, test for correctness and security and eventually publish to registry for easy consumption and deployment to different target environments.  \nAfter model generation exercise is complete, model artifacts can be stored at any storage location, the primary among them are Azure Storage Account and Azure machine learning workspace.  \nThe asset provides two pipelines:  \nDownloading the model artifacts from a storage account  \nDownloading the model from the Azure machine learning workspace.  \nIt is also possible to extend the existing asset for model storage by adding additional storage locations. This asset comes with a sample model and related files for both the pipelines.  \nThe pipeline generates a docker image with all model artifacts baked into it, ready for deployment by uploading the image to a container registry.  \nAzure DevOps - Model Processing pipeline  \nThe pipeline is responsible for executing the following steps on a build server:  \nDownload the model artifacts from storage account or Azure Machine Learning workspace.  \nCreate a new Docker image with a copy of the model, its artifacts and related configuration.  \nRun a container from newly created Docker image.  \nExecute test on newly running container by sending http requests to the model using its scoring file and validating the same.  \nOptionally security scan the image.  \nUpload the image to container registry. It could be a Docker registry, Azure Container Registry, JFROG artifactory or any other that implements the docker standards.  \nSalient features  \nThe entire pipeline runs as a single unit with a model as input and a deployable docker image uploaded to a container registry as an output.  \nFlexibility to incorporate additional capabilities like security scans for images, intense testing capabilities and validation before uploading the image to registry.  \nFail the pipeline is any of the step fails.  \nAbility to bake model into docker images and test them with both CPU as well as GPU based models.  \nAbility to consume new docker image from a variety of deployment targets including - AKS, EDGE, ACI, Azure App Server, Azure functions and even On-Premises environments including Azure stack.  \nImplementation of fail fast, fail early principle related to models thereby reducing cost of technical debt\u200b.  \nImproves overall quality, speed and reliability in deployment process while reducing cost.  \nQuick Model verification and image generation capability\u200b.  \nReduce operational cost for models on edge\u200b.  \nThe repo Model Processing pipeline for the asset consists of two folders:  \nmodel-aml: This pipeline should be used for downloading model artifacts from an Azure storage account.  \nmodel-storage: This pipeline should be used for downloading model artifacts from Azure machine learning workspace.  \nApplicability  \nIn general, this asset can be used in any engagement/project that deals with machine learning models and operationalizing them. The models can be generated using supervised, un-supervised, or deep-learning learning and the pipeline should be able to build a new docker image after proper validation and making it ready for deployment to different type of target platforms and services.  \nEdge deployment involves building IoT modules with references to docker images from Image repositories and this asset can be used to implement best practices for model processing before deploying and using it on a target as a solution.  \nThe pipeline in its current form expects the model and its artifacts to be available in a particular folder structure. However, this can be changed by modifying the pipeline code.  \nPrerequisites  \nAzure subscription with contributor rights.  \nAzure resources \u2013 Service Principal using App Registration for access to subscription from Azure DevOps pipeline, ACR with admin login enabled, Key vault for storing secrets, storage account or Machine learning workspace for storing the model artifacts, Azure DevOps project.  \nService connection in Azure DevOps configured with Service Principal having access to Azure resources and resource group.  \nA repo to host the code.  \nA local environment with visual studio code to update the code.  \nThe pipeline expects the model artifacts - model and scoring file along with test data available at the storage.  \nReference  \nModel Processing pipeline  \nAzure Machine Learning  \nAzure DevOps pipelines  \nDockerfile  \nAzure Machine learning CLI V2  \nAzure AD Service Principal",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\serving-models-on-edge\\model-pipeline.md"
    },
    {
        "chunkId": "chunk305_0",
        "chunkContent": "Smoke Testing ML Models on Edge Devices  \nIntroduction  \nIoT Edge deployments to Ubuntu virtual machines should be validated before opening traffic to the users on production environment. ML Models deployed on Edge devices should be validated as well alongside deployment validations. Edge devices generally have outbound internet connectivity, and it is difficult to initiate requests from external sources to the device. We want to quickly test the deployed ML Model on the Edge device to check if it is available to serve requests to the users. Smoke Testing the deployed ML Model on the Edge device is a quick way to test the availability of the model on an Edge device.  \nIn the context of deploying an ML Model on Edge devices, Smoke Testing is a quick test to check if the Model is available for inferencing and can respond with appropriate results to the caller. Smoke Testing routines can be implemented as Edge modules and deployed alongside the ML Models on the same Edge device. This strategy helps in testing multiple ML Models together and introduces process isolation for test execution.  \nWhy do we need Smoke Testing  \nSmoke Testing is primarily done for the following reasons:  \nTo be able to perform quick and basic tests to ensure the service is not failing. It is not a comprehensive test.  \nTo be able to identify issues before opening traffic to the users.  \nTo have higher confidence and predictability in Edge deployment scenarios.  \nSmoke Testing Strategy  \nWe need a Smoke Testing strategy for ML Models deployed on the Edge devices that have the following features:  \nTests can be initiated from external sources ideally from an automated pipeline like Azure DevOps.  \nTests can be implemented as a separate Edge module.  \nTest results can be reported back to the caller.  \nTests can be executed with sample data.  \nSmoke Testing Steps  \nImplementing Smoke Testing for ML Models on Edge devices includes:  \nBase infrastructure environment comprising of:  \nAzure Container Registry.  \nAzure IoT Hub with a registered Edge device.  \nUbuntu based virtual machine as Edge device configured with Edge runtime and Edge related binaries.  \nAzure DevOps project to generate deployment configuration, build Docker images for Smoke Testing module and deploy it as part of the solution to Edge device.  \nAuthoring Smoke Test cases as IoT Edge modules.  \nDeployment of Smoke Testing IoT Edge module to the Edge device.  \nInitiating Tests from Azure DevOps pipelines.  \nRefer to the IoT Smoke Testing asset for sample implementation alongside its usage details.  \nReference  \nSmoke Testing ML models on Edge devices  \nAzure IoT Hub  \nAzure IoT Edge documentation  \nAzure Container Registry  \nAzure Machine learning  \nAzure DevOps pipelines  \nDockerfile  \nAzure Machine learning CLI V2  \nAzure IoT CLI  \nAzure AD Service Principal",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\serving-models-on-edge\\smoke-testing-on-edge.md"
    },
    {
        "chunkId": "chunk306_0",
        "chunkContent": "GenICam implementation with .NET  \nThe supported cameras are Gen\\<i>Cam interface cameras because it is what is used and sold in factories. With this interface different kind of cameras can be used using the same interface. This also makes it easy to change cameras as long as they support Gen\\<i>Cam.  \nThe proposed solution consists of one \"Camera Controller Module\" and a GeniCam enabled Camera connected to the CVT using GigE. As you will read in this document those camera have quite some challenges and the fact they are using UDP and optimize to stream as much content as possible makes it challenging in terms of networking.  \nWhy GigE?  \nFor the connection we propose to use cameras with the GigE Protocol which connect to the CVT via Ethernet. Advantage is that ethernet is a common interface and the GigE Protocol provides very low latency and high bandwidth.  \nThis document gives more details about the implementation for .NET. In short, we can leverage the work done to support Gige Vision cameras which are GenICam based. The code is a mix of MIT and Apache license. Part of the code can be fully reused and new code has to be added. Quite a lot of optimization and bug fixes has been done.  \nAssumptions & Guardrails  \nWe assume as that all camera implement GenICam interfaces allowing to take pictures, adjust some of the elements.  \nWe assume that the cameras are on the same network as the VM/Edge/Kubernetes and the UDP protocol is not filtered and freely accessible as the GenICam protocol is UDP based.  \nDesign  \nWe propose to reuse and leverage the code from Gige Vision cameras. While a nuget is available for basic GenICam implementation, the recommendation is not to use directly the nuget as quite some modifications has been done.  \nThe license associated to the GenICam part is MIT (according to the nuget), while the license associated to the GigeVision implementation is an Apache v2. Both licenses are compatible with the work we will do. The Apache license will have to be attached with the Apache license for this specific sub directory of the project. Contributions back to main repository have been done but they are limited as the code has been expurgated of UWP elements.  \nTechnology  \nSo the technology which will be used to connect, gather the metadata, read, write the register and run the commands on the camera will be purely .NET managed code. We are using Aravis implementation for the simulator as it does offer a perfect camera simulator supporting all the basic GenICam feature needed and supposed to be supported by all the cameras which will be used with CVT.  \nAravis is a glib/gobject based library for video acquisition using Genicam cameras. It currently implements the gigabit ethernet and USB3 protocols used by industrial cameras. It also provides a basic ethernet camera simulator and a simple video viewer. More information about Aravis can be found here. Aravis GitHub Repo. Using Aravis in a container directly and a C++ implementation would have limitations because the Aravis implementation uses statics and does not allow to connect with multiple cameras at the same time.  \nThe last option would be to use vendors specific SDK, that is a good option if you are vendor locked, now this forces to use C++ and for a lot of them, there is as well the limitation for the streaming which does not allow to connect to multiple cameras at the same time.  \nNon-Functional Requirements  \nDepending on camera type, some may support more advance protocols. So far the available implementation supports all what is needed including setting specific parameters like exposure, region of interest, etc.  \nNetwork infrastructure must not filter the UDP protocol on port 3956 as it's the one used by GenICam protocol.  \nDependencies  \nThe main dependency is with the camera simulator that runs using the Aravis tools and currently under development.  \nNote that to run properly the camera simulator, you'll have to run the container with the network host and the UDP port 3956 properly mapped:  \n```shell\n\nhere to run it in interactive mode for debug purpose\n\ndocker run -it --network=host -p 3956:3956/udp containername\n\nand now you'll run the simulator -d all allow to get all debug elements\n\n./aravis/build/src/arv-fake-gv-camera-0.8 -i eth0 -d all\n```  \nWhile debugging and accessing the camera, you will have output like this:  \ntext\n[17:33:06.399] \ud83c\udd78 device> GVSP address = 10.10.10.10\n[17:33:06.399] \ud83c\udd78 device> GVCP address = 10.10.10.10:3956\n[17:33:06.399] \ud83c\udd78 device> Global discovery address = 10.10.10.10:3956\n[17:33:06.399] \ud83c\udd78 device> Subnet discovery address = 10.10.10.10:3956\n[17:33:06.399] \ud83c\udd78 device> Listening to 3 sockets\n[17:33:56.574] \ud83c\udd78 device> [GvFakeCamera::handle_control_packet] Read memory command 512 (512)\n[17:33:56.574] \ud83c\udd78 device> [GvFakeCamera::thread] Control packet received\n[17:33:56.577] \ud83c\udd78 device> [GvFakeCamera::handle_control_packet] Read memory command 65536 (536)\n[17:33:56.577] \ud83c\udd78 device> [GvFakeCamera::thread] Control packet received\n[17:33:56.578] \ud83c\udd78 device> [GvFakeCamera::handle_control_packet] Read memory command 66072 (536)\n[17:33:56.578] \ud83c\udd78 device> [GvFakeCamera::thread] Control packet received\n[17:33:56.579] \ud83c\udd78 device> [GvFakeCamera::handle_control_packet] Read memory command 66608 (536)\n[17:33:56.579] \ud83c\udd78 device> [GvFakeCamera::thread] Control packet received\n[17:33:59.492] \ud83c\udd78 device> [GvFakeCamera::handle_control_packet] Write register command 2360 -> 10000\n[17:33:59.493] \ud83c\udd78 device> [GvFakeCamera::thread] Control packet received\n[17:33:59.495] \ud83c\udd78 device> [GvFakeCamera::handle_control_packet] Read register command 2560 -> 2\n[17:33:59.495] \ud83c\udd78 device> [GvFakeCamera::thread] Control packet received\n[17:34:36.751] \ud83c\udd86 device> [GvFakeCamera::handle_control_packet] Heartbeat timeout  \nCamera requirements regarding networking setup  \nThis section is about bringing clarity on network requirements for GigeVision cameras. In short, the camera needs to be in the same network without any firewall or other filtering element between the VM or physical machine running our Camera Interface and itself. More details in this document.  \nBased on the fact that GigeVision and GenICam protocols used to stream pictures and control the camera are high speed UDP based protocols, it is important to avoid any element in the network that will slow down or analyzes this traffic.  \nAlso, the GenICam protocol forces to use the same port on all camera 3956 in UDP to control and pilot the camera settings. The mechanism used for the answer is to generate on the requester side a random port. This mechanism is important especially when accessing multiple cameras at the same time. This is illustrated in the network capture below:  \nThe streaming mechanism with the GigeVision protocol is different as the requester can properly setup a streaming port. So a range of ports can be selected for this manner.  \nSpeed and large traffic is all what is important when GigeVision protocol has been designed. As an example, a Basler camera has a resolution of 2448x2440 and in 8 bits color, the raw image transferred will be: 2448 - 2440 - 8 - 3= 143 354 880 bytes (this is an example, size can vary). Count overhead of about 36 bytes per packet during transfer and to be able to read 1 image you'll stream about 3 of them, you'll quickly saturate a firewall or any element opening and interpreting the packets to route them.  \nSo it's *fundamental- to have the cameras in the same network as the VM or the machine running CVT. In the case of physical machine, it is even recommended to have a specific network dedicated to those cameras, see operationalization section below.  \nOperationalization  \nWhile installing the camera, the same network as the VM/Edge/Kubernetes or the machine has to be used. It is important that the VM will be granted access to the Azure cloud resources. The camera has no need to get any internet access. The key point is to have both VM and associated cameras on the same network without any filtering element between them.  \nIn the case of a VM, having all elements in the DMZ is one of the possible scenario  \nIn the case of a physical machine running the same configuration as in a VM (not standalone but here, a connected scenario like in a VM but on a physical machine), the physical machine will have to have access as well to the Azure cloud resources. It can be in the DMZ or in another network but the could connectivity is a must as well. We expect those scenarios to be very consuming in terms of bandwidth as the main interest is to have a stronger machine being able to compute more elements on a dedicated use case.\nDMZ is a possible scenario as well for cameras and the physical machine but the network will most likely be used a lot by the cameras.\nAnother possibility is to have the physical machine with 2 network card, one having access to the Azure cloud resources, DMZ network is a possibility. And as in the case of the physical machine, the camera may be close to the machine, it make sense to isolate them in a dedicated network connected to the second network card of the dedicated machine.  \nIn the case of CVT Standalone, any network, including fully isolated can be used. That say it may be handy to have the standalone version accessing the Azure cloud resources as well.  \nIn all cases during the installation of the camera, it is strongly recommended to use the tools provided by the different vendors. Each vendor has specific settings and will allow a proper configuration. For this, a direct connection between the camera and a laptop is largely enough. Once ready, all camera support a change of their IP address directly from those tools. The camera can then be unplugged from the laptop and plug into the proper destination network.  \nRisks & Mitigation  \nThe main risks with those GigeVision cameras is a network traffic saturation. They can output 30 raw images per seconds taking 1G of network bandwidth. This is mitigated by the fact we only take 1 image. Now to get 1 image, on some cameras, this requires to gather 2 to 3 images as they don't support single picture mode.  \nSo network separation needs to be done in a good way to allow a set of cameras and other equipments like a VM to properly operate together. There may be need of more network separation or having few dedicated hardware where camera do connect directly and acting like a gateway. This will technically look like the current Camera Interface (the Image Taker) to be deployed on a dedicated hardware edge. The API would be exposed as TCP exposing compressed images to its consumers. This will require to add a proper authentication on the current API. The rest of the tool chain, so the Orchestrator would have to add also this authentication to support this scenario. No other changes would be required.  \nLearnings and important elements regarding the GigeVision/GenICam cameras  \nThis section aims to collect learnings and important elements regarding the GigeVision/GenICam cameras used in CVT. This is a collection of dos and don'ts and processes to improve the reliability of the cameras used in the production line.  \nDo  \nAs explained in the CVT deployment typology, cameras *must- be connected in the same network as the IPC/VM they are attached to.  \nCameras *must- be connected directly to the IPC/VM without any proxy or firewall opening and checking the packets.  \nCameras streaming ports *must- be different for each camera. A large enough range of ports should be available for this. It is technically possible to use any port for this purpose.  \nFor maintenance, make sure you disconnect the cameras from the network first and *then- connect to it with the vendor tools.  \nDon't  \nDuring the camera setup, if you are using a configuration application from the camera vendor, *DO NOT- configure them in the network where they are supposed to be attached. If you do this, it will stream and saturate the network during the configuration.  \nFirst setup the camera in an independent network, not connected to the final network.  \nOnce ready, change the camera IP address for the final IP address.  \n*Never update the camera when it is connected to the network. See the point on dofor the maintenance. The camera has to be disconnected first from the network, then connected directly to a PC and finally, the update process can be done.  \nSelf fixing solution in code  \nCameras like any other elements can become unstable. This can be the case when multiple network errors occur during its usage, when a connection is not properly closed. If the camera starts to timeout on a regular basis (it depends on the camera), but lets say 3 times in a row, you should reboot the camera.  \nTo reboot a camera, for most cameras, a command is available, usually named DeviceReset, the reset value is 1. So when you use this command, the camera will proceed to a reboot. This is taking a couple of milliseconds. Refer to the vendor manual to understand long much you need to wait. Once rebooted, most cameras need a warmup phase. In short, it consist of taking multiple pictures (this depends of the vendor and camera type) before being able to have a proper image. The typical time found so far is to take 20 to 30 pictures which is the equivalent of a few seconds. This is mainly due to the auto brightness that is setup in most cameras.  \nAdjusting settings on the fly  \nMost cameras do support to adjust some settings on the fly. It can be the resolution, the brightness, the color scheme, the image format. While some of them won't need a warm up (changing the pixel format or the resolution for example on all cameras tested so far), some will require as well a warmup like when it is rebooted. The only way to understand this is to test for each cameras and adjust this.  \nNote that it is not really recommended to change those settings on the fly related to the sensitivity of controlling those cameras. It seems more efficient to have a second camera with different settings if needed.  \nNetwork collisions and camera streams  \nThe protocol used to stream the pictures is using UDP and is optimize to consume *all- the available bandwidth and output as many images as possible. So it is clearly not recommended to let the cameras stream but rather take individual pictures when supported by the cameras and stop the streaming as soon as the number of pictures required is achieved.  \nThis does means as well that the number of cameras that can be connected in the same network and taking the pictures should be properly orchestrated. With a small number of cameras, it seems reasonable not to have a deep control but with few of those, you may have to orchestrate taking the pictures with a small delay between each of them to avoid any incident in the network.  \nThere is as well a way for most cameras to setup a number of frames per seconds. By setting a smaller number of images per second, we can decrease the network spamming. This will result in a better network manageability. Now, most camera brands have a different way to set this up. so the code will have to handle the brand used. This will have the inconvenient to make receiving an image in case the image is partial and need another one a bit slower than with the maximum number of images per second. It seems reasonable to set a value around 5 images per seconds.  \nDetecting potential socket issues  \nAnother element to take into consideration is the detection of failure in the sockets. There is an existing instrumentation about failures based on events. An implementation should take this into consideration to reset/clear/clean sockets and even potentially restart the container itself.  \nReferences  \nBasler ACA2440 20GC  \nBasler GigeVision features available",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\serving-models-on-edge\\computer-vision-on-edge\\camera-interface.md"
    },
    {
        "chunkId": "chunk307_0",
        "chunkContent": "CVT Bundle Simulator  \nWe need a mechanism to:  \nRun and test bundles in a runtime  \nFetch metadata  \nMeasure and store CPU/RAM metrics of bundle  \nGoals / In-Scope  \nDesign of Simulator service  \nTechnology choice  \nSecurity and permission constraints of simulator  \nRelated Designs  \nCVT High level architecture  \nCVT Bundle Format  \nAssumptions & Guardrails  \nThe bundle is a containerized application with REST API. This application can be traditional computer vision or Machine Learning based.  \nBundles are stored in the Global Azure Container Registry and accessible for all slots.  \nThe Simulator should be deployed on platform Slot level.  \nProposed Design / Suggested Approach  \nThe main parts of the bundle simulation approach are:  \nCVT Simulator Orchestrator  \nSimulated Bundle Application  \nCVT Simulator Orchestrator  \nCVT Simulator Orchestrator is an API application deployed\nto an AKS on Slot level. This application can deploy any provided bundle as\nSimulated Bundle Applications by exposing an HTTP POST endpoint.  \nFlow and interaction between Client Application, Simulator Orchestrator, Simulation Environment and Simulated Bundles:  \nClient Application can request Simulator Orchestrator to start the simulation of a provided bundle from Bundle Registry.  \nSimulator Orchestrator will deploy bundle with Helper application to Simulation Environment as Simulated Bundle Application.  \nWhen deployed, Simulated Bundle Application will do testing of the bundle and will report results back to Simulator Orchestrator.\nIt can be done via HTTP Requests, ServiceBus Messages or with direct access to the database. For that, Managed Identity might be added to the Helper Application.  \nSimulator Orchestrator calls the Simulation Environment to get CPU/Memory Metrics of the deployed bundle.\nAfter the simulation is done\u00b9, results and metrics are collected and saved, Simulator Orchestrator will stop and delete Simulated Bundle Application.  \nClient Applications can get results from Simulator Orchestrator.  \n\u00b9 all API endpoints will have specified timeouts, to ensure the simulations finish.  \nSimulated Bundle Application  \nFor running bundles on a simulated environment we will use a sidecar pattern, i.e. multi-container application with two applications inside:  \nBundle application  \nHelper application to test bundle API  \nTwo applications are running together and can communicate via 'localhost'.  \nThe diagram below shows this flow:  \nKubernetes approach  \nSimulator Orchestrator is an application running on AKS cluster and this application can connect to Kubernetes API.\nSimulation Environment is AKS Cluster.\nSimulated Bundle Application is a multi-container pod.  \nAKS Security  \nThe recommended approach to connect to Kubernetes API from within a pod is using official Client Libraries.\nKubernetes C# Client allows to connect to Kubernetes API from within a pod using code:  \ncsharp\nvar client = new Kubernetes(KubernetesClientConfiguration.InClusterConfig());  \nA Secure connection will be established using the attached Service Account to the pod on which Simulator Orchestrator is running.\nA separate Service Account should be created to restrict the possibilities of Kubernetes Client.\nFor example, the next Service Account is limited to only any Create pods in a specified namespace, Delete listed pods and Get Metrics of pods:  \n```yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: cvt-bundle-simulator-service-account\nnamespace: namespace-aa\nautomountServiceAccountToken: false\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\nnamespace: namespace-aa\nname: cvt-bundle-simulator-role\nrules:\n- apiGroups: [\"metrics.k8s.io\"]\nresources: [\"pods\"]\nverbs: [\"get\"]\n- apiGroups: [\"\"]\nresources: [\"pods\"]\nverbs: [\"create\"]\n- apiGroups: [\"\"]\nresources: [\"pods\"]\nresourceNames: [\"cvt-simulated-bundle-0-pod\", \"cvt-simulated-bundle-1-pod\", \"cvt-simulated-bundle-2-pod\"]\nverbs: [\"delete\"]\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\nname: cvt-bundle-simulator-role-binding\nnamespace: namespace-aa\nsubjects:\n- kind: ServiceAccount\nname: cvt-bundle-simulator-service-account\nnamespace: namespace-aa\nroleRef:\nkind: Role\nname: cvt-bundle-simulator-role\napiGroup: rbac.authorization.k8s.io\n```  \nThis approach allows for having a limited number of 'slots' to which Simulator Orchestrator is\nable to deploy Simulated Bundles. The Simulator Orchestrator will not be able to destroy any other resource in AKS cluster.\nUnfortunately, it's not possible to restrict 'Create' by resource names via k8s role bindings, Simulator Orchestrator will therefore be able to deploy any pod.  \nAKS Deploy  \nTo deploy pods in a runtime Kubernetes C# Client allows using yaml templates, which will be\npopulated with container image URI and Environment Variables.  \n```csharp\nvar pod = await KubernetesYaml.LoadFromFileAsync(\"pod_template.yml\");\npod.Metadata.SetNamespace(namespace);\npod.Metadata.Name = podName;\npod.Metadata.Labels[\"app\"] = name;\nvar bundleContainer = pod.Spec.Containers.First(x => x.Name == \"bundle-container\");\nbundleContainer.Image = image;\nvar helperContainer = pod.Spec.Containers.First(x => x.Name == \"helper-container\");\nhelperContainer.Env.Add(new V1EnvVar(\"db_connection_string\", _dbConnectionString));\n\nawait _client.CoreV1.CreateNamespacedPodAsync(pod, _namespace);\n```  \nIt takes several seconds to start a new pod in AKS.  \nSimulated Bundle Application should have Managed Identity assigned to be able to connect to ServiceBus/Database/Simulator Orchestrator in a secure way to save results of testing.  \nAKS Metrics  \nSimulator Orchestrator can get CPU/Memory Metrics from Kubernetes Metrics Server.  \n```csharp\nvar metricsObject = (JsonElement)await _client.CustomObjects.GetNamespacedCustomObjectAsync(\"metrics.k8s.io\", \"v1beta1\", _namespace, \"pods\", podName);\nvar metrics = metricsObject.Deserialize();\nvar bundleContainerMetrics = metrics?.Containers.First(x => x.Name == \"bundle-container\");\n\nreturn new\n{\nCpu = bundleContainerMetrics.Usage[\"cpu\"].Value,\nMemory = bundleContainerMetrics.Usage[\"memory\"].Value\n};\n```  \nThe code above is the equivalent to the kubectl top pod --containers <podName> -n <_namespace> command. Metrics from the Kubernetes Metrics Server are available with a delay and show only average values within a specified time window. It means Simulator Orchestrator should be polling for metrics every N seconds from Kubernetes Metrics Server, store and find the highest number of CPU/Memory and potentially also store the averages. It's important that the simulation pod stays alive for a while to ensure all (delayed) metrics are obtained.  \nAzure Container Instances approach  \nSimulator Orchestrator is an application running on AKS cluster and this application is able to connect to Azure.\nSimulation Environment is Azure (Azure Resource Manager, Slot Resource Group, Azure Monitor).\nSimulated Bundle Application is a multi-container Azure Container Instance.  \nACI Security  \nSimulator Orchestrator should have Service Principal / Managed Identity assigned with permissions to only create/delete Azure Container Instances in cloud platform Stamp/Slot resource group in Azure. This requires a Custom Azure AD Role with specific Microsoft.ContainerInstance/* permissions.  \nSimulated Bundle Application should have Managed Identity assigned to be able to connect to ServiceBus/Database/Simulator Orchestrator in a secure way to save results of testing.\nManaged Identity for ACI is in Preview  \nACI Deploy  \nDeployment of Azure Container Instances at runtime from .NET application can be done in several ways:  \nAzure.ResourceManager.ContainerInstance SDK - to create ACI via C# objects  \nAzure.ResourceManager.Resources SDK + json ARM template  \nAzure Resource Manager REST API  \nThe recommended approach will be to use ContainerInstance SDK, at the time of writing just out of preview and a lot cleaner than using the raw API or the resources SDK with ARM templates. We need to assign a User Assigned Identity to these ACI resources that has the ability to ACRPull from the cloud platform and CVT Azure Container Registries.  \nDeployment takes some time, about 1-2 minutes. This can be considered as a limited time, the test will anyway take from a few seconds to hour(s) for very heavy models.  \nACI Metrics  \nSimulator Orchestrator can get CPU/Memory Metrics from Azure Monitor for ACI.  \n```csharp\nvar metricsClient = new MetricsQueryClient(new DefaultAzureCredential());\nstring resourceId = $\"/subscriptions/{configuration[\"subscriptionId\"]}/resourceGroups/{configuration[\"resourceGroup\"]}/providers/Microsoft.ContainerInstance/containerGroups/{name}\";\nvar metricsOptions = new MetricsQueryOptions()\n{\nMetricNamespace = \"Microsoft.ContainerInstance/containerGroups\",\nTimeRange = new QueryTimeRange(DateTimeOffset.UtcNow.AddMinutes(-30), DateTimeOffset.UtcNow),\n};\n\nmetricsOptions.Aggregations.Add(Azure.Monitor.Query.Models.MetricAggregationType.Maximum);\nmetricsOptions.Filter = \"containerName eq '*'\";\n\nvar results = await metricsClient.QueryResourceAsync(resourceId, new[] { \"MemoryUsage\", \"CpuUsage\" }, metricsOptions);\n// results variable contains timeseries values of MAX Memory and MAX CPU for each container in ACI for specified TimeRange.\n```  \nSimulator Orchestrator can get Metrics after testing is done, not needed to poll multiple times - Azure Monitor provides metrics for TimeRange.  \nNote: Metrics should be fetched before ACI is deleted.  \nAzure Monitor for ACI is in Preview\nPreview limitations: Azure Monitor metrics are only available for Linux containers.\nNot an important limitation as Linux Containers are used in the cloud platform.  \nComparison AKS vs ACI approach  \nACI-\n*AKS-  \nSecurity-\nPod in AKS needs MSI that has specific RBAC rights to create ACI resource\nPods need specific role bindings with permissions, plus additional node pool for isolation  \nCosts-\nCheaper, only pay for what you use\nMore costly, since Simulator will need its own node pool  \nResources-\nNeed extra ACI instances\nRequires extra node pool for isolation  \nFeature completeness-\nManaged Identity and Azure Monitor still in-preview\nFeature complete  \nDeployment time-\nDeployment takes around 1-2 minutes\nSimulator bundle deployed within several seconds  \nMetrics-\nBuilt-in via Azure Monitor (with History)\nOnly real-time metrics available via Kubernetes Metrics Server  \nBoth solutions have advantages and disadvantages, most notably:  \nACI will be cheaper\u00b9, because you only pay for the workloads that are running, while AKS needs to have a separate node pool for security reasons.  \nDeployments on ACI take longer, but this is not an issue because the simulations aren't time-sensitive  \nMetrics for CPU/RAM are more accurate and easier via ACI because we can use Azure Monitor to request historic metrics, while AKS only provides real-time metrics that we manually need to track.  \nManaged Identity for ACI is still In-Preview and only available on Linux machines which might be a risk.  \nWith ACI, it's easier to specify the specific workload we want the simulator to run on (e.g. enable GPU, specific CPU/RAM requirements, etc). On AKS this is more complex since we'll then need e.g. GPU node pools which are very costly.  \n\u00b9 e.g. running 1 simulation per day, which lasts 2 minutes will cost < $0.05 on ACI, whereas the smallest VM node on AKS will be at least $3 for that entire day (and idle 99% of the time)  \nWith the above comparison in mind, the recommendation is to implement the ACI approach. With this approach, a Simulator Orchestrator pod in AKS exists that is able to spin up ACI resources for simulations. This is because the ACI approach is cheaper, easier to get historic metrics and it's more sandboxed and therefore more secure. Deployment does take a bit longer and some features are in preview, but this doesn't outweigh the advantages.  \nThe AKS approach could still be worth implementing alongside the ACI approach at a later stage when the requirements change (e.g. faster simulations or higher simulation frequency).  \nRisks & Mitigation  \nAzure Monitor and Azure Managed Identity are In-Preview, but there is no sign of these features disappearing anytime soon.  \nNeed managed identity with custom AD RBAC in Azure able to create Container Instance in Azure without being able to delete other resources in that resource group. This can be solved by creating a custom Azure AD Role with Microsoft.ContainerInstance privileges.  \nAdditional References  \nAccessing the Kubernetes API from a Pod  \nKubernetes Client Libraries  \nKubernetes C# Client",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\serving-models-on-edge\\computer-vision-on-edge\\cvt-bundle-simulator.md"
    },
    {
        "chunkId": "chunk308_0",
        "chunkContent": "CVT deployment typology and network configuration  \nThis document presents the possible configuration possible for the CVT deployment. In short, we can distinguish 3 deployments:  \nCVT standard: a central deployment using a VM, shared across multiple use cases, this is the best when cycles are long allowing multiple ones to run on the same machine.  \nCVT dedicated: a deployment on a dedicated physical industrial PC. This is best when cycles are very short or prediction need specific GPU resources for example.  \nCVT standalone: a deployment on a dedicated physical industrial PC. This is mainly for non non connected factories or pure research.  \nThose 3 types of deployments have an impact on the connectivity of the cameras and the machine. This document will bring clarity on all those elements. Note that 3 types of deployments can either work with IoT Edge, Kubernetes or a simple Docker Compose.  \nGoals / In-Scope  \nBring clarity on the 3 types of CVT typology.  \nBring clarity on the network requirements for those 3 types.  \nBring clarity on the camera specific networking needs.  \nUse a concrete example to clarify all this.  \nBring clarity on what are the possible configurations for screens.  \nRelated Designs  \nHigh-Level Architecture  \nCamera network requirements  \nAssumptions & Guardrails  \nWe assume that all the cameras used are GenICam GigeVision cameras. If they are \"classical\" TCP/IP cameras with a simple endpoint to get an already ready picture, this may not fully not apply as networking could be different.  \nWe assume that there is a physical possibility (network cable) for the cameras to be connected to the network.  \nWe assume that the network team/operation team can adjust/add networking elements, setup them properly in order to achieve a proper connection.  \nApproaches  \nIn this section, we will describe in details the requirement, pros and cons for each of the 3 CVT typologies and show the associated requirements in terms of networking for the different elements including the cameras.  \nThis graphics shows a simplified version of the CVT deployment typologies and some of the networking elements:  \nCVT Standard  \nThis is the deployment of CVT in a shared VM. It means multiple use cases (bundles) can run on the same virtual machine. This virtual machine is in the DMZ, in its own zone and can communicate with the Connectivity edge as well. It has a proper access to the cloud resources in Azure.  \nWhen to use CVT Standard  \nAdvantages of CVT Standard:  \nShared infra so you can have mutualization of workloads decreasing the cost drastically. No physical maintenance, OS update part of the process. No need of physical Industrial PC (IPC) and no additional network elements. Only cameras need to be connected.  \nFully automated deployment from provisioning the VM, installing it (standard script) to DPS provisioning for the runtime.  \nEasily scalable to numerous number of VM.  \nInconvenient:  \nCameras need to be in same network. Because of the bandwidth used by cameras, there are limitations on how many cameras can be operated at the exact same time not to overkill the network. This number depends on the number of pictures that needs to be taken at a specific moment. While this is not an absolute number, we can estimate that 1 picture every second is no issue. Meaning that in case of multiple cameras triggered at the same time, 3 cameras in a period of 1 to 3 seconds should work. UDP protocol used by the cameras is non trivial when it comes to setup properly a firewall.  \nNeed a bit of optimization to run all this and possibly delays for the cameras.  \nBest for relatively long cycle scenario like few minutes where transferring the picture and doing the prediction is not time critical.  \nNetworking requirements for CVT Standard  \nHere is a first example of the needed configuration with 1 camera:  \nThis is another example with 3 cameras:  \nCVT Dedicated  \nCVT Dedicated is similar to CVT Standard but rather than deployed in a VM, it is deployed on a real industrial PC (IPC). It uses as well the connection to the cloud platform and the same tool chain as CVT Standard and same way of deploying it. The difference in in the provisioning of the IPC and maintenance of it. It does require manual intervention to install the base OS, run the installation script and configuration. This requires as well to clear the machine for a cloud connectivity.  \nWhen to use CVT Dedicated  \nAdvantages of dedicated IPC:  \nEasier setup for cameras as they can be in a NAT connected to the IPC.  \nLower latency than the CVT Standard.  \nBest for short cycle where a lot of power is needed all the time or need GPU for example.  \nCan add a screen for real time tagging. Even if this is not yet implemented in CVT, this is a possible scenario.  \nInconvenient:  \nProxy edge needs to be available in the DMZ to have access to the cloud platform in general (nested edge scenario) OR IPC should be part of the DMZ network.  \nNeed to manage OS update and initial installation manually. Need to manage any potential issue manually as well.  \nCost of dedicated IPC, of more network elements.  \nHigher installation and maintenance costs.  \nSecurity.  \nMachine connectivity (feedback loop) needs to be handled / solved separately.  \nNetworking requirements for CVT Dedicated  \nDMZ scenario  \nCVT Dedicated deployment with access to the cloud platform with IPC in DMZ:  \nCVT Standalone  \nIn order to bring clarity like for the other deployment, we'll as well describe here in a summary. CVT Standalone is similar to CVT Dedicated but without any connection to the cloud. It then requires fully manual deployment, maintenance and update. CVT Standalone is then a solution for small standalone tests, factories not yet connected to the cloud or where a network connection is purely impossible.  \nWhen to use CVT Standalone  \nAdvantages of dedicated IPC:  \nBest if the factory is not cloud enabled or if the network connectivity is strictly impossible.  \nInconvenient:  \nNeed to manage OS update and installation manually. Need to manage any potential issue manually as well. Need to manage any update manually.  \nNeed to deploy everything manually from pre-build solutions.  \nNo out of the box way to publish the pictures taken.  \nCost of dedicated IPC, of more network elements.  \nHigher installation and maintenance costs.  \nSecurity.  \nMachine connectivity (feedback loop) needs to be handled / solved separately.  \nIf a network connection is needed, all this should be solved separately as well. This may introduce challenges if cloud connectivity is needed and PLC connectivity at the same time.  \nNetworking requirements for CVT Standalone  \nHere is an example of networking requirements with 3 cameras in a CVT Standalone deployment:  \nNote that in this case, there should not be requirements to exit the local networking layer 3. The main switch and the external firewall is represented for completion and comparison with the 2 other CVT typologies.  \nNon-Functional Requirements  \nNetworking typology need to be known and understood.  \nNo firewall should be between cameras and VM/IPC.  \nOperationalization  \nCameras need to be setup, installed, cabled and configured manually.  \nPossible costs of adding cabling, local switch need to be budgeted.  \nIt is important to anticipate any installation as they require time and specific setup.  \nIn case of CVT Standard with VM, only cameras need to be connected properly on the same network as the VM.  \nIn case of CVT Dedicated with IPC, the IPC must be able to connect to the DMZ and to the cloud. Ports should be open as well for the MQTT broker (1884) and for the OPC UA Server (can be adjusted).  \nIn case of CVT Standalone, only local network is needed, it is optional to add any other network configuration.  \nIn case of CVT Dedicated or Standalone, it is strongly recommended to have cameras on a dedicated sub network accessible only by the IPC. This will guaranty a better network isolation to avoid \"pollution\" of camera frames in the rest of the network.  \n[!NOTE]  \nThere are 2 different options allowing a screen displaying parts and allowing tagging/classification. Below are the 2 options and the network impact. Those 2 options are complementary and can be used at the same time. The light portable tablet like style is more for the longer cycles and naturally aligns with CVT Standard. The dedicated fixed screen on an IPC is best for very short cycles or complex tagging that requires to be done on the edge and naturally aligns with CVT Dedicated/Standalone.  \nLight portable screen like a tablet with cloud connection  \nA light portable screen, think of a tablet that only requires an internet connection can be used in the CVT Standard or Dedicated configuration. This is possible when the production cycle is not too short as the picture and the associated data and annotations will be uploaded to the cloud platform first and the screen will display what has been uploaded in the cloud. See the schema below for a better understanding:  \n[!IMPORTANT]  \nIn terms of networking, it is NOT necessary (and not possible) to have this tablet/IPC connected to the production network. The only required access is the cloud platform so targeted network is either office network or DMZ.  \nThis will work as well for a PC in the Office network and for a phone connected to the normal data network or wifi.  \nA fixed screen attached to an IPC with or without cloud connection  \nThis is a typical physical screen attached to an IPC. This is a possible solution for CVT Standard, CVT Dedicated and CVT Standalone. The case of CVT Standard is the same as the previous case. So see above for more information. In case of CVT Dedicated or Standalone, the IPC on which CVT is deployed is used as well with a screen allowing to display a local UI. This scenario is best for low latency scenarios or very short production cycles needing a very quick display of the picture for tagging/classification/adjustments. The network typology is then the same as the one described for CVT Standard and CVT Standalone.\nThe difference here is just the ability to have a display as well.  \nCVT Dedicated configuration is the most common way to have a screen directly attache to the CVT deployment. Same configuration from CVT Standalone:  \nThe other option represented here is a dedicated IPC connected to the production network but without CVT deployed on it, so a configuration like CVT Standard or even CVT Dedicated but on separate machine:",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\serving-models-on-edge\\computer-vision-on-edge\\cvt-deployment-typologies.md"
    },
    {
        "chunkId": "chunk309_0",
        "chunkContent": "CVT Operational Logging  \nThis document will covers the observability aspects like logging, metrics for CVT components running on the edge device. The focus will be on operational logging.  \nGoals / In-Scope  \nFor troubleshooting, traceability, responsible AI, we need to be able to capture various metrics and logs so they can be monitored and analyzed. As mentioned the focus will be on operational logging first.  \nWe want to be able to capture and log the following metrics:  \nCPU usage of the CVT IoT Edges modules (including the bundles)  \nMemory usage of the CVT IoT Edges modules (including the bundles)  \nBundle specific:  \nResponse times when calling the Bundle API  \nResult of the Bundle execution:\nOK/NOK\nConfidence  \nError/exception messages  \nAll logging should include sufficient metadata to be able to get insights in the behavior of specific deployments and bundles. This metadata should include (when available):  \nTimestamp  \nIoT Edge device name  \nModule / Bundle identification (name and version)  \nAssumptions & Guardrails  \nThe cloud platform is using Log Analytics for logging and monitoring.  \nCVT is using and running on IoT Edge devices, which are online and have connectivity to a Log Analytics workspace in Azure.  \nUsers which have access to the Log Analytics workspace can query the data, so the logged data and metrics don't contain any sensitive information.  \nDesign  \nWe'll make use of 3 *layers- of logging:  \n1) IoT Edge Metrics Collector Module, provides the base performance metrics out-of-the-box. You can use a customized version that can run independently of IoT Edge if you are on a Kubernetes cluster for example. You can as well replace this component by a Prometheus collector that can push the detail logs to Azure.\n2) Orchestrator logging, the orchestrator will log the bundle execution results, response times and error/exception messages caused by bundle execution.\n3) (optional) Logging in the bundles, the creators of the bundles can choose to log additional information in their bundles.  \nIoT Edge Metrics Collector Module  \nIOT edge team provides metrics collector module which is currently in Preview. Here is the link to get more info about the collector module.  \nThis metrics-collector module can be added to IOT edge deployment to collect metrics and send them to Azure Monitor. This is also provided as multi-arch Docker container image that supports Linux X64,ARM32,ARM64 and Windows 64.  \nMetrics-module we can perform following steps  \n```json\n\"IoTEdgeMetricsCollector\": {\n\"settings\": {\n\"image\": \"mcr.microsoft.com/azureiotedge-metrics-collector:1.0\",\n\"createOptions\": \"\"\n},\n\"type\": \"docker\",\n\"env\": {\n\"ResourceId\": {\n\"value\": \"$RESOURCE_ID\"\n},\n\"UploadTarget\": {\n\"value\": \"AzureMonitor\"\n},\n\"LogAnalyticsWorkspaceId\": {\n\"value\": \"$LOG_ANALYTICS_WORKSPACE_ID\"\n},\n\"LogAnalyticsSharedKey\": {\n\"value\": \"$LOG_ANALYTICS_SHARED_KEY\"\n},\n\"MetricsEndpointsCSV\": {\n\"value\": \"http://edgeAgent:9600/metrics,http://edgeHub:9600/metrics\"\n},\n\"OtherConfig\": {\n\"value\": \"\"\n}\n},\n\"status\": \"running\",\n\"restartPolicy\": \"always\",\n\"version\": \"1.0\"\n},\n\n```  \nOrchestrator logging  \nSince the Orchestrator is the component that's responsible for orchestrating the flow of data (e.g. sending the image data to the bundle(s) and capturing the result), it makes sense to centralize logging functionality in the Orchestrator. This will avoid having to repeat logging functionality in the bundle(s).  \nWe can use the Application Insights SDK from the C# code of the Orchestrator and it should log the Bundle specific items using the following features of the Application Insights API:  \nResponse times when calling the Bundle API, , to be sent using TrackRequest  \nResult of the Bundle execution:  \nOK/NOK, to be sent using TrackMetric  \nConfidence, to be sent as a TrackMetric  \nError/exception messages, to be sent as a TrackException  \nAnd it should include this metadata:  \nTimestamp (happens out of the box)  \nIoT Edge device name, added as a Property  \nBundle identification, , added as a Property  \nLogging in the bundles  \nAdditional logging can be added to the bundles, it would be up to the bundle creators to connect to Log Analytics and log the additional information. We could use the Python OpenCensus library to send logs to Log analytics. Here are some sample steps to enable logging:  \nCreate Application insights resources which is tied to existing Log analytics workspace, use this python logger class to implement logging in our python modules, add environment settings in modules in deployment json.  \npython\n\"env\": {\n\"APPINSIGHTS_INSTRUMENTATION_KEY\": {\n\"value\": \"$APPINSIGHTS_INSTRUMENTATION_KEY\"\n}\n}  \nInitialize logger in python file as  \npython\ntry:\ndevice_id = getenv(\"IOTEDGE_DEVICEID\",\"\")\nmodule_id = getenv(\"IOTEDGE_MODULEID\",component_name)\nlogger = APP_LOGGER.get_logger(\ncomponent_name=component_name,\ncustom_dimensions={\n\"deviceid\": device_id,\n\"moduleid\": module_id\n\"bundle_id\": bundle_id\n},\n)\ntracer = APP_LOGGER.get_tracer(\ncomponent_name=component_name,\n)\nexcept Exception as ex:\nprint(ex)\nraise Exception(f\"ERROR - in initiate app logger - {ex}\") from ex  \nAs mentioned above add custom dimensions so that deviceid and moduleid are attached to every log.  \nRemark 1:- based on the chosen library (e.g. OpenCensus) it could be that the moduleid is already included in the log by default, so we don't explicitly need to add it as a custom dimension.  \nRemark 2:- logging added by bundles is out-of-control for CVT so the Log Analytics workspace could become 'polluted' with log information coming from the bundles. If this becomes a problem in the future, it can be solved by having the bundles log their logs to a separate Log Analytics workspace. The recommendation is to solve this only when it becomes a problem, so initially have the bundles log to the CVT Log Analytics workspace.  \nRemark 3:- after there are some bundles created which implement custom logging, it's probably worthwhile to reflect if common code and functionality can be abstracted and added to the CVT library.  \nWe can use custom queries to filter logs based on deviceid or moduleid:  \nkusto\ntraces\n| extend moduldeid = customDimensions.moduleid\n| where timestamp > ago(30m)\n| where moduldeid == 'PythonModule2'\n| limit 1000  \nWe encountered an issue where opencencus was taking too much CPU. After reading about similar issue and we disable local storage when initializing AzureLogHandler. This helped to resolve high CPU issue.  \npython\nlog_exporter = AzureExporter(\nconnection_string=app_insights_cs,\nenable_local_storage=False\n)  \nRisks and Mitigation  \nCost: it's worth noting that additional logging will cost money. Factors that impact the pricing of Azure Monitor are ingestion & retention. The more data you send, and the longer you want to use it, the more money it will cost. There is functionality available to manage both the amount of data that is sent (e.g. sampling) and the time it's stored (e.g. retention policies). Initially probably we'll want to send all data and keep it available for a longer time. After a while we could tune sampling and retention to manage the cost.  \nReferences  \nMetrics collector Module  \nAdding custom metrics  \nBuilt in metrics  \nIot Edge Logging and monitoring solution(ELMS)  \nIot Edge observability  \nPython logger class  \nAzure Monitor Pricing",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\serving-models-on-edge\\computer-vision-on-edge\\cvt-monitoring.md"
    },
    {
        "chunkId": "chunk310_0",
        "chunkContent": "CVT Picture Store  \nThe purpose of this document is to explain the type of cloud resource for CVT Picture\nStore and the deployment layer (global, deployment stamp, slot).  \nGoals / In-Scope  \nDecision on the CVT Picture Store  \nDecision on the cloud platform deployment layer (global, deployment stamp, slot)  \nDecision on Folder Structure  \nRelated Designs  \nCVT High Level Architecture  \nAssumptions & Guardrails  \nWe want to leverage existing platforms where a stamp with a slot model deployment has been done as much as possible to avoid duplicate setup work.  \nDesign  \nPicture Store  \nWe propose to use Azure Data Lake Gen2 as the picture store for the CVT data. You can find a lot of other good reasons in the Data Ops documentation.\nThe main reasons for this are the Optimized storage for big data, ACL based on\nActive Directory and low analysis cost  \nBlob Storage ADLS Gen2 Flat namespace object store Hierarchical namespaces General purpose object store for a wide variety of storage scenarios, including big data analytics Optimized storage for big data analytics workloads. Good storage retrieval performance Better storage retrieval performance High cost for Analysis Low cost for Analysis Access Control Lists (ACLs) based on Azure Active Directory Identities can be set at the file and folder level Account, Container or Blob level authorization using shared access tokens  \nWe propose to deploy the picture data and picture metadata stores on the platform\nslot layer.  \nFolder Structure  \nWe propose to use the below folder structure considering the picture store at\nslot level with multiple plants connected per slot. This structure helps to\neasily identify the pictures and metadata received per day and use for model\ndevelopment and training.  \nThe below structure will have multiple iterations and changes based on the\nrequirements. This is the initial proposal for both the short term platform storage\nand long term Data lake storage.  \n```txt\n.\n\u2514\u2500\u2500 CVT (Storage Account)\n\u2514\u2500\u2500 pictures (Storage Container)\n\u2514\u2500\u2500 plant=PlantId\n\u2514\u2500\u2500 machine=MachineId\n\u2514\u2500\u2500 y=YYYY\n\u2514\u2500\u2500 m=MM\n\u2514\u2500\u2500 d=DD\n\u2514\u2500\u2500 h=HH\n\u2514\u2500\u2500 o=OPERATIONID\n\u2514\u2500\u2500UUID.jpg\n\n```  \nReasoning for Hierarchy Elements  \nPictures: We will have metadata, pictures and annotations. We keep them\nseparate by using separate containers or folders  \nPlant: We need this for aggregation and access levels  \nMachine: We need this for aggregation  \nYYYY/MM/DD/HH this is used for time based partitioning and helps finding\npictures  \nOperationId: This keeps pictures close to each other that were created due to\nthe same event (e.g. a machine trigger or an on demand picture taken by a\nuser)  \nAdvantages  \nEasy identification of pictures and metadata per day for Data Science Process  \nData segmentation and access control per plant  \nFinding data while troubleshooting  \nDisadvantages  \nLarge hierarchical structure  \nRequired to search multiple folders for getting data from more than one\nplant/machine/date  \nNon-Functional Requirements  \nYou may not know how large datasets will be or how fast they will grow, however,\nplacing the picture archives at the platform slot level allows us to scale a lot.\nStorage accounts have a maximum capacity of 5 PiB (Pebibyte) and allow for 60\nGbps (ingress) and 120 Gbps (egress) traffic.",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\serving-models-on-edge\\computer-vision-on-edge\\cvt-picture-store.md"
    },
    {
        "chunkId": "chunk311_0",
        "chunkContent": "Deployment patterns  \nWhen speaking about deploying a cloud solution, there are few existing patterns for the resources and the associated services in which the code and the data are deployed.  \nGlobal deployment  \nA global deployment is a deployment that happens only in one central place, so in one specific Azure region and only once (or 2 times if redundancy is required for some of the services). Those kind of deployment are usually used for few services in a larger deployment like some global Azure Container Registry or some global Azure Keyvault.  \nIn general, other services are then deployment in a stamp or slot pattern.  \nStamp deployment  \nThe stamp deployment basically recreates the services to be deployed with copies of each others but isolated from each others. A detailed article explaining the pattern can be found in the Azure architecture  documentation.  \nAs a summary, a stamp deployment will look like this:  \nThe issues and considerations from the article point out some elements like the increase of the cost as the infrastructure is always redeployed and some services have a high initial cost of deployment regardless of its usage. Another element is traffic routing and shared component. And to address those specific elements, the slot model can be used.  \nSlot model  \nThe slot model takes place in a stamp deployment. It is always recommended to use the stamp pattern first as it solves quite some problems like latencies. But then some services can have a high cost and should be better shared across multiple services. That's for example the case for an Azure Kubernetes Cluster.  \nA slot model is like a slot in a Azure web application, the idea is to share the same infrastructure element and have a logical separation in this component. You can create such slots across a complex application. For example, using a specific namespace in a Kubernetes cluster to separate the application while still sharing the same cluster, using an Azure SQL Pool while having each slot having its own SQL Database attached to the Azure SQL Pool.  \nA slot can be seen as very similar to a stamp, difference is on the shared infrastructure and common elements in the slot:  \nAnd when you put everything in perspective with global, stamp and slots, the picture looks like this:  \nThe slot model tries to address some of the challenges that the stamp model has especially on getting access to shared resources across the slot and decreasing the cost. As any model, it has also some inconvenient as a bit more complexity in managing the deployments but it also brings flexibility and allow ring deployment in the same infrastructure.",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\serving-models-on-edge\\computer-vision-on-edge\\global-stamp-slot.md"
    },
    {
        "chunkId": "chunk312_0",
        "chunkContent": "Computer Vision Toolkit (CVT)  \nCVT is a cloud and edge based platform for packaging, deploying and running computer vision, OCR, ML/AI and other types of models within production facilities. The main goal of CVT is to support use cases especially in the area of quality inspection and traceability. By leveraging Azure cloud platforms, CVT offers a scalable computer vision solution potentially enabling 100% inspection rates across multiple factories globally.  \nBusiness Problem  \nComputer Vision Toolkit is solving the problem of deploying and running at scale any kind of computer vision, OCR, ML/AI on the edge. When reading at scale, think of lots of hundreds or thousands models in different locations where you can, in a standard way run those regardless with which technology they've been built with. There is also another aspect which is about the deployment of a standard edge solution that can run in a virtual machine in a shared environment or a physical machine, all in the same way.  \nChallenges  \nThere are multiple challenges to overcome. The first one is to find a good way to standardize running computer vision on the edge with a standard set or interfaces leaving the freedom to anyone to use the best technology for it. And at the same time, you want to make sure that the interfaces are working the way they are supposed to and being able to run security checks.  \nAnother challenge is to optimize the hardware and the processes to run this workload on the edge. Thing of a factory where you have a machine producing one part ever 10 minutes, another one with part every minute and a third one once per day. Rather than having to deploy 3 different hardware attached to the machines, you will want to optimize and group everything on a single virtual or physical machine. This is a large cost scenario optimization that in most patterns available is not been taken into consideration.  \nAnother important challenge to overcome is the integration with Programmable Logic Controller (PLC) used in the industrial world to control and pilot production machines. Those exposes signal and speaks standards like OPC and OPC-UA. You want to be able to take pictures when a part is ready and instruct back the PLC if the part is a good one or a bad one.  \nSolution  \nTo solve the first challenge on the standardized way of running computer vision on the edge, the solution is to use a Bundle.  \nTo solve the challenge of optimizing the cost and workloads running at the same time, we solve this problem designing a solution that can run multiple computer vision workload, shared on the same machine rather than having to manage multiple machines and maintaining them. The solution works as well when deployed in an on-premise data-center in a Kubernetes style pattern or on an Azure IoT Edge pattern or even with a simple docker compose or any other solution that can deploy and run containers.  \nThe solution allows to use at scale industrial GigeVision cameras. Those cameras are standard in the industrial world.  \nThe all up solution is designed to integrate in a factory, it is designed to take pictures based on triggers and output the model prediction through OPC-UA.  \nArchitecture  \nWe want to establish a generalized solution approach for computer vision\nprojects. CVT (Computer Vision Toolbox) is supposed to offer this approach by\ncombining the ability to work with pictures and cameras on the shop floor with a\nstandardized cloud-based configuration service and ML templates as well as\ninterfaces that enable the user to easily configure the solution.  \nBundle  \nIn short, a bundle is a container containing one or multiple models with a standard interface and metadata. For ease of deployment of Machine Learning models a common way of packaging the models and all related assets must be defined. This will facilitate the generalization of the deployments as well as make it easier for teams to operationalize them.  \nYou will find more information on the format, the pattern in those documents:  \nBundle Interface  \nBundle Registry  \nDesign  \nThe team aligned to partition the CVT solution into three larger buckets:  \nThe machine learning (ML) components  \nCloud-based components  \nComponents that are running on the edge.  \nMachine Learning  \nData lake: Machine Learning utilizes existing Data lake infrastructure for more information you can check cvt picture store. Data Science Teams\nshould be able to use their own workspaces if they are capable of meeting the\nrequired interfaces.  \nLibraries: We will develop common libraries to access various CVT\nfunctionality (e.g. how to create a bundle that will work on CVT).  \nCloud  \nConfiguration Service: Central part of the CVT is the configuration\nservice. This service is the central point in CVT to bring cameras,\nmachines (via Asset Registry) and bundle information (via Bundle Registry)\ntogether and configure the edge devices (via own UI and config pipeline).  \nCVT cloud infrastructure is deployed using a stamp and slot model.  \nMost CVT services is deployed on platform slot level.  \nThe Bundle registry is deployed on a platform global level. The reason is\nbecause the bundles should be available globally.  \nBundle Registry: A global service that stores CVT bundles and be used\nfrom each slot. Since the bundle format and authentication means with the\nregistry are not yet clear, detailed\ndesign for the bundle registry can be found here.  \nPicture & Meta Store: CVT is hosting dedicated stores for pictures, annotation and meta\ndata for short-term processing and immediate interaction. For long-term\nprocessing and model training purposes, all data is forwarded to a Data lake. More details here.  \nAnnotation Service: CVT offers a solution to annotate pictures. We used an annotation format to annotate pictures, this is happening on the edge in orchestrator after predictions, also this format supports manual annotation.  \nPerformance Monitor: CVT offers a performance monitoring component\nthat analyzes model performance on the edge. The performance monitor is\ndetailed Here. Performance monitor leverages operation ID that are propagated in\nall the components including the bundle. See details here.  \nSignals: CVT acquires data via a trigger signal. The\nprinciple is to subscribe to an MQTT broker and wait for a specific element.  \nSimulator: CVT incorporates a simulator to test deployments in a safe\nenvironment before actually deploying that to the edge.\nOne of the roles of the simulator is to\ntest that the bundles are properly formed and running properly. The simulator\nmay have other roles like extracting metadata from the bundle.  \nEdge  \nOrchestrator: The heart of the CVT Edge is the bundle runtime called Orchestrator. The\norchestrator enables CVT Edge to run multiple Computer Vision Bundles at\nthe same time.  \nBundle: Bundles are a defined container having standard interfaces, exposing metadata and containing\ntest elements so it is possible in the Simulator to check that it actually works as expected.  \nCamera Interface: One of CVT's core functionalities is the connection to\nindustrial cameras. In a pre-assessment we determined that initially, only\ncameras that can be interfaced via IP is supported. But if there is a way to connect physical\ncameras as well depending on the deployment typology.  \nConfig Retriever: The edge modules including all bundles need to be\nconfigured via the configuration service. This service is actually done on the cloud side.\nEach module has its own configuration through a file or a twin. And each module has a fully independent\nconfiguration so they can be used separately and scaled separately as well.  \nMetrics collector: Collect metrics from various edge modules and make them\navailable in the cloud.  \nMetadata publisher & file publisher: This module forwards the bundle outputs called annotation,\nthe pictures and there metadata to\nconfigured sinks. One of the sinks is the picture store hosted on the\nrespective platform slot and published via OPC UA. Other sinks might include local\nnetwork targets, other cloud-based or even physically attached storage options\nor messaging systems.  \nOPC UA server: OPC Unified Architecture is a cross-platform, open-source, standard for data exchange from sensors to cloud applications developed by the OPC Foundation. You can find more detail about OPC-UA and Industrial IoT.  \nIn order to integrate with the rest of the factory but also other OT\ntechnologies, CVT exposes its results as OPC UA signals. This will allow\neasy integration (see Signals above).\n- Trigger: The trigger module is fed by data (machine signals like OPC UA) flowing in from the\nconnectivity edge or from other means defined by individual use case teams.\nThese may include APIs, and physical triggers connected to the cameras. The connectivity edge is\na dedicated edge processing the machine signals. This is out of the code shared here. As\navailable in the code, this is just an MQTT message and can be produced by a module watching\nspecific signals on a machine.  \nEdge - Camera Interface  \nCloud - Bundle Simulator  \nCloud and edge - Deployment Typologies  \nEdge - File uploading from edge to cloud using blob uploader  \nCloud and edge - Monitoring  \nCloud - Picture Store  \nCloud deployment patterns",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\ai-on-the-edge\\serving-models-on-edge\\computer-vision-on-edge\\index.md"
    },
    {
        "chunkId": "chunk313_0",
        "chunkContent": "Automated information extraction from forms  \nBusiness Problem  \nRetrieving information from documents and forms has long been a challenge, and even now at the time of writing, organizations are still handling significant amounts of paper forms that need to be scanned, classified and mined for specific information to enable downstream automation and efficiencies. Automating this extraction and applying intelligence is in fact a fundamental step toward digital transformation that organizations are still struggling to solve in an efficient and scalable manner.  \nTypically organizations will have built text mining and search solutions, which are often tailored for a scenario, with baked-in application logic, resulting in an often brittle solution that is difficult and expensive to maintain.  \nSolution  \nThanks to the breakthroughs and rapid innovation in the machine learning fields of Computer Vision and Natural Language Processing (NLP), reliable options are now available to provide data driven solutions that generalize and provide high degrees of accuracy in extracting information from structured forms.  \nCoupled with Azure services this provides rapidly deployable, cost-efficient and scalable solutions ready for production workloads.  \nDescription  \nThe goal of this Solution is to build a set of guidance, tools, examples and documentation that illustrate some known techniques for information extraction, all of which have been applied in real customer solutions.  \nThe Customer Pattern  \nCustomers want to automate the organization and extraction of information from these scanned forms to streamline their operations, and save costs and time associated with processing the data manually.  \nValue Proposition  \nThis Solution aims to provide step-by-step guidance for each phase of a typical Forms Extraction project alongside typical considerations, key outcomes and code accelerators per phase.  \nImplementation  \nKnowledge Extraction for Forms Accelerators & Examples",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\cognitive-services\\automated-Information-extraction-from-forms\\index.md"
    },
    {
        "chunkId": "chunk314_0",
        "chunkContent": "Automation and Optimization of Custom Translator Service  \nBusiness Problem  \nMicrosoft Custom Translator is a machine translation solution that helps businesses to translate their content into multiple languages. This service solves the problem of language barriers that businesses face when trying to reach a global audience. With Custom Translator, businesses can quickly translate their websites, documents, customer support interactions, and other content, which helps them to expand their reach, increase engagement, and grow their customer base. Additionally, the custom translator model can be trained on specific domain-specific terminology to increase translation accuracy, making it easier for businesses to communicate with their target audience in their native language.  \nChallenges  \nSome major challenges with building enterprise solutions with Custom Translator are as follows:  \nIntegration with Other Systems: Technical expertise and customization is required to integrate Custom Translator with other enterprise systems.  \nMaintenance and Upkeep: Training and maintaining the custom translation model requires ongoing efforts and resources, which can be time-consuming and difficult for some businesses.  \nLack of Expertise: Some businesses may not have the specialized knowledge and expertise in machine translation.  \nTo lessen these challenges, automation, enterprise assets, and guidance are needed. Automation will provide a training and deployment process that is easier to maintain. Enterprise assets and guidance will democratize specialized knowledge in technology and machine translation and allow businesses to focus on integration. Very little automation or enterprise assets and guidance exists for getting the best results of out of the Custom Translator service. This solution aims to provide some tools and pipelines to enable MLOps for this service.  \nSolution  \nCustom Machine Translation Recipes contains guidance, CI/CD pipelines, and scripts to optimize the selection of data and training of the Custom Translator service.  \nCapabilities by Stage  \nStage Capability Description Data Curation Data Engineering Cleaning Translation memory files and generating train/test/tune datasets Experimentation in Azure ML Illustrates unsupervised approach to building a Phrase Dictionary Inference and Feedback Target language multi-model evaluation pipeline",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\cognitive-services\\automation-and-optimization-of-custom-translator-service\\index.md"
    },
    {
        "chunkId": "chunk315_0",
        "chunkContent": "Image classification experimentation with Azure Custom Vision  \nBusiness Problem  \nAzure Cognitive Services provide pre-trained, and in some cases fine-tunable, machine learning (ML) models for common AI scenarios and provides them as a service. Using Cognitive Services in your solution can save time and effort compared to building a custom ML model from scratch, even when models are fine-tuned to specific scenarios, which reduces the time and effort required to get a ML solution into production.  \nFor example, Azure Custom Vision is an image recognition model that you can fine tune to your scenario, often with less data and in less time than creating a custom image recognition model from scratch.  \nWhen working with Cognitive Services such as Azure Custom Vision, project teams require the MLOps capabilities described in this playbook. Guidance and code assets on how to engineer these MLOps capabilities are not well documented or supported via code assets, which greatly increases the engineering effort and Custom Vision subject matter expertise required to build a well performing model.  \nChallenges  \nWhen working with Azure Custom Vision, guidance and code assets for delivering key MLOps capabilities such as Experimentation, Model Development, and Deployment are not provided.  \nSolution  \nMLOps for Custom Vision provides guidance and code assets for MLOps capabilities Experimentation, Model Development, and Deployment when using Custom Vision. Use this repo to create a fully configured Azure Custom Vision experimentation environment for your project with the ability to deploy models between environments. Infrastructure as Code scripts are provided for quick environment setup. Sample data is provide to quickly confirm your deployment is working end-to-end. Documentation on how to apply the solution to your own data and scenario are provided.  \nFor more guidance on using Cognitive Services, see Accelerating with Cognitive Services in this playbook.  \nFor more guidance on Custom Vision, see Custom Vision in this playbook.",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\cognitive-services\\image-classification-experimentation-custom-vision\\index.md"
    },
    {
        "chunkId": "chunk316_0",
        "chunkContent": "Langchain Custom Hybrid Retriever using Azure AI Search  \nA custom Langchain search implementation was developed leveraging Azure AI Search (formerly known as Azure Cognitive Search) & FAISS Vector Search in order to implement hybrid searching using an open source vector store.  \nOverview  \nThis solution was developed during an LLM MVE engagement with a US Government customer in order to surface relevant documents and document chunks from a keyword or natural language based search. Typically, Azure AI Search service would suffice in meeting the goal above, however, due to limitations in access to the vector search feature in Azure Government subscriptions at that time, the hybrid keyword-vector search functionality would be decoupled from Azure AI Search and built using the FAISS in-memory similarity search library.  \nTechnical Solution Summary  \nIn order to surface relevant documents for a given search fitting the above criteria, search indices must first be built and populated with document chunk data and their vectors for the Azure AI Search Service and FAISS respectively. Next, the AI toolkit Langchain is used to create a Retriever for each of the search stores, which is simply an interface that returns documents given an unstructured query. Having created a Retriever for both search features, we are able to use the Ensemble Retriever also from Langchain, to re-rank results using the Reciprocal Rank Fusion algorithm. Reciprocal Rank Fusion combines rankings from the multiple results with different relevance indicators into a single resulting set.",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\cognitive-services\\langchain-custom-hybrid-retriever\\index.md"
    },
    {
        "chunkId": "chunk317_0",
        "chunkContent": "tags:\n- Speech to Text\n- Real-time transcription\n- ASR\n- Automatic Speech Recognition\n- React\n- Javascript  \nReal Time Transcription  \nReal-time transcription is an essential part of many modern applications.  \nPeople want to speak to their devices and get answers. When they ask OpenAI for a recommendation of a restaurant, having to type information on a keyboard feels like picking up a self-driving car, by-passing all the tech and driving it manually. Slow, and decidedly old-fashioned.  \nOverview  \nThe objective of this solution is to demonstrate the minimum work required to deploy a real-time speech to text solution on the web browser.  \nThis solution uses the popular framework React with Javascript (it would equally work with Typescript) and Azure Speech to Text.  \nThe application uses the local computer\u2019s microphone to capture audio input, which will then be transcribed in real-time using Azure Speech to Text.  \nThe transcribed text will be displayed on the computer screen as it is generated. The user is also be able to download the transcription as a text file.  \nThe Repository  \nThis publicly available repository contains the solution:  \nhttps://github.com/Azure-Samples/real-time-transcription-simple  \nThe Technical Solution in a Nutshell  \nThe app uses Azure Speech to Text\u2019s Continuous Recognition feature, which allows the service to transcribe speech in real-time as it is being spoken.  \nThis is the piece of code that contains transcription: https://github.com/Azure-Samples/real-time-transcription-simple/blob/main/webapp/src/Components/Transcription.js  \nIn that code, we define a speech configuration with code like this:  \njavascript\nconst speechConfig = sdk.SpeechConfig.fromSubscription(\n\"YourSpeechKey\",\n\"YourSpeechRegion\"\n);  \nIn order to capture the computer microphone, we use the navigator media services API. This is done when the website is first rendered, via this code:  \njavascript\nconst getMedia = async (constraints) => {\nlet stream = null;\ntry {\nstream = await navigator.mediaDevices.getUserMedia(constraints);\n// stream is then passed to the recognizer\n} catch (err) {\n/* handle the error */\nalert(err);\nconsole.log(err);\n}\n};  \nWe define the audio configuration to read the stream defined above. This is done with code like this:  \njavascript\n// configure Azure STT to listen to an audio Stream\nconst audioConfig = AudioConfig.fromStreamInput(stream);  \nThen we put it all together in one recognizer:  \njavascript\nconst recognizer = new sdk.SpeechRecognizer(speechConfig, audioConfig);  \nAnd create callbacks for when continuous recognition is running:  \njavascript\nrecognizer.recognizing = (s, e) => {\n// uncomment to debug\n// console.log(`RECOGNIZING: Text=${e.result.text}`)\nsetRecognisingText(e.result.text);\ntextRef.current.scrollTop = textRef.current.scrollHeight;\n};\nrecognizer.recognized = (s, e) => {\nsetRecognisingText(\"\");\nif (e.result.reason === sdk.ResultReason.RecognizedSpeech) {\n// uncomment to debug\n// console.log(`RECOGNIZED: Text=${e.result.text}`)\nsetRecognisedText((recognisedText) => {\nif (recognisedText === \"\") {\nreturn `${e.result.text} `;\n} else {\nreturn `${recognisedText}${e.result.text} `;\n}\n});\ntextRef.current.scrollTop = textRef.current.scrollHeight;\n} else if (e.result.reason === sdk.ResultReason.NoMatch) {\nconsole.log(\"NOMATCH: Speech could not be recognized.\");\n}\n};\nrecognizer.canceled = (s, e) => {\nconsole.log(`CANCELED: Reason=${e.reason}`);\nif (e.reason === sdk.CancellationReason.Error) {\nconsole.log(`\"CANCELED: ErrorCode=${e.errorCode}`);\nconsole.log(`\"CANCELED: ErrorDetails=${e.errorDetails}`);\nconsole.log(\n\"CANCELED: Did you set the speech resource key and region values?\"\n);\n}\nrecognizer.stopContinuousRecognitionAsync();\n};\nrecognizer.sessionStopped = (s, e) => {\nconsole.log(\"\\n    Session stopped event.\");\nrecognizer.stopContinuousRecognitionAsync();\n};  \nThen, whenever we want the recognizer to run, we run:  \njavascript\nrecognizer.startContinuousRecognitionAsync();  \nAnd, to stop it we run:  \njavascript\nrecognizer.stopContinuousRecognitionAsync();  \nYou can find help information on how this works in Azure Speech to Text \u2014 Use continuous recognition  \nChallenges  \nOne challenge that this solution addresses is to enable applications to interact with the user in real-time, by speaking to the application. Speaking feels natural, is faster, and gives the user quicker responses.  \nAzure Speech to Text is accurate, can support several languages (future solutions added to the playbook will demonstrate multiple languages - this solution works in English only).  \nFurthermore, as more applications expect the user to enter their requests in natural language, typing has become more tedious and verbose, and by giving the users the ability to speak instead of typing, can lead to more productive applications.  \nWe are focusing on front-end solutions, because most of the speech applications at this moment are based on front-end technologies, and most front-end applications use Javascript or Typescript-based frameworks, such as React.  \nDevcontainers  \nThe repository leverages Devcontainers.  \nDevcontainers is an open-source extension provided by Microsoft that enables developers to create a development environment that is consistent across multiple platforms.  \nDevcontainers ensures that all the required dependencies are installed in the development environment, which saves developers the hassle of installing dependencies on their local machine.  \nReferences  \nhttps://github.com/Azure-Samples/real-time-transcription-simple/blob/main/webapp/src/Components/Transcription.js  \nGithub codespaces - Using Codespaces and Devcontainers  \nAzure Speech-to-Text Continuous Recognition - Azure documentation on Continuous Recognition  \nSpeech to Text Technology Guidance",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\cognitive-services\\real-time-transcription\\index.md"
    },
    {
        "chunkId": "chunk318_0",
        "chunkContent": "Data Obfuscation Pipeline Solution for scanned forms and images  \nIntroduction  \nThe Data Obfuscation Pipeline Solution provides data redaction and data obfuscation\ncapabilities for unstructured data such as forms scanned as images and PDFs.  \nData Redaction is the process of masking sensitive information with a\nuniform-colored box, making the data useless to malicious actors.  \nData Obfuscation is the process of replacing sensitive information with\nrealistic synthetic data, making the data useless to malicious actors, but\nuseful to developers and testers.  \nSolution Overview  \nThis solution includes a pipeline and all components required to redact / obfuscate a large number of documents via a batch process at scale.  \nSample document redaction & obfuscation  \nGiven a document such as a W2 form:  \nand a known set of PII fields:  \njson\n{\n\"name\": \"DOCTORED B. MONEY\",\n\"address\": \"80 WORKHOURS WAY SLEEPLESS HOLLOW, NY, 11222\",\n\"ssn\": \"123-45-6879\"\n}  \nData redaction  \nMask ALL INSTANCES of the known PII fields with a masking box.  \nData obfuscation  \nObfuscate ALL INSTANCES of the known PII fields with synthetic and appropriate\nvalues, with indistinguishable styling.  \nWorkflow  \nThe document processing pipeline consists of several mandatory and optional\nsteps to support both redaction and obfuscation. Document(s) and associated\nknown PII are inputs to the solution. The solution will provide\nredacted/obfuscated document(s) as output.  \nData redaction workflow  \nRead document and convert to image format  \nExtract text from document using OCR  \nAdjust document rotation  \nDetect PII in the document based on the OCR results and given PII data  \n(Optional) Combine closely located bounding boxes  \nCV-preprocessing  \nRedaction (masking)  \nRestore original rotation  \nThe workflow for Obfuscation is similar. Extra steps are highlighted in\nbold.  \nData obfuscation workflow  \nRead document and convert to image format  \nExtract text from document using OCR  \nAdjust document rotation  \nDetect PII in the document based on the OCR results and given PII data  \nGenerate synthetic identities to replace PII with  \n(Optional) Combine closely located bounding boxes  \nCV-preprocessing  \nRedaction (masking)  \nRecognize font and font attributes  \nFind font in library (optional, see fonts for more information)  \nImprint synthetic values  \nRestore original rotation  \nGetting Started  \nRun in Azure / batch processing  \nThe below links provide instructions on how to configure an Azure\nenvironment to process a batch of documents and how to work with\nthe Azure solution locally.  \nAzure Deployment  \nLocal Azure Development  \nRun locally using Docker  \nThe below link provides details on how to run locally to explore &\nextend the underlying data redaction & obfuscation Python library/code.  \nInstructions for running locally",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\cognitive-services\\redacting-and-obfuscating-scanned-forms-and-images\\index.md"
    },
    {
        "chunkId": "chunk319_0",
        "chunkContent": "author: shanepeckham\ntitle: Automating and Monitoring ML Model Development\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Enterprise customers continue to advance their ML capabilities to improve their operational efficiency and automation. We have seen developers face the most friction when scaling these capabilities across an enterprise. This solution is designed to help Software Engineers and Data Scientists to create repeatable ML experiments, reusable code, and scalable deployments of their ML models. The templates and guidance are based on the experiences of multiple large enterprise engagements.\nrings:\n- public  \nAutomating and monitoring ML model development  \nOur customers build ML capabilities to improve operational efficiency and automation. When scaling these capabilities across an enterprise developers face the most friction. This solution helps software engineers and data scientists create repeatable ML experiments, reusable code, and scalable deployments of their ML models.  \nBusiness problem  \nTraditional software development is not well-suited to the unique requirements of machine learning models. Often, machine learning models are built in a research environment and then manually moved to production, where they need to be maintained. So, manual deployment:  \nLeads to inefficiencies in deployment and management.  \nLimits reproducibility and transparency of ML models.  \nInhibits collaboration between data science and operations teams.  \nReduces the ability to monitor and detect drift in ML models for our customers.  \nSolution overview  \nThe broader practice of MLOps provides a framework for managing the complete lifecycle of machine learning models, from development to deployment to monitoring and maintenance. This solution uses a combination of CI/CD pipelines, ML pipelines, and cloud-based orchestration to train, retrain, and monitor machine learning models. Alongside these tools, it is important to develop a process for managing the ML lifecycle, as these tools cannot fully enforce proper ML development and operations on their own. Organizations use these tools to build accurate, scalable, and secure ML solutions that deliver business value over time.  \nValue proposition  \nFor data scientists and data engineers, this solution enables:  \nTransparent and reproducible ML model training.  \nAutomated model retraining and deployment to reduce human error and inefficiency.  \nA simplified path from experimentation to deployment of a new model.  \nScaled impact of Data Scientists who define and develop ML models.  \nStandards for governance and security of ML models.  \nLogical Architecture  \n```mermaid\n\nflowchart LR\nsubgraph Experimentation\ndirection TB\nExperimentationInMLOps([\"Experimentation in MLOps\"]) --- ExperimentationInAzureML([\"Experimentation in Azure ML\"])\nend\nsubgraph ModelDevelopment[\"Model Development\"]\ndirection TB\nMLPipelines([\"ML Pipelines\"]) --- MLTestingScenarios([\"ML Testing Scenarios\"])\nend\nsubgraph Deployment\ndirection TB\nReleasePipelines([\"Release Pipelines\"]) --- SyncAsync([\"Sync/Async MLOps Pattern\"]) --- ModelRelease([\"Model Release\"]) --- DeploymentAzure([\"Model Deployment in Azure ML\"]) --- Flighting([\"Model Flighting\"])\nend\nsubgraph InferenceAndFeedback[\"Inference and Feedback\"]\ndirection TB\nInf([\"Inference and Feedback\"])\nend\nsubgraph MLLM[\"ML Lifecycle Management\"]\ndirection TB\nDriftMonitoring([\"Drift Monitoring\"])\nend\nExperimentation o==o ModelDevelopment o==o Deployment o==o InferenceAndFeedback\n\nclick ExperimentationInMLOps \"../../../capabilities/experimentation/experimentation-in-mlops\" _self\nclick ExperimentationInAzureML \"../../../technology-guidance/mlops/working-with-azure-ml/experimentation-in-azure-ml\" _self\nclick MLPipelines \"../../../capabilities/model-development/ml-pipelines/\" _self\nclick MLTestingScenarios \"../../../capabilities/model-development/model-engineering/mlops-testing-scenarios\" _self\nclick ReleasePipelines \"../../../capabilities/deployment/release-pipelines/\" _self\nclick SyncAsync \"../../../technology-guidance/mlops/working-with-azure-ml/sync-async-mlops-patterns\" _self\nclick ModelRelease \"../../../capabilities/deployment/model-release/\" _self\nclick DeploymentAzure \"../../../technology-guidance/mlops/working-with-azure-ml/azure-ml-deployment\" _self\nclick Flighting \"../../../capabilities/deployment/model-flighting/\" _self\nclick Inf \"../../../capabilities/inference-and-feedback/\" _self\nclick DriftMonitoring \"../../../capabilities/ml-lifecycle-management/drift-and-adaptation/drift-overview\" _self\n\n```  \nSolution building blocks  \nThis solution is based on building real MLOps solutions and incorporates capabilities.  \nStage Capability Description Experimentation Experimentation in MLOps Learn how to manage Jupyter Notebooks and Model experimentation in an MLOps framework Experimentation in Azure ML Review an example implementation of a code repo for ML development using Azure ML Model Development ML Pipelines Build a Pipeline to train a model ML Testing Scenarios Create Unit Tests for your ML Model Deployment Release Pipelines Integrate the ML pipeline into a CI/CD pipeline Sync - Async MLOps Pattern Understand when to use async jobs in AzDo for model training Model Release Learn about the various model release options Model Deployment in Azure ML How to deploy a model using Azure ML Model Flighting Use managed online endpoints to flight versions of the model for deployment ML Lifecycle Management Drift Monitoring Understand the basics of Data Drift monitoring and how to implement it  \nImplementations  \nThis example solution has been implemented in several GitHub code repos.  \nCore MLOps Templates (Azure ML)  \nThese two templates provide the code structure necessary to create a production-level automated model training pipeline. They use Azure Machine Learning and Azure Pipelines (or GitHub Actions) as the core services. Both provide example pipelines, and a folder structure suited to most ML tasks.  \nMLOps Template for Azure ML CLI v2  \nJenkins Implementation: this folder contains files necessary to implement the CLI template, using Jenkins for CI/CD  \nMLOps Template for Azure ML SDK v2  \nThe SDK (Software Development Kit) based template and CLI (Command Line Interface) based template are two different approaches to using MLOps templates.  \nHere are some differences of an SDK-based MLOps template compared to a CLI-based template:  \nCustomization: An SDK-based template allows for greater customization since it provides access to underlying code and allows engineers to modify it to suit their specific needs. This flexibility is useful when creating complex applications requiring advanced features or functionality. For example, the SDK allows conditional steps in a pipeline, as well as making querying and filtering pipeline results easier.  \nIntegration: An SDK-based template is easier to integrate into other applications, frameworks, and systems since it is built using standard programming languages and libraries. These features make it easier to incorporate into larger projects or environments.  \nPlatform independence: An SDK-based template can be used on multiple platforms, such as Windows, Linux, and macOS, without requiring any modifications. An SDK is designed to be cross-platform and used across different operating systems.  \nSimplicity: CLI-based templates are typically easier to use since they only require engineers to enter commands into the command-line interface. Developers with less programming experience may find this method more accessible.  \nFast prototyping: CLI-based templates can be used for quick prototyping of ideas and do not need complex programming. Proof-of-concept applications or exploring new ideas can benefit from the quick turnaround.  \nAutomation: CLI-based templates can be automated to perform repetitive tasks quickly, such as building and deploying applications. This automation  saves engineer's significant amounts of time and effort in the development process.  \nNon-Python development: Both SDK and CLI-based MLOps templates are built on top of Azure Machine Learning provider REST API. Details of these REST APIs are available at AML REST API. CLI-based templates or a direct REST API should be used for non-python based development.  \nSome more comparison details are available in the official documentation - SDK VS CLI.  \nThe choice between an SDK-based or CLI-based template depends on the needs of the project and the expertise of the engineers involved. Both approaches have their own strengths and engineers should evaluate each option carefully before deciding.  \nMLOps Model Factory Template  \nLink to Template  \nThe MLOps Model Factory template helps to deploy a model platform for automatically building, training, and deploying ML models at scale. It includes features for creating and managing large numbers of ML models, and automates the model building process.  \nModel Data: Each Model can define its own data source and data with no linkage to other models.  \nModel framework: Each Model can be based on its choice of framework. Models can be built using widely used frameworks like PyTorch, Keras and Scikit-learn among others.  \nDependency: Each Model can determine its own environment and includes its required dependencies into it.  \nModel pipeline: Each Model can have its own customized pipeline. The template code includes data preparation, transformation, Model training, scoring, evaluation and registration.  \nPath to production: Each Model can evolve separately and differently across time.  \nDatabricks Templates  \nThese templates provide similar capabilities as the Azure ML templates, but use Databricks as the main compute interface for training the machine learning models:  \nDatabricks MLOps Template based on Workflows and Repos  \nLearn More  \nIf this is your first time exploring MLOps, the bulk of this information is meant to explain and explore MLOps; consider reviewing more of the capabilities and guidance before implementing this solution. Good places to start include:  \nMLOps 101 is our high-level overview of MLOps and why it matters.  \nWorking with Azure ML is an overview of the Azure ML service, which is crucial to several implementations.  \nDatabricks vs Azure ML breaks down the key features and differences between the Azure Machine Learning service and Databricks.  \nFor customers who are using Databricks:  \nMLOps in Databricks using Workflows and Repos uses Databricks features like Workflows and Repos to build an end-to-end MLOps process.  \nMLOps in Databricks using MLFlow Projects applies MLOps best practices in Databricks using MLFlow Projects and job clusters.  \nDatabricks Step Configuration integrates a Databricks workload into an Azure ML pipeline.",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\custom-machine-learning\\automating-model-training\\index.md"
    },
    {
        "chunkId": "chunk320_0",
        "chunkContent": "author: shanepeckham\ntitle: MLOps Model Factory\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Enterprise customers continue to advance their ML capabilities to improve their operational efficiency and automation. Often, developers face challenges in scaling these capabilities across an enterprise in a repeatable, reliable manner. Enterprises want to manage multiple ML Models along with their versions, lineage and trace. They want to deploy ML Models consistently across environments, optimize resource cost, automate repetitive tasks. They want a standardized framework across their suite of ML Models for better manageability and operations.\nrings:\n- public  \nMLOps Model Factory  \nOur customers build ML capabilities to improve operational efficiency and automation. Developers often face challenges in scaling these capabilities across an enterprise in a repeatable, reliable manner. Enterprises want to manage multiple ML Models along with their versions, lineage and trace. They want to deploy ML Models consistently across environments, optimize resource cost, automate repetitive tasks. They want a standardized framework across their suite of ML Models for better manageability and operations. The following solution helps software engineers and data scientists to use Azure ML and Azure DevOps to create these items:  \nRepeatable ML experiments for multiple ML Models  \nReusable code across ML Models  \nScalable deployments of their ML Models  \nBusiness Problem  \nTraditional software development is not well suited to the unique requirements of machine learning models. The models can change rapidly as new data becomes available and as model performance evolves. Often, machine learning models are developed in a research environment and then moved to a production environment manually. Because models need to be maintained and updated in production, this manual method of deployment leads to:  \nInefficiencies in deployment and management.  \nLimited reproducibility and transparency of ML models.  \nInhibited collaboration between data science and operations teams.  \nReduced ability to monitor and detect drift in ML models.  \nSolution overview  \nMLOps Model Factory provides a structured platform for generating, organizing, versioning, and tracking multiple ML models. It makes it easier to manage and keep track of models throughout their lifecycle. Implementing MLOps Model factory addresses the following challenges:  \nModel management complexity  \nScalability  \nDeployment consistency  \nCollaboration  \nGovernance  \nExperimentation  \nContinuous improvement  \nCost optimization  \nIt streamlines the management and deployment of multiple models. The factory improves efficiency, reliability, and agility across environments.  \nValue proposition  \nMLOps Model Factory provides help with:  \nEnabling transparent and reproducible ML model training for many models.  \nAutomating model retraining and deployment to reduce human error and inefficiency.  \nSimplifying the path from experimentation to deployment of a new model.  \nScaling Model experimentation for Data Scientists who define and develop ML models.  \nApplying standards for governance and security of ML models.  \nLogical Architecture  \nThe MLOps process the Model Factory follows is:  \n```mermaid\n\nflowchart LR\nsubgraph Infrastructure\ndirection TB\nBuildCloudInfrastructure([\"Build Cloud Infrastructure\"]) --- BuildEdgeInfrastructure([\"Build Edge Infrastructure\"])\nend\nsubgraph Experimentation\ndirection TB\nExperimentationInMLOps([\"Experimentation in MLOps\"]) --- ExperimentationInAzureML([\"Experimentation in Azure ML\"])\nend\nsubgraph ModelDevelopment[\"Model Development\"]\ndirection TB\nMLPipelines([\"ML Pipelines\"]) --- MLTestingScenarios([\"ML Testing Scenarios\"])\nend\nsubgraph ModelPackaging[\"Model Packaging\"]\ndirection TB\nPackagePipeline([\"Package Pipeline\"]) --- BuildBundle([\"Build Bundle Docker Image\"]) --- TestBundle([\"Test Bundle\"]) --- StoreBundle([\"Store Bundle\"])\nend\nsubgraph Deployment\ndirection TB\nReleasePipelines([\"Release Pipelines\"]) --- ModelRelease([\"Model Release\"]) --- DeploymentEdge([\"Model Deployment on Edge ML\"]) --- SmokeTest([\"Model Test on Edge\"])\nend\nsubgraph InferenceAndFeedback[\"Inference and Feedback\"]\ndirection TB\nInf([\"Inference and Feedback\"])\nend\nsubgraph MLLM[\"ML Lifecycle Management\"]\ndirection TB\nDriftMonitoring([\"Drift Monitoring\"])\nend\nInfrastructure o==o Experimentation o==o ModelDevelopment o==o ModelPackaging o==o Deployment o==o InferenceAndFeedback\n```  \nImplementation  \nThe MLOps Model Factory implementation is built on Azure DevOps and Azure Machine Learning. It is possible to extend the code to execute in GitHub or GitLab by writing workflows for the same.  \nMLOps Model Factory provides a systematic approach to generating and managing machine learning models for a production environment. It ensures that models are built consistently, reliably, and efficiently across different environments and platforms. It enables organizations to scale their machine learning workflows by automating repetitive tasks, optimizing resource allocation, and streamlining the deployment process. It helps in efficiently managing computational resources and handling large-scale machine learning workloads.  \nLearn more  \nTo learn more, refer to these articles:  \nCase Study \u2013 Accelerating ML Model Development and Deployment on the edge using MLOps Model Factory Accelerator  \nUnderstanding production AI building blocks  \nMLOps: Model management, deployment, and monitoring with Azure Machine Learning  \nSetting up MLOps with Azure DevOps  \nAzure Machine Learning  \nAzure Pipelines  \nCreate a service principal using the Azure portal  \nAzure Machine Learning SDK v2",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\custom-machine-learning\\mlops-model-factory\\index.md"
    },
    {
        "chunkId": "chunk321_0",
        "chunkContent": "Streamlining Feature Management through Automation  \nBusiness Problem  \nIn the realm of machine learning operations, organizations often grapple with challenges in feature management. The lack of automation in feature extraction and registration processes, difficulties in feature sharing across teams, and complexities in feature engineering are common issues.  \nDependency on manual process of obtaining data.  \nLack of automation in data ingestion, cleaning, and transformation processes can lead to inefficiencies.  \nDifferent teams may have varied processes and tools to obtain features, lacking a common interface to share features across teams.  \nCreating new features requires a standardized validation process to ensure quality and relevance.  \nThese challenges highlight the necessity of automating the process of data ingestion prior to feature engineering, along with establishing a standardized repository for the discovery and registration of features.  \nSolution  \nThe solution to these challenges lies in the integration of an automated data pipeline which automates the prior process of data ingestion, cleaning, and transformation to extract features; and a managed feature store service to store features and allow sharing.  \nThe Azure Machine Learning managed feature store facilitates feature engineering and sharing. Additionally, it provides a range of functionalities for feature management such as versioning of feature sets, online/offline materialization, and more.  \nValue Proposition  \nThe integration of a comprehensive feature pipeline and a managed feature store offers several value propositions:  \nAutomates the process of data ingestion, cleaning, and transformation.  \nFacilitates feature sharing across teams, enhancing collaboration within the organization.  \nSimplifies the process of feature engineering by providing a centralized store for machine learning features.  \nEnhances the reliability of machine learning models by ensuring consistency in the features used for model training and inference.  \nLogical Architecture  \n```mermaid\ngraph LR\nsubgraph Pipeline\nA[Data Ingestion] --> B[Data Cleaning]\nB --> C[Data Transformation]\nC --> D[Feature Registration]\nend\n\nF[Train Model A]\nG[Train Model B]\nH[Train Model ...]\nI[Inference Model A]\nJ[Inference Model ...]\n\nD --\"Feature Registration\"--> E[Feature Store]:::featurestore\n\nE --\"Feature Retrieval\"--> F\nE --\"Feature Retrieval\"--> G\nE --\"Feature Retrieval\"--> H\nE --\"Feature Retrieval\"--> I\nE --\"Feature Retrieval\"--> J\n\nclassDef featurestore fill:#f96,stroke:#333,stroke-width:4px;\n```  \nImplementations  \nFeature Engineering on Microsoft Fabric  \nLink  \nThis implementation shows a feature engineering system using AML Managed Feature Store and Microsoft Fabric.  \nFeature engineering is a crucial process in machine learning where domain knowledge is used to extract features from raw data. These features are then used to train models for predicting values in relevant business scenarios.  \nThe system architecture involves a data pipeline running on Microsoft Fabric that lands, ingests, and transforms incoming public NYC taxi data into features. These features are built, registered, and stored in AML Managed Feature Store, and are used for model training and inferencing. The data pipeline is tracked and monitored by Azure Purview, which also captures and stores the feature lineage.  \nLearn More  \nData Pipeline: Designing and building Data Pipelines  \nFeature Management: What is a Feature Store?  \nAzure ML Managed Feature Store: https://learn.microsoft.com/azure/machine-learning/concept-what-is-managed-feature-store?view=azureml-api-2  \nFabric: https://learn.microsoft.com/fabric/get-started/microsoft-fabric-overview",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\custom-machine-learning\\streamlining-feature-management\\index.md"
    },
    {
        "chunkId": "chunk322_0",
        "chunkContent": "author: shanepeckham\ntitle: Data Discovery Solution for Unstructured Data\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Every machine learning project requires a deep understanding of the data. Successful ML solutions need training data that is representative of real-world data. Data must also be sufficient to deliver the desired outcome. As ML teams learn about the data, they get a sense of what outcomes are possible and which approaches to follow on the project.\nrings:\n- public  \nData Discovery solution for unstructured Data  \nEvery machine learning project requires a deep understanding of the data. Successful AI solutions need training data that is representative of real-world data. Data must also be sufficient to deliver the desired outcome. As AI teams learn about the data, they get a sense of what outcomes are possible and which approaches to follow on the project.  \nAI teams learn about data during Exploratory Data Analysis (EDA). During this phase, the data is cleaned, outliers are identified, and the suitability of the data is assessed. This work helps teams create initial hypotheses and plan experiments to test them.  \nBusiness problem  \nCustomers have large volumes of unstructured data such as documents, scanned forms, videos and images that contain a wealth of information. Performing EDA on such unstructured data is very challenging and time consuming. There are many approaches to follow, and many tools to utilize. Exploring and analyzing data can easily involve a data science team for weeks or months. Using the Data Discovery Toolkit which uses various Machine Learning techniques to discover insights in unstructured data helps streamline EDA for unstructured data.  \nSolution  \nUnstructured data consists of artifacts like documents, images and videos. Examples include content from news and social media sites and legal discovery content.  \nThe repo provides well-documented code that is ready to use.  \nValue Proposition  \nThe Data Discovery toolkit provides value in many areas:  \nAccelerates time to value:  \nAccelerating the Exploratory Data Analysis (EDA) phase.  \nValidating that the data is representative of the business problem.  \nRapidly providing a labeled dataset for ML experimentation.  \nEnables EDA at scale:  \nProviding Azure Synapse Notebooks to allow performant access to large volumes of data by taking advantage of the in-memory and distributed nature of Azure Synapse and Spark.  \nFor scaling MLOps best practices gained from multiple deployments and tracks all experiments, parameters and hyperparameters.  \nCommoditizing common data science functions for consistency and in cases where no data scientist is available.  \nMakes tools and analysis accessible by:  \nAllowing a domain expert or product owner to rapidly access the data to see broader patterns and insights.  \nAllowing non-technical interactive access to the data.  \nFacilitating communication between customer and project teams.  \nLogical architecture  \nThe Data Discovery toolkit breaks EDA down into a sequence of easily understood tasks, as shown in the following logical architecture diagram.  \nImplementation  \nThe Data Discovery toolkit is provided as a GitHub repo that you can clone and begin using:  \n:fontawesome-brands-github: View GitHub repo{ .md-button .md-button--primary }  \nThe Data Discovery Solution provides code to quickly discover data. This discovery is usually part of the Exploratory Data Analysis phase of the project. The overall approach is to take a large unstructured dataset that has no labels available. Then, to iterate over the data using various techniques to aggregate, cluster, and ultimately label the data in a cost effective and timely manner. Labeling is achieved by using these processes:  \nUnsupervised ML clustering algorithms  \nHeuristic approaches  \nDirect input and validation by a domain expert  \nAsking questions of the data in natural language is also possible, if text based, using semantic search features in Azure AI Search.  \nBy combining these approaches, you can apply structure and labels to large datasets so the data may either be indexed for:  \nDiscovery via a search solution such as Azure AI Search.  \nTraining a supervised ML model to be trained so that future unseen data can be classified accordingly.  \nThe following list illustrates this approach at a high level for a text-based problem where large amounts of unstructured data exist.  \nCluster and explore the data quickly in the generated interactive Power BI report.  \nAsk specific questions of your data from within the Synapse notebooks using Azure AI Search and Azure SynapseML.  \nAssess the data to determine whether some simple heuristics may be applied to classify the data with a semantically relevant term (see the Heuristics notebook).  \nApply the heuristic classification to the underlying data and remove the data from the larger corpus that could be classified.  \nRun text clustering in the remaining data and generate word clouds; iterate until an ideal number of clustered data appears and the clusters make sense to a domain expert.  \nDomain expert assesses the word clouds in more detail, and makes obvious corrections to word clouds by programmatically moving terms between clusters.  \nDomain expert labels the clusters with a semantically relevant term, which is programmatically propagated to the underlying records within the dataset.  \nMerge steps 1 and 6, which now allows for a classification model to be trained.",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\data-discovery\\data-discovery-and-classification-for-unstructured-data\\index.md"
    },
    {
        "chunkId": "chunk323_0",
        "chunkContent": "Alternate Reference Designs  \n1. Decentralized Gateway  \nOne approach is to create a common utility library that contains the GenAI standards of the company. It can then be packaged as a lightweight container image that does the following:  \nLoad balances queries across multiple Azure OpenAI (AOAI) instances (using Managed Identities for authentication)  \nA thin wrapper on top of the API calls that can log the generated prompts, output, and token usage to a data store destination (Cosmos DB, Log Analytics, etc.)  \nAdditional useful functions such as APIs for sending user feedback (comments, thumbs up, thumbs down, etc.)  \nThis container image can then be deployed as a gateway by individual use-cases (or at Business Unit (BU) levels). In this approach there isn\u2019t a big, centralized gateway managed by the platform team, but multiple gateways that follows the enterprise\u2019s standards and convention.  \nPros Cons Cost of the GenAI gateway is spread across the use cases (no shared GenAI gateway cost) Load balancing is per use case, not true load balancing across use cases. No APIM subscription key required since developers can use the utility library directly. DevOps maturity is expected, as use cases need to redeploy the latest container version when there is an update  \nThis approach was used in one of Microsoft's banking customer's in Asia. Instead of using Container Images, they used Azure Functions which is deployed to different use cases using Azure DevOps and Terraform.  \n2. Other Custom Solutions  \nPowerProxy for Azure\nOpenAl  \nAOAI Service Multi-tenant Load Balancing and Token Per\nMinute Tracking via Prometheus\nMetrics  \nBoost up 4x Request per minute for your AOAI\nResources  \n3. References  \nLoad Balancing AOAI using Application\nGateway  \nLoad Balance request across multiple OpenAI deployment models via\nL7 LB App\ngateway",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\genai-gateway\\alternate-ref-designs.md"
    },
    {
        "chunkId": "chunk324_0",
        "chunkContent": "Reference Design for Key Individual Capabilities  \nThe selection below describes the reference design for key GenAI gateway\ncapabilities as discussed in this document leveraging Azure API\nManagement (APIM) service as a foundational technology.  \n1. Scalability  \n1.1 Supporting High Consumer Concurrency  \nThe Premium tier of APIM provides the capability\nto extend a single APIM instance across multiple Azure regions.\nConsequently, a single Premium Tier APIM service instance is equipped to\nsupport multi-region, multi-Azure AOAI account configurations,\nfacilitating efficient traffic routing across various regions and\nensuring support for high consumer concurrency.  \nThe below diagram illustrates this setup, where APIM efficiently routes\ntraffic to multiple AOAI instances, deployed in distinct regions. This\ncapability enhances the performance and availability of the service by\nleveraging geographical distribution of resources.  \nMore info about multiple regions can be found\nhere.  \nFigure 1: Handling High Consumer Concurrency  \nScenario: Managing spikes with Provisioned Throughput Units (PTUs) and Pay As You Go (PAYG) endpoints  \nThis diagram shows the implementation of spillover strategy which\ninvolves initially routing traffic to PTU-enabled deployments, but in\ncases where PTU limits are reached, the overflow is redirected to TPM\n(Tokens Per Minute)-enabled Azure OpenAI (AOAI) endpoints, ensuring all\nrequests are processed.  \nMore information can be found\nhere.  \nFigure 2: Managing Spikes on PTUs with PAYG  \n1.2 Load Balancing across Multiple AOAI Instances  \nA simple round robin load balancing can be configured using APIM.\nAn APIM policy which distributes traffic across several AOAI\ninstances can be configured to achieve scale. Refer to this\narticle\nfor a detailed implementation of round robin load balancing in APIM.\nAlso, a random load balancing approach can be found\nhere.  \n2. Performance Efficiency  \n2.1 Quota Management for Consumers  \nAPIM policies can be leveraged to rate limit based on RPM and TPM.\nDifferent rate limit values can be set for different use cases based on\ntheir subscription IDs. In the below policy snippet, rate limiting is\ndone based on both RPM and TPM and throttling is expected when either of\nthese limits is crossed.  \n2.1.1 Rate Limit based on TPM consumption  \nThrottling can be implemented based on token consumption as well. This\ncode snippet shows the XML policy defined in APIM to set up a rate limit\nbased on the number of calls per minute (TPM - Transactions Per Minute).\nThe value used for counter-key concatenates the subscription ID with \"tpm\" to\nuniquely identify the limit. The increment-condition value increments the\ncount only if the HTTP response status code falls within the 200 to 399\nrange, and it tracks the usage by extracting the total tokens from the\nresponse body. Additionally, it specifies header names for remaining TPM\nand total TPM counts to be included in the response.  \n```xml\n\n())\"\nremaining-calls-header-name=\\\"remainingTPM\\\" total-calls-header-name=\\\"totalTPM\\\" >\n```  \nTo mitigate the AOAI quota limits,\nretries\nbecome an essential tool to ensure service availability. Given the fact\nthat the request throttling happens for a window of a few seconds or\nminutes, a retry strategy with exponential backoffs can be implemented\nat the Gateway layer to ensure that the request is served for the\nconsumers.  \nRefer this link for more Sample APIM Policies for\nAOAI  \n3. Security and Data Integrity  \n3.1 Authentication  \nAPIM in conjunction with AOAI provides several\noptions for authentication including API keys, Managed Identities and\nService Principal Managed identity approach can be leveraged to\nauthenticate between APIM and managed identity supported backend Azure\nservices.\nThe Managed Identity can be given the right access -- \"Cognitive Service\nUser\" to the AOAI instance as mentioned\nhere.\nAPIM then transparently authenticates to the backend i.e. AOAI.  \n3.2 PII and Data Masking  \nThis diagram depicts how PII detection and data masking is enabled using\nGenAI Gateway. Upon receiving a data request, the information is\ntransmitted to an Azure function for PII detection. This function has\nthe capability to employ Azure Text Analytics or a custom machine\nlearning model to identify PII data. The detected information is then\nutilized to mask the request, and the masked data is forwarded to Azure\nAPIM, which subsequently sends it to AOAI.  \nFigure 3: PII and Data Masking  \n3.3 Data Sovereignty  \nThis diagram depicts how data is restricted to customer specific regions\nusing GenAI Gateway. Here we have two regions, and each region hosts\nAI-Enabled application, APIM and AOAI. Traffic is routed to\nregion specific APIM and OpenAI using Traffic manager. APIM helps with\nrouting requests to the region-specific Azure OpenAI instance.  \nTo know more about APIM multi-regional deployment, please refer to this\ndocument.  \nFigure 4: Data Sovereignty via Multiple APIMs  \nFigure 5: Data Sovereignty via Multi-instance APIM  \nTo mitigate the AOAI quota limits,\nretries\nbecome an essential tool to ensure service availability. Given the fact\nthat the request throttling happens for a window of a few seconds or\nminutes, a retry strategy with exponential backoffs can be implemented\nat the gateway layer to ensure that the request is served for the\nconsumers.  \n4. Operational Excellence  \n4.1 Monitoring and Observability  \nAzure Monitor Integration  \nBy using the native integration of APIM with Azure Monitor, the\nrequests, and responses (payload) as well as APIM metrics can be logged\ninto Azure Monitor. In addition, Azure Monitor can be used to collect\nand log metrics from other Azure services like AOAI making it the\ndefault choice for monitoring and observability.  \nFigure 6: Monitoring using Azure Monitor  \nWhile configuring Azure Monitor provided a low-code/no-code way of\ngenerating insights, this approach has the following possible\nlimitations:  \nAzure monitor may introduce latency ranging from 30 sec to 15 mins\nfor its consumers intending to consume this information. This\nlatency factor can be a crucial aspect to account for real-time\nmonitoring and decision-making processes.  \nEmploying Azure Monitor for capturing request/response payload\nrequires configuring the desired sampling rate in APIM. A high\nsampling rate has potential impact on throughout of\nAPIM\nand increased latency for consumers.  \nWhen dealing with large payload in the request or response, it's\ncrucial to be aware that the complete data may not be logged since\nAPIM imposes a limitation on the log size limit at Azure Monitor\nwhich be logged to 9182 bytes.  \nMonitoring via Custom Events  \nFigure 7: Monitoring using Custom Events  \nIn this approach, requests, responses and other data from Azure API\nManagement (APIM) can be logged as custom\nevents\nto a messaging system like Event Hub. The event stream from Event Hub can be\nconsumed by other services to perform various operation like\ndata aggregation in near-real-time fashion, generating alerts or\nperforming other actions.  \nWhile this approach offers more near-real-time experience, it requires\nwriting custom aggregation services.  \n5. Cost Optimization  \n5.1 Tracking Consumption  \nNon-Streaming endpoints  \nA Product in APIM can have multiple subscriptions. Metrics at a product\nlevel can be tracked and aggregated for consumption at consumer level.\nTo measure the consumption, the response body of the payload that has\nthe total token count can be logged to Azure Event Hub.  \nThese techniques have been discussed in maximizing PTU and Monitoring\nsections.  \nStreaming endpoints  \nThere is no built-in option for tracking consumption within streaming\nendpoints with APIM as of now. However, there are alternatives.  \nOne alternative has been discussed here  \nWorkaround presented in the article:  \nFigure 8: Handling Streaming  \nThe other alternative is PowerProxy, a low-latency, open-source gateway based on Python and available on GitHub.",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\genai-gateway\\designs-key-individual-capabilities.md"
    },
    {
        "chunkId": "chunk325_0",
        "chunkContent": "GenAI Gateway Reference Architectures  \n1. Reference Architectures using Azure API Management  \nThis section presents the reference architectures of a GenAI gateway for an\nenterprise that needs to access both Azure OpenAI (AOAI) resources and the\ncustom LLM deployments on their own premises. There could be many\npossible ways to design a GenAI gateway using a combination of various\nAzure services. This section demonstrates leveraging Azure API\nManagement (APIM) Service as the main component to build the necessary features\nfor a GenAI gateway solution.  \n1.1 Cloud based GenAI Gateway  \nThis design shows how to use APIM to create a\nGenAI gateway that smoothly integrates with AOAI services in the\ncloud and any on-premises custom LLMs that are deployed and available\nas REST endpoints.  \nFigure 1: Cloud Based GenAI using APIM  \nAPIM products and subscription features can enable\nvarious Generative AI scenarios in an enterprise. Different products can\noffer different functionalities such as creating content, producing\nembeddings, searching, etc. and subscriptions can allow different teams\nto access these functionalities.  \nScalability: APIM can use the combination of products and\nsubscription keys to intelligently route the load to Azure OpenAI\nProvisioned Throughput Units (PTUs) and  Pay As You Go (PAYG) instances. This section deep dives into the implementation details.  \nPerformance Efficiency: Using APIM policies, it is possible to apply rate limits for subscription on their TPM consumption. This section covers the implementation details of this feature.\nAnd also by orchestrating Azure Monitor, custom Azure Functions, it is possible to prioritize one request over another amongst various client applications.  This section covers various methods to handle prioritization and effective PTU utilization.  \nSecurity and Data Integrity: APIM can abstract consumers from complexity of authenticating against AOAI endpoint or custom LLM deployments. Authentication can be done by the gateway itself either through OAuth or an access key. Implementation of keyless authentication, handling PII data using APIM is covered here.  \nOperational Excellence: On-premises LLM deployments can be integrated with APIM through VNet. APIM can emit custom metrics to Azure Monitor to enable observability of the applications consuming LLM resources. This approach is elaborated here  \nCost Optimization: APIM can record the consumption of AOAI resource in Azure Monitor for each subscription and product, which allows allocating the costs to the right business unit within the organization. This section covers the implementation details.  \n1.1.1 Key Considerations  \nOne important thing to keep in mind with this method is that the gateway\ncomponent is on the cloud. This means that the Azure network has to\nprocess every request before any of the gateway policies apply and this\ncan impact the total latency when other services are running on-premises. Besides that, if there are LLM models\ndeployed on-premises, then the right network setup has to be there to\nenable inbound connection from the gateway to on-premises network.  \n1.2 On-Premises GenAI Gateway using APIM Self Hosted Gateways  \nMany enterprises would like to leverage existing in-house capabilities\nwhile also having network constraints to allow inbound connection from\nAzure to their internal network.  \nAzure API Management (APIM) self-hosted gateways\ncan be used to create a GenAI gateway that seamlessly integrates with\nAOAI services as well as on-premise applications. The\nself-hosted APIM gateway acts as a crucial component, bridging AOAI services with the enterprise's internal network.  \nFigure 2: On-Premises Self-Hosted APIM Gateway  \nWith APIM self-hosted gateway, the requests from the enterprise's\ninternal network stay within the network unless they reach out to the\nAOAI resource. This approach enables all the features of the\ngateway inside the network and eliminates the need for inbound\nconnection from the cloud.  \nThe gateway can leverage any existing on-premises deployment of\nqueue\nfor scheduling requests and connect with enterprise-wide monitoring\nsystem.\nThis would enable gateway logs and metrics to be combined with existing\nconsumer application logs and metrics.  \n1.2.1 Key Considerations  \nWith this approach, the organization is responsible for deploying and\nmaintaining the self-hosted gateway. This means scaling the gateway\ncomponent horizontally to handle the load and keeping it elastic to\nhandle surges in requests. If metrics and logs are sent to custom\nmetrics store, each organization will have to build their own monitoring\nand alerting solution to support dynamic scheduling of request and for\nmaking reports for chargeback.  \n2. Explore Approaches for Maximizing PTU Utilization  \nAOAI resources have a reserved capacity called Provisioned\nThroughput Units (PTUs). It is advisable to use this capacity as fully\nas possible since it is already allocated. A single PTU instance can\nsupport different use cases for an enterprise. Many of the use cases can\nbe classified as either real-time (high priority) or batch (low\npriority). High priority requests need to be handled right away, while\nlow priority requests can be delayed until later. A common scenario is\nto have more batch requests than real-time requests. In this case, batch\nrequests can easily take up all the capacity of PTU. There should be a\nconstant balancing of capacity between high and low priority requests to\nmake the most of the PTU's available capacity. A gateway should exist\nthat can optimize PTU utilization by intelligently dividing the capacity\nbetween high and low priority requests based on their ratio. The gateway\nshould be able to reduce the throughput of low priority requests to\ncreate space for and prioritize more real-time requests when they\narrive.  \nMore detailed approaches are described here.  \n3. Reference Design for Key Individual Capabilities  \nThe section describes the reference design for key GenAI gateway capabilities as discussed in this document leveraging APIM service as a foundational technology which are covered here.  \n4. Alternate Reference Designs  \nThis section describes some of the alternate design options which are covered here.  \n5. APIM Solution References  \nAPIM Sample Policies for\nAOAI  \nAPI Management policy\nreference  \nAzure-OpenAI-Enterprise-Governance  \nUtilize API Management to make Azure OpenAI load-balanced and\nredundant  \nAddressing Azure OpenAI Token\nLimitations  \nAzure APIM with AOAI Streaming\nEndpoints  \nLog events to Azure Event Hubs in Azure API\nManagement  \nEnterprise Azure\nOpenAI  \nAzure OpenAI Using PTUs/TPMs With API Management - Using the\nScaling Special\nSauce",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\genai-gateway\\index.md"
    },
    {
        "chunkId": "chunk326_0",
        "chunkContent": "Approaches for Maximizing PTU Utilization  \n1. Approach  \nIn this approach, the AI gateway must be able to maximize the use of an Azure Open AI Provisioned Throughput Unit (PTU) while also\nmaintaining good performance for high priority requests. When there are\nfewer high priority requests, the gateway should let delayed low\npriority requests take up the leftover capacity of PTU and when there\nare more high priority requests, the gateway should free up some\ncapacity by reducing the speed of low priority requests.  \nYou can use Azure API Management (APIM) as part of the GenAI Gateway, and route\nrequests based on their priority. You can split the requests into high\npriority and low priority by using header information like APIM\nsubscription key or by having separate APIM endpoint for different\npriority requests. Then, you can apply different policies with APIM to\nthem. High priority requests are sent straight to Azure OpenAI resource\nto finish their execution while low priority requests are put into a\nqueue. A queue consumer will handle the requests from this queue only if\nthere is enough capacity in the PTU. When capacity is reduced, the\nqueue consumer should slow down and reduce the rate of low priority\nrequests until it stops completely when the capacity reaches a critical\nthreshold.  \nBelow we will discuss how to monitor the PTU capacity and control the\nqueue consumer's throughput in more detail.  \n1.1 Measuring PTU Utilization through Azure Monitor  \nFigure 1: Measuring PTU Utilization via Azure Monitor  \nOne approach to measure PTU utilization is by using Azure OpenAI\nresource capability to send metrics into Azure\nMonitor.\nThe key metrics we are interested in are Azure OpenAI Requests,\nProcessed Inference Tokens and Provision-managed Utilizations.  \nWith this approach, APIM is a gateway for any calls to Azure OpenAI. As mentioned above,\neach request can be classified as high priority or low priority based on\nthe header or the request path. All high priority requests go straight\nto Azure OpenAI and all low priority requests are put into Azure Event\nHub. An orchestration service handles the message from Event Hub when\nthere is capacity in PTU and returns the result from Azure OpenAI to the\nrequestor.  \nThere are two ways by which orchestration service can calculate the\ncapacity --  \nAzure Alert: Azure alerts can monitor key metrics and notify a\nstate in the orchestration service when the metrics exceed a certain\nrange. The state can help the orchestration service determine the\navailable capacity and adjust the processing speed of low priority\nrequest based on the metrics.  \nPeriodic Check of PTU availability: Orchestration service can\nregularly call Azure Metrics API to get the current usage of the\nPTU. Based on this it can find out the free capacity and change the\nthroughput of low priority requests as needed.  \n1.1.1 Key Considerations  \nOne should think about the delay that comes with Azure\nMonitor.\nIt can take between 30 seconds and 15 minutes for metrics from Azure\nOpenAI resource to be ready for querying. Azure Alert can also cause\nmore delay on its own. So, this approach works well for situations where\nthe ratio of high and low priority requests changes gradually over time\nand there is no need to balance the capacity in near real-time.  \n1.2 Measuring PTU Utilization through Custom Events  \nIf the delay in the approach using Azure Monitor is not acceptable an\nalternative approach could be to use GenAI Gateway to capture the\nmetrics and process it to calculate the utilization of the PTUs.\nNote that this approach hasn't yet been validated as part of an ISE\nengagement.  \nThis approach involves APIM generating custom events that count the\ntokens used in both input and output and sending them to Azure Event\nHub. Azure Stream Analytics can then aggregate the token usage within a\nspecified window period to estimate the PTU consumption. Azure Stream\nAnalytics can then store the consumption in a state store that can be\naccessed by the orchestration service to regulate its throughput. This\nwill let near real-time balancing of PTU utilization between different\npriorities messages.  \nFigure 2: Measuring PTU Utilization through Custom Events  \n1.2.1 Key Considerations  \nIt is important to remember that the utilization calculation is\nperformed by a different system than Azure OpenAI Metrics. This means\nthat the calculated utilization will not be the same as the real\nutilization calculated by PTU itself. Since these two are different\nsystems that use different windowing methods to calculate the\nutilization, there is no way to make custom calculation of utilization\nmatch the true utilization. The gateway and orchestration service should\ntake this into account and keep enough buffer to avoid rate limiting\nerror from Azure OpenAI Service.  \n1.3 Graceful degradation of throughput of Low Priority Request  \nAfter computing the utilization of PTU with either one of the two\nmethods, orchestration service should reduce the processing speed of low\npriority request gradually as the utilization goes up. This will help to\nmaintain a balance between high and low priority requests while making\nthe most of the PTU utilization.  \nHere are the steps to allocate PTU capacity among different priority levels of requests:  \nDetermine the maximum number of low priority requests that we can process simultaneously without compromising the high priority ones, based on the demand trends and potential peaks of our organization.  \nDefine two limits for PTU consumption: a lower one and an upper one. These will help us regulate the throughput of low priority requests depending on the available capacity.  \nChange the pace of low priority requests according to the PTU consumption. When the consumption is below the lower limit, we process them at the highest speed. When the consumption is between the lower and upper limits, we reduce the speed. When the consumption is above the upper limit, we pause the processing. The upper limit acts as a cushion to avoid being throttled by Azure OpenAI service.  \nFor example, suppose we can handle up to 10 low priority requests at the same time. And we set our lower limit at 20% and upper limit at 90% of total PTU consumption. Then we decrease the number of low priority requests we process concurrently when PTU consumption exceeds 20% and stop when it reaches 90%. This is illustrated below.  \nFigure 3: Throughput Graph",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\genai-gateway\\maximise-ptu-utilization.md"
    },
    {
        "chunkId": "chunk327_0",
        "chunkContent": "Data Enrichment for Azure Cognitive Search  \nData enrichment is the application of Machine Learning (ML) models over content that isn't fully text-searchable in its raw form. Through enrichment, analysis and inference are used to create searchable content and structure where none previously existed.  \nBecause Azure Cognitive Search (ACS) is a full text search solution, the purpose of AI enrichment is to improve the utility of your content in search-related scenarios:  \nTranslation and language detection for multi-lingual search\nEntity recognition extracts people, places, and other entities from large chunks of text\nKey phrase extraction identifies and then outputs important terms\nOptical Character Recognition (OCR) recognizes printed and handwritten text in binary files\nImage analysis describes image content and outputs the descriptions as searchable text fields  \nWhat is the Data Enrichment for Azure Cognitive Search Solution  \nA collection of end-to-end pipelines using a variety of Azure services and standalone ML models that can be invoked in isolation or as part of a data enrichment pipeline, including the Azure Cognitive Search pipeline as they adhere to the Azure Cognitive Search PowerSkills format.  \nThe Customer Pattern  \nCustomers want to be able to search across multi-modal data such as images, videos and documents using search terms that are meaningful to their user base.  \nTypical challenges faced  \nLimited understanding of the data - unstructured data can be difficult to work with  \nThe ACS pipeline can be difficult to debug and troubleshoot  \nThe ACS pipeline requires a synchronous invocation of a PowerSkill which has a timeout value  \nHow does the Data Enrichment for Azure Cognitive Search Solution help?  \nOffers a variety of alternate pre-built enrichment pipelines that can be used independent of the ACS pipeline  \nAlternate pipelines offer more control and easy of debugging and troubleshooting  \nThe PowerSkills template offers a pre-built, production ready code template allowing the Data Scientist/ML Engineer to only focus on adding custom logic  \nThe Data Enrichment for Azure Cognitive Search Process  \nSolution locations  \nAI Enrichment Pipeline tutorial - Complete sample for processing text, image and video files through a full enrichment pipeline with event grid, service bus, functions, logic apps, cognitive services and video indexer  \nImproving Video Search using AI - A full solution that automates the analysis of videos with Azure Video Indexer, creates and improves the search index with Video Indexer insights and optional metadata. Provides a web UI application to search and see results and logs the behavior and feedback of users (in order for the admin or data science team to improve the search index). Lastly it also provides a dashboard which displays statistics on platform usage and search performances.  \nAzure Cognitive Search Powerskills - Power Skills are a collection of useful functions to be deployed as PowerSkills for Azure Cognitive Search.  \nEnd to end Knowledge Mining for Video- A video discovery pipeline that includes Azure Search and user feedback.  \nEvaluating Azure Search - A deployable feedback and evaluation solution for Azure Cognitive Search.",
        "source": "..\\data\\docs\\code-with-mlops\\solutions\\search\\data-enrichment-for-azure-cognitive-search\\index.md"
    },
    {
        "chunkId": "chunk328_0",
        "chunkContent": "author: shanepeckham\ntitle: AI technology guidance\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Often guidance is needed according to the technology not to the capability. For instance, someone may want to know how models are deployed on Azure ML. To facilitate this, this section provides guidance for various technologies, including Azure ML.\nrings:\n- public  \nAI technology guidance  \nOften guidance is needed according to the technology not to the capability. For instance, someone may want to know how models are deployed on Azure ML. To facilitate this, this section provides guidance for various technologies, including Azure ML.  \nTopics include:  \nMLOps 101, an introduction to MLOps for Azure ML and Databricks.  \nWorking with Large Language Models (LLMs), a guide to effectively building applications with OpenAI and other LLMs.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\index.md"
    },
    {
        "chunkId": "chunk329_0",
        "chunkContent": "CVT Bundle Interface  \nThis library contains building blocks to make it easier to build consistent APIs using Python and Bundle interface.  \nThe Bundle format has a REST API endpoint to interface with the underlying prediction service.. CVT Edge modules will use this endpoint to call the model/traditional computer vision code and get the predictions. This REST API endpoint provides endpoints:  \n/predict: This endpoint is used to call the model/traditional computer vision code and get the predictions.  \n/metadata: This endpoint is used to get the metadata of the model/traditional computer vision code and bundle.  \n/testdata: This endpoint is used to expose sample images and expected results in as a zip file.  \n/health/live: This endpoint is used to check the status of the bundle.  \n/health/ready: This endpoint is used to check if a bundle has finished the initialization and is ready to get requests.  \nWe propose the REST interface for interaction with other CVT Modules. The endpoints are described in cvt-openapi-1.0.1.yaml swagger file.  \nAll endpoints (except /metadata, which will serve as a static endpoint) are expected to be versioned (in case we need to change the interface definition at a later stage) and will thus be prefixed with a version tag, e.g. v1.  \nBundle Interface will expose port 5000 to communicate with Orchestrator over HTTP.  \n1. predict endpoint  \n/v{version}/predict endpoint is used to call the model/traditional computer vision code and get the predictions.  \nRequest body includes a single image or images with a camera identifier for multiple camera scenarios. Since we don't have actual models/traditional computer vision code and how those can run with multiple cameras, camera information (optionally) which can be used for multiple camera scenario. We'll need further investigation after we decide how models/traditional computer vision code run with multiple cameras with different angles.  \nA sample output from /predict endpoint will have minimal information from model/traditional computer vision code prediction, data field is dynamic and it will contain classification, object detection, OCR outputs, status is the main outcome from the prediction which we can store and use in decision making. Also, bundleId, imageId are critical information to correlate data with other data at ABC.  \nImportant: those names may change and evolve during the implementation. The key principle here is to keep a small number of root level elements and a dynamic, per usage and highly customizable data object.  \nRequest body:  \n```text\nPOST /v{version}/predict HTTP/1.1\nHost: {hostname}\nContent-Type: multipart/form-data;boundary=\"boundary\"\nContent-Length: 1464\n\n--boundary\nContent-Disposition: form-data; name=\"files\"; filename=\"image1.raw\"\nContent-Type:\n\n(data)\n--boundary\nContent-Disposition: form-data; name=\"files\"; filename=\"image2.raw\"\nContent-Type:\n\n(data)\n--boundary\nContent-Disposition: form-data; name=\"files\"; filename=\"image3.raw\"\nContent-Type:\n\n(data)\n--boundary\nContent-Disposition: form-data; name=\"input_description\"\n{\n\"properties\": [\n{\n\"inputName\": \"image1\",\n\"resolutionX\": 2448,\n\"resolutionY\": 2048,\n\"format\": \"BayerBG8\",\n\"bitDepth\": 8,\n\"additionalProperties\": {\n\"color\": \"rgb\"\n}\n},\n{\n\"inputName\": \"image2\",\n\"resolutionX\": 2448,\n\"resolutionY\": 2048,\n\"format\": \"BayerBG8\",\n\"bitDepth\": 8,\n\"additionalProperties\": {\n\"color\": \"rgb\"\n}\n},\n{\n\"inputName\": \"image3\",\n\"resolutionX\": 2448,\n\"resolutionY\": 2048,\n\"format\": \"BayerBG8\",\n\"bitDepth\": 8,\n\"additionalProperties\": {\n\"color\": \"rgb\"\n}\n}\n]\n}\n--boundary--\n```  \nResponse json for classification:  \njson\n{\n\"status\": \"NOK\",\n\"score\": 0.8136,\n\"data\": {\n\"OK\": {\n\"predictionType\": \"classification\",\n\"className\": \"OK\",\n\"score\": 0.1864\n},\n\"NOK\": {\n\"predictionType\": \"classification\",\n\"className\": \"NOK\",\n\"score\": 0.8136\n}\n}\n}  \nResponse json for OCR:  \njson\n{\n\"status\": \"NOK\",\n\"score\": 0.8136,\n\"data\": {\n\"SerialNo\": {\n\"imageId\": \"e6fd3074-2af8-4634-8c42-3d5f40f61213\",\n\"predictionType\": \"ocr\",\n\"boundingBox\": \"462,379,497,258\",\n\"text\": \"12-345-6789\",\n\"score\": 0.8864\n},\n\"Brand\": {\n\"imageId\": \"e6fd3074-2af8-4634-8c42-3d5f40f61213\",\n\"predictionType\": \"ocr\",\n\"boundingBox\": \"162,79,197,58\",\n\"text\": \"ABC\",\n\"score\": 0.964\n}\n}\n}  \nResponse json for object detection:  \njson\n{\n\"status\": \"NOK\",\n\"score\": 0.8136,\n\"data\": {\n\"SCRATCH\": {\n\"imageId\": \"e6fd3074-2af8-4634-8c42-3d5f40f61213\",\n\"predictionType\": \"detection\",\n\"boundingBox\": \"12,109,107,38\",\n\"label\": \"scratch\",\n\"score\": 0.8864\n},\n\"CRACK\": {\n\"imageId\": \"e6fd3074-2af8-4634-8c42-3d5f40f61213\",\n\"predictionType\": \"detection\",\n\"boundingBox\": \"42,100,42,20\",\n\"label\": \"crack\",\n\"score\": 0.964\n}\n}\n}  \nResponse json for a mixed model including OCR and object detection:  \njson\n{\n\"status\": \"NOK\",\n\"score\": 0.8136,\n\"data\": {\n\"SCRATCH\": {\n\"imageId\": \"e6fd3074-2af8-4634-8c42-3d5f40f61213\",\n\"predictionType\": \"detection\",\n\"boundingBox\": \"12,109,107,38\",\n\"label\": \"scratch\",\n\"score\": 0.8864\n},\n\"CRACK\": {\n\"imageId\": \"e6fd3074-2af8-4634-8c42-3d5f40f61213\",\n\"predictionType\": \"detection\",\n\"boundingBox\": \"42,100,42,20\",\n\"label\": \"crack\",\n\"score\": 0.964\n},\n\"SerialNo\": {\n\"imageId\": \"e6fd3074-2af8-4634-8c42-3d5f40f61213\",\n\"predictionType\": \"ocr\",\n\"boundingBox\": \"462,379,497,258\",\n\"text\": \"12-345-6789\",\n\"score\": 0.8864\n},\n\"Brand\": {\n\"imageId\": \"e6fd3074-2af8-4634-8c42-3d5f40f61213\",\n\"predictionType\": \"ocr\",\n\"boundingBox\": \"162,79,197,58\",\n\"text\": \"ABC\",\n\"score\": 0.964\n}\n}\n}  \nNOTE:- properties of data object are dynamic and can be customized per usage. Also these properties are not required in the response, we can make them optional.  \n2. metadata endpoint  \n/metadata endpoint will provide information about bundle, model/traditional computer vision code and requirements for the environment to host and run the model/traditional computer vision code:  \nRequest:  \nGET {hostname}/metadata  \nResponse json:  \njson\n{\n\"bundle\": {\n\"description\": \"water glass detection with classification\",\n\"maintainers\": [\n{\n\"name\": \"John Doe\",\n\"email\": \"john.doe@abc.com\"\n}\n],\n\"repoUrl\": \"https://dev.azure.com/Abccompany/CVT/_git/water-glass-detection\"\n},\n\"resourceRequirements\": {\n\"requiredCpu\": 0.5,\n\"requiredMemory\": \"2GB\"\n},\n\"metadataVersion\": \"1.0.0\",\n\"numberOfCameras\": 3,\n\"inputs\": [\n{\n\"name\": \"image1\",\n\"description\": \"First camera\",\n\"type\": \"image\",\n\"recommendedSettings\": {\n\"angle\": 0,\n\"aperture\": \"5.6\",\n\"orientation\": \"top\",\n\"resolution\": \"1280x720\",\n\"shutterSpeed\": {\n\"max\": \"1/125\",\n\"min\": \"1/500\",\n\"suggested\": \"1/250\"\n}\n}\n},\n{\n\"name\": \"image2\",\n\"description\": \"Second camera\",\n\"type\": \"image\",\n\"recommendedSettings\": {\n\"angle\": 120,\n\"aperture\": \"5.6\",\n\"orientation\": \"top\",\n\"resolution\": \"1280x720\",\n\"shutterSpeed\": {\n\"max\": \"1/125\",\n\"min\": \"1/500\",\n\"suggested\": \"1/250\"\n}\n}\n},\n{\n\"name\": \"image3\",\n\"description\": \"Third camera\",\n\"type\": \"image\",\n\"recommendedSettings\": {\n\"angle\": 240,\n\"aperture\": \"5.6\",\n\"orientation\": \"top\",\n\"resolution\": \"1280x720\",\n\"shutterSpeed\": {\n\"max\": \"1/125\",\n\"min\": \"1/500\",\n\"suggested\": \"1/250\"\n}\n}\n}\n],\n\"outputs\": [\n{\n\"name\": \"quality\",\n\"type\": \"classification\"\n},\n{\n\"name\": \"defect1\",\n\"type\": \"detection\"\n},\n{\n\"name\": \"partNumber\",\n\"type\": \"ocr\"\n}\n],\n\"targetedParts\": [\n\"010.010.010.010-10\",\n\"010.010.010.010-11\"\n]\n}  \n3. testdata endpoint  \n/v{version}/testdata endpoint will provide test images and testcase status labels as a compressed package. All bundles have test images under test folder.  \nRequest:  \nGET {hostname}/v{version}/testdata  \nResponse:  \nThe response will be a zip file with images and expected status results.  \nFolder structure for testdata:  \ntestdata/  \ninputs/testcases1/image1.raw  \ninputs/testcases1/image2.raw  \ninputs/testcases1/image3.raw  \ntestcases.json  \nAll testcases and expected status are available in testcases.json file. Images can be single image or multiple images from different cameras, so test cases will support multiple images as list of images.  \njson\n{\n\"version\": \"1.0.0\",\n\"testcases\": [\n{\n\"inputs\": [\n{\n\"fileName\": \"inputs/testcase1/image1.raw\",\n\"inputProperties\": {\n\"inputName\": \"image1\",\n\"resolutionX\": 2048,\n\"resolutionY\": 2448,\n\"bitDepth\": 8,\n\"format\": \"Bayer\",\n\"additionalProperties\": {\n\"color\": \"rgb\"\n}\n}\n},\n{\n\"fileName\": \"inputs/testcase1/image2.raw\",\n\"inputProperties\": {\n\"inputName\": \"image2\",\n\"resolutionX\": 2048,\n\"resolutionY\": 2448,\n\"bitDepth\": 8,\n\"format\": \"Bayer\",\n\"additionalProperties\": {\n\"color\": \"rgb\"\n}\n}\n},\n{\n\"fileName\": \"inputs/testcase1/image3.raw\",\n\"inputProperties\": {\n\"inputName\": \"image3\",\n\"resolutionX\": 2048,\n\"resolutionY\": 2448,\n\"bitDepth\": 8,\n\"format\": \"Bayer\",\n\"additionalProperties\": {\n\"color\": \"rgb\"\n}\n}\n}\n],\n\"output\": {\n\"status\": \"NOK\",\n\"score\": 0.9999,\n\"data\": {\n\"OK\": {\n\"predictionType\": \"Classification\",\n\"className\": \"OK\",\n\"score\": 0.0001\n},\n\"NOK\": {\n\"predictionType\": \"Classification\",\n\"className\": \"NOK\",\n\"score\": 0.9999\n}\n}\n}\n}\n]\n}  \n4. health/live endpoint  \nv{version}health/live: This endpoint is used to check the status of the bundle. A response code 200 indicates that the bundle is live.  \nRequest:  \nGET {hostname}/v{version}/health/live  \nResponse:  \njson\n{\n\"status\": \"live\"\n}  \n5. health/ready endpoint  \nv{version}health/ready: This endpoint is used to check if a bundle has finished the initialization and is ready to get requests. A response code 200 indicates that the bundle is ready.  \nRequest:  \nGET {hostname}/v{version}/health/ready  \nResponse:  \njson\n{\n\"status\": \"ready\"\n}",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\ai-on-the-edge\\custom-vision-edge-bundle\\bundle-interface.md"
    },
    {
        "chunkId": "chunk330_0",
        "chunkContent": "CVT Bundle Registry  \nThe bundle registry is a central component within the Computer Vision Toolbox.\nIt allows us to develop bundles in one place and subsequently deploy them to\nmany edge devices throughout the whole deployed base. The bundle registry is a\nglobal service and therefore it needs to be highly available with very stable\nAPIs.  \nRelated Designs  \nCVT High Level Architecture  \nCVT Bundle Format  \nCVT Bundle Interface  \nAssumptions & Guardrails  \nBundles are packaged computer vision solutions that will be shipped in the\nform of docker images. Each bundle will have multiple versions, each version\nwould be a separate docker tag.  \nCVT Projects are logical wrappers around one or multiple bundles. They will be\nused as a prefix for the bundle names.  \n```shell\nCVT Project = Docker Repository Prefix\nBundle Registration = Docker Repository\nBundle = Docker Image\n\n.\n\u251c\u2500\u2500 aa [Slot]\n\u2502   \u251c\u2500\u2500 aa-project-a # [Project, Repo-Prefix, Scope Map]\n\u2502   \u2502   \u251c\u2500\u2500 aa-project-a/bundle-1 # [Bundle Registration, Repo]\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 bundle-1:1.0.0 # [Bundle, Tag]\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 bundle-1:1.1.0\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bundle-1:1.2.0\n\u2502   \u2502   \u2514\u2500\u2500 aa-project-a/bundle-2\n\u2502   \u2502       \u251c\u2500\u2500 bundle-2:1.0.0\n\u2502   \u2502       \u2514\u2500\u2500 bundle-2:2.0.0\n\u2502   \u2514\u2500\u2500 aa-project-b\n\u2502       \u2514\u2500\u2500 aa-project-b/bundle-3\n\u2502           \u251c\u2500\u2500 bundle-3:1.0.0\n\u2502           \u2514\u2500\u2500 bundle-3:1.0.1\n\u2514\u2500\u2500 ba\n\u2514\u2500\u2500 ba-project-c\n\u2514\u2500\u2500 ba-project-c/bundle-6\n\u251c\u2500\u2500 bundle-6:3.0.1\n\u251c\u2500\u2500 bundle-6:3.1.0\n\u2514\u2500\u2500 bundle-6:3.2.0\n```  \nEvery bundle will originate from one home slot  \nDesign  \nWe are using Azure Container Registry as the technology for the bundle\nregistry as it can hold docker images and provides additional functionality that\nease the process of governing bundles for us.  \nWith its scope-map and tokens functionality, we can easily manage write-access\nto the registry without using RBAC permissions. This allows us to manage access\nto bundles at runtime without having to run a new deployment for each new\nproject.  \nEach bundle will originate from one home slot. This puts each slot's\nconfiguration service in charge of managing the bundle but allows all slots to\ndownload the bundle from the registry.  \nPermissions  \nUsers will not get dedicated permissions to the bundle registry. Instead, they\ncan use scope maps to access the registry to upload new bundle versions. Some of\nthe bundle life cycle will be realized using permission flags. Updating those\nrequires the metadata/write permission. This will be explicitly not given via\nscope maps. Instead, users will only get the content/write permission via the\nscope map, allowing us to control the bundle life cycle. Users will explicitly\nnot have the content/delete permission so that the configuration service will\ngovern deletions.  \nProposed bundle naming  \nScope maps  \nScope maps allow fine grained access to image repositories. Each CVT\nimplementation project will get its own scope map and associated credentials.  \nFormat: <slot>/<project-slug>  \nExample: aa/cogwheel-quality  \nRepository / Bundle  \nFormat: <slot>/<project-slug>/<bundle-slug> or <scope-map>/<bundle-slug>  \nExample: slot/water-glass-detection/ok-nok  \nScope maps and tokens will be managed by each slot's configuration\nservice. Scope maps will be created in\na slot namespace. Bundles will be registered by adding them to a scope map,\nallowing users to upload bundles to that specific repository (or create it if it\ndoesn't exist).  \nBundle Status  \nAzure Container Registry gives us little options to manage the status of a\nrepository. Therefore a few conventions will be used in each slot's\nconfiguration service, which will use read/write/delete attributes of the\nrepositories to indicate their status:  \nStatus parameters  \nquarantineState (Q) [not changeable via CLI]  \nlist-enabled (L)  \ndelete-enabled (D)  \nread-enabled (R) [not changeable via CLI]  \nwrite-enabled (W)  \nStatus meanings  \nNewly uploaded: Bundle version newly uploaded and not touched by any\nconfiguration service\\\nStatus: QLDRW \\\na home slot's configuration service should take care of the bundle version and\nperform tests, etc. Other slot's configuration services should ignore this\nbundle for now except for registering its existence. The Container Registry\nshould be using the quarantine\nflow.  \nQuarantine: Bundle version newly uploaded and recognized by it's home\nconfiguration service.\\\nStatus: Q--R- \\\nThe home slot's configuration service will disallow any further changes to the\nbundle version until the tests have finished. The home slot's configuration\nservice will keep track of the bundle. Other services or users will not be\nable to list it.  \nTests successful \\\nStatus: -L-R- \\\nThe tests ran successfully. The bundle version may be deployed to the\nedge. Other slots may now list the bundle and run their tests. The bundle\nis readable but not writeable. This ensures that no other versions of the\nbundle will be uploaded. Any future updates require a new version (==\nDocker Tag).  \nTests failed \\\nStatus: -L--W \\\nThe tests failed. The bundle may not be deployed to the edge. Other slots\nshould continue to ignore the bundle. The bundle is re-enabled for write\naccess. Developers may upload a new version of the bundle under the same\ntag. The image is out of quarantine but not readable and should still be\nignored by other slots.  \nTests failed (Security Finding) \\\nStatus: Q-D-- \\\nThe tests failed and there was a security finding. Image cannot be read or\nwritten. All slots should remove the bundle from their services. Should be\ndeleted after 14 days.  \nDeprecated: Once a bundle version should no longer be used, for various\nreasons, it shall be marked as deprecated. \\\nStatus: LD- \\\nThis happens by marking the bundle version as deletable. The bundle is still\nreadable but not writeable. The bundle should no longer be used by\nconfiguration service for new deployments.  \nDeprecated (Security Finding) \\\nStatus: -LD-- \\\nDuring regular checks, a security finding arose. Images can no longer be read.\nAll configuration services should mark them as deprecated and stop any\nexisting deployments after 14 days.  \nStatus Overview  \nQ L D R W Meaning \u2714 \u2714 \u2714 \u2714 \u2714 Quarantine -- Home Slot should run tests (initial state after uploading) \u2714 \u274c \u274c \u2714 \u274c Quarantine -- Testing mode. Home slot acknowledged the bundle version and is now testing. Users cannot upload changes. \u2714 \u274c \u2714 \u274c \u274c Quarantine (Security Finding during upload test) -- All slots should immediately remove. \u274c \u2714 \u2714 \u274c \u274c Security Finding -- All slots should remove (inform owner & users); Automatic removal of deployments after 14 days. \u274c \u274c \u2714 \u2714 \u274c Marked for deletion (will be deleted in short time) \u274c \u2714 \u274c \u2714 \u274c Tests successful \u274c \u2714 \u274c \u2714 \u2714 Tests failed \u274c \u2714 \u2714 \u2714 \u274c Deprecated \u274c \u2714 \u2714 \u2714 \u2714 Initial Upload  \nBundle Life Cycle and automatic deletion  \nWe will use Azure Container Registry's Retention policies to delete docker\nmanifests that don't have any tag assigned to them automatically. We will\nintroduce a soft-limit of max 20 active bundles (tags) per bundle registration\n(repository). Users can deprecate older versions of a bundle through the\nconfiguration service which will be picked up by other configuration services.\nIf a bundle gets deprecated, due to non-security related reasons, the following\nprocedures should be executed:  \nImmediately mark the bundle as deprecated and inform all users via the\nnotification service.  \n7 days after deprecation, no new deployments of the bundle should be\naccepted.  \n90 days after deprecation, the bundle should be deleted.  \nIf the bundle was deprecated due to a security finding:  \nImmediately mark the bundle as deprecated and inform all users via the\nnotification service.  \nImmediately disable read/write access to the bundle. This will automatically\ndisable any new deployments.  \n14 days after deprecation, the bundle should be deleted from the registry.  \nLimitations  \nAs there are certain\nlimits\nin ACR, those should be taken into account for the configuration service. By the\ntime of writing, we assume that we might end up having up to 100 slots per\nenvironment (assuming plants are usually grouped together using Slots 2.0). This\nwould limit the total number of tokens that should be issued per slot to 200\n(20,000 / 100). Assuming that each project would get access to up to two\ntokens (with two passwords each for key rotation) that means that there should\nnot be more than 100 projects per slot on average. Each project (scope map) can\nhave up to 500 repositories / bundles (repositories). The default limit shall be\nset to 20 in order to give headroom for slots that are more active.  \nAs these limits apply per container registry and not on subscription or global\nlevel, this decision will not affect other teams. If the limit is reached in the\nfuture, a second global instance can be added to cope with the increased demand.\nThis scenario is not considered yet. as it seems highly unlikely.  \nTechnology  \nWe will use Azure Container Registry as the underlying service for our bundle\nregistry. We will use scope maps and tokens to manage access to the registry.  \nNon-Functional Requirements  \nGlobal availability. There should only be one instance. ACR provides\nzone-redundancy and geo-replications with custom locations in order to\nminimize lag.  \nLoad on the ACR is expected to be low due to the low frequency of updates once\na bundle is in production.  \nOperationalization  \nDeleting bundles will require additional work if a bundle is deployed to\ndifferent slots. It's recommended to use the deprecation flow and set a\nspecific date after how long a bundle may actually be deleted.  \nACR instances cannot be deleted if there are --write-enabled=False tags in\nthe registry. This will lead to additional work shall a slot ever be\ncompletely deleted.  \nThe resources should be monitored in terms of ingress / egress / storage usage\nand # of requests via Azure Dashboards.  \nDue to the component not having its own custom functionality but being\ninfrastructure at heart, the operations effort is expected to be quite low.  \nRisks & Mitigation  \nImage Spam: As mentioned before, the storage usage should be monitored. If one project\nconsumes highly unusual amounts of storage, their scope map can be disabled in\norder to investigate the issue.  \nScope Maps & Tokens are a preview feature: They are in preview since early  \nDuring the preview, it is not possible to assign scope maps to AAD\nIdentities. They are used like API Keys. In order to minimize risk, they\nshould use a limited lifetime and not be issued indefinitely.  \nAdditional References  \nACR Scope Maps & Tokens  \nACR redundancy  \nACR limits",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\ai-on-the-edge\\custom-vision-edge-bundle\\bundle-registry.md"
    },
    {
        "chunkId": "chunk331_0",
        "chunkContent": "Computer Vision Toolkit Bundle Format  \nBy definition a \"Bundle\" is a package containing a software and everything it needs to operate. The Computer Vision Toolkit Bundle includes packages needed to run models on the edge but as well standard interfaces, self description and test data. The CVT Bundle has a specific format. This uniqueness of format and the way everything is packaged in a container allows high scalability of deployment on the edge of a lot of different models on a lot of different machines/virtual machines across many different locations all operated in a standard and scalable way.  \nFor ease of deployment of Machine Learning models in the CVT (Computer Vision Toolbox) a common way of packaging the models and all related assets must be defined. This will facilitate the generalization of the deployments as well as make it easier for teams to operationalize them.  \nSee the CVT High Level Architecture to understand how the bundle is used in the overall architecture.  \nAssumptions & Guardrails  \nFor now the assumption is that all ML related and traditional computer vision code will be provided in Python. Bundles are designed to support different languages and different platforms. They just have to stick with the REST API and the needed data.  \nAll bundles should have unique identifiers. For 3rd party models, during Bundle Registry process, unique identifier and name for bundles should be defined to make sure there won't be any duplicated content.  \nabc.cvt.[bundleName] name format with versioning can be used to identify the bundle and its version.  \nDesign  \nIn CVT solution we may have different techniques to solve computer vision problems for different production lines. These different techniques may bring different tool sets as solutions. Main focus is packaging and running these models on the edge to enable computer vision.  \nBased on the above assumption we will create a bundle format that will be used to package the models or traditional computer vision code and all related assets. Bundle format is composed of a docker image that will be used to run the model or traditional computer vision code on the edge. We've chosen docker image comparing with below other options. For the beginning due to ease of implementation but *might- consider different formats in the future.  \nThe Bundle uses a standalone Docker Container approach. It is in short a container with specific elements in it.  \nWith standalone docker container we can provide all needed information for running models. Environment, language, requirements, scripts and metadata. All will be packaged in a  Docker image will provide the following:  \nPros  \nFlexibility to run on different platforms  \nWill work on CVT Edge (Azure IoT Edge is expected, but tbd) as module  \nUse Azure Container Register (ACR) to store and retrieve the bundles  \nEasy to manage deployment on through IoT Edge Deployment  \nEasy to manage scale  \nOut of the box versioning  \nCons  \nPerformance on memory space footprint  \nBundle High Level Architecture  \nA bundle is a docker image that facilitates the ease of deployment of ML models and traditional computer vision code to CVT. Here's a high level architecture of the bundle process: We cover two scenarios:  \nTraining and deployment of a model using platforms such as Azure ML, Azure Databricks etc.  \nUsing an existing model/traditional computer vision code developed outside of CVT platform, such as Custom Vision AI.  \nIn all cases, we will use a bundle interface to package the model or traditional computer vision code, and all related assets as a Bundle. Please check CVT High-Level Architecture for more details for the components. When we have the Image in Bundle Registry we'll check the REST API endpoint extracting metadata and test cases from the bundle, then we'll test this bundle image on a Simulator. You can check the CVT Simulator document for more information.  \nOnce Bundle is available in Bundle Registry and tested, we can use it to deploy the image to CVT Edge. On CVT edge there can be multiple modules, a controller, module communicate with Bundle Image through REST API over HTTP protocol. See the Bundle Registry document.  \nBundle Format  \nA bundle package includes:  \nmodel(s)/traditional computer vision code,  \nrequirements,  \nscoring file and script (if needed) to run the model,  \nthe settings metadata,  \ntest images/test cases.  \nTo run this model on the edge we need a dockerfile to build an image and REST API endpoint to call the model from CVT Edge Modules. Also it'll include some test images and test cases to validate the package in a Simulator.  \nAs a sample, a Python bundle contains:  \nThe ML/CV model(s)/traditional computer vision code;  \nEnvironment requirements (poetry);  \nScoring file (score.py);  \nDockerfile to create the bundle container image.  \nTest images  \nREST API interface  \nMetadata  \nBundle Rest API  \nAll Bundles will have REST API endpoint to call the model. CVT Edge modules will use this endpoint to call the model and get the predictions.  \nActual Bundle Interface definition is available in CVT Bundle Interface documentation.  \nImportant: To have a standard for all ML projects a sample template should have blueprint template and bundle interface library to share same REST API for ML projects.  \nThis REST API endpoint provides below endpoints:  \n/predict: This endpoint is used to call the model and get the predictions.  \n/metadata: This endpoint is used to get the metadata of the model and bundle.  \n/testdata: This endpoint is used to expose sample images and expected results in as a zip file.  \n/health/live: This endpoint is used to check the status of the bundle.  \n/health/ready: This endpoint is used to check if a bundle has finished the initialization and is ready to get requests.  \n1. /predict endpoint  \nImportant:  Those fields and names may change due to need your project. The key principle here is to keep a small number of root level elements and a dynamic, per usage and highly customizable data object.  \n/predict request body includes a single image or images with camera options, as shown in the bundle interface description. We'll need further investigation after we decide how models run with multiple cameras with different angles.  \n2. /metadata endpoint - Bundle Metadata  \n/metadata endpoint will provide information about bundle, model and requirements for the environment to host and run the model:  \nThis metadata will be tested by Simulator and will be used by CVT Edge modules on the edge.  \nBundle metadata requirements will be provided by data scientists, CVT Implementation team and some of them will be auto-generated. These information will be stored in a metadata config file in a Bundle.  \nAbove is not the final list for Bundle Metadata, we'll iterate on this list and add more as we need.  \n3. /testdata - Bundle Testing  \n/testdata: This endpoint is used to expose sample images and expected results in as a zip file.  \nOnce Bundle is available in Bundle Registry, Simulator will be used to test the bundle. We'll check health and ready status of the container then, extract metadata and test data on a Simulator.  \nBundles will be able to run model/traditional computer vision code. Bundles include some test images and testcase labels.  \nTest images are under test folder in the Bundle and test folder has a testcases.json file. This file contains a list of testcases and expected labels. With these tests, the purpose is not model evaluation, but rather, a lightweight test of reliability (along other systems tests) of the model by observing that previously produced (and not necessarily ground truth) results, are indeed reproducible. That is why we have only expected labels, not the ground truth.  \nImages can be single image or multiple images from different cameras, so test cases will support multiple images as list of images.  \nAlso, Simulator will check memory usage and performance metrics. Details of the Simulator are provided in this document.  \nObservability  \nBundle Images will be consumed by CVT Edge modules and we would like to enable observability for Bundles. Once bundle is ready it'll provide a REST API endpoint, we consider providing logging into REST API interface. You can find more details in the CVT monitoring document.  \nTechnology  \nDocker  \nAzure Container Registry (ACR)  \nAzure Machine Learning (AML)  \nDataBricks  \nAzure DevOps Pipelines  \nAdditional References  \nUsing OpenCensus with Python FAST API and Azure Monitor  \nCog: Containers for machine learning",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\ai-on-the-edge\\custom-vision-edge-bundle\\index.md"
    },
    {
        "chunkId": "chunk332_0",
        "chunkContent": "Accelerating with Cognitive Services  \nAzure Cognitive Services are \"Models as a Service\"  \nAzure Cognitive Services provide pre-trained, and in some cases fine-tunable, machine learning (ML) models for common AI scenarios and provides them as a service. Cognitive Service models are built leveraging very large datasets and years of machine learning research and design that would be difficult or impossible to replicate on most projects.  \nCognitive Services Accelerate ML Projects  \nUsing Cognitive Services in your solution can save time and effort compared to building a custom ML model from scratch, even when models are fine-tuned to specific scenarios, which reduces the time and effort required to get a ML solution into production.  \nCognitive Services Projects are ML projects  \nWhile Azure Cognitive Services provide pre-trained ML models hosted as a service, and many steps associated with creating and using custom machine learning models are accelerated (or unnecessary) with Cognitive Services, following the end-to-end guidance laid out across the AI section of the Microsoft Solutions Playbook is required for project success.  \nThis section provides best practices and code assets from real world machine learning projects to support the suggested machine learning lifecycle when leveraging Cognitive Services.  \nContact ISE MLOps Support for more details.  \nAzure Cognitive Services  \nOpenAI  \nAzure OpenAI Service is a powerful tool that provides REST API access to OpenAI's advanced language models, including the GPT-3, Codex, and Embeddings series. The service enables users to leverage the power of these models for a variety of language-related tasks, such as content generation, summarization, semantic search, and natural language to code translation.  \nSpeech  \nAzure Speech Services provide speech capabilities such as speech-to-text, text-to-speech, speech translation, and speaker recognition.  \nVision  \nAzure Vision Services provide computer vision capabilities such as image analysis, optical character recognition (OCR), facial recognition, and spatial analysis as well as the ability to fine tune image classification to your specific scenario.  \nLanguage  \nAzure Cognitive Services for Language provides Natural Language Processing (NLP) features for understanding and analyzing text through capabilities such as named entity recognition, summarization, sentiment analysis, and PII detection.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\cognitive-services\\index.md"
    },
    {
        "chunkId": "chunk333_0",
        "chunkContent": "tags:\n- Financial\n- Healthcare  \nAzure Cognitive Service for Language Services  \nOverview  \nAzure Cognitive Services for Language provides Natural Language Processing (NLP) features for understanding and analyzing text through capabilities such as named entity recognition, summarization, sentiment analysis, and PII detection.  \nCapabilities  \nNamed Entity Recognition (NER) identifies and categorizes entities in unstructured text. A pre-trained universal model is provided for common scenarios such as identifying people, places, organizations, and quantities. A fine-tunable model, Custom NER, is provided to build custom AI models that extract domain-specific entities from unstructured text, such as contracts or financial documents.\nFine tuning often requires less data and less time than creating a custom model.  \nSummarization produces a summary of documents and conversation transcriptions. It extracts sentences that collectively represent the most important or relevant information within the original content.  \nSentiment Analysis mines text for clues about positive or negative sentiment, and can associate them with specific aspects of the text. The sentiment analysis feature provides sentiment labels (such as \"negative\", \"neutral\" and \"positive\") based on the highest confidence score found by the service at a sentence and document-level. Opinion mining  provides more granular information about the opinions related to words (such as the attributes of products or services) in text.  \nText analytics for health is a pre-configured feature that extracts and labels relevant medical information from unstructured texts such as doctor's notes, discharge summaries, clinical documents, and electronic health records.  \nPersonally Identifying (PII) and Health (PHI) Information Detection  identifies, categorizes, and redacts sensitive information in unstructured text.  \nLanguage Detection detects the language a document is written in, and returns a language code for a wide range of languages, variants, dialects, and some regional/cultural languages.  \nConversational language understanding (CLU) enables users to build custom natural language understanding models to predict the overall intention of an incoming utterance and extract important information from it.  \nNOTE: The Language Understanding Intelligence Service (LUIS) is being retired in favor of the Conversational language understanding (CLU) feature of Azure Cognitive Service for Language. Migration details can be found in the LUIS documentation. For more details on creating conversational AI solutions, see  Conversational AI  \nQuestion Answering enables users to create a conversational layer over their frequently asked question (FAQ) and knowledge base (KB) data to allow users to find information via natural language questions.  \nNOTE: The QnA Maker is being retired in favor of the Question Answering feature of Azure Cognitive Service for Language. Migration details can be found in the QnA Maker documentation. For more details on creating conversational AI solutions, see  Conversational AI",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\cognitive-services\\language\\index.md"
    },
    {
        "chunkId": "chunk334_0",
        "chunkContent": "Azure Speech Services  \nOverview  \nAzure Speech Service provides capabilities such as speech-to-text, text-to-speech, speech translation, and speaker recognition.  \nCapabilities  \nAzure Speech to Text (STT) transcribes speech audio into text. A pre-trained universal model is provided for common  scenarios. A fine-tunable model is provided to tune recognition for scenario-specific transcription (such as recognizing industry acronyms and technical terms) and acoustic environments (such as noisy backgrounds). Fine-tuning often requires less data and less time than creating a custom model.  \nAzure Text to Speech (TTS) converts text into lifelike human speech audio. Pre-trained models are provided for many languages in multiple voices to match your scenario needs. Custom voice models can be trained to create voices that are unique to a solution or company brand. Customization of TTS models using Azure Speech often requires less data and less time than creating a custom TTS model from scratch.  \nAzure Speech Translation provides real-time speech translation for your solution.  \nAzure Speaker Recognition provides speaker identification in audio based on their unique voice characteristics.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\cognitive-services\\speech\\index.md"
    },
    {
        "chunkId": "chunk335_0",
        "chunkContent": "tags:\n- Automotive\n- Manufacturing\n- Speech to Text\n- Real-time transcription\n- ASR\n- Automatic Speech Recognition  \nSpeech to Text  \nOverview  \nAzure Speech to Text (STT) transcribes speech audio into text. A pre-trained universal model is provided for common scenarios. A fine-tunable model is provided to tune recognition for scenario specific transcription (such as recognizing industry acronyms and technical terms) and acoustic environments (such as noisy backgrounds). Fine tuning often requires less data and less time than creating a custom STT model.  \nDegree of Difficulty  \nDuring the Feasibility Phase of a machine learning project, evaluate candidates for their ability to contribute to the scenario and solution. Consider Azure Speech Speech to Text capability in the following scenarios based on their degree of difficulty:  \nLow  \nTranscription - Transcribing speech that is spoken in a conversational style, for example transcribing in-person conversations, phone calls, video, or meeting audio. Both real-time and batch transcription are supported.  \nCustom Transcription - Transcription customized to improve accuracy when considering ambient noise and/or industry-specific vocabulary. Factory floors, cars, or noisy streets can be accommodated via a custom acoustic model. Topics like biology, physics, radiology, product names, and custom acronyms can be accommodated via a customized vocabulary. Both real-time and batch transcription are supported.  \nMedium  \nLocal/Edge Containers - Speech Service Containers enable some of the Speech service capabilities to run in containers to support security and data governance scenarios not possible in a Microsoft hosted cloud. Speech containers enable customers to build a speech application architecture that is optimized for both robust cloud capabilities and edge locality. This capability has a medium degree of difficulty since not all speech to text features are supported in the container, so extra care is needed to design for and test against the containerized model. Refer to the the latest container capabilities for more details on what features are supported in containers.  \nHigh  \nSTT \"Chaining\"/Pipelines - Do not use Speech Service speech-to-text outputs as the input to another Cognitive Service or machine learning model. This form of \"chaining\" or pipelining models compounds inaccuracy across the pipeline, reducing the overall accuracy of the solution. See speech translation and Voice Assistants below for common scenarios often approached via chaining/pipelining. Consider machine learning ensemble methods to create accurate predictive models across several models.  \nSpeech Intent Recognition/Speech Commanding/Speech to Stateful Action - Speech Service speech-to-text should not be used in scenarios that require extracting the meaning from a spoken audio utterance and taking some action in the system where the conversation is open ended or complex. For speech assistants, or bots, based on limited/scoped conversations, use Voice Assistants.  \nRelated  \nNatural Language Processing (NLP)/Understanding (NLU) - Build applications capable of understanding natural language.  \nText-to-Speech - Build solutions that speak using synthesized human speech. Leverage 110+ built-in voices and over 45 supported languages. Customize speaking styles and emotional tones to fit your scenario. Create custom voices to match your brand.  \nSpeech Translation - Build real-time, multi-language speech-to-speech and speech-to-text translation of audio streams into your solutions. Translate audio from 30+ supported languages and customize your translations for the specific needs of your scenario.  \nSpeaker Recognition - Verify and identify speakers by their unique voice characteristics using voice biometry. Speaker Recognition is used to answer the question \"who is speaking?\".  \nVoice Assistants/Bots - Create natural, human-like conversational interfaces for your application or device.  \nData Curation  \nData collection is vital to any ML project, including projects that leverage Cognitive Services like Azure Speech to Text. Data that is representative of what the model will see in production, including the transcription and the audio used to create the transcription, is required for project success. Therefore, a data collection strategy is required for every project in order to meet the data need. STT data is used to benchmark the base model and to inform (and perform) any experimentation to improve against that base model using Custom Speech.  \nA good pattern for data collection is to build a minimal viable product (MVP) using the base speech-to-text model while also collecting data (transcription and user feedback). The data collected can then be used in future milestones for benchmarking, analysis, and fine-tuning of a Custom Speech model if needed.  \nTo support this pattern, use the Azure Speech logging feature of Custom Speech to collect transcription/audio data while using the base model of Custom Speech.  \nNOTE: Logging requires Custom Speech, so you need to start with Custom Speech base model log endpoint data. Custom Speech does cost more per transaction than Base STT when paying as you go, but is priced the same under the Commitment Tiers. Custom Speech supports the same transaction levels and region/language support as the Universal Model.  \nTo create a Custom Speech project with logging enabled, do the following:  \nCreate Custom Speech Project - Create a create a Custom Speech project in your resource group/resource using either:  \nSpeech Studio  \nCreate Project REST API  \nCreate a Base Model Endpoint with Logging - Deploy a Base Model (similar to the Universal STT model) so you have an model to reference at that endpoint using either:  \nSpeech Studio\nNavigate to your project at Speech Studio/Your Project/Deploy models and deploy the base model while checking \"Enable Logging\".  \nREST API\nCreate Model REST API with loggingEnabled = true when creating.  \nGet Endpoint Data via Logs - When logging is turned on, the endpoint logs the transcription and a *.wav audio file for each call to that endpoint. You can access these logs via:  \nSpeech Studio/Your Project/Deploy Models/Your Base Model and click \"Download Log\" button.  \nGet Custom Model Endpoint Logs REST API  \nConfirm logging is working as expected by programmatically hitting the endpoint and downloading the logs.  \nBringing your own storage (BYOS) is also supported when using the logging feature to enable storage in a Storage Account Resource in your Resource Group.  \nFollow these steps to configure BYOS logging:  \nRequest BYOS Access for your Subscription  \nFill out the Azure Speech BYOS Request Form.  \nCreate BYOS Enabled Speech Resource  \nNOTE: Requires Azure Subscription Owner access.  \nAccess Azure Portal via the BYOS Enabled Link.  \nCreate Azure Speech Resource with New Storage Account\nYou must use the Create New link to create the storage.\nCreates the resources and links the SA to the Speech Resource for storage of the logging data.\nNOTE: The SA cannot be changed after creation. Changes require create a new Speech Resource.  \nCreate Speech Project/Base Model Endpoint  \nUse the process above.  \nReview Log Data  \nNavigate to the SA\\Containers\\customspeech-audiologs\\ container.  \nReview .json/.txt/.wav for each call to the endpoint  \nNOTE: The \"Download Logs\" button no longer works when using BYOS, but the logs API and SA Explorer have access as expected.  \nResources  \nReal Time Transcription - Provides solution on creating a web-based speech to text application with React and Javascript  \nReal Time Transcription - Provides solution on creating a web-based speech to text application with React and Javascript  \nDevOps for Custom Speech - Provides solution acceleration for the Deployment phases of the MLOps Playbook when fine-tuning speech to text models using Azure Speech to Text. For full Experimentation and Deployment support, the MLOps for Custom Vision solution accelerator can be modified to support other services. Contact ISE MLOps Support for details.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\cognitive-services\\speech\\speech-to-text.md"
    },
    {
        "chunkId": "chunk336_0",
        "chunkContent": "tags:\n- Financial Services  \nCustom Vision  \nOverview  \nAzure Custom Vision lets you build, deploy, and fine-tune your own image classifiers. An image classifier is a Machine Learning model that applies labels (a classification) to what is recognized in the image. Use Custom Vision to easily create and deploy custom classification and object detection models specific to your solution needs.  \nUnlike the Computer Vision Cognitive Service, Custom Vision allows you to specify your own labels and train custom models to detect them. Alternatively, the Computer Vision Cognitive Service (CogS) provides a prebuilt model with prebuilt labels for common scenarios, but does not support customization.  \nBefore building a custom model with Custom Vision, review the capabilities and supported scenarios for Computer Vision CogS to see if that service supports your needs without the need for customization.  \nDegree of Difficulty  \nDuring the Feasibility Phase of an Machine Learning project, candidate Machine Learning models are evaluated for their ability to contribute to the scenario and solution. Consider Azure Custom Vision in the following scenarios based on their degree of difficulty:  \nLow  \nImage classification - Recognize what is in a given image and apply one or more labels, along with a confidence score for each label.  \nObject detection - Similar to image classification, with the location coordinates (a bounding box) for each labeled object in the image provided.  \nMedium  \nLocal/Edge Containers - Export your Custom Vision model to be run locally on a mobile device, edge device, or Docker container. Exported models are designed to run within the constraints of real-time classification on model/edge devices and may be less accurate than the hosted model. This capability is considered medium difficulty due to the potential difference is performance between the hosted and exported model. Extra care needs to be taken to test the exported model on the device, in the solution environment, during the course of the project to ensure accuracy metrics are met or accounted for.  \nHigh  \nDefect Detection/Quality Inspection/Maintenance Inspection - Building a solution where a Machine Learned model is trained to detect quality control, manufacturing defects, or maintenance issues from images or video. Custom Vision is not recommended for this scenario. Defects in a recognized object are much more difficult to detect, in many cases these defects are very small compared to the overall size of the image, increasing the difficulty in identifying those defects. For example, if the the size of the object or defect is less than 5% of the overall image size, Custom Vision likely be able to identify the defect. Building a custom object/defect machine learning model that can be parameterized and tuned for the scenario is recommended.  \nRelated  \nOptical Character Recognition (OCR) - Extracting printed or handwritten text from images, such as license plate numbers from images or serial numbers from product containers, as well as from documents - invoices, bills, financial reports, and articles. For this capability, use the Computer Vision OCR API.  \nFacial Recognition - Detecting, recognizing, and analyzing human faces in images. Facial recognition can be leveraged in scenarios such as security, natural user interface, image content analysis and management, mobile apps, and robotics. For this capability, use the Azure Face Service.  \nResources  \nMLOps for Custom Vision - Provides solution acceleration for the Experimentation and Deployment phases of the MLOps lifecycle when using Custom Vision. Use this repo to create a fully configured Azure Custom Vision experimentation environment for your project with the ability to deploy models between deployment environments. Infrastructure as Code scripts are provided for quick environment setup. Sample data is provide to quickly confirm your deployment is working end-to-end. Documentation on how to apply the solution to your own data and scenario are provided. Contact ISE MLOps Support for more details.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\cognitive-services\\vision\\custom-vision.md"
    },
    {
        "chunkId": "chunk337_0",
        "chunkContent": "Azure Vision Services  \nOverview  \nAzure Vision Services provide pre-trained models for common vision scenarios, including image understanding, optical character recognition (OCR), and spatial analysis (such as people counting) as well as image classification models that can be fine-tuned to your specific scenario, often with less data and in less time than creating a custom model from scratch.  \nCapabilities  \nAzure Computer Vision provides pre-trained models for common vision scenarios, including image understanding, optical character recognition (OCR), and spatial analysis (such as people counting).  \nAzure Custom Vision is an image recognition model that you can fine tune to your scenario, often with less data and in less time than creating a custom image recognition model from scratch.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\cognitive-services\\vision\\index.md"
    },
    {
        "chunkId": "chunk338_0",
        "chunkContent": "Conversational AI  \nConversational AI (a.k.a. \"chatbot\") is one of the most prevalent uses of Artificial Intelligence in the corporate world.  \nGartner Predicts Conversational AI Will Reduce Contact Center Agent Labor Costs by $80 Billion in 2026  \nIn fact, closer to home, \"Gartner Forecasts Conversational AI End-User Spending to Reach Nearly $2 Billion in 2022\"  \nMicrosoft has been providing chatbot building capabilities since the introduction of the Bot Framework in 2016, and now is converging its technologies into a unified experience, comprised of both the Power Virtual Agents and Bot Framework.  \nPower Virtual Agents  \nPower Virtual Agents enable you to create virtual agents using a guided graphical interface with no code, and for advanced scenarios, extend using Microsoft Power Automate and Azure Bot Services.  \nChannels  \nWith Power Virtual Agents, you can publish bots to engage with your customers on multiple platforms or channels. These include live websites, mobile apps, and messaging platforms like Microsoft Teams and Facebook.  \nCustom Applications  \nPower Virtual Agent bots can use the Direct Line channel to integrate with custom applications.  \nThis repository: https://github.com/microsoft/PowerVirtualAgentRobotsSamples contains examples of connecting a Power Virtual Agent to three types of custom applications:  \nA generic Android application, with Java  \nA \"Furhat\" robot (from https://furhat.io, using Kotlin  \nA \"Misty\" robot (from https://mistyrobotics.com, using C#",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\conversational-ai\\index.md"
    },
    {
        "chunkId": "chunk339_0",
        "chunkContent": "rings:\n- public  \nGenerative AI  \nIntroduction  \nThis section serves as a comprehensive guide for launching, operating, and enhancing Generative AI applications in a production environment. It encapsulates essential capabilities, from delivering innovative experiences to ensuring robust security and privacy, as well as managing the lifecycle of AI solutions.\nNavigating the complexities of distributed systems is a formidable task, further compounded by the unpredictable nature of Generative AI models. This section aims to consolidate crucial information and best practices, making them readily accessible and comprehensible. It includes links to additional resources, aiding diverse teams across numerous disciplines to access the insights they need for success.  \nGenerative AI Solutions  \nLarge Language Models (LLMs) are the core of enterprise Generative AI applications. They can process and generate natural language, but they require additional components to handle user interactions, security, and other functionality to respond to or act on user inputs. The collection of these components and services that form a functional solution is called a Generative AI application. A best practice when developing a Gen AI application is to follow a standard AI Lifecycle, while utilizing Large Language Model Operations (LLMOps) tools and processes to facilitate and simplify the steps.  \nFigure 1: Generative AI Application Stack  \nThe absence of a unified toolset for overseeing the development of individual components and services means that creating a comprehensive end-to-end solution demands the use of connective code and custom functions. These are essential for seamlessly integrating diverse products and services into a high-quality Generative AI application tailored for enterprise use.  \nThe sections below will explore each of these components along with the tools and processes that help our customers develop, deploy, and maintain a Generative AI solution. We focus on the top use cases that ISE is seeing with our customers which are predominantly language-to-language or language-to-action solutions. The underlying components are similar for image generation, but we do not explore the differences here.  \nAI Lifecycle  \nWe will base our capabilities in the context of the Gen AI application lifecycle that has standard stages for custom ML, and Large Language Model solutions. This lifecycle represents the typical iterative approach to preparing, deploying, and improving a Gen AI application over time.  \nFigure 2: AI solution lifecycle: Data Science and ML Engineering  \nEnterprise Generative AI Application  \nManaged Services  \nManaged services enable access to Large Language Models and provide built-in services to adapt the models to specific use cases and may include capabilities for integrating with existing tooling and infrastructure. Enterprise deployments also require services to manage the ML lifecycle (MLOps). Examples of Microsoft\u2019s Generative AI and ML lifecycle managed services include Azure Cognitive Search, Azure OpenAI Service and Azure ML Service.  \nAI Solution Framework  \nThe backend system orchestrates the data workflow in a large language model application by facilitating access to Language Model (LLMs), overseeing data processing, and enabling cognitive skills. It breaks down tasks and makes calls to agents or libraries, enhancing the capabilities of the LLM. Examples of such integrations include Semantic Kernel and LangChain. {% if extra.ring == 'internal' %}Please see Libraries and Languages for more information.{% endif %}  \nClient Applications  \nApplications provide the \u201ccanvas\u201d for how the user will interact with the solution, and all the supporting functions needed to make the solution useful. The most common examples of user interfaces are chat based. Application examples and frontend tooling include Microsoft Co-pilot, Teams, and Power Virtual Agents (PVAs).  \nMonitoring & Observability  \nThis involves tracking the performance and health of a Generative AI application. It includes collecting metrics, logs, and traces to gain visibility into the system\u2019s operations and to understand its state at any given moment. In Generative AI, this could mean monitoring the model\u2019s performance, data throughput, and response times to ensure the system is functioning as intended.  \nSecurity & Privacy  \nThese are critical aspects that encompass protecting the Generative AI application from unauthorized access and ensuring the confidentiality, integrity, and availability of data. It also involves safeguarding the privacy of the data used by the application, which includes implementing measures to prevent data breaches and leaks and ensuring compliance with data protection regulations.  \nData Platform  \nThis is the underlying infrastructure that supports the storage, processing, and analysis of large volumes of data used by Generative AI applications. It includes databases, data lakes, and other storage systems, as well as the tools and services for data ingestion, transformation, and querying. A robust data platform is essential for prompt engineering, fine-tuning and operating Generative AI models effectively.  \nLLMOps  \nLarge Language Model Operations (LLMOps) acts as the orchestrator that manages all above components cohesively. LLMOps refers to the set of practices, techniques, and tools that facilitate the development, integration, testing, release, deployment, and monitoring of LLM-based applications. It establishes the operational framework, ensuring a smooth and efficient interplay among these elements throughout the application lifecycle. LLMOps ensures that LLMs remain reliable, efficient, and up to date as they are integrated into Generative AI applications.  \nThese components work together to create a stable, secure, and efficient environment for Generative AI applications to develop, deploy, operate, and evolve.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\index.md"
    },
    {
        "chunkId": "chunk340_0",
        "chunkContent": "rings:\n- internal  \nGlossary  \nBag-of-words Model  \nThe bag-of-word model represents text by a bag (multi-set) of the words contained in the text.  \nSee Bag-of-words model (Wikipedia).  \nBM25  \nBM25 is a bag-of-words retrieval functional that ranks a set of documents because on the query terms appearing in each document.  \nSee Okapi BM25 (Wikipedia).  \nDocument Embeddings  \nDocument embeddings are a real-value vector that represents the  potentially correlated features in a more compact representation while preserving important relationships and patterns.  \nSee Word embeddings (Wikipedia).  \nK-means Clustering  \nK-means clustering is a method of partitioning n observations into k clusters in which each observation is associated to the nearest cluster center.  \nSee K-mean clustering (Wikipedia).  \nPrincipal Component Analysis (PCA)  \nPCA is a statistical technique for analyzing large datasets by reducing the dimensionality of the dataset.  \nSee Principal Component Analysis (Wikipedia).  \nPrompt Engineering  \nA prompt is natural language text describing a task that an AI model should perform. Prompt engineering is the process of building the prompt so that the task is performed correctly.  \nSee Prompt Engineering (Wikipedia)",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\lab\\llm-lab\\glossary.md"
    },
    {
        "chunkId": "chunk341_0",
        "chunkContent": "rings:\n- internal  \nAbout ISE Engineering Labs  \nISE Engineering Labs simulate real-world customer scenarios integrated into an open-learning format validated through code-with engagements. Each lab consists of several modules that build on each other. You will need to complete each module before moving on to the next one. In every module, you will apply the tools, processes and functionality that are essential for the customer solution you are creating.  \nWhat to expect  \nThese labs are designed to help you learn by doing, not by following instructions. We have tested this approach in many 'OpenHack' deliveries and found it effective. Instead of telling you what to do step by step, we give you an overview of the goals and outcomes of each module, a list of resources and experts that you can consult if you need help, and a collaborative environment where you can work with your peers to solve problems.  \nAs you go through the lab, you will also have the opportunity to contribute to our products and documentation by giving feedback and making PRs to docs or adding to the Microsoft Solutions Playbook.  \nDelivery guidance  \nISE Engineering Labs offer flexible delivery options, whether online or in-person. You can choose between a facilitated hack, or a self-serve experience based on your specific needs.  \nSelf-serve guidance  \nIf you choose self-serve it's best to work as a group of 4-6 individuals, we recommend that you assign a \u2018lead\u2019 who has some prior experience and can coordinate and mentor the group. You can set your own pace, but make sure you allocate enough time to prepare, deliver, and provide feedback. The estimated time commitment is about 3.5 days in total (1 hour for preparation, 3 days for delivery, 15 minutes for feedback).  \nProgress through the content according to the criteria and guidance within the content itself. You can use the existing infrastructure or your own Azure subscription (if needed). However, under this self-serve model, you are responsible for all the delivery and resourcing aspects. Please note that some content may require you to use your own Azure subscription. You can use one subscription for the whole group if you work as a team. You can deliver the content virtually via Microsoft Teams with live or asynchronous facilitation, or in-person at your local Microsoft office using Microsoft Teams as a collaboration tool.  \nExample agenda  \nDay 1 Day 2 Day 3 AM Welcome + Day 1 Kickoff Day 2 Kickoff Day 3 Kickoff Code Time Code Time Code Time Break Break Break Code Time Code Time Code Time PM Lunch Lunch Lunch Code Time Code Time Code Time Break Break Break Wrap & Day 1 Close Wrap & Day 2 Close Provide Feedback / Log PRs + Wrap & Day 3 Close",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\lab\\llm-lab\\index.md"
    },
    {
        "chunkId": "chunk342_0",
        "chunkContent": "rings:\n- internal  \nGetting Started and What to Expect  \nDisclaimer  \nThis content is for Microsoft employees only. However, the dataset is restricted to ISE employees because it contains confidential customer data. If you are not in ISE, you will need your own dataset of unstructured documents. If you are in ISE, you will need to copy the data from ISE Artifact Hub (linked in Module 2a) to your own blob storage account.  \nGetting started  \nWelcome to the ISE Engineering Lab for Azure OpenAI! This module will help you learn how to use the amazing capabilities of OpenAI to create innovative solutions. Whether you join a facilitated session or explore on your own, you will find plenty of guidance, resources, and tools to support your learning journey. Get ready to have fun and discover new possibilities with OpenAI!  \nWhat should I expect from an ISE Engineering Lab?  \nISE Engineering Labs are challenging. They don't have ready-made solutions. You will create your own solution as if you were working with a real customer, using your knowledge of architecture and technology.  \nISE Engineering Labs simulate real-world customer coding scenarios. You will make design decisions to fulfill the requirements for each module. What you build in each module will contribute to a solution that addresses the customer problem described in Module 1.  \nWhat do I need to know?  \nThis lab requires you to write Python code and work with GitHub. You should have a good grasp of these tools and practices.  \nPython Essential Training (LinkedIn Learning): Course that provides an introduction to Python.  \nYou should also be familiar with the fundamental principles of AI and ML.  \nBeginner:  \nAI Foundation (Viva Learning): Course that provides foundational information about AI, responsible AI principles, generative AI and machine learning, Azure AI, and how to start an initial AI conversation with customers.  \nIntroduction to Artificial Intelligence (LinkedIn Learning): Course that supplies baseline knowledge of Machine Learning.  \nAzure ML Fundamentals (YouTube): Microsoft Developer YouTube video  \nThe Artificial Intelligence Wiki: Glossary for AI terms.  \nIntermediate:  \nAzure OpenAI Service in a Day Workshop Deck (CSU Resources): Consolidated Azure OpenAI service slide decks  \nGet started with Azure OpenAI Service (MS Learn): Modules to help get you started with OpenAI Services.  \nAzure OpenAI Documentation (MS Learn): Robust documentation and guidance for OpenAI Services.  \nHow can I make informed architecture and technology decisions?  \nTo help you make smart choices about architecture and technology, we suggest you:  \nReference the resources in the lab: They contain best practices, examples, and code that we use in real customer scenarios.  \nCollaborate with peers: Work in groups where you can share ideas and solve problems with team members who have different skill sets (like in customer engagements).  \nGet guidance from experts: Leverage the coach and SME community who can give you advice (but not solutions). Teams channels are a great place to chat with Microsoft colleagues (lab graduates, experts and AI fans).  \nSelf-serve guidance  \nIf you choose self-serve it's best to work as a group of 4-6 individuals, we recommend that you assign a \u2018lead\u2019 who has some prior experience and can coordinate and mentor the group. You can set your own pace, but make sure you allocate enough time to prepare, deliver, and provide feedback. The estimated time commitment is about 3.5 days in total (1 hour for preparation, 3 days for delivery, 15 minutes for feedback)  \nProgress through the content according to the criteria and guidance within the content itself. You can use the existing infrastructure or your own Azure subscription (if needed). However, under this self-serve model, you are responsible for all the delivery and resourcing aspects. Please note that some content may require you to use your own Azure subscription. You can use one subscription for the whole group if you work as a team. You can deliver the content virtually via Microsoft Teams with live or asynchronous facilitation, or in-person at your local Microsoft office using Microsoft Teams as a collaboration tool.  \nWhat should I do before the lab?  \nBefore you begin the lab, you need to do some preparation. Don't wait until the last minute, because some of these steps can take a long time and you might need to get approval. Here's what you need to do:  \nEnvironment Setup:  \nDownload Python 3.10 or 3.11  \nLearn how to use the OpenAI python package https://pypi.org/project/openai/  \nFor facilitated labs, you do not need to provision an environment. You will be given access to your assigned resource group. Each environment will have:  \nResource Description Link Azure Storage Account A storage account for your solutions Learn More Cognitive Search account A search service for your solutions Learn More Azure OpenAI Large Language Models for your solutions Learn More Azure Machine Learning workspace A place to collaborate on Python notebooks, etc. for your solutions Learn More  \nOther Azure products and concepts that you should be familiar with:  \nResource Description Link Prompt flow Development tool fo AI applications powered by LLMs Learn More Document Intelligence (formerly Form Recognizer) Intelligent document processing solution builder Learn More RAG Pattern Pattern that uses prompt engineering to guide the model Learn More  \nYou will also have access to a blob store that contains source data for this lab.  \nWatch LangChain101, which will familiarize you with many of the concepts and tools that will be part of the lab.  \nAfter Setup, Move to Module 1  \nModule 1",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\lab\\llm-lab\\module-0.md"
    },
    {
        "chunkId": "chunk343_0",
        "chunkContent": "rings:\n- internal  \nModule 1: Adatum Corporation Generative Customer Patterns  \nSuggested Time Box: 2 hours  \nThis module will provide the background you need to understand what you are building with your customer. You will then use Azure Machine Learning Prompt Flow to automate the creation of a demo-ready version of the solution.  \nHow do we know what to build?  \nMachine Learning engagements have a unique lifecycle due to the experimentation stage where we iteratively make changes and measure the outcomes to identify the best performing solution. In custom ML engagements, we optimize our models through feature engineering and data processing. Because Large Language Models are pre-trained at a massive scale, our experimentation stage is focused on prompt engineering and data processing.  \nThe first stage in these engagements is investigation, where we determine the customer challenge, success criteria and assess their readiness to build and deploy an ML engagement. The solution definition is an output of the investigation. Please see the supplement to Module 1 for more information on the questions to ask and success criteria for the stage.  \nFor the purposes of this lab, we make the following assumptions and present the results of the investigation stage next:  \nYou have completed the investigation stage with your customer. We provide a summary of the outcomes below.  \nYou have determined that your customer has a safe and secure data estate that enables data sharing across their organization. AI is 80% data, so this is a critical step that we take with our customers. Any issues identified here would need to be addressed before we start an AI engagement. To learn more about this, you can refer to Data Sharing.  \nAdatum Corporation  \nAdatum Corporation is a Global Systems Integrator that builds software solutions for their end-customers across all geographies and industries. They have experienced a significant increase in customer demand for their services to implement a variety of AI-enabled solutions. To meet this demand, Adatum needs to hire and upskill people, and invest in building solutions that can accelerate customer engagements. They want to analyze customer requirements from prospective customers and completed engagements to make informed investments in the right skills and accelerators to solve their customers\u2019 top problems.  \nInvestigation Summary  \nYour investigation phase can be summarized as:  \nCustomer Scenario:\nAdatum Corporation wants to identify the most common customer challenges and requirements, or Customer Patterns, across detailed engineering documents so they can apply scarce resources to solve the highest impact problems. The engineering documents (game plans) are written by engineers, and do not consistently adhere to a format, which makes it difficult to extract information from them. The Adatum team is looking for tools to more easily query information from these documents, using natural language. Eventually, they would like to use this tool to extract information for each document and build up a knowledge base of customer patterns.  \nSuccess Criteria:  \nUsers can enter a natural language question about an engineering topic they are interested in.  \nThe solution analyzes a corpus of Adatum engineering documents (game plans) to identify the appropriate document, and information within it to answer the question.  \nThe solution only returns information that is found in the game plans.  \nThe solution returns the information in a format that is easy to read and understand.  \nIf the information is not available in the game plans, the solution returns a message indicating that the information is not available.  \nAdatum Generative Customer Patterns Solution  \nThe MVP solution will have a chat interface where users enter a customer pattern related question they need answered. These include:  \nWhat was the goal of the HSBC Cloud Agnostic Apps project?  \nWhat Azure technologies were used in Connected Container?  \nWhich technologies did Peacock use?  \nWhat open source assets components were used in the Maersk engagement?  \nWhat was the goal of the connected container engagement?  \nAn example of questions and the expected answers are provided in the aoai-lab-storage container, linked below.  \nProposed Architecture  \nAdatum's scenario, a chatbot that provides answers based on a proprietary dataset, is a candidate for the Retrieval Augmented Generation (RAG) pattern of LLM prompting. In this pattern a search query is performed based on the user's question to find relevant document content (chunks) to be included in the prompt that gets sent to the LLM. There are a few important pieces to understand about this pattern before continuing:  \nThe source data is the set of documents that you are enabling search over. These documents can be of various formats (pdfs, markdown, docx, etc.)  \nThere is an offline pipeline for converting the unstructured source data into a searchable set of indexed chunks. This is outside of the LLM solution and resembles a classical search problem. Key steps:\nCracking - This step extracts text context from the various formats of the source information.\nChunking - LLM solutions charge based on the number of tokens passed into them, so it is common practice to break a document into smaller chunks that can fit in the prompt context.\nEnriching - Some structured data bases (like Azure Cognitive Search) allow the user to build search indices out of key fields. These fields can be extracted from the source document to give better search results.  \nWhen an end user submits a question, the LLM solution will first use the question as a query to the structured chunk store to find the most relevant chunks.  \nThe chunks get stitched into a prompt that gets passed to an LLM hosted in Azure OpenAI to generate the answer for the user.  \nThe remaining modules of this lab will walk through each piece of the solution, with a focus on what should be tested and measured for each solution. Like any other solution that multiple components are pieced together, each piece should be individually tested.  \nWhat you should know  \nOnly some use cases need AOAI/LLM solutions. After reviewing Adatum\u2019s problem statement and success criteria, you determine that this is a Retrieval Augmented Generation (RAG) use case. This use case is well aligned with an Azure OpenAI enabled solution.  \nAOAI/LLM solution responses are only as good as your data. You will spend time exploring your data and implementing your strategy to manage incorrect or incomplete data.  \nAOAI/LLM solution responses are created from search results. Incorrect or unreliable results, including hallucinations, are a result of poorly indexed data. You will spend time iteratively improving a search index.  \nSolution performance improvements are iterative. You will define metrics and build an evaluation framework to run build, test and refine loops to improve the performance of your solution.  \nWhat will I learn in this module?  \nIn this module you will use Azure ML prompt flow to build an example of the Retrieval Augmented Generation (RAG) pattern. This is the pattern that will be implemented for Adatum, but here we will use analyze the pattern on the example AML data set.  \nGain knowledge needed to guide investigation discussions in each pillar of this stage.  \nSee an example of a customer business challenge and success criteria.  \nUnderstand how a use case can map to a business challenge.  \nStart here  \nThese links provide a starting point. Remember to use your peers and the Lab Teams channel for additional resources:  \nSource Data for this lab (For ISE employees only) NOTE: The data is hosted inside of a SharePoint site. Please copy this data to your own blob storage account, rather than directly reading from SharePoint.  \nUse the ISE ArtifactHub Link to confirm:\nThe full dataset is available in the gameplans folder for review.\nThe gameplans_subset folder contains a curated selection of game plans that you will use for this lab, both to improve performance, and reduce the amount of data preparation necessary for success.\nThe evaluation dataset is available  \nRead an overview of the RAG pattern. This is the LLM-based solution framework we will be following in this lab.  \nWhat is Rag?  \nReview the overview of Azure Machine Learning prompt flow.  \nFollow the guidance on how to use RAG in Prompt Flow to create a Q&A chat.  \nYou do not need to connect the Gameplan data to this yet, just use the pre-generated sample data in the template.  \nNOTE: Use a Compute Instance for the runtime - the managed online endpoints will run into permissions issues due to this lab's setup.  \nUse this sample to understand the flow in the RAG pattern.\nWhat are the basic operations being done on the data?\nWhat core services are required?  \nConsider what areas will customers want to customize.  \nDefinition of Done  \nYou have created an end to end Prompt Flow solution that can be used to answer questions about sample data using the Prompt Flow UI.  \nYou have a high level understanding of the RAG pattern.  \nWhat does RAG stand for?  \nWhat are they key components in a RAG solution?  \nResources & Support  \nTopic Description Format What is Rag? An overview of the RAG pattern Documentation How to use RAG in Prompt Flow A brief tutorial on how to setup the RAG pattern Documentation  \nGuidance & support:  \nProvided reference materials  \nYour team chat and breakout room  \nThe Lab channel (asynchronous)  \nYour coach  \nIterate & Improve  \nPlease help us improve this module, our references and resources and the product and accelerators you used.  \nModule 1 feedback, references and resources.  \nProvide Product Feedback to the PG here.  \nOnce Complete, Move to Module 2  \nModule 2",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\lab\\llm-lab\\module-1.md"
    },
    {
        "chunkId": "chunk344_0",
        "chunkContent": "rings:\n- internal  \nModule 2a: Data Ingestion, Discovery, & Enrichment  \nSuggested Time Box: 4 hours  \nIn this module, you will write code to extract text from documents and get it into a format required for subsequent steps. We assume that our customers have a robust data platform via the previous module and the data is stored in a secure Data Lake as PDFs.  \nLet's Get Started  \nIn Module 1, you used Azure Machine Learning prompt flow to, essentially, automatically put together a RAG solution. Now, you are going to break down those steps into their component parts, and build it from \"scratch\". As is often the case with automated solutions, they do a good job of getting started, but often fall apart at enterprise scale due to a lack of flexibility. By building the full pattern yourself from beginning to end, you can adjust each piece, all the way down to how the initial data preparation is done.  \nWhat's in Our Data?  \nAdatum Corporation has a large corpus of game plans that they have used to deliver solutions to their customers. These game plans have been exported as PDF documents, and contain a variety of information. The main focus of the gameplan documents are to outline a customer's problem statement, and the solution that Adatum will provide to solve that problem. The documents also contain information about the customer, the solution, and the impact of the problem. The format of the documents is not consistent, and there are many different templates that have been used over the years, and the documents may have mixes of images, text, tables and more.  \nWe need to extract the relevant information from these documents, and then use that information to build our solution.  \nWhy do we need to do data discovery?  \n\"Garbage in, garbage out\" (GIGO): The performance of any Machine Learning model, custom or pre-trained (like an LLM), is only as good as the data. This is not a copy/paste error from the previous module, it is a tenet of successful AI solutions that bears repeating.  \nIf you still need convincing, ask your closest Data Scientist and they will echo: \u201cIf 80 percent of our work is data preparation, then ensuring data quality is the important work of a machine learning team.\u201d - Andrew Ng, Professor of AI at Stanford University, and founder of DeepLearning.AI.  \nAdatum Corporation has not verified that every document was exported correctly, and left that up to you to sort out. Data discovery is how we explore our data to see:  \nIs it clean? This means no duplication, missing values, unreadable characters, or other issues. There will be issues, so you will need to determine a strategy for cleaning the data.  \nIs it relevant, which means the documents are related to your topic? Any unrelated content should be removed so it does not impact your outcomes.  \nWhere does AI fit in?  \nWith the arrival of tools like OpenAI's embedding models, taking unstructured text, and converting it into numerical vectors has become borderline trivial. This means that we can now use AI to help us understand what is in our data, and to help us clean it up. Using these embeddings in combination with clustering algorithms, we can group similar documents together, and use those clusters to better understand what is in our data. Keep in mind, these embeddings may be useful for more than just clustering - they will come up again in future modules.  \nAn important concept to consider is that these game plan documents may be multiple pages long - much longer than the token limits of an LLM service. This means that we will need to chunk the documents into smaller pieces, and then use the LLM to generate embeddings for each chunk. There are many different chunking strategies - it is up to you to explore and see what works best for your data.  \nWhat will I learn in this module?  \nAfter completing this module, you will:  \nUnderstand some strategies for extracting content from documents.  \nLearn about helpful methods for exploring unstructured data, such as the Data Discovery Toolkit. Optionally using tools like Spark (e.g. via Synapse) to increase speed of these processes.  \nBe able to cluster and explore the data to understand what is in the dataset using tools such as PCA or K-means.  \nKnow how to use OpenAI or another LLM to generate embeddings to help with the clustering process.  \nUnderstand how to chunk documents into smaller pieces for processing.  \nModule Objective(s)  \nYou will use various tools and processes to extract and explore the data contained in the provided corpus. At the end of this module, you will understand what data you have, and you will have removed any unrelated or incorrect data.  \nDefinition of Done:  \nYou have extracted the text from the documents into a machine-readable format such as CSV, JSON, or TXT.  \nYou have enriched the data with embedding vectors that represent the content of the documents.  \nYou have stored the enriched data in a secure location (this can be the same file/location as the base extracted documentation).  \nYou have used visualization tools to explore the data and understand what is in it.  \nStart here  \nThese links provide a starting point. Remember to use your peers and the Lab Teams channel for additional resources:  \nSource Data for this lab (For ISE employees only) NOTE: The data is hosted inside of a SharePoint site. Please copy this data to your own blob storage account, rather than directly reading from SharePoint.  \nFor ISE Employees - ISE ArtifactHub Link\nFolder: gameplans_subset  \nConnect the storage account to your Azure ML Workspace as a Datastore, once you have copied the data over.  \nCreate a Compute Instance in your Azure ML Workspace, and import the Tutorial Notebook (right click, 'Save As' \"Tutorial_Notebook.ipynb\").  \nFollow the steps in the notebook, and use the resources and support guidance to understand the various steps in the notebook. This notebook covers content for modules 2a and 2b.  \nWhen running the notebook for yourself, be sure to reduce the size of your dataset by slicing your dataframe. This will avoid hitting constraints with different Azure resource quotas.  \nOnce you have explored your subset of the data, you may load in the pre-embedded full dataset for the work in Module 2b. A link to the pre-embedded dataset is provided in the notebook, as well as in Module 2b.  \nThe steps in the notebook help you build the offline pipeline for building a searchable index, which will be done in Module 2b. All of the steps in the notebook are subject to experimentation to try and optimize the resulting search solution, which will be the subject of module 3. For example, there are many python packages for extracting text from PDFs and some may perform better or worse than the form recognizer solution depending on the scenario. For customer engagements, make sure to test a variety of solutions.  \nPlugging in the technology choices in this notebook, we update our generic RAG pattern architecture to the following:  \nResources & Support  \nTopic Description Format Data Discovery Toolkit A repository containing guidance and code assets that use a variety of Machine Learning techniques to discover insights in unstructured data such as documents, images, and videos Code Repo LangChain Document Transformers LangChain includes tools for extracting content from PDFs and chunking data. Document Azure AI Document Intelligence Azure AI Document Intelligence is a service that can extract content from documents. Document OpenAI Embeddings An overview of embeddings in the OpenAI service. Document Getting Started with Embeddings Using OpenAI to generate embeddings Document  \nIterate & Improve  \nPlease help us improve this module, our references and resources and the product and accelerators you used.  \nModule 2a feedback, references and resources.  \nProvide Product Feedback to the PG here.  \nOnce Complete, Move to Module 2b  \nModule 2b",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\lab\\llm-lab\\module-2a.md"
    },
    {
        "chunkId": "chunk345_0",
        "chunkContent": "rings:\n- internal  \nModule 2b: For More Context  \nSuggested Time Box: 3 hours  \nIn this module, you will write code to create a search index to query across the data you enriched in the previous modules.  \nWhat is Search doing in my AI solution?  \nAn LLM on its own has no context of the content inside the documents, so we need to build a system to extract and search through the documents for relevant context to answer user questions. This is where search tools come in - building off of the enriched data we created in part A, we can use search to find the most relevant documents to answer a user's question. Later, we will explore how to use the LLM to generate answers from the search results.  \nWith the RAG pattern, the core of the solution is Search. The OpenAI LLMs are not magically able to understand the content of a gameplan, they rely on being sent the relevant information to answer a user's questions. This means that if our search solution is not effective, our AI solution won't be effective! For our search to be effective, we need to have properly understood, and prepared our data, and then created an effective search index.  \nIn part A, you generated embeddings of your data. Using those embeddings, you can use cosine similarity to compare the semantic structure of a given query to the content in documents. By combining this \"vector search\" with a traditional keyword search, you can build a very powerful search capability.  \nWhat will I learn in this module?  \nAfter completing this module, you will:  \nUnderstand how to create a search index using Azure Cognitive Search  \nBe able to enrich a search index with vector embeddings  \nExplore the different results from different search styles (such as BM25, Vector, Semantic, and Hybrid)  \nModule Objective(s)  \nBy the end of this module, you will have built a search index that can be used as the base for your AI solution. The search solution will be able to understand natural language queries and return the most relevant documents to answer the question.  \nYou will use the enriched data created in part A to populate this index, and then use the index to answer questions about the data.  \nSuccess Criteria:  \nCreate a Cognitive Search Index  \nEnrich the index with vector embeddings  \nRun the following queries and review what is returned:  \nWhat was the goal of the HSBC Cloud Agnostic Apps project?  \nWhat Azure technologies were used in Connected Container?  \nWhich technologies did Peacock use?  \nWhat open source assets components were used in the Maersk engagement?  \nWhat was the goal of the connected container engagement?  \nStart here  \nLearn about Azure Cognitive Search and vector search.  \nTo save you from needing to embed every document, we have provided a pre-computed set of embeddings for you to use. You can find them in the same blob store as the game plans, in the aoailabcontainer container, as the searchdf.gzip file.  \nResources & Support  \nTopic Description Format Azure Cognitive Search An overview of Azure Cognitive Search Document Vector Search An overview of vector search in Azure Cognitive Search Document Data Discovery Toolkit Search Evaluation A Spark notebook which evaluates the performance of different search strategies, and includes evaluation metrics Code Repo  \nIterate & Improve  \nPlease help us improve this module, our references and resources and the product and accelerators you used.  \nModule 2b feedback, references and resources.  \nProvide Product Feedback to the PG here.  \nOnce Complete, Move to Module 3  \nModule 3",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\lab\\llm-lab\\module-2b.md"
    },
    {
        "chunkId": "chunk346_0",
        "chunkContent": "rings:\n- internal  \nModule 3: Experimentation: Search  \nSuggested Time Box: 3 hours  \nIn this module, you will write scripts to evaluate the search results of the index you created in previous modules, and you will execute pre-defined steps to evaluate and compare outcomes.  \nWhy do we need to do search evaluation?  \nYour Azure OpenAI solution needs to search for patterns that match your query.  Bad search results now, means bad answers in the RAG solution later. If the correct answer isn't somewhere in the returned search results, we may see unexpected behavior such as hallucinations.  \nAOAI solutions are only as good as their search capabilities, so we need to be objectively sure we have optimized search results.  \nHow do we know if our search is good? We define metrics, and we measure tests queries against these metrics. These metrics are generated by evaluating the results of a test set of queries against a set of known \"good\" answers. By using a variety of metrics (such as cosine similarity, fuzzy matching, and more), we can get a good idea of how well our search is performing.  \nWhat will I learn in this module?  \nAfter completing this module, you will:  \nUnderstand the importance of a good search index.  \nKnow how to define evaluation metrics for search, including how to calculate different distance measures for search results.  \nUnderstand where different search tools perform better.  \nUnderstand some of the techniques for improving search.  \nModule Objective(s)  \nMeasure the performance of search against your evaluation metrics.  \nCreate a script to test/evaluate the search index.  \nCompare the results of various search types (e.g. BM25, Vector, Semantic, Hybrid).  \nSuccess Criteria  \nYour team has created a Cognitive Search index and are able to execute queries against it.  \nIf you followed the notebook in the previous modules to completion, the index should already exist. If not:\nUse the Pre-prepared Dataset as the base of your index.\nUse the searchdf.gzip that is in the aoailabcontainer container - the same that was used for the previous module.  \nYou may choose to further enrich this data with additional metadata, but this is not required. It may help improve your search results!  \nUse the evaluation_data.csv for a set of test queries and expected results.\nUse the 'evaluation_data.csv' that is in the aoailabcontainer container.  \nYour team has a test script (or scripts) that output the results of a set of test queries against the index, and uses various performance metrics to compare those results to the expected results.  \nIdentify key metrics that may be useful for evaluating search results. Examples include:\nEmbedding Distance Metrics\nPrecision and Recall\nBoth at 1 results and up to N results (e.g. top 5)\nFuzzy Matching\nMean Average Precision\nMean Reciprocal Rank  \nYou should be comparing the expected DOCUMENT that is returned, not necessarily the exact answer. For example, if the expected answer is \"What was the goal of the HSBC Cloud Agnostic Apps project?\", the expected document is the document that contains the answer to that question. We will evaluate the answer generation in a later module.  \nStart here  \nYou created embeddings in the previous module. Your next step would be to optimize these embeddings through iterative improvements. In the interest of time, we have provided optimized embeddings for you to use for the remainder of the lab.  \nThese links provide a starting point. Remember to use your peers and the Lab Teams channel for additional resources:  \nLearn about Evaluating Azure Cognitive Search Results.  \nUse the\\ Data Discovery Toolkit examples for inspiration on how to implement a test strategy for evaluating search.  \nIn the Data Discovery Toolkit, the linked BBC_OpenAI_EDA notebook has a lot of content - the related code is in the 3rd and 10th code cells, which calculate many different metrics (e.g. Fuzz ratio, Jaccard Distance, and others). Use this as a starting point for some more research. The notebook itself relies on Spark to run, so don't try running it in the provided compute, as it will not work.  \nOnce you have defined your metrics, try different search strategies - such as using BM25, Vector, Semantic, and Hybrid search - to see how they perform against your metrics.  \nResources & Support  \nTopic Description Format Pre-prepared Dataset ISE Employees Only Pre-extracted OpenAI embeddings. These embeddings have undergone the default settings in the Module 2 notebook, in case you were facing any roadblocks. Saved as searchdf.gzip . Upload this to your own blob storage. PARQUET file Evaluation Dataset A set of test queries and expected results. Saved as evaluation_dataset.csv Upload this to your own blob storage. CSV  \nIterate & Improve  \nPlease help us improve this module, our references and resources and the product and accelerators you used.  \nModule 3 feedback, references and resources.  \nProvide Product Feedback to the PG here.  \nOnce Complete, Move to Module 4  \nModule 4",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\lab\\llm-lab\\module-3.md"
    },
    {
        "chunkId": "chunk347_0",
        "chunkContent": "rings:\n- internal  \nModule 4: Experimentation: Prompt Engineering  \nSuggested Time Box: 5 hours  \nWhy do we need to do prompt engineering?  \nPrompt Engineering is the process of manipulating the inputs to an LLM to give it context for the expected outcome of a query. Using a Prompt Engineering framework, we can iteratively refine the prompt context.  \nWhat will I learn in this module?  \nAfter completing this module, you will:  \nHave experience doing iterative prompt development.  \nKnow how to use tools such as few-shot context, session history, and other techniques to improve an LLM prompt.  \nKnow how to build a context programmatically using Prompt Engineering pipeline tools.  \nUnderstand that prompt engineering is not equal to model \u201cfine tuning\u201d a model.  \nModule Objective(s)  \nYou have an FFModel or Azure Machine Learning Prompt flow pipeline and your engineered prompt allows for a chat-like interface, including context from previous messages in the session.  \nSuccess Criteria  \nYour prompts return results that provide answers you deem reasonable.  \nStart here  \nThese links provide a starting point:  \nPrompt Engineering section of the Microsoft Solutions Playbook provides an overview of prompt engineering.  \nRemember to use your peers and the Lab Teams channel for additional resources:  \nReferences and Resources  \nTopic Description Format Prompt Engineering An overview of Prompt Engineering from the Solutions Playbook Article OpenAI RAG Guidance A repo with guidance on how to build a RAG (Retrieval Augmented Generation) solution Code Repo OpenAI RAG Example An example implementation of the RAG model Code Repo Azure Machine Learning prompt flow What is Azure Machine Learning prompt flow (preview) - Azure Machine Learning Microsoft Learn  \nFurther learning (optional)  \nTopic Description Format An Introduction to Large Language Models: Prompt Engineering and P-Tuning NVIDIA Technical Blog An overview of prompt engineering from Nvidia Article Semantic Kernel Semantic Kernel is an OSS SDK from Microsoft that can help orchestrate LLMs to string skills together for improved prompts (separate from evaluation) Documentation LangChain LangChain is a popular OSS tool for improved prompt engineering, similar to Semantic Kernel Documentation Learn how to customize a model for your application Azure OpenAI Service lets you tailor our models to your personal datasets using a process known as fine-tuning. This customization step will provide higher quality results than what you can get just from prompt design, ability to train on more examples than can fit into a prompt and lower-latency requests. Documentation  \nIterate & Improve  \nPlease help us improve this module, our references and resources and the product and accelerators you used.  \nModule 4 feedback, references and resources.  \nProvide Product Feedback to the PG here.  \nOnce Complete, Move to Module 5  \nModule 5",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\lab\\llm-lab\\module-4.md"
    },
    {
        "chunkId": "chunk348_0",
        "chunkContent": "rings:\n- internal  \nModule 5: Experimentation: Environment & Evaluation  \nSuggested Time Box: 4 hours  \nIn this module, you will Read & Explore information on defining evaluation frameworks and you will execute pre-defined steps to set up an experimentation environment.  \nWhy do we need an evaluation framework?  \nFrom the previous modules we know our search results pass our metrics and our prompt is returning good results, but we need to evaluate the outputs of the overall AOAI solution automatically. Evaluating the performance of a model requires an informed strategy of how we will query, and what we expect to return. Defining an informed strategy requires the knowledge and experience that data scientists have accumulated through many projects.  \nOnce we define the metrics (query/response) we need to have a way to evaluate the results. This can be \u201chuman in the loop\u201d or automated. The combination of the metrics, how we run a test against the metrics, and how we evaluate the results is our framework. These iterative test/evaluate loops are part of the overall experimentation stage.  \nWhy do we need an experimentation environment?  \nThe performance of a model is improved through iterative build, test, and evaluate loops. The specific actions that are done in this phase vary depending on the type of model and the solution(s) that will use it. The build, test and evaluate loops require managing tooling, versioning and other operational processes which can be time consuming and for some, monotonous. An experimentation framework enables engineers to programmatically create experimentation pipelines, which can be versioned and managed. More crucially, these frameworks allow us to put our evaluation tools within our pipeline. Simply put:  \nExperimentation frameworks save time and automate repetitive processes.  \nWhat will I learn in this module?  \nAfter completing this module, you will:  \nHave exposure to defining an evaluation framework.  \nHave knowledge to select an experimentation framework.  \nUnderstand how to use your chosen framework to test and evaluate your solution  \nUnderstand why a mature evaluation strategy is crucial in prompt engineering.  \nModule Objective(s)  \nMeasure the performance of your AOAI solution by automating test/evaluation loops against your evaluation framework.  \nSuccess Criteria  \nYou have a suite of tests written.  \nYou have an initial pipeline of evaluation components.  \nNote that your evaluation tests will likely be failing (Think Test Driven Development methodology). Start with a simple test and build from there, such as a manual human-in-the-loop evaluation.  \nStart here  \nThese links provide a starting point:  \nModel Experimentation to understand the overall approaches for the Experimentation stage.  \nFramework for Foundation Models and FFModel Tutorial to learn about this experimentation framework.  \nAzure Machine Learning prompt flow to learn about this LLM development framework.  \nRemember to use your peers and the Lab Teams channel for additional resources.  \nReferences and Resources  \nTopic Description Format MLOps for OpenAI Prompt Engineering MLADs session that gives a good overview into MLOps process for components that should be developed using Prompt Engineering techniques Vector Search with Azure Cognitive Search Performing Vector Search with Azure Cognitive Search Learning  \nFurther learning (optional)  \nTopic Description Format Retrieval Augmented Generation using Azure Machine Learning prompt flow RAG is a feature that enables you to harness the power of LLMs with your own data. Video FFModel Docs Link to walkthrough of FFModel, and official repo Code Repo Conversational AI Test & Training Tool (DSTK) Data Science Toolkit accelerator  \nIterate & Improve  \nPlease help us improve this module, our references and resources and the product and accelerators you used.  \nModule 6 feedback, references and resources.  \nProvide Product Feedback to the PG here.  \nOnce Complete, Move to Module 6  \nModule 6",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\lab\\llm-lab\\module-5.md"
    },
    {
        "chunkId": "chunk349_0",
        "chunkContent": "rings:\n- internal  \nModule 6: Deploy into an Enterprise Environment  \nDeploying the search and prompt solution to an enterprise-grade endpoint, so that it can called by the client application (e.g., PVA)  \nWhy do we need to do enterprise deployment?  \nWhat you will learn  \nDeployment of AML endpoints using Prompt flow or CI/CD or building Azure Functions  \nProvision AML endpoints and deployments  \nAML Environments  \nTroubleshoot and test deployments  \nTraffic allocation between deployments  \nWriting Client consuming endpoints  \nModule Objective  \nThis is the endpoint that Power Virtual Agents or the front-end application will call.  \nSuccess Criteria  \nYour search/prompt application is accessible via a REST API call, and can be called from PVA or a Chat Application  \nTopic Description Format Prompt flow - Deploy a managed endpoint Deploy a flow as a managed online endpoint for real-time inference (preview) - Azure Machine Learning Microsoft Learn Azure Machine Learning online endpoints What are online endpoints? - Azure Machine Learning Microsoft Learn  \nFurther learning (optional)  \nTopic Description Format Create a Python Azure function from CLI Quickstart: Create a Python function in Azure from the command line. Microsoft Learn Create Azure Functions with CLI Create a Python function from the command line - Azure Functions Microsoft Learn Call Azure Function with Postman Running Azure Functions with Postman Microsoft Learn  \nIterate & Improve  \nPlease help us improve this module, our references and resources and the product and accelerators you used.  \nModule 6 feedback, references and resources.  \nProvide Product Feedback to the PG here.  \nOnce Complete, Move to Module 7  \nModule 7",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\lab\\llm-lab\\module-6.md"
    },
    {
        "chunkId": "chunk350_0",
        "chunkContent": "rings:\n- internal  \nModule 7: Customer Pattern Application  \nExpose results of existing AI experiment through Natural Language conversational interface using Power Virtual Agents (PVA).  \nWhat do I need to connect to a Power Virtual Agent?  \nAPI for existing experiments  \nPower Platform environment (for Power Virtual Agents and Power Automate)  \nAzure Active Directory access  \nWhat will I learn in this module?  \nHow to create a chatbot using Power Virtual Agents (PVA)  \nHow to call external services from PVA  \nHow to surface PVA chatbot in multiple channels (Teams, web, etc.)  \n(optional) How to create a custom connector to access APIs  \nModule Objective(s)  \nSuccess Criteria  \nYou can use a conversational interface to interrogate the AI model you created in earlier modules  \nStart here  \nThis module assumes that there is an existing prompt engineering pipeline with an evaluation framework in place and is exposed via an API. The References and Resources section includes several options for sending requests to the pipeline API.  \nReferences and Resources  \nTopic Description Format Create a Power Automate Flow for PVA Consumption Steps for creating a Power Automate Flow for PVA consumption. Flows can perform HTTPS requests. Documentation Create a Custom Connector for Power Platform Overview of how to create a custom connector for Power Platform. Custom connectors eliminate the need for HTTP headers and response parsing. Documentation Create a custom connector from an OpenAPI definition Steps for using a swagger file to create a custom connector Documentation Power Virtual Agents in a Day Update \u2013 New Canvas, New Content! Microsoft Power Virtual Agents Introduction to PVA Documentation and labs  \nFurther learning (optional)  \nTopic Description Format Quickstart guide for building bots with GPT (preview) - Power Virtual Agents Minimal steps necessary to get started quickly in creating and boosting a chatbot with expanded natural language understanding (NLU) capabilities Documentation Quickstart: Create and deploy a bot on the portal (contains video) - Power Virtual Agents Quick guide for creating a bot for the first time. Includes adding topics, testing content changes, deploying to a test page, and analyzing performance Documentation  \nIterate & Improve  \nPlease help us improve this module, our references and resources and the product and accelerators you used.  \nModule 7 feedback, references and resources.  \nProvide Product Feedback to the PG here.  \nOnce Complete, Move to Module 8  \nModule 8",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\lab\\llm-lab\\module-7.md"
    },
    {
        "chunkId": "chunk351_0",
        "chunkContent": "rings:\n- internal  \nModule 8: Share What You Learned  \nFeedback is a gift  \nYour feedback is crucial for our ongoing improvement of content, tools, and the overall experience for future participants. You will get to see what our customers will see when they create and deploy solutions. You will encounter the same difficulties, obstacles and issues as our customers, so please take time to help us enhance our products, solutions and trainings.  \nImprove the overall lab experience by providing feedback @ aka.ms/ISEUpskillingFeedback  \nImprove our products by logging product feedback @ aka.ms/ISEUpskillingProductFeedback and creating a new Product Feedback work item as a Child link  \nImprove our Microsoft Solutions Playbook by opening a PR",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\lab\\llm-lab\\module-8.md"
    },
    {
        "chunkId": "chunk352_0",
        "chunkContent": "Supplemental Learning",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\lab\\llm-lab\\supplemental\\index.md"
    },
    {
        "chunkId": "chunk353_0",
        "chunkContent": "Module 1: Investigation Stage  \nThis lab is designed to be a simulated customer Machine Learning engagement. The first stage in these engagements is investigation. This is a Read & Explore module to give you the background context needed for the lab.  \nThe investigation stage for LLM engagements is covered in depth in the Customer Solutions Planning Tutorial (see schedule and sign-up).  \nWhy do we need an investigation stage?\nApplying a rigorous plan for each of the five pillars in this stage is critical for responsible, safe and secure deployments of LLM solutions into an enterprise environment.  \nBefore we start defining an architecture or writing code, we need to help our customers answer questions that we have identified as indicators of successful deployments in ML engagements:  \nDo they have the right experience for an ML project? ML projects require Data Science and ML engineering expertise. If the customer does not have these skills in-house, they will need an engineering partner or extra support from Microsoft. Lack of dedicated resources with these skills is a blocker.  \nWhat is the business challenge they need to solve? A clear and concise business challenge is a keystone which defines business value (differentiates PoC from deployed solution) and informs subsequent investigation questions. Understanding the customer\u2019s success criteria (\u201cwhat must be true to solve this challenge\u201d) will help us define work, evaluate our solution, and determine when we are done.  \nCan LLMs help solve this problem, or should we use another approach? Not all problems are LLM problems. We need to understand the underlying reason they have not been able to solve the business challenge. For example, we often see attempts to use AI to mitigate a bad data platform. We need to fit the solution to the problem (i.e. in this example, solve the data platform issues first \u2013 more on this in module 2).  \nDo we have enough of the right data for this challenge? We need to understand data quality, quantity, and profile. We need significant volume of clean, relevant data to use ML. If they do not have enough of the right data to solve the business challenge, we may explore synthesizing it or pausing the engagement until they have the required data.  \nWhat technologies can help us build and test a solution that meets our success criteria? This is where we look for the \u201cengineering truth\u201d. While there is a lot of excitement around LLMs and OpenAI, there may be a better solution for their challenge. This could be a different pre-trained model, custom ML model, or another non-AI approach.  \nHow do we ensure our solutions are responsible, safe, and secure? Responsible AI is a core value for all engagements. We need to explore each pillar with our customers with compliance as a dependency for ISE\u2019s involvement.  \nWhat do we need to do to ensure we can deploy our solution into an enterprise environment? Pre-trained models are ML models. This means we need an end-to-end infrastructure to build, iteratively improve, deploy, and use our LLM solutions. This includes DataOps, MLOps, DevOps, Security principles and practices. We need to understand our customer\u2019s existing infrastructure, policy and regulation requirements as well as any other dependencies for enterprise deployments.  \nWhat will I learn in this module?  \nIn this module you will:  \nGain knowledge needed to guide investigation discussions in each pillar of this stage.  \nSee an example of a customer business challenge and success criteria.  \nUnderstand how a use case can map to a business challenge.  \nInvestigation Stage Outcomes  \nYou can explore simulated customer investigations in the Customer Solutions Planning Tutorial (see schedule and sign-up). For this lab, we have done the work for you with the following outcomes:  \nPrinciples & Agreements  \nWe have worked with our customer to explore our Responsible AI (RAI) pillars. We used the knowledge and guidance from ISE\u2019s Responsible AI wiki page to help us understand how to apply RAI to our customer engagements.  \nMachine Learning Readiness  \nWe are confident that our customer had the skills and experience for a successful AI engagement. We used the AI readiness survey (aka.ms/customerAIreadiness) to guide our discussions. This was an active discussion where we filled out the survey as we would not have the same in-depth understanding of their readiness if we had simply asked them to fill it out.  \nBusiness Challenge  \nYou have worked with your customer to identify a real-world challenge that is impacting their business.  \nProblem Statement:\nIndustry Solutions wants to understand the business problems that we repeatedly see with our top customers . Customer Patterns are currently not captured, or are in undiscoverable locations, and are often defined at different levels of granularity. This means we have incomplete or incorrect information about the highest priority problems. This limits the customer impact we drive with our investments.  \nISE records the problem statement and success criteria for each of our customer engagements in our Game Plan documents. These are the discrete signals that are analyzed to identify customer patterns. The number of documents and variances on how problems and success criteria are written limits the speed and accuracy of patterns created through manual analysis.  \nUse Case:\nThis lab will focus on the Retrieval Augmented Generation (RAG) use case, where attendees will work in groups to build a solution that retrieves Game Plan documents and other relevant content and submits them to the Azure OpenAI service for analysis and returns a natural language summary of the most common Customer Patterns in the format:  \nFunctional Architecture:\nWe have worked through the requirements for this solution and have identified the following technologies we want to explore before selecting the products and services:  \nAzure OpenAI service\n\u2022 Promptflow  \nStart here  \nThese links provide a starting point. Remember to use your peers and the Lab Teams channel for additional resources:  \nSource Data <link> for this lab  \nResources & Support  \nGuidance & support:  \nProvided reference materials  \nYour team chat and breakout room  \nThe Lab channel (asynchronous)  \nYour coach  \n[Lab Feedback]    [Product Feedback]   [Suggest Resources for this module]",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\lab\\llm-lab\\supplemental\\module-1a-investigation.md"
    },
    {
        "chunkId": "chunk354_0",
        "chunkContent": "LLMOps: Operational management of LLMs  \nLLMOps is the collection of tools and processes that manages the end-to-end process of developing, deploying, and maintaining LLM-based applications. It is a multifaceted orchestration that navigates the complexities of developing and deploying LLMs and other solution components. In summary, LLMOps provides a cohesive workflow that facilitate the entire lifecycle of LLM applications, from initiation to production.  \nThe stages of LLMOps workflow are categorized into inner and outer loops. The inner loop focuses on the iterative process to develop, test, and refine the solution. The outer loop focuses on deploying and managing solutions in production. The following diagram represents our current understanding of these stages and workflow:  \nData Curation includes exploratory data analysis to understand what data is available, data transformation to create a clean and consistent dataset and enrichment to add additional, pertinent data.  \nExperimentation aims to identify the best potential LLM solutions through careful monitoring and auditing of rapid iterations of testing various techniques, such as prompt engineering, information retrieval optimization, relevance improvements, model selection, model fine tuning, and hyperparameter tuning.  \nEvaluation is the process defining tailored metrics, and selecting methods of comparing results to them at key points that contribute to overall solution performance. This is an iterative process to see how changes impact solution performance such as optimizing a search index as information retrieval for RAG implementations or refining few-shot examples through prompt engineering.  \nValidate & Deploy stage involves rigorous model validation to evaluate performance in production environments and A/B testing to evaluate new and existing solutions before deploying the most performant ones into various environments.  \nInference involves providing an optimized model tailored for consistent, reliable, low-latency and high-throughput responses, with batch processing support and compatibility with edge devices (if needed).  \nMonitor covers tools and practices to assess and report on system and solution performance and health. Monitored areas include tracking resource utilization, raising real-time alerts for anomalies or data privacy breaches, and evaluating queries and responses for issues such as inappropriate responses.  \nFeedback & Data Collection requires seamless mechanisms for user feedback collection, capturing user provided data for insights and to enrich the validation datasets to improve the LLM solution\u2019s performance, while ensuring privacy and compliance.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\mlops-in-openai\\index.md"
    },
    {
        "chunkId": "chunk355_0",
        "chunkContent": "LLMOps for RAG  \nLLMs are powerful tools in NLP. RAG is a pattern that uses pre-trained LLMs and your own data to generate responses. It integrates information retrieval into LLM text generation. In this guidance, we will explore the basics of LLMOps for RAG pattern using LLMs and how to operationalize and manage them effectively.  \nTechnology agnostic implementation  \nRetrieval Augment Generation (RAG) is using your own data. The problem is that the initial data is not always ready (in fact, they are always not ready) to be used by the RAG pipeline. In most cases, data should be prepared and transformed in many ways. The most common steps for data preprocessing pipelines are:  \nExtract data from documents like grab text from pdfs, extract entities and additional metadata, convert documents into a special format, and generate summaries.  \nChunk all big data elements into smaller blocks. Obviously, the chunking mechanism can be different and requires many tries to pick the best one.  \nConvert texts into embeddings. We need to represent texts in vector format (a sequence of numbers).  \nStore vector data in a special data store. We need to store not just vector data but references to the initial documents as well.  \nMake data available to a search system and define a search strategy.  \nThe primary goal of this preprocessing pipeline is to make sure that our RAG pipeline will finally get needed data only to start doing its job. Counting that there are many chunking strategies, search algorithms and even techniques to generate embeddings, we need to run many experiments and do evaluation of their results. Once we find the best configuration of the pipeline, it should be available in production to preprocess all existing data as well as to preprocess all new incoming documents upon request. Let\u2019s call this pipeline as the Data Injection Pipeline, and it\u2019s going to be our first LLMOps artifact.  \nThe second artifact is the RAG pipeline that can vary from a basic single step to a complex multi-steps flow. As a basic example we can look at a pipeline that gets a question from a user and generates an answer based on a response from a search service. More complex pipeline can have several steps and call various APIs. Building a RAG pipeline, you often need to apply prompt engineering techniques and run many experiments. Results should be validated as well. Once the RAG pipeline is ready it can be published as a service.  \nTherefore, we have two pipelines that should be supported by LLMOps: the Data Injection Pipeline and the RAG pipeline. At the same time, we mentioned that experimentation results should be validated during the development process. It means that we need two more LLMOps artifacts: the Data Injection Validation Pipeline and the RAG Validation Pipeline. So, we have four artifacts in total.  \nData Injection and its validation pipelines example.  \nImplementation using Azure Machine Learning  \nLet\u2019s look at some important characteristics of the Data Injection Pipeline:  \nIt\u2019s a batch pipeline that must work with many documents at the same time, but all the documents can be processed independently.  \nIt requires different cluster sizes to preprocess documents: the number of documents to process usually have high variance. For example, when we preprocess the initial data source it can be millions of documents, but after that it can be a few documents per week. That\u2019s why we need a flexible way to pick a cluster with a different number of instances.  \nIt should be an ability to publish the pipeline as a service and execute by schedule. This ability is very important if we are waiting for new incoming documents in the production environment where we don\u2019t have a DevOps system to orchestrate pipeline runs.  \nAn SDK should be available. It\u2019s important to have a tool to build the pipeline, execute it, deploy it, and orchestrate resources.  \nA straightforward way to add a validation pipeline. RAG pipeline or its components should be reused for validation with no modifications. In this case we don\u2019t need to build two different pipelines from scratch.  \nLogging should be available. It\u2019s important during the experimentation phase when we need to run many experiments and compare their results.  \nLooking at the requirements from above, we would recommend Azure ML as the primary platform to build and deploy the Data Injection Pipeline. Thanks to Azure ML we will be able to:  \nImplement a pipeline with as many steps as needed where each step can be executed in a single node or on a cluster with many nodes in parallel. For example, parallel job is a good way to preprocess documents, chunk them and save into a vector DB.  \nStarting a pipeline, it\u2019s possible to specify a cluster for execution on per step basis as well as a few nodes. So, we can use a bigger cluster for initial injection and smaller cluster later. Azure ML can wake up the cluster automatically, and customers must pay for real processing time only.  \nPipeline can be published as published batch endpoint, and it can be reused later. There is a way to invoke pipelines automatically by schedule or due to an event.  \nPipeline can be defined in YAML format or in Python. All the code outside the pipeline is Azure ML independent. An SDK and CLI are available to develop, execute and deploy pipelines.  \nEach step of the pipeline can be presented as a component that can be published and reused by other pipelines. So, we can construct the validation pipeline just using existing components of the injection pipeline and adding a few more steps to do validation.  \nAzure ML has integration with MLFlow, and AI Studio can represent all the results in a UI.  \nThe Data Injection Pipeline can be implemented using our standard MLOps process for Azure ML. This link provides a very detailed guideline about how to implement it. End-to-end coding template is available here.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\mlops-in-openai\\llmops-rag.md"
    },
    {
        "chunkId": "chunk356_0",
        "chunkContent": "LLMOps Concepts and Design  \nLLMOps is the collection of tools and processes that manages the end-to-end process of developing, deploying, and maintaining large language model-based applications. It encompasses data processing using data pipelines, model selection, experimentation and automated iterative process of prompt engineering, A/B tests, closed-loop fine-tuning, and continuous model evaluation. It involves deploying models/chains/flows through APIs and endpoints, ensuring seamless integration with diverse downstream applications. The process is automated end-to-end through CI/CD pipelines. It provides version control mechanisms to facilitate transparency and enable the rollback of changes if needed. LLMOps performs query logging, performance monitoring, and observability, and automates real-time assessments and insights generation.  \nLLMOps vs DevOps for Common Scenarios  \nIn this section we will focus on LLMOps for the backend. Let\u2019s address the question on whether there really any difference between LLMOps and for this kind of backends general DevOps is. Let\u2019s look at a few prompt engineering tasks:  \nPrepare a list of samples that can be used in few-shot learning: for example, you would like to use OpenAI to convert a raw user input into a specific format that satisfies your organization requirements. Obviously, OpenAI models will not be able to understand your requirements if you don't supply a few samples. The quality of the responses will depend on the sample set provided. It is an iterative process to pick an initial set of few-shot samples and tune them until they produce the desired result.  \nGenerate a system message: the system message is a special text block in the beginning of a prompt where you supply context, instructions, or other information that make the user's request relevant to a desired use case.  \nExtend user's requests: the user\u2019s questions will not contain all the context necessary for the LLM to produce the right result. An important part of the prompt that the application backend needs to provide is to provide enough context for the user\u2019s request to make sense to the LLM, and for the LLM to produce the right result.  \nThe list of the tasks is not complete, but it allows us to understand that even in the case of very basic scenarios it's important for the developer to have means to run various experiments, try different prompts or even models, and compare their results, eventually picking the best prompt.  \nTherefore, the first significant difference when compared with traditional DevOps is the need for an Experimentation Platform. It should bring abilities to compare runs of each experiment, support different computes and feed a different set of data to generate as many requests as needed per a single run.  \nThe second important distinction is data availability. To test a generic web service, it\u2019s good enough to develop a few unit tests and implement a basic integration test, but in the case of a LLM backend it\u2019s important to have a representative dataset to see if the backend is performing consistently well. Small tweaks to prompts can produce vastly different results, and this can completely make or (unfortunately) break a solution. Therefore, testing the solution with a statistically significant number of samples is critical. It is critical that we have a metric that accurately conveys the objective of the application, and that metric is evaluated with adequate number and variety of samples. In some cases, it can be more than one dataset. For example, a toy dataset to run experiments on a local computer and a full dataset to run experiments in the cloud.  \nA very valuable concept of Prompt Flow is the concept of evaluation flows. Whilst the application itself consists of a standard flow (i.e. the flow as it can be converted directly into an endpoint and deployed), an evaluation flow is one that calls the standard flow, compares the result with a given ground truth, or produces a groundless evaluation, aggregates the result and reports on it. In the case of prompt flow, that result is stored as an Azure AI Studio Experiment, and can be compared with others.  \nTherefore, experimentation platform, data and evaluation flows are the most important aspects that bring differences into DevOps making it as LLMOps.  \nTechnology Agnostic LLMOps for Prompt Engineering Implementation  \nPrior to building the architecture, we need to build a few assumptions regarding to components that must be presented in any implementation. At least two of them should be available: the DevOps and Experimentation platform.  \nThe first component is the DevOps system, which is an obvious component. We assume that Git is one of the tools that should be used for prompt engineering. A prompt is not located in the vacuum, but it can be found in a code repository alongside experimentation scripts and notebooks. Therefore, if a software engineer or a data scientist is willing to create or tune prompts or related code, the first step there is to create a new branch. We would like to emphasize this obvious step here, because the branch name is an element that will allow us to build some rules around experiment naming. Any service that works on the top of Git including GitHub, Azure DevOps, Jenkins, GitLab, is good enough to host branches, invoke builds/flows and orchestrate the whole end-to-end process up to deployment into production.  \nAnother important tool is a service that allows us to log experiments alongside with their results. The tool should have an interface as well as API. It's hard to tell in advance what we will need to log (depends on metrics availability), but it's important to make sure that the service supports the ability to store experiment names and related runs including evaluation metrics and artifacts like prompts themselves and models. Looking at the existing technology stack, tools like MLFlow and Azure Machine Learning have solid UIs and SDKs. In general, it's possible to use any ML related service that supports experiment tracking for standard Machine Learning training workloads.  \nA code repository and service for experiment tracking are not sufficient on their own. It is equally important to introduce naming conventions and rules for experiments to enable parallel work by multiple engineers. Our recommended approach is in combining the branch name and the name of an LLM model. For example, if there is a branch branch_1 and code-davinci-002 model, the experiment name can be branch_1-code-davinci-002. This approach allows you to differentiate your own experiments from experiments of other developers in the team. Moreover, you can do different experiments using different models staying in one branch. It's important to note that every experiment may have many runs - run names can be generated automatically by the pattern.  \nThe goal of LLMOps at the first stage is to provide an experimentation environment, alongside scripts to programmatically run experiments, and log their results for simple evaluation.  \nAs an output of the experimentation phase, we have an updated LLM flow (inferencing flow) as well as related evaluation flow. The updates might include updated prompts, LLM configurations, additional steps in the LLM flow, better evaluation code and so on.  \nAll successful experiments should lead to Pull Requests (PRs) to move code and related artifacts from a feature branch into the development branch. The PR should include several important checks:  \nStandard checks like linting and unit testing.  \nInvoke the LLM flow with provided prompts using a toy dataset. The toy dataset should be good enough to decide about the performance of the flow. At the same time, the toy dataset should allow us to execute the PR build in a few minutes rather than hours.  \nExecute evaluation flow based on results from the previous step.  \nOnce the PR has been approved and merged, we would recommend updating the development environment that has all solution components deployed in their current state.  \nDeployment into the production environment may happen by schedule, by request or automatically.  \nMLOps at this stage must provide implementation for at least two things:  \nA/B testing is essential: it's a recommended practice to validate how the new prompt will generalize queries from real users. Depending on the platform that you are using to host the backend for your solution, it's possible to use Blue/Green deployment or Staging deployment, where a selected group of users (internal group or 10-20% of external users) start using the updated version of the service based on the new prompts. During A/B testing users may provide additional feedback or engineers can use their request-response records to assume about the new prompts\u2019 performance.  \nFeedback tracking: a service to collect users' feedback should be deployed and available for engineers to tune the prompt in the next iteration.  \nSome additional components that are related to observability/monitoring are required as well, but it\u2019s a common situation that they are part of the selected deployment platform.  \nTherefore, we can complete our MLOps process diagram:  \nUsing Prompt flow to implement LLMOps  \nBased on the architecture diagram, we can assume that LLMOps implementation is a straightforward process, and it\u2019s possible to use MLFlow as an experimentation platform and any DevOps system to implement DevOps pipelines. As a host for MLFlow it\u2019s possible to use Azure Machine Learning (Azure ML) that provides endpoints to log everything into Azure ML using MLFlow APIs. Moreover, it\u2019s possible to log local experiments as well, and Azure ML compute is not needed. Azure DevOps or GitHub can be used as a DevOps system.  \nTherefore, most of the work should be concentrated on the development of LLM flows. We already mentioned some libraries like Semantic Kernel and LangChain, but these libraries do not fully solve all the challenges. Let\u2019s look at some of them:  \nLocal experimentation environment: When developing a backend, you always have a deployment target in mind like AKS or WebApps. To test such a service in a local environment you need to extend it with some added tools like docker, a web service component and so on. Moreover, you need to develop scripts that will invoke a local version of the service several times to collect results from all the requests that can be generated based on the toy dataset. Also, in some cases (when you don\u2019t have data on a local computer) you would like to start experiments from a local computer but execute them in the cloud to make sure that the backend has access to all the resources.  \nSharable components: Some components of your backend or the flow itself might be a good candidate to be shared as components for other flows. In some cases, it\u2019s not enough to create coding artifacts, but a place to publish and make them available is also required.  \nAccess to data: A toy dataset to evaluate LLMOps flows can be stored in the repository itself. This might not be true for the full dataset. It means that all scripts that test the backend and run evaluation should have access to local data as well as to data in the cloud.  \nDeployment targets: It will be great to use the same code base to deploy it as a backend using various deployment targets including AKS, WebApps, and Online endpoints.  \nPrompt Templates: This challenge has been solved in LangChain as well as Semantic Kernel, but it\u2019s still important to emphasize it here: prompts should not be mixed with code, and they should be stored in separate template files. It helps to log them every run and pick the best prompt later.  \nManage connections: All LLMOps flows require the storage of connections at least to a service that hosts LLMs, but it can be more than that. For example, connections to search services or databases should be stored as well. Moreover, it should be a way to provide the connections on a local host as well as in the cloud with abilities to change them with no redeployment.  \nRoadmap from experiments to inferencing: In a perfect world we should be able to use the same flow to run experiments as well as to be executed as a service in production.  \nLogging infrastructure: The LLMOps flow must log some debug information and metrics, and it\u2019s OK to use MLFlow as a platform for that. But, once the service has been deployed into production, a real-time, enterprise-grade logging system should be used (such as Application Insights. Importantly, the change between logging systems should be done automatically.  \nTherefore, there are still some challenges that are related to code orchestration, deployment, and execution. Some of these challenges can be solved by custom code, and some of them can be ignored by losing flexibility (for example, you can fix a deployment platform and even duplicate code that you would like to reuse), but there is a tool that allows us to solve all the challenges from above out of the box. This tool is Prompt flow that is a part of Azure ML service, but it can be used even independently (with reduced number of features).  \nLet\u2019s look at the challenges from above and see how Prompt flow can solve them:  \nLocal experimentation environment: Prompt flow supports SDK and CLI to execute LLMOps flows locally or in the cloud using a single input record or a batch. In both cases, you have access to output from every run of your experiment and can use it to execute an evaluation flow just by providing a run id. Moreover, using a single line in CLI you can spin up a development service to serve your flow. It\u2019s important to implement integration tests even on a local computer.  \nSharable components: Prompt flow flows can be published as Azure ML components, and it\u2019s possible to reuse them across the organization thanks to Azure ML Registries. Moreover, each flow step can be represented as a reusable tool  \nAccess to data: When executing LLMOps flows, it\u2019s possible to use a local batch of data as well as a data asset from Azure ML if you execute your flow in the cloud. You should not modify your code to have this feature.  \nDeployment targets: Prompt flow supports abilities to publish LLMOps flows using Azure Kubernetes Service (AKS), WebApps, or Azure ML Online Endpoints. Prompt flow provides a way to build an image based on your flow. So, potentially, you can deploy your image in any environment that supports containers.  \nPrompt Templates: Prompt flow has a few tools to simplify flows development, and one of them is ability to use Jinja templates that allows us to store templates separated from our code and generate some components of the templates dynamically.  \nManage connections: Azure ML provides a central repository that allows us to manage connections to other services. These connections can be used if you execute your flow in the cloud or deploy it as an online endpoint. If you prefer local execution and deploying your flow in a container, there are some commands that allow us to create and encrypt needed connections to use them locally.  \nRoadmap from experiments to inferencing: In Prompt flow you are using the same code to run experiments locally or in the cloud as well as to deploy the backend into production  \nLogging infrastructure: Prompt flow supports independent logging systems that can utilize MLFlow, Application Insight or log everything locally depending on how you execute the flow. In any case, no code changes are needed.  \nFinally, Prompt flow supports a UI designer that is available in Azure ML Studio as well as in Visual Studio Code. The designer allows us to build complex flows and convert them into code and back.  \nTo show how to implement LLMOps for Prompt Engineering we would like to recommend a template that includes all needed code and DevOps pipelines (Azure DevOps and GitHub). The coding template is available by this link",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\mlops-in-openai\\llmops.md"
    },
    {
        "chunkId": "chunk357_0",
        "chunkContent": "Tools & Resources  \nISE  \nTool/Accelerator Description Workflow Stages - Mapping RAG Experiment Accelerator The RAG Experiment Accelerator is a versatile tool designed to expedite and facilitate the process of conducting experiments and evaluations using Azure Cognitive Search and RAG pattern. Stages 1(Data Curation), 2 (Experiment), and 3 (Evaluate) Rag Learnings This repository contains collective learnings from Microsoft engagement teams (Flipkart, CTC, Moody\u2019s, NAB-Expert Finder, Coles) building Retrieval-Augmented Generation (RAG) solutions. It documents approaches to deal with common architecture, engineering, experimentation, and deployment challenges. All stages FFModel-Data (WIP) The FFModel Data System is a dataset management tool for creating, labeling, editing, and reviewing your evaluation datasets for experimenting with foundation models. It fills the gap between inference and experimentation in the development cycle of Large Language Models (LLMs) based solutions. It is designed to work with various development tools including FFModel and Prompt Flow. Stage 1(Data Curation) FFModel FFModel is a framework to support Foundation Model Ops. The goal is to accelerate the development, experimentation, operationalization, and life-cycle management of solutions based around Large Language Models (LLMs). Stages 1(Data Curation), 2 (Experiment), 3 (Evaluate), 4 (Validate & Deploy), 5 (Inference) and 7 (Feedback & Data Collection) MLOps accelerator for FFM MLOps Accelerator for FFModel(Framework for Foundational Models) helps with prompt experimentation, evaluate and compare prompts, human in loop prompt approval, deploy prompt configuration to Azure Machine Learning, test prompt configuration and availability after deployment. Stage 4 (Validate & Deploy) LLMOps basic template This code asset help implement MLOps process for Azure Open AI workloads using Prompt Flow SDK and code-first approach. The repository contains all needed components to help developing and deploying one or more Prompt Flow pipelines starting from local execution and up to online endpoint deployment. Stage 4 (Validate & Deploy) LLMOps advanced template LLMOps with Prompt Flow is a \"LLMOps template and guidance\" to help you build LLM-infused apps using Prompt Flow. It offers a range of features including Centralized Code Hosting, Lifecycle Management, Variant and Hyperparameter Experimentation, A/B Deployment, reporting for all runs and experiments and so on. Stage 4 (Validate & Deploy) MLOps template with semantic kernel A code repository demonstrating process to tune and engineer prompts, log and evaluate results, and publish a service to serve users' requests using Semantic Kernel. Please note that this repo will be significantly updated/enhanced as soon as we have our next material customer opportunity. Stage 4 (Validate & Deploy) ISE Observability Accelerator The sample is designed to demonstrate how to instrument data automatically and manually in variety of languages within a distributed application, as well as provide similar examples for visualization and alerts based on this incoming data. Stages 5 (Inference), and 6 (Monitor) Building GenAI Gateway.docx This document serves as an essential guide for engineering teams tasked with designing and implementing a gateway solution involving Azure OpenAI resources. It aims to equip teams with the essential guidance and reference designs required to build a Generative AI (GenAI) gateway that can efficiently handle and optimize GenAI resources utilization, facilitate seamless integration and distribution of workloads across various deployments, as well as enable a fine-grained monitoring and billing. Stages 5 (Inference), and 6 (Monitor) LLM Security Guidance Security guidance and an intro to high level LLM specific threats Stages 1 (Data Curation), 5 (Inference), and 6 (Monitor) Threat Assessment Guidance High-level intro to various AI/ML specific attacks Stages 5 (Inference), and 6 (Monitor) ISE Secure Score Stages 5 (Inference), and 6 (Monitor) GenAI Data Toolkit Stage 1(Data Curation)  \n1st/3rd Party  \nWorkflow Stages 1st/3rd Party Tooling 1. DATA CURATION Microsoft Presidio PII Detection cognitive skill - Azure Cognitive Search Data obfuscation in Data Factory with Delphix Compliance Services Vector Database Samples in Azure Excel/CSV 2. EXPERIMENT PromptFlow Methodologies: PEFT , LoRA , QLoRA bitsandbytes (Python package) 3. EVALUATE LangChain Evaluators Evaluation Flows Azure Managed Grafana 4. VALIDATE & DEPLOY Prompt flow debugging Prompt flow using Kubernetes Prompt flow as a Web App LangSmith RAI Red Teaming Content Filtering Azure Event Hub Azure Data Explorer Azure API Management AOAI- APIM Azure Traffic Manger Cost Monitoring Monitor Azure Open AI Azure Cost Monitoring and Billing Git, Azure ML Model Repository 5. INFERENCE Guardrails AI Nemo-Guardrails 6. MONITOR Model Monitoring Azure monitor Open Telemetry 7. FEEDBACK & DATA COLLECTION Monitor Overview",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\mlops-in-openai\\tooling-resources.md"
    },
    {
        "chunkId": "chunk358_0",
        "chunkContent": "Large Language Model Operations (LLMOps) Workflow Capabilities  \nLLMOps  \nLLMOps refers to the set of practices, techniques, and tools that facilitate the development, integration, testing, release, deployment, and monitoring of LLM-based applications. For an enterprise Generative AI Application deployment, the following are comprehensive list of capabilities needed:  \nWorkflow Stages Capabilities 1. DATA CURATION Guardrails to ensure data privacy and security Handling noisy and incomplete data (e.g. spelling errors, punctuation, abbreviations etc.) Data enrichment and augmentation Data store, review, and management 2. EXPERIMENT Iterative prompt engineering Closed loop fine tuning LLMs optimization for speed, accuracy, and resource efficiency 3. EVALUATE Solution evaluation Query logging and performance monitoring 4. VALIDATE & DEPLOY Error analysis, debugging Configuration management Rollback Mechanism Advanced deployment strategies (Kubernetes/WebApp) A/B experimentation Continuous Delivery Pipeline Containerization & container orchestration Integration testing Automated testing A/B testing for state changes Version control & code quality analysis 5. INFERENCE Inference Guardrails 6. MONITOR Model Inference monitoring Observability   - (Real-time, transparent, extensible, AI assisted assessment) 7. FEEDBACK & DATA COLLECTION Reporting and Insights generation  \nMonitoring and Observability  \nMonitoring and observability afford consistent observation of performance metrics for the LLM, including detection of data drifts and relaying information about the model's performance to relevant stakeholders. For an enterprise Generative AI Application to reach an observable state, the following capabilities need to be implemented and enabled:  \nWorkflow Stages Capabilities 1. DATA CURATION Logs : collect Logs of activities and orchestration workflow to help with debugging, troubleshooting, and auditing Data Collection and Storage : collect and store data for analysis and historical reference, helping with performance optimization and auditing. Metric Collections : define and capture relevant metrics related to response times, resource utilization, and other performance indicators 2,3. EXPERIMENT/EVALUATION Logs : collect Logs of activities and orchestration workflow to help with debugging, troubleshooting, and auditing Data Collection and Storage : collect and store data for analysis and historical reference, helping with performance optimization and auditing. Metric Collections : define and capture relevant metrics related to response times, resource utilization, and other performance indicators Error Tracking : Identifying and tracking errors, exceptions, and issues with Generative AI Applications Distributed tracing : monitor and trace requests across multiple orchestrator workflow, providing insights into performance and dependencies Visualization : Create visual representations of data and metrics for a clear view of LLM\u2019s and Generative AI Applications\u2019 performance and behavior Anomaly detection : use advanced machine learning and statistical techniques to detect unusual behavior or patterns that may indicate issues or opportunities for improvement 4. VALIDATE & DEPLOY Logs : collect Logs of activities and orchestration workflow to help with debugging, troubleshooting, and auditing Data Collection and Storage : Collect and store data for analysis and historical reference, helping with performance optimization and auditing. Metric Collections : Define and capture relevant metrics related to response times, resource utilization, and other performance indicators Error Tracking : identifying and tracking errors, exceptions, and issues with Generative AI Applications Distributed tracing : monitor and trace requests across multiple orchestrator workflow, providing insights into performance and dependencies Visualization : Create visual representations of data and metrics for a clear view of LLM\u2019s and Generative AI Applications\u2019 performance and behavior Alerting : setup alerts to notify DevOps or concerned teams when predetermined thresholds or anomalies are detected Anomaly detection : use advanced machine learning and statistical techniques to detect unusual behavior or patterns that may indicate issues or  opportunities for improvement Traffic analysis Quota Management Load Balancing Billing and usage Managed Service Monitoring Monitoring Cost : monitoring cost for LLM models along with other cloud resources 5. INFERENCE Logs : collect Logs of activities and orchestration workflow to help with debugging, troubleshooting, and auditing Data Collection and Storage : Collect and store data for analysis and historical reference, helping with performance optimization and auditing. Metric Collections : Define and capture relevant metrics related to response times, resource utilization, and other performance indicators Distributed Tracing : Monitor and trace requests across multiple orchestrator workflow, providing insights into performance and dependencies. Visualization : Create visual representations of data and metrics for a clear view of LLM\u2019s and Generative AI Applications\u2019 performance and behavior Anomaly detection : use advanced machine learning and statistical techniques to detect unusual behavior or patterns that may indicate issues or opportunities for improvement Traffic analysis : Analyze incoming and outgoing traffic to understand how the Generative AI Application is used and identify usage patterns  \nSecurity & Privacy  \nWhen building solutions that incorporate LLMs and Generative AI securing best practices must be applied. Implementations must not neglect the basics such as authentication, authorization, protecting customer data, etc., like any other cloud solution. After we have addressed the basics, there are specific aspects for LLM and Generative AI requiring special attention. Details vary depending on the implementation and are constantly changing in this fast-growing field, but understanding something about the properties allows us to address specific implementation needs. One way to think about securing it is to view the model as essentially intelligence in a black box that has two defining characteristics:  \nAI models are stochastic/non-deterministic, requiring an engineer to account for them spontaneously failing or being incorrect. In some sense it doesn't matter if the failure is intentional (due to poisoned training data, adversarial input, etc.) or unintentional (the model is not accurate). Engineers need to expect failure, and it is likely best to assume failure in the worst and most spectacular way.  \nIf you're giving it any data at all, either in the context window (via patterns like RAG) or by fine-tuning the model, it may include/leak data in its response.  \nBuilding on these two attributes of the following table provides guidance mapped to the phases of the Generative AI lifecycle:  \nWorkflow Stages Capabilities 0. INVESTIGATION & EXPLORATION Establish a strong security foundation for AI ecosystems Address the unique security considerations for AI Continuous assessment of the security posture of AI 1. DATA CURATION Guardrails to ensure data privacy and security Secure Data Processing Pipeline Secure AI Supply Chain Perform Threat Assessments 2,3. EXPERIMENTATION/EVALUATE Ensure the integrity of the training and evaluation data Secure AI Model development and training environments Adequate AI Alignment AI Model code and config is secured Secure AI Supply Chain Secure LLM Plugins Perform Threat Assessments RAI Impact Assessments 4. VALIDATE & DEPLOY AI Model validation and deployment environment is secured Secure LLM Outcomes and Responses Red Teaming LLMs 5,6. INFERENCE/MONITOR Ensure AI Model availability and integrity Secure LLM Plugins Threat Detection and Response Bounty Program Train users to quickly identify an anomaly Prevent Model Theft  \nData Platform Fundamentals  \nHaving the right dataset is crucial for any AI project, and GenAI is no exception. For general guidance on building data platforms, please check our playbook - https://playbook.microsoft.com/code-with-dataops/.\nBesides the typical hurdles encountered when building a data platform, the GenAI use cases introduce particular challenges that hold great significance within this domain.  \nWorkflow Stages Capabilities 1. DATA CURATION Vector Storage Data Privacy Data Chunking Data Pipeline Considerations Data Modalities Data Modeling 2. EXPERIMENTATION Data Virtualization",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\mlops-in-openai\\workflow-capabilities.md"
    },
    {
        "chunkId": "chunk359_0",
        "chunkContent": "Getting started with OpenAI  \nTo be successful implementing an OpenAI solution requires the same fundamentals to be in place as with any ML project. That said, working with a managed OpenAI solution does not present the same complexity as a custom ML solution in that the service is hosted and managed within Azure infrastructure, is predominantly used for inference only and is well integrated with a number of 1st party Azure services and that a large number of Open-Source solutions exist.  \nNevertheless, understanding the data, its suitability and representativeness to the problem to be solved, implementing a Hypothesis Driven Development Approach, tracking experiments and implementing CI/CD around ML Processes will always be fundamental to building an ML solution that yields a return on investment and is adopted by users.  \nSee our Before an engagement starts section for more info.\nSee our Before an engagement starts section for more info.  \n{% if ring.extra == 'internal' %}\nThis tutorial is based on using the Microsoft Framework for Foundational Models (FFModel).\n{% endif %}  \nEngagement Phases  \nThis section outlines the phases of an engagement primarily focused on utilizing large language models (LLMs) like Azure OpenAI Service. We will pay special attention to the data needs and recommended tools for each phase.  \nThis is meant to be a knowledge base for information targeted to data scientists and software engineers working on LLM engagements. For that reason, the format is kept straight forward. Before getting started, we recommend that engagement teams include software engineers, data scientists, and if possible, a user experience designer. Most, if not all, scenarios will require bespoke infrastructure and integrations (engineers), measurement and experimentation of LLM solutions (data scientists), and challenges with validating and handling model failures (designers).  \nAssumptions  \nThe project has already been identified as a candidate for utilizing LLMs.  \nEngagement Phase Glossary  \nCustomer \u2013 The enterprise that a dev crew works with. Customers own the end user experience.  \nEnd user \u2013 The users of the customer\u2019s feature or app, like employees or the customer\u2019s customers. They interact with the LLM-powered feature.  \nSolution or Use Case \u2013 An application of an LLM that you are building to address your end user scenarios.  \nProblem Formulation  \nThe goal of Problem Formulation is to refine the initial scope of the problem and define the specific scenario that will be solved. The success criteria that will be met should be defined here as well. Make sure to consider the specific value added to the end user that the solution needs to address. This should include an understanding of the UX and the success criteria from a user perspective.  \nRecommendations:  \nExpect your first formulation of the problem to be overly ambitious. The hype around these models and the reality of their performance differs significantly in enterprise settings. Therefore, you should focus on the minimum functionality required to meet the business objective. There is another phase to refine the problem statement later.  \nThink through the UX for the scenario that you are enabling, with a focus on the value that you are providing to the end users. No need to prototype or build anything at this stage. Use this to answer questions like:  \nWhat information about the user\u2019s session is available at the time of inference?  \nWhat is an acceptable user experience? What are you looking for in terms of \u201ccorrectness\u201d? Make sure to note these down as they will be key evaluation criteria later.  \nHow can you track the performance of your solution with end users? Some things to think about:  \nExplicit signals like thumbs up/down or accept/reject feedback.  \nImplicit signals like how many edits the end user makes to the response, if they bounce from the page, etc...  \nAsk yourself \u201cWhat task-specific information or data would an expert need to be successful in solving the use case, and how do I retrieve that information or data? \u201d This will help you refine what sources of information must be included in the solution to get the right answer.  \nRemember that the outputs from a LLM are non-deterministic and will occasionally be wrong. Make sure to think through any potential risks of showing the end user the results directly. Mitigation plans will be different depending on the scenario and the customer\u2019s risk tolerance.  \nStart thinking about failure cases now. Solutions around non-deterministic models require special attention to unhappy path handling. Are there any ways that you can minimize the impact of an incorrect generation?  \nIf possible, consult with the end users of the solution to understand their specific pain points. Also, the problem statement should come from the business objectives of the customer. Resist the urge to propose scope and scenarios based on the LLM hype. Everything must tie back to the customer\u2019s needs.  \nDeliverables:  \nAgreement on what success looks like for the end users. In addition, make sure you have answers to the following questions:  \nCan the user validate the accuracy?  \nHow is success measured?  \nWhat is success when the model is inevitably wrong?  \nClear enumeration of the targeted scenarios. Be as specific as possible.  \nA set of example user inputs and expected outputs to cover each scenario. Ideally these examples come from an end user or a project owner.  \nEnumeration of responsible AI concerns. You should start formulating your mitigation plan at this point, even if the details need to be experimented with.  \nData:  \nExample user input, output pairs for each scenario that you are targeting.  \nIt is important to capture variability in the data and have representation of all use cases.  \nWe recommend having multiple variations of prompt language, e.g. 10, for these points.  \nThese pairs should be sourced from multiple individuals to get a diversity of phrasings  \nIn addition, if your scenario requires additional data/context to be included in the prompt submitted to the LLM, make sure to have representative samples. For instance, in a Q&A, or a summarization case, it is useful to have samples of their documents.  \nTools:  \nConsider how you are going to capture and manage your data at this phase.  \nThe gold standard is a mechanism to capture data in-situ. This allows people in the same context as your end users to formulate their requests and give examples of what they expect back.  \nPreliminary Exploration  \nIn this stage, the goal is to explore different prompt ideas and qualitatively assess the output of the LLM. More specifically, a small dataset can be used to test different prompts and observe the output diversity, coherence, and relevance. This can help in understanding the power and limits of the LLM, defining data requirements, and planning for experimentation. Simple Jupyter notebooks or the Azure OpenAI playground can be used to interact with the LLM.  \nRecommendations:  \nAs you use the playground to explore the model\u2019s performance, make sure that you add any new data points you create to your dataset. When in the playground, it is often tempting to tweak the input string (that you\u2019d normally get from an end-user) to get the correct output. These should be thought of as different data points, as we have no control over what the user inputs.  \nThe goal of the exploration should be to get an idea of the model\u2019s performance and limitations. Also, take note of what types of additional information you need to pass in the prompt to get the correct answer. This will inform your solution.  \nThe less that the model knows about your subject, the more information you will need to pass through prompt engineering (or fine tuning). Also, the chance of hallucinations is higher in this scenario.  \nLook for key words or phrases to ground the model into your specific task at this point. A great example is specifying import statements for code generation scenarios.  \nLook for different ways you can encode this information when passing it to the LLM. A good example of this is providing table schema information as comments vs. as a CREATE TABLE statement in a SQL code generation scenario.  \nPerform Exploratory Data Analysis on your data set to understand the distributions.  \nNote down how you determine if the generation is correct. These will help dictate what post-processing and automated evaluation you should create for running experiments. A few guiding questions:  \nWhat pieces of the completion are you looking for?  \nAre there any red flags that tell you the result will be wrong? For example, invalid format or syntax, or an empty output?  \nHow do you think about correctness? Is it binary? Relative? Is close good enough?  \nAre there available metrics that can measure the above? or is there a need to implement custom metrics? For more information on possible metrics, please see section 5. When the playground becomes a limitation, consider moving to an experimentation framework {% if ring.extra == 'internal' %}like FFModel{% endif %}. This is when you move to the next phase and set your baseline.  \nA common limitation at this phase is working with many data points. More than a couple of data points gets tough to iteratively try in the playground.  \n\u26a0\ufe0f Warning: don't just work with a single example. Individual data points may have unknown quirks that either allow or prevent some prompt strategies that may not apply to other test cases. The key to building a successful project is finding a prompt strategy that works across all data points.  \nSee the Data Curation for more info.  \nDeliverables:  \nA preliminary decision on the feasibility of the outlined scenario to be accomplished. If not, return to the first phase with your new learnings.  \nIdeas for what elements may be included in the solution. This includes prompt engineering techniques that were promising in the playground and thoughts around if fine tuning is an option to consider.  \nIdentification of knowledge bases that need to be available for inclusion in the prompt. This could be documents to reason over, other examples of input/output pairs, or any other information that the LLM needs to generate a correct answer. In addition, identify any additional information that you need from the user\u2019s interaction. This could be things like what page they are on, previous interactions, or who they are.  \nData:  \nBeginning of your knowledge stores, which include any additional data/context need to be included in the prompt.  \nMinimum Viable Evaluation Set (MVES). The evaluation set should have good coverage of the expected user inputs.  \nConsider adding more examples to underrepresented categories of your scenario. This should be informed by the exploratory data analysis work.  \nThe data points that you created while testing in the playground.  \nRefer to the Exploratory Data Analyses section for more info.  \nTooling:  \nPython environment with Jupyter notebooks.  \nAzure OpenAI Studio \u2013 Gives a playground environment for completions and chat interfaces.  \nAzure Fabric -  Data source and analytics service together\u2014on a single, AI-powered platform.  \nAzure Synapse - Azure Synapse Analytics is an enterprise analytics service that accelerates time to insight across data warehouses and big data systems. It brings together the best of SQL technologies used in enterprise data warehousing, Apache Spark technologies for big data, and Azure Data Explorer for log and time series analytics.  \nExample EDA with OpenAI in Azure Synapse  \nEstablishing a Baseline  \nIn this stage, the goal is to establish baseline performance using a simple solution (e.g., static prompt with zero-shot learning). To measure the performance, a minimum viable evaluation set and evaluation metrics are needed. The evaluation set is the output of the previous phase. The evaluation metrics should capture the quality aspects of the output, such as accuracy or informativeness. Tools or environments that can efficiently call the LLM API are needed to generate and score the outputs.  \nRecommendations:  \nResearch different solution elements that worked for others in a similar scenario. This applies to both the prompting techniques and how you evaluate the outputs. Remember to review existing literature for guidance on evaluation metrics that can apply to your scenario. Refer to prompt engineering and fine tuning for detailed guidance on these topics.  \n{% if ring.extra == 'internal' %}  \nCheck the FFModel tutorials to see if one overlaps with your scenario. This can be used as a baseline. Make sure to swap in your data set and expand on the evaluation metrics for what success looks like to you.\n{% endif %}  \nPerform human evaluation on the output results. This serves a few roles:  \nHelps you identify which types of points are underperforming in which ways.  \nGives a ground truth evaluation metric that you can look for correlations with against your automated metrics. The stronger the correlation, the more you can trust your metrics.  \nBuild a PoC for the end user experience to consume your inference endpoint. You should not invest too much time on building this PoC. One goal is to use the PoC to collect user inputs. The outputs generated by the model at this phase will not be good, so experts will need to clean the output data to power experimentation.  \nMoving forward, create a regular release cadence to the endpoint to see how the experience changes over time.  \nEvaluate the need for fine-tuning and explore the amount of data required. Additional guidelines about fine-tuning can be found in here.  \nDeliverables:  \n{% if ring.extra == 'internal' %}  \nFirst experiment in FFModel, including analysis of the output results.\n{% endif %}  \nEnd to end experience of user interactions through the baseline solution. The outputs will not be good. Improving the outputs is the goal of Hypothesis-driven Experimentation phase.  \nSecond check in on if fine-tuning needs to be considered. See fine tuning for more details on fine tuning and fine tuning recommendations.  \nData:  \nMinimum Viable Evaluation Set  \nMake sure you have enough data to gather meaning out of the generated results. The exact number needed depends on the complexity of your specific scenario but think upper double digits to low triple digits.  \nTooling:  \n{% if ring.extra == 'internal' %}  \nFFModel\n{% endif %}  \nPoC of the end-user\u2019s experience  \nRefine the Problem Space  \nApplications providing a variety of functionality from an LLM can become challenging. After your initial baseline, take some time to evaluate if the initial scope is still realistic. Interviewing potential end users is a great way to focus on the portions of the scenario that provide the most value.  \nThis phase could be as simple as a meeting with the whole team.  \nRecommendations:  \nYou should always be considering if your scenario is too broad and if it should be narrowed. When narrowing, focus on the use cases that were easiest to get correct output.  \nIf you started with a narrow scope, this is the phase where you can expand it based on the outcome of the previous step.  \nHave a dialogue between all disciplines (data science, engineering, and user experience) to see what is possible from each lens.  \nDeliverables:  \nAgreement on a potentially refined scope.  \nReady to ramp up data production.  \nData:  \nYou may be filtering out scenarios and the data associated with them from your test set.  \nYou might also add data points to hone-in on your refined problem space.  \nData Enrichment  \nThe success of an OpenAI engagement will come down to data. Even though you are accumulating data, you will need an explicit data set creation phase to get meaning from the upcoming experimentation. While the exact number of data points depends on the situation, you need enough data to capture the expected variability in the data, such as using different phrasings of each task that you want the model to perform. Ideally, this new data should be sourced from end users to make sure it is representative.  \nIn addition to data points for evaluation, plan and build any supporting knowledge base that needs to be available to the model. Examples include a vector store filled with embedding representations of document chunks and examples of correct input/output pairs. Further, if LLM fine-tuning is expected, you need to build a large training dataset for that. More details about data requirements for fine-tuning can be found here.  \nRecommendations:  \nCollecting data can be challenging, so investigate the need and feasibility for generating synthetic data to augment your user data.  \nWhen possible, capture the data in-situ. If necessary, you can mock the interaction. The goal is to capture data in as realistic a setting as possible, including supporting information available at the time. If you created a PoC in previous phases, consider using that for your data collection.  \nEven with the presence of tools for automated data validation and preparation, it is recommended that data be reviewed by human experts.  \nWe also recommend that people who work on the prompt engineering of the solution should not create these data points. It becomes challenging to write unbiased data points once you have an idea of what will and will not work.  \nIt is also recommended to perform additional exploratory data analysis to better understand your data distribution.  \nInclude examples of your responsible AI concerns to make sure your solution addresses them.  \nDeliverables:  \nEvaluation data set is created and agreed upon.  \nYou have created the knowledge banks that will be needed for experimentation.  \nAll scenarios are represented in the data set.  \nData:  \nA representative evaluation dataset with different phrasings of each task.  \nInclusion of negative test examples. These are inputs that the solution should not respond to. Examples are prompt injection attacks or things in violation of your terms of use/responsible AI considerations.  \nTooling:  \n{% if ring.extra == 'internal' %}  \nFFModel data system for storage, reviewing, and tracking of data points.\n{% endif %}  \nIdeally: In-situ tooling for capturing user data inputs.  \nHypothesis-driven Experimentation  \nIn this stage, multiple experiments can be implemented, executed, and evaluated to improve the performance of the solution. This stage is iterative and data driven. For each iteration, different experiments are evaluated and compared. This may also involve defining different experimental configurations and performing hyperparameter sweeping.  \nAfter that, exploratory results analysis can be performed for selected experiments to better understand performance issues, such as revealing patterns of errors, biases, or gaps in the results. Finally, insights can be used to define new hypotheses for improvement and/or the need for additional data. An experimentation framework {% if ring.extra == 'internal' %}(e.g., FFModel){% endif %} is needed at this stage to enable large-scale experimentation.  \nRecommendations:  \nWe recommend using hypothesis driven experimentation where we start with simple hypotheses and iteratively and objectively add more complexity.  \nSee the section Experimentation as part of MLOps for more info.  \nEvaluate and compare experimental results for different hypothesis. In the case of benchmarking based on many experimental configurations (i.e., parameters), consider running hyperparameter sweeping.  \nWhen choosing an experiment for deployment, make sure to take different criteria into consideration including performance and cost.  \nDeliverables:  \nIdentify one or more candidate solutions for the first deployment. There may be business objectives around cost or latency that determine which solution gets deployed.  \nIdeally: The evaluation metrics meet the agreed upon acceptance criteria.  \nRealistically: The generated outputs feel good enough that you are ready to put it in front of users.  \nData:  \nAn evaluation dataset  \nYou may need to add data points for underperforming use cases or to fill any gaps we discover as part of the results analysis.  \nTooling:  \n{% if ring.extra == 'internal' %}  \nFFModel\n{% endif %}  \nAny custom result analysis tooling that you build for the scenario  \nReal World Testing  \nIn this stage, the goal is to test and evaluate the solution that has been deployed in production. This can start by deploying the solution in a controlled environment before releasing it (e.g., a beta release). Observability tools can be used to track and monitor the solution\u2019s behavior and performance (e.g., detect drifts). Further, user data and feedback can be exported for exploratory data analysis (EDA) to assess the performance of the solution.  \nAs information about the performance and requests that users are making become available, return to hypothesis driven experimentation to improve user satisfaction. Consider verifying the end user requests and adding them to the data set used in experimentation (e.g., added to the evaluation sets), creating new evaluation criteria/metrics that map to user satisfaction, or improving your prompting to address shortcomings.  \nRecommendations:  \nIt is critical to record user interactions to provide data for experimentation.  \nInstrument the user experience to capture metrics on how your solution is performing. While explicit feedback from users is the clearest form of feedback, look for implicit signs of user satisfaction as well.  \nThere is a risk that the evaluation data set you used to this point is not representative of the distribution of real-world user data.  Therefore, be ready to have a quick turnaround on a few more sets of experiments.  \nDeliverables:  \nInsights about real-world testing performance  \nDecision on the need to add new features and/or perform further experimentation.  \nData:  \nThis phase is where we get real user data that gets captured and fed back into experimentation.  \nTooling:  \n{% if ring.extra == 'internal' %}  \nFFModel inference  \nFFModel data system\n{% endif %}  \nTools to analyze user data  \nEnd user experience in the customer\u2019s product",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\openai\\getting-started-with-openai.md"
    },
    {
        "chunkId": "chunk360_0",
        "chunkContent": "tags:\n- Azure OpenAI\n- LLM\n- Large Language Models\n- Semantic Search  \nAzure OpenAI  \n{% set thisPage = self._TemplateReference__context.page.url %}  \nOverview  \nAzure OpenAI Service is a powerful tool that provides REST API access to the following series of OpenAI's advanced language models:  \nGPT-3  \nCodex  \nEmbeddings series  \nThe service enables users to use the power of these models for various language-related tasks. The following are examples of tasks:  \nContent generation  \nSummarization  \nSemantic search  \nNatural language to code translation.  \n{% if extra.rings == 'internal' %}\nBefore starting an OpenAI engagement, review the Getting Started Guide for OpenAI.\n{% endif %}  \nService Objectives  \nOne of the main objectives of Azure OpenAI is to make advanced language AI more accessible to businesses and organizations of all sizes, while ensuring the responsible use of AI-driven language models. The service is designed to help users get the most out of OpenAI's models. It also provides a secure and scalable platform for deploying and managing AI-powered language applications.  \nFeatures  \nOne of the main features of Azure OpenAI is its extensive range of language models. These models include the following series:  \nGPT-3 base  \nCodex  \nEmbeddings  \nGPT-4 (currently in preview)  \nEach of these models has its own unique capabilities and strengths, making them well suited for different types of language-related tasks.  \nAnother important feature of Azure OpenAI is its ability to fine-tune language models using various pre-built models. The following are examples of pre-built models (though some are currently unavailable):  \nAda  \nBabbage  \nCurie  \nDavinci  \nThis fine-tuning allows users to adapt the models to their specific use case and improve their accuracy and effectiveness.  \nAzure OpenAI also supports virtual network and private link support. This support is critical for many enterprise-level applications that require secure, private communication channels. The service also includes a managed identity feature via Azure Active Directory. Azure AD-managed identities provide an extra layer of security for accessing and managing language models.  \nAzure OpenAI Studio  \nIn terms of user experience, Azure OpenAI provides a web-based interface in the Azure OpenAI Studio. This web-based interface makes it easy for users to explore and fine-tune language models. The service also includes an Azure portal for account and resource management. Account and resource management via Azure portal makes it easy to create and manage resources and deployments.  \nYou can access Azure Open AI studio here: https://oai.azure.com  \nUse Cases  \nSome of the main use cases for Azure OpenAI include content generation, which can be used to create articles, product descriptions, and other types of written content. The service can also be used for summarization, which is useful for condensing large amounts of text into shorter summaries. It can also be used for semantic search, which can help users find information that is relevant to their search query.  \nAnother key use case for Azure OpenAI is natural language to code translation. This translation is a powerful tool for developers who want to write code in a more natural language. The service can also be used for chatbots and other conversational AI applications. Chatbots can help businesses automate customer support and other types of interactions.  \nNatural Language Processing (NLP)  \nAzure Open AI provides several NLP services, such as text analytics, language understanding, and machine translation. These services can be used to analyze text data and extract valuable insights, such as sentiment analysis, key phrases, and entity recognition.  \nChatbots and Virtual Assistants  \nAzure Open AI offers tools and services for building intelligent chatbots and virtual assistants. These assistants can converse with users and provide them with helpful information. These chatbots can be integrated with various channels, such as websites, mobile apps, and messaging platforms.  \nComputer Vision  \nAzure Open AI provides the following computer vision services that can be used to analyze images and videos:  \nImage recognition  \nObject detection  \nThese services can be applied to various use cases, such as security and surveillance, retail, and healthcare.  \nSpeech Recognition and Synthesis  \nAzure Open AI offers services for speech recognition and synthesis that can be used to do the following actions:  \nTranscribe audio files  \nConvert text to speech  \nBuild voice-enabled applications  \nThese services can be applied to various industries, such as education, entertainment, and customer service.  \nAnomaly Detection  \nAzure Open AI provides services for anomaly detection that can be used to detect unusual patterns and behaviors in data. These services can be applied to various use cases, such as fraud detection, predictive maintenance, and cybersecurity.  \nEmbeddings to Search and Analyze Complex Documents  \nAzure Open AI offers embeddings that can be used to do search and analyze complex documents. Some examples of complex documents are:  \nLegal contracts  \nMedical records  \nScientific articles  \nEmbeddings are numerical representations of words and phrases that capture the meaning and context of the text. These embeddings can be used to build powerful search and analysis tools that can extract insights from large volumes of text data.  \nOverall, Azure Open AI provides a wide range of services and tools for doing the following items:  \nBuilding intelligent applications  \nExtracting valuable insights from data  \nThese use cases are just a few examples of how these services can be applied to various industries and use cases.  \nResponsible AI  \nAzure OpenAI is also designed with responsible AI in mind. Microsoft has made significant investments to help guard against abuse and unintended harm. Some examples of these investments are  \nRequiring applicants to show well-defined use cases  \nBuilding content filters to support customers  \nProviding responsible AI implementation guidance to onboarded customers  \nThe service also includes content filtering capabilities that evaluate prompts and completions against a content policy with automated systems. This policy helps ensure that the language models are not used to generate harmful or inappropriate content.  \nAzure OpenAI Service provides a powerful and accessible platform for applying advanced language AI models in a responsible and secure way. Its extensive range of models and fine-tuning capabilities make it well suited for various language-related tasks. Also, its support for virtual network and private link communication channels and managed identity features make it a secure option for enterprise-level applications.  \nLearn More  \nMicrosoft Azure OpenAI Services Documentation  \nAzure OpenAI Services Quickstart Templates  \nAzure Open AI Portal  \nGeneral Prompt Engineering Resources  \nPrompt Engineering Playbook article on Prompt Engineering  \nIntroduction to Prompt Engineering Microsoft article on prompt engineering  \nSemantic kernel Semantic Kernel (SK) is a lightweight SDK enabling integration of AI Large Language Models (LLMs) with conventional programming languages.  \nLearn Prompting  Introductory course on prompt engineering  \nPrompt Engineering Guide Guides, papers, lecture, notebooks and resources for prompt engineering",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\openai\\index.md"
    },
    {
        "chunkId": "chunk361_0",
        "chunkContent": "author: shanepeckham\ntitle: Working with LLMs\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: This guide aims to provide users with comprehensive resources and learnings for Large Language Model (LLM) applications\nrings:\n- public  \nGuide to working with Large Language Models  \nThis guide aims to provide users with comprehensive resources and learnings for Large Language Model (LLM) applications covering the following topics:  \nIntroduction:  \nLLMs Overview  \nPrompt Engineering  \nFine Tuning  \nEvaluation Metrics  \nRecommendations and Learnings  \nSecurity  \nExperimentation  \nUse Cases  \nPrompt Engineering  \nFine-Tuning\n{% if extra.ring == 'internal' %}  \nLatency\n{% endif %}  \nOverview of LLMs  \nLarge Language Models (LLMs) are deep learning models trained using large text corpora to generate text. They are based on the idea of auto-regressive models, where they have been trained to predict the next word (or the most probable ones) given the previous ones. LLMs can be used to process large amounts of text and learn the structure and syntax of human language.  \nThe development of LLMs has been a gradual process, especially over the past decade. The first LLMs were relatively small and could only perform simple language tasks. However, with the advances in deep neural networks, larger and more powerful LLMs were created. The figure below shows examples of LLMs that have been introduced over the past few years with their sizes.  \nThe 2020 release of the GPT-3 (Generative Pre-trained Transformer 3) model marked a significant milestone in the development of LLMs. GPT-3 also represents a family of models of different sizes. GPT3 demonstrated the ability to generate coherent and convincing text that was difficult to distinguish from text written by humans.  \nLike other foundation models, LLMs such as GPT-3 are trained in a self-supervised/unsupervised manner and can be adapted to perform different tasks and/or applications. This training process is in contrast to traditional machine learning models where different models are trained for different tasks using labeled data. For example, LLMs can be used to generate text for chat-bots, language translation, and content creation. They can also be used to analyze and summarize large amounts of text data, such as news articles or social media posts. In addition, LLMs can be used to write programming code from natural language requests.  \nAdapting the LLMs for different tasks or applications can be done in two ways: in-context learning through prompt engineering or fine-tuning. Regardless of the approach to be used, developers and data scientists should learn about and adopt new techniques.  \nLearn More  \nRefer to the Azure OpenAI section for more info on the working with Azure Managed ML Services.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\working-with-llms\\index.md"
    },
    {
        "chunkId": "chunk362_0",
        "chunkContent": "OpenAI/LLM Engagement Approach",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\working-with-llms\\approach\\approach-details.md"
    },
    {
        "chunkId": "chunk363_0",
        "chunkContent": "Libraries and Languages  \nThe backend itself is not CPU or memory intensive. In theory, it would be possible to use any programming language of your personal preference. Whilst this is true on one side, it is also important to consider that not all languages support common LLM libraries and platforms that can significantly simplify and automate development and deployment. It is still possible to implement LLM scenarios using only REST calls, and not the tools and libraries, but choosing to work without them can lead to additional overhead of development additional tools rather than focusing attention on the solution itself. At the time this document is written, Python, Javascript/Typescript, and .NET (C#) are well supported, and Java is starting to receive some attention. Other languages must rely on REST calls.  \nLet\u2019s pick Azure Open AI as an example of a LLM service. The service itself has an API as well as a small number language-specific libraries that provide wrappers around the REST API. At the same time, even basic backend might need to implement the following functionalities:  \nHistory management, including history summarization, truncation, and database storage.  \nChaining several requests together to treat the chain as a black box with inputs and outputs.  \nImplement document loaders in different formats and from different sources that can include embeddings, chunking, and connection management.  \nInteract with a vector database to store and retrieve data based on embeddings.  \nGeneralize tools to work with various APIs in order to iterate through them and pick the most relevant results.  \nThe most popular libraries that can provide the above functionalities with minimal code are LangChain and Semantic Kernel.  \nBoth libraries are open source, support a small number of languages and provide several.  \nLet\u2019s look at some benefits in details.  \nLanguage Support LLMOps allows us to run experiments and evaluate results using code that will be publish into production. Some companies have very strict rules regarding programming languages that can be for production deployments, and what packages and libraries can be used. Semantic Kernel supports .NET (C#) as the first party programming language. Its Python implementation has almost the same set of features but lags slightly behind. it also implements a Java option, that has moved into the main repo from an experimentation one and is catching up very fast. This page shows more details about supported languages. LangChain supports Python and JavaScript/TypeScript. The framework is trying to match all available features in both languages. Actions Orchestration A common case is when two or more actions need be performed to address a question, and the sequence of actions is not always hardcoded. For example, this could be a series of mathematical steps to implement certain operations, but their sequence will depend on the questions asked themselves and cannot be predicted in advance. So, frameworks use LLMs to generate a sequence of actions based on the actual question to pick actions, one by one, using not just the question but responses from previous actions and questions. Semantic Kernel supports planners , including SequentialPlanner, to generate end-to-end plans of actions, and StepwisePlanner to add steps incrementally until the final result is sent to the user. Conversational planners are not supported. LangChain supports agents that can be used in conversations, to find the best tool or to define a sequence of steps. Memory Management Azure Open AI and similar services are stateless services. It means that they cannot preserve history of conversations. To provide context to LLMs about previous steps in a conversation, history must be transferred to the service alongside with the new input. In many cases, there can a requirement to persist history to support long conversations, and vector format can be very useful in this case. Semantic Kernel natively supports text memory and conversation summary skills as well as various vector stores . LangChain supports various memory types including conversation buffer and conversation summary, that can be persisted. This website lists supported vector stores. Reusable Components It can be very handy for development teams to have the ability to pack certain LLM functions into reusable components, that can be shared among projects. Semantic Kernel supports plugins that include native and semantic functions. According to the documentation Open AI plugins standard has been implemented. LangChain support tools with abilities to create custom tools . Prompt Templates The ability to separate code and prompt can be very critical for a prompt engineering approach. It allows developers to execute the same code using many different prompts and pick the best prompt as a result. Semantic Kernel semantic functions (a part of plugins) are designed so that prompts as well as LLM parameters are stored in separate files using a special folder structure. LangChain supports various prompt templates as well as serialization for them. Document Loaders Documents are a key component of the RAG pattern, and they must be loaded to the search tool, or to the framework documents repository. Whilst documents come in various formats (text, pdf, images, sound, video, word documents, etc.), the existing tools focus on a limited number of formats.  (Thus, the documents must be converted in those before ingesting into the existing document loaders). Semantic Kernel C# implementation handles Word and MSGraph. LangChain supports a wide list of loaders mostly text based, such as: raw text, markdown, json, pdf (no OCR, only the text part) Document Transformers It\u2019s not always possible to send the entire document to the document loader, as it needs to be embedded and the embedders can only handle a limited number of tokens. So, the frameworks offer transformers that chunk, and perform other pre-processing tasks. Semantic Kernel implements a few methods that allow us to split plain text and markdown by lines or paragraphs. More skills are coming into the repo. LangChain supports a few splitters including some based on spacy and tiktoken.  \nThe following table summarizes the above benefits (In some cases, similar functionality is provided, but are called by a different name (e.g., Semantic Kernel Plugins and LangChain Tools)):  \nCharacteristics Semantic Kernel LangChain Language support C#, Java, Python Python, Java, JavaScript/TypeScript Actions Orchestration Planner (Conversational is not supported) Agents Memory Management Short term memory and vector stores Short term memory and vector stores Reusable Components Plugins Tools Prompt Templates Semantic Functions (Plugins) PromptTemplate Document Loaders Word\u202fand MSGraph (C# only) CSV, HTML, Markdown, PDF, JSON Document Transformers Plain text and markdown by lines and paragraphs Token based, context-aware splitting Open Source MIT License MIT License  \nOne more important piece of technology in the tool\u2019s arsenal is Prompt flow. Prompt flow is part of Azure AI Studio and is primarily an orchestrator of LLM flows. It can be very handy in building, experimentation, prompt engineering and deployment of solutions with LLMs. It can be used independently or jointly with LangChain or Semantic Kernel. We will refer more to Prompt flow in the LLMOps for Prompt Engineering topic.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\working-with-llms\\approach\\libraries-languages.md"
    },
    {
        "chunkId": "chunk364_0",
        "chunkContent": "rings:\n- public  \nRecommendations for Experimentation  \nExperimenting with large language models (LLMs) can be challenging, and may require careful design choices. Here are some recommendations for experimenting with LLMs effectively and efficiently.  \nCollect as much good quality data as possible then augment with synthetic data as needed  \nLarge language models are trained on massive amounts of text from diverse sources. This massive training may not have enough data for your specific domain or task. Therefore, it is important to collect as much relevant and high-quality data as possible to provide the model with sufficient context and examples. Data collection can be costly and time-consuming. You may also consider augmenting your data with synthetic data generated by one of the following methods:  \nLLM  \nOther generative or deterministic methods (e.g., grammar-based)  \nSynthetic data can help increase the diversity and robustness of your data, and fill in the gaps or imbalances in your data distribution.  \nDefine/use different evaluation metrics that fit your application  \nWhen using LLMs, you need to define or use different evaluation metrics that can capture the quality and performance of your model outputs. Depending on your application, you may use one of the following automatic metrics:  \nLevenshtein distance  \nBLEU  \nROUGE  \nHuman evaluation, such as ratings, rankings, or feedback  \nYou may also use multiple metrics to get a comprehensive and holistic assessment of your model. For more information, see evaluation metrics.  \nStart with in-context learning  \nStart simple to establish a baseline \u2013 start with simple prompt designs and use that as a baseline. A baseline can be a quick and easy way to gauge the model's capabilities and limitations.  \nGradually increase complexity. Once you have a baseline, you can experiment with increasing the complexity of your task or domain. You can add complexity by providing more context or examples, or introducing constraints.  \nUse different prompt designs to optimize the performance \u2013 different prompt designs can elicit different responses from the model, and some may be more suitable or effective for your task or domain than others. Therefore, try different prompt designs and compare their results.  \nDo benchmarking using different configurations and evaluate different models. You can use different prompt designs, model parameters, datasets, metrics, etc. to benchmark the model. See how it performs on different aspects of your task or domain. You can also evaluate and compare different versions or variants of GPT-3, or other large language models.  \nPerform fine-tuning if needed  \nWhile there are use cases where fine-tuning can help improve the model's performance and adaptability, it has limitations due to the costs, the need for more data, computational resources, and hyperparameter tuning. Fine-tuning may also cause over-fitting or catastrophic forgetting. Therefore, we advise doing fine-tuning only if needed, and only after you have exhausted the in-context learning methods. Below are a few recommendations for fine-tuning. For more information, see fine-turning recommendations.  \nStart with smaller models especially for simple tasks. Smaller models can be faster, cheaper, and easier to use and fine-tune, and they can also be more interpretable and controllable.  \nTry fine-tuning using different data formats. Different data formats can affect the model's input and output representations, and some may be more suitable or effective for your task or domain than others. For example, you can use plain text, structured text, or semi-structured text as your data format. You can also use different separators, delimiters, or tokens to indicate the boundaries or labels of your input and output.  \nOptimize the hyper-parameters of your model and your fine-tuning process, such as the learning rate, the batch size, the number of epochs, the weight decay, or the dropout rate.  \nStages for Experimenting with LLMs  \nLarge language model (LMM) experimentation is a multi-stage process. While the number of stages may vary from one application to another, we can define at least four main stages.  \nPreliminary ideation  \nIn this stage, the goal is to explore different prompt ideas and qualitatively assess the output of the LLM. More specifically, a small toy dataset can be used to test different prompts and observe the output diversity, coherence, and relevance. This test dataset can help in defining data requirements and planning for experimentation. Simple Jupyter notebooks or the Azure OpenAI playground can be used to interact with the LLM.  \nEstablishing a baseline  \nIn this stage, the goal is to establish baseline performance using a simple solution (e.g., static prompt with zero-shot learning). To measure the performance, an evaluation set, and evaluation metrics are needed. The evaluation set should contain representative examples of the task, with ground-truth or references completions (outputs). The evaluation metrics should capture the quality aspects of the output, such as accuracy or informativeness. Tools or environments that can efficiently call the LLM API are needed to generate and score the outputs.  \nHypothesis-driven experimentation  \nIn this stage, multiple experiments can be implemented, executed, and evaluated to improve the performance of the solution. This stage is iterative and data-driven. For each iteration, different experiments are evaluated and compared. It may also involve defining different experimental configurations and performing hyperparameter sweeping. After that, exploratory results analysis can be performed for selected experiments to better understand performance issues. Performance issues such as revealing patterns of errors, biases, or gaps in the results may be found. Finally, insights can be used to define new hypotheses for improvement and/or the need for more data. An experimentation framework {% if ring.extra == 'internal' %}(e.g., FFModel){% endif %} is needed at this stage to enable large-scale experimentation.  \nReal-world testing  \nIn this stage, the goal is to test and evaluate the solution that has been deployed in production. Observability tools can be used to track and monitor the solution\u2019s behavior and performance (e.g., detect drifts). Further, user data and feedback can be exported for exploratory data analysis (EDA) to quantitatively assess the performance of the solution. The EDA may also help us identify new data to be used in experimentation (e.g., added to the evaluation sets), new evaluation criteria/metrics, or improvement opportunities.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\working-with-llms\\experimentation\\experiment-recommend.md"
    },
    {
        "chunkId": "chunk365_0",
        "chunkContent": "author: shanepeckham\ntitle: LLM Fine-Tuning Recommendations\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: In some cases, LLMs may not perform well on specific domains, tasks, or datasets, or may produce inaccurate or misleading outputs. In such cases, fine-tuning the model can be a useful technique to adapt it to the desired goal and improve its quality and reliability.\nrings:\n- public  \nRecommendations for LLM fine-tuning  \nIn some cases, LLMs may not perform well on specific domains, tasks, or datasets, or may produce inaccurate or misleading outputs. In such cases, fine-tuning the model can be a useful technique to adapt it to the desired goal and improve its quality and reliability.  \nWhen to consider fine-tuning  \nBelow are some scenarios where fine-tuning can be considered.  \nHallucinations: Hallucinations are untrue statements output by the model. They can harm the credibility and trustworthiness of your application. One possible mitigation is fine-tuning the model with data that contains accurate and consistent information.  \nAccuracy and quality problems: Pre-trained models may not achieve the desired level of accuracy or quality for a specific task or domain. This shortfall can be due a mismatch between the pre-training data and the target data, the diversity and complexity of the target data, and/or incorrect evaluation metrics and criteria.  \nHow fine-tuning can help  \nFine-tuning the model with data that is representative and relevant to the target task or domain may help improve the model's performance. Examples include:  \nAdding domain-specific knowledge: Teaching the model new (uncommon) tasks or constraining it to a smaller space, especially complex specialized tasks may require the model to learn new skills, concepts, or vocabulary that are not well represented in the model's original training data. Some examples are legal, medical, and technical texts. These tasks or domains may also have specific constraints or requirements, such as length, format, or style, that limit the model's generative space. Fine-tuning the model with domain-specific data may help the model acquire the necessary knowledge and skills and generate more appropriate and coherent texts.  \nAdd data that doesn't fit in a prompt: The LLM prompt is the input text that is given to the model to generate an output. It usually contains some keywords, instructions, or examples that guide the model's behavior. However, the prompt has a limited size, and the data needed to complete a task may exceed the prompt's capacity. This happens in applications that require the LLM to process long documents, tables, etc. In such cases, fine-tuning can help the model handle more data and use smaller prompts at inference time to generate more relevant and complete outputs.  \nSimplifying prompts: Long or complex prompts can affect the model's efficiency and scalability. Fine-tuning the model with data that is tailored to the target task or domain can help the model provide quality responses from simpler prompts, and potentially use fewer tokens and improve latency.  \nBest practices for fine-tuning  \nHere are some best practices that can help improve the efficiency and effectiveness of fine-tuning LLMs for various applications:  \nTry different data formats: Depending on the task, different data formats can have different impacts on the model\u2019s performance. For example, for a classification task, you can use a format that separates the prompt and the completion with a special token, such as {\"prompt\": \"Paris##\\n\", \"completion\": \" city\\n###\\n\"}. Be sure to use formats suitable for your application.  \nCollect a large, high-quality dataset: LLMs are data-hungry and can benefit from having more diverse and representative data to fine-tune on. However, collecting and annotating large datasets can be costly and time-consuming. Therefore, you can also use synthetic data generation techniques to increase the size and variety of your dataset. However, you should also ensure that the synthetic data is relevant and consistent with your task and domain. Also ensure that it does not introduce noise or bias to the model.  \nTry fine-tuning subsets first: To assess the value of getting more data, you can fine-tune models on subsets of your current dataset to see how performance scales with dataset size. This fine-tuning can help you estimate the learning curve of your model and decide whether adding more data is worth the effort and cost. You can also compare the performance of your model with the pre-trained model or a baseline. This comparison shows how much improvement you can achieve with fine-tuning.  \nExperiment with hyperparameters: Iteratively adjust hyperparameters to optimize the model performance. Hyperparameters, such as the learning rate, the batch size and the number of epochs, can have significant effect on the model\u2019s performance. Therefore, you should experiment with different values and combinations of hyperparameters to find the best ones for your task and dataset.  \nStart with a smaller model: A common mistake is assuming that your application needs the newest, biggest, most expensive model. Especially for simpler tasks, start with smaller models and only try larger models if needed.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\working-with-llms\\fine-tuning\\fine-tuning-recommend.md"
    },
    {
        "chunkId": "chunk366_0",
        "chunkContent": "author: shanepeckham\ntitle: Getting started with LLM fine-tuning\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Large Language Model (LLM) Fine-tuning is the process of adapting the pre-trained model to specific tasks. This process is done by updating its parameters on a new dataset. Specifically, the LLM is partially retrained using <input, output> pairs of representative examples of the desired behavior. Hence, it involves updating the model (i.e., LLM) weights.\nrings:\n- public  \nGetting started with LLM fine-tuning  \nLarge Language Model (LLM) fine-tuning involves adapting the pre-trained model to specific tasks. This process takes place by updating parameters on a new dataset. Specifically, the LLM is partially retrained using <input, output> pairs of representative examples of the desired behavior. Hence, it involves updating the model weights.  \nData requirements for fine-tuning  \nBefore fine-tuning an LLM, it is essential to understand data requirements to support training and validation.  \nHere are some guidelines:  \nUse a large dataset: The required size for a training and validation dataset depends on the complexity of the task and the model being fine-tuned. Generally, you want to have thousands or tens of thousands of examples. Larger models learn more with less data as illustrated in the examples in the figure below. They still need enough data to avoid over-fitting or forgetting what they learned from the pre-training phase.  \nUse a high-quality dataset: The dataset should be consistently formatted and cleaned of incomplete or incorrect examples.  \nUse a representative dataset: The contents and format of the fine-tuning dataset should be representative of the data on which the model will be used. For example, if you are fine-tuning a model for sentiment analysis, you want to have data from different sources, genres, and domains. This data should also reflect the diversity and nuances of human emotions. You also want to have a balanced distribution of positive, negative, and neutral examples, to avoid skewing the model\u2019s predictions.  \nUse a sufficiently specified dataset: The dataset should contain enough information in the input to generate what you want to see in the output. For example, if you are fine-tuning a model for email generation, you want to provide clear and specific prompts that guide the model\u2019s creativity and relevance. You also want to define the expected length, style, and tone of the email.  \nThis diagram shows illustrative examples of text classification performance on the Stanford Natural Language Inference (SNLI) Corpus. Ordered pairs of sentences are classified by their logical relationship: either contradicted, entailed (implied), or neutral. Default fine-tuning parameters were used when not otherwise specified.  \nHow to format data to fine-tune OpenAI  \nTo fine-tune a model using Azure OpenAI (or OpenAI), you'll need a set of training examples that have certain formats. For more information, see Customize an Azure OpenAI model with fine-tuning.  \nHere are some additional guidelines:  \nUse JSON: The training examples should be provided in JSON format, where each example consists of a <prompt, completion> pair. The prompt is a text fragment that you want the model to continue. The completion is a possible continuation that the model should learn to produce. For example, if you want to fine-tune an LLM for story generation, a prompt could be the beginning of a story, and a completion could be the next sentence or paragraph.  \nUse a separator and stop sequence: To inform the model when the prompt ends and the completion begin and when the completion ends, use a fixed separator and a stop sequence. A separator is a special token or symbol that you insert at the end of the prompt. A stop sequence is a special token or symbol that you insert at the end of the completion. For example, you could use \\n##\\n as the separator and \\n###\\n as the stop sequence, as shown below:  \ntext\n{\"prompt\": \"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do.\\n##\\n\",\n\"completion\": \"She wondered if they should just go back home.\\n###\\n\"}  \nDon't confuse the model: Make sure the separator and stop sequence aren't included in prompt text or training data. The model should only ever see them as a separator and stop sequence.  \nUse a consistent style and tone: You should also make sure that your prompts and completions are consistent in terms of style, tone, and length. They should match the task or domain for which you want to fine-tune the model.  \nKeep training and inference prompts consistent: Make sure that the prompts you use when using the model for inference are formatted and worded in the same way that the model was trained, including using the same separator.  \nFine-tuning hyperparameters  \nWhen fine-tuning an LLM such as GPT-3, it is important to adjust hyperparameters to optimize the performance of the model on a specific task or domain. Many hyperparameters are available for the user to adjust when fine-tuning a model. The table below lists some of these parameters and provides some recommendations for each. (Source: Best practices for fine-tuning GPT-3 to classify text)  \nParameter Description Recommendation n_epochs Controls the number of epochs to train the model for. An epoch refers to one full cycle through the training dataset. \u2022 Start from 2-4 \u2022 Small datasets may need more epochs and large datasets may need fewer epochs. \u2022 If you see low training accuracy (under-fitting), try increasing n_epochs. If you see high training accuracy but low validation accuracy (over-fitting), try lowering n_epochs. batch_size Controls the batch size, which is the number of examples used in a single training pass. \u2022 Recommendation is set the batch size in the range of 0.01% to 4% of training set size. \u2022 In general, larger batch sizes tend to work better for larger datasets. learning_rate_multiplier Controls learning rate at which the model weights are updated. The fine-tuning learning rate is the original learning rate used for pre-training, multiplied by this value. \u2022 Recommendation is experiment with values in the range 0.02 to 0.2 to see what produces the best results. \u2022 Larger learning rates often perform better with larger batch sizes. \u2022 learning_rate_multiplier has minor effect compared to n_epochs and batch_size. prompt_loss_weight Controls how much the model learns from prompt tokens vs completion tokens. \u2022 If prompts are long (relative to completions), try reducing this weight (default is 0.1) to avoid over-prioritizing learning the prompt. \u2022 prompt_loss_weight has minor effect compared to n_epochs and batch_size.  \nChallenges and limitations of fine-tuning  \nFine-tuning large language models scan be a powerful technique to adapt them to specific domains and tasks. However, fine-tuning also comes with some challenges and disadvantages that need to be considered before applying it to a real-world problem. Below are a few of these challenges and disadvantages.  \nFine-tuning requires high-quality, sufficiently large, and representative training data matching the target domain and task. Quality data is relevant, accurate, consistent, and diverse enough to cover the possible scenarios and variations the model will encounter in the real world. Poor-quality or unrepresentative data leads to over-fitting, under-fitting, or bias in the fine-tuned model, which harms its generalization and robustness.  \nFine-tuning large language models means extra costs associated with training and hosting the custom model.  \nFormatting input/output pairs used to fine-tune a large language model can be crucial to its performance and usability.  \nFine-tuning may need to be repeated whenever the data is updated, or when an updated base model is released. This involves monitoring and updating regularly.  \nFine-tuning is a repetitive task (trial and error) so, the hyperparameters need to be carefully set. Fine-tuning requires much experimentation and testing to find the best combination of hyperparameters and settings to achieve desired performance and quality.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\working-with-llms\\fine-tuning\\fine-tuning.md"
    },
    {
        "chunkId": "chunk367_0",
        "chunkContent": "rings:\n- internal  \nAzure OpenAI Latency - A Case Study  \nThis case study was an engagement where our team was tasked with improving the latency of a chatbot just prior to go-live. The bot was designed for origination of of general insurance for a pan-geo digital native insurer. The bot consisted of a multi-turn (~25 turns) directed conversation orchestrated and implemented via the prompting of GPT-4 with various structured DSLs emitted to the output to be further processed by downstream channels (Facebook messenger, WhatsApp etc...). While the bot was \"functional\" in sofar as it could complete a full transactional conversation, the per-turn latency was at such a level that it was not able to be deployed to customers. Through a series of experiments, we identified key factors that impacted the chatbot\u2019s speed and implemented solutions to address these issues. The goal was to enhance the chatbot\u2019s performance and ensure its readiness for launch.  \nOverview  \nPrior to engagement the chatbot application would incur latency of between 10s and 30s per chat-turn.  \nThe resolution was to re-architect the approach to LLM prompting to use the Azure Open AI Function Calling pattern. This in turn allowed the application to change from using GPT-4 to the GPT 3.5 Turbo model which is significantly faster.  \nAs a result, per-turn latency was reduced from 10s-30s down to ~3s.  \nOutline  \nThe application in question is a multi-turn commerce chatbot for general insurance policy origination. The chatbot is designed to provide information about policy options and collect information, including PII, before providing for quotation and policy purchase. The chatbot is intended to be delivered through a range of chat-like channels, including Line, WhatsApp, Facebook Messenger, and the insurers website.  \nThe chatbot is highly interactive, with a typical origination conversation requiring 20-30 chat turns. The application was initially built using complex explicit prompting to execute a step-by-step conversation with the user, with generated output from the LLM model (GPT-4 8k) always returned in pre-defined formatted JSON text. This JSON text was then consumed by downstream channels for presentation to the user. Chat history was included in subsequent prompt turns.  \nThe bot is multi-lingual and targets English along with a range of East Asian and Southeast Asian languages including Thai, Malay and Bahasa Indonesia.  \nThe problem presented to our team for assistance was that the chatbot was taking upwards of 8 seconds per chat-turn. While the chatbot was fully functional as built, the latency of chat-turns was such that it was not able to go to production. There were also concerns with the exhaustion of the 8k token-limit as other larger use-cases are to be implemented. While not raised with us, we also believe that the bot as implemented would be cost prohibitive to operate at scale.  \nInitial Assessment  \nAn initial workshop with the user identified a range of potential issues for investigation. Out team members was engaged alongside the user\u2019s engineering team.  \nFollowing the workshop, our team presented several hypotheses for investigation by the user's engineering team. These included:  \nthe elimination of unnecessary cross-cloud hops  \noverall reduction in the prompt context  \nhistory compression/summarization  \nre-evaluation of the use of GPT-4 vs GPT 3.5 or other smaller models  \nOf these proposals, only the reduction in the prompt context had been investigated prior to the engagement, resulting in a marginal improvement in latency.  \nOn-site engagement  \nDuring the on-site engagement, it was discovered that while the user had logging in place for the application, there was no ability to easily measure the end-to-end latency of a single chat-turn, the latency of a full origination conversation, or the latency of single components of a chat-turn.  \nThe initial on-site work from the team focused on establishing an isolated baseline of latency for the key component(s) of the system. Given limited access to the application itself, the team established two baseline measurements measured at the Orchestrator1: single chat-turn latency and full simulated conversation latency.  \nIn addition to measuring the latency incurred, the team implemented a mechanism to score the similarity of generated output against the conversation baseline output. This allowed changes to be made to the prompt and the impact on the baseline output to be measured over the course of the conversation. In other words, this provided a quantitative measure of how much the application was affected by some of the aggressive changes to the prompting approach.  \nThe baseline measurement mechanism was implemented as an Azure ML Studio experiment. The team initially tried to implement this using Azure Prompt Flow and identified challenges which have been raised with the Prompt Flow team. The baseline output comparison was implemented by embedding the ground truth and the generated output, and then calculating a cosine distance between the two embedding vectors.  \nExperiments  \nThe team conducted several experiments to improve the latency of the chatbot, including:  \nestablishing a baseline for latency  \nchat history summarization  \nchanging the location of the GPT4 service  \nforcing the use of GPT3.5, and using Function calling  \nBaseline  \nThe baseline measures showed that the initial baseline latency ranged from ~10,000ms at the first prompt to ~30,000ms at the last prompt, with a consistent pattern of timeouts and unexpected results in all initial baseline runs.  \nChat history summarization  \nThe chat history summarization experiment aimed to reduce the size of outgoing prompts to improve response times. However, the summarization algorithm did not improve response times and caused incorrect answers from turn 10 onwards. This indicated that the number of outgoing tokens had a limited impact on response time, while the quality of responses suffered.  \nChanging the location of the GPT4 service  \nThe experiment to change the location of the GPT4 service aimed to reduce latency by running the service out of Japan and Canada instead of the UK.  \nHowever, both Canada and Japan had worse response times than the UK, with no significant change in cosine similarity. The team interpreted this as occurring because when it is daytime in Singapore, it is night-time in the UK, so the user's services faced less competition for resources with other users.  \nForcing the use of GPT3.5, and using Function calling  \nThe experiment to force the use of GPT3.5 Turbo aimed to explore the known fact that GPT3.5 Turbo is faster than GPT4. The team explored two approaches: changing the master system prompt to force the result to be returned as a JSON by adding one-shot suggestions, and using OpenAI Function calling to force the results to come in JSON format. Initial trials of both approaches produced promising results, and the team decided to explore function calling first. An Azure Function was built and deployed to Australia East, and the user started working on rebuilding the orchestrator to accommodate function calling. The results of the first experiment showed a significant improvement in response time, averaging 3.3s.  \nSummary of Experiments  \nThe summary of our experiments is show below:  \nHypothesis Assumptions Result Graph Reduce physical distance between Orchestrator, GPT-service, OpenAI The shorter the distance the faster it is Limited improvement. And any improvement (~200-400ms) washed out by prompt-response latency (8,000-28,000ms) Summarize History The shorter the outgoing prompt, the faster it is Limited latency improvement. Loss of information from history context caused failure of conversations. \u201cStepless\u201d prompt Less precise prompt, less steps to focus on, faster it is Limited improvement GPT3.5 Turbo instead of GPT4 GPT3 Turbo is smaller model, hence faster Significant improvements, but even greater improvement and conversational flexibility achieved by using Function Calling GPT3.5 + Function Calling Smaller GPT3 model is faster, Function Calling enforces structured response Final Approach  \nKey Learnings  \nFactors that impact performance  \nThe key learnings from the experiments conducted by the team showed that several factors had a significant impact on the speed of the chatbot. These included:  \nthe number of tokens returned (the more token is return, the slower the response is)  \nthe size of the large language model - i.e. GPT4 (larger) will be slower than 3.5 (smaller)  \nwhether a structured (in our case JSON formatted) response was expected (the more structured is the response, the slower it is)  \nthe complexity of the prompt; the more complex, the slower it is. We did not endeavour to 'measure' complexity quantitatively but this may be interesting further work.  \nFactors that do not have significant impact on performance  \nOn the other hand, factors that we initially thought would have a significant impact on performance did not have such an impact:  \nThe number of tokens sent to completion. Smaller and larger requests (GPT 4) behaved similarly which was a surprise to us.  \nThe physical distance between the server process and the Azure OpenAI service (this is counter-intuitive, as it really depends on whether that particular server is overloaded or not). Overall the latency introduced by routing requests around the globe was washed out by other latency in the system. We've updated our priors with respect to cross geo-routing (e.g. PAYG load balancing etc) and feel comfortable recommending this.  \nFunction Calling  \nFunction calling was the mechanism that we found to return a fully formatted JSON response, without negatively impacting the performance of the solution.  \nFor more information on Function Calling on AzureOpenAI, please refer to this link: How to use function calling with Azure OpenAI Service  \nFurthermore, we used it as a mechanism to chain the responses, so that the model would know which question to ask at which point of the conversation, emulating a workflow solution.  \nIt worked in both GPT-3.5 and GPT-4, but could sometimes fail in GPT-3.5 with hallucination or empty values, even when the arguments were sent as \u201crequired\u201d. The team had to implement workarounds to address this problem. A thorough evaluation of the model, more prompt engineering, and extra coding were necessary to ensure the solution worked well with GPT-3.5. However, the better response time compared to GPT-4 made this effort a good investment.  \nConclusion  \nThe team conducted several experiments to improve the latency of the chatbot, including establishing a baseline for latency, chat history summarization, changing the location of the GPT4 service, and forcing the use of GPT3.5. The results showed that the number of tokens returned, the size of the model, whether a formatted response was expected, and the complexity of the prompt had a significant impact on performance. On the other hand, factors such as the number of tokens sent to completion, the physical distance between the server process and the Azure OpenAI service, and the use of functions in GPT3.5 and GPT4 had little effect on performance.  \nFunction calling worked in both GPT3.5 and GPT4, but could sometimes fail in GPT3.5 with hallucination or empty values, even when the arguments were sent as \u201crequired\u201d. The team had to implement workarounds to address this problem. A thorough evaluation of the model, more prompt engineering, and extra coding were necessary to ensure the solution worked well with GPT3.5. However, the better response time compared to GPT4 made this effort a good investment.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\working-with-llms\\latency\\openai-latency-a-case-study.md"
    },
    {
        "chunkId": "chunk368_0",
        "chunkContent": "rings:\n- internal  \nAzure OpenAI Latency and Performance  \nOverview  \nA common challenge when working with large language models (LLMs) is to reduce the latency of a completion request or maximizing the throughput of the LLM deployment. This guide will go through some important factors of both latency and performance, especially when working with Azure OpenAI.  \nWhat makes up the request latency?  \nBefore stepping into methods to reduce latency, it is essential to understand what makes up the latency of a completion request. Here are the main components:  \nTime to process the prompt tokens  \nTime to generate completion tokens  \nNetwork round-trip time (time from client to LLM instance and back)  \nFactors affecting overall latency  \nLet's take a closer look at the main and a few supplementary factors that affect latency on Azure OpenAI calls.  \nPrompt effects  \nOne obvious attribute of the prompt that affects the latency is the length. Assuming that prompts of higher sizes are similar in complexity and the size of the response generated is fixed, increasing the prompt size will increase the overall latency, but not at a significant rate.  \nAnother aspect of the prompt, the complexity, is a major factor. This may seem a bit abstract but the takeaway here is that not all prompts are created equal. For instance, you have two following prompts:  \nWho is the first president of the US and the years he served?  \nGive me a summary of the first president of the US.  \nAzure OpenAI will be able to generate a response quicker for the first prompt over the second one, even though the prompts are similar in length and refer to the same topic. This is where benchmarking can help, where by using a specific prompt, you can run a quick load test on Azure OpenAI and get a baseline of how long it generally takes for the prompt to get processed.  \nTo reduce the prompt\u2019s complexity and overall latency, one can streamline their prompt using prompt engineering techniques.  \nText completion effects  \nThe text completion phase takes place when Azure OpenAI generates the response after parsing and processing the prompt. In general, prompt processing is classified (along with handling prompt complexity) under the text completion phase. For this guide, we decided to simplify this perspective by classifying the core Azure OpenAI logic to understand and process the prompt as part of prompt complexity. Therefore, text completion is made up of Azure OpenAI generating the response and outputting the response content.  \nSince Azure OpenAI generates the completion tokens one at a time, latency will increase as the text completion response goes longer. Latency will scale at a higher rate for larger responses compared to the rate latency increases given larger prompt sizes.  \nThere are several parameters you can use to reduce the number of tokens generated for the text completion response:  \nLower max_tokens to reduce the upper limit of tokens Azure OpenAI will generate when possible. If you expect a prompt to return with a consistent response, try to minimize the max_tokens setting.  \nGenerate fewer completions: The n (number of completions to generate) and best_of (how many responses to choose/consider from all completions) parameters control how many tokens Azure OpenAI will generate. You can consider the number of generated tokens as\u202f[ max_tokens * max (n, best_of) ]  \nAdditional factors  \nAzure OpenAI offers different models, each with varying levels of complexity and generality. Models like gpt-4 can generate more complex and diverse text completions but at the cost of higher latency. gpt-3.5-turbo generates responses faster, but the results may be less accurate or relevant given the prompt. When choosing the model, you need to be aware of the tradeoff between quality of answers and speed.  \nAzure OpenAI currently offers deployments in certain regions and support for newer ones coming soon. Using a deployment region that is closer to where the client is sending calls from will help minimize the network round trip time.  \nThe concurrency of requests may also impact the latency, provided the requests do not exceed the model deployment\u2019s tokens per minute (TPM) and requests per minute (RPM) limits.  \nAnother contributor to latency and more so Azure OpenAI performance is the type of instance provisioned. Any instance of the public Azure OpenAI service offering is shared amongst many users. Since the Azure OpenAI resources are shared within a region, you can encounter the noisy neighbor issue when many other users are actively using the service. Moreover, Azure OpenAI instances that are deployed within the non-production tenant will have limited throughput compared instances in the production tenant. The other, dedicated offering of Azure OpenAI is the Provisioned Throughput Unit (PTU), where the instance and its resources are reserved solely for the user. The PTU will be more efficient and incur less latency but at a higher cost.  \nAzure OpenAI Performance  \nThe two different instance types, shared and PTU, heavily dictate the Azure OpenAI instance's performance. PTUs are offered per 100 units. Allocating a specific number of units will come with the corresponding TPM and RPM quotas, which are also influenced by the model deployed for the PTU. For instance, a 300 unit gpt-3.5-turbo deployment has a TPM quota range of 900K \u2013 2700K. The higher the unit allocation, the higher the TPM quotas will be for the instance.  \nThe reason we have a range instead of a fixed TPM quota is because the PTU can process different prompts at different rates. We have seen earlier that not all prompts are similar, some will take longer than others even if similar in length and topic. This is why benchmarking your Azure OpenAI instance, whether shared or PTU, is salient. By testing your custom prompt through a quick benchmarking test, you can obtain a general baseline of how long the specific model and Azure OpenAI instance type will take with the prompt. With this baseline, you can then derive more relevant TPM quotas for your custom prompts and accordingly plan how to manage traffic flow into the instance.  \nTo run a quick load test using a custom prompt, use a load testing tool that ideally has parameters to adjust the concurrent requests per second (RPS) and the overall duration of the test. For the following example, we use the Apache Bench command-line tool to send traffic at 5 RPS up to 1200 requests (about 5 mins) with a chat completion request defined inside test-prompt.json.  \nab -c 5 -n 1200 -e results.csv -H \"api-key: api-key\" -k -T \"application/json\"\u202fhttps://openaiendpoint.openai.azure.com/openai/deployments/gpt-35-turbo/chat/completions?api-version=2023-07-01-preview  \nBest Practices  \nBased on the findings that we have presented in this guide, we would like to reiterate and summarize a list of best practices to improving Azure OpenAI latency and performance.  \nLatency Reduction  \nUse faster models like *-turbo variants, such as gpt-3.5-turbo instead of gpt-4, where possible. Depending on the specific model training parameters, you may have to balance the tradeoff between speed and quality of response.  \nReduce text completion tokens by adjusting parameters max_tokens, n, and best_of. Limiting these values will also let you send more requests given the model deployment\u2019s TPM and RPM rate limits.  \nDeploy the Azure OpenAI model and the client closer to each other to reduce network round-trip time.  \nMaximize Throughput  \nDeploy a PTU instance over a shared Azure OpenAI instance if the costs are manageable. The dedicated hardware in the PTU will eliminate any noisy-neighbor issues.  \nFind the baseline of how your custom prompts perform on the Azure OpenAI instances. Run a quick benchmarking test without hitting any rate limits and assess how long each request takes. Using these baselines, you can better plan your traffic flow, specifically how many requests and prompts to send to a particular instance.  \nOptimize any complex prompts through prompt engineering. Using Azure ML Prompt flow, you can easily create prompt variants and evaluate their performance through large-scale testing to streamline the prompt optimization process.  \nLook out for the number of prompt tokens you are sending to Azure OpenAI and the tokens being generated in the response. Ensure that this token rate along with the request rate won\u2019t hit the TPM and RPM rate limits so you can preemptively avoid 429s. If applicable, increase the workload gradually rather than having sharp changes in the traffic flow.  \nProvision more Azure OpenAI instances in other regions to maximize the overall TPM quota. Add a routing component as necessary that manages traffic to multiple endpoints.  \nObservability  \nEnable server-side observability on your Azure OpenAI instance, through the built-in metrics and logs via the diagnostic settings. Using this data, you can setup an Azure dashboard with indicators of how the instance is performing given the traffic patterns and overall token loads.  \nLog Azure OpenAI requests and responses using Azure API Management. Tag the requests to determine the latency profile of each unique prompt from the logs and in turn, use the latency data to better plan out the overall workload sent to your instance.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\working-with-llms\\latency\\openai-latency-performance.md"
    },
    {
        "chunkId": "chunk369_0",
        "chunkContent": "author: shanepeckham\ntitle: AI Evaluation Metrics\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Evaluating the performance of machine learning models is crucial for determining their effectiveness and reliability. To do that, quantitative measurement with reference to ground truth output (also known as evaluation metrics) are needed. However, LLM applications are a recent and fast evolving ML field, where model evaluation is not straightforward and there is no unified approach to measure LLM performance. Several metrics have been proposed in the literature for evaluating the performance of LLMs. It is essential to use the right metrics that are suitable for the problem we are attempting to solve.\nrings:\n- public  \nDefining and understanding LLM evaluation metrics  \nEvaluating the performance of machine learning models is crucial for determining their effectiveness and reliability. To do that, quantitative measurement (also known as evaluation metrics) with reference to ground truth output are needed. However, LLM applications are a recent and fast evolving AI field, where model evaluation is not straightforward and there is no unified approach to measure LLM performance. Several metrics have been proposed in the literature for evaluating the performance of LLMs. It is essential to use the right metrics suitable for the problem we are attempting to solve.  \nThis document covers several evaluation metrics and recent methods that are useful for evaluating these large models over various Natural Language Processing tasks.  \nTraditional NLP and classification metrics  \nTraditional metrics include:  \nClassification metrics (for example, accuracy, F1-score) for scenarios where there is ground truth.  \nText similarity (for example, bleu, rouge, fuzzy) and semantic similarity (for example, cosine similarity) for scenarios where there is ground truth.  \nText similarity metrics  \nText similarity metrics evaluators focus on computing similarity by comparing the overlap of word or word sequences between text elements. They\u2019re useful for producing a similarity score for predicted output from an LLM and reference ground truth text. These metrics also give an indication as to how well the model is performing for each respective task.  \nLevenshtein Similarity Ratio  \nThe Levenshtein Similarity Ratio is a string metric for measuring the similarity between two sequences. This measure is based on Levenshtein Distance. Informally, the Levenshtein Distance between two strings is the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other. The Levenshtein Similarity Ratio can be calculated using Levenshtein Distance value and the total length of both sequences in the following definitions:  \nLevenshtein Similarity Ratio (Simple Ratio):\n$$Lev.ratio(a, b) = {(|a|+|b|)-Lev.dist(a,b) \\over |a|+|b|}$$\nwhere |a| and |b| are the lengths of a and b.  \nA few different methods are derived from Simple Levenshtein Similarity Ratio:  \nPartial Ratio: Calculates the similarity by taking the shortest string, and compares it against the sub-strings of the same length in the longer string.  \nToken-sort Ratio: Calculates the similarity by first splitting the strings into individual words or tokens, sorts the tokens alphabetically, and then recombines them into a new string. This new string is then compared using the simple ratio method.  \nToken-set Ratio: Calculates the similarity by first splitting the strings into individual words or tokens, and then matches the intersection and union of the token sets between the two strings.\n{% if extra.ring == 'internal' %}  \nffmodel Fuzzy Distance Evaluator: Fuzzy evaluator calculates the similarity between the prompt and the completions.\n{% endif %}  \nBLEU Score  \nThe BLEU (bilingual evaluation understudy) score evaluates the quality of machine-translated text from one natural language to another. Therefore, it\u2019s typically used for machine-translation tasks, however, it\u2019s also being used in other tasks such as text generation, paraphrase generation, and text summarization. The basic idea involves computing the precision, which is the fraction of candidate words in the reference translation. Scores are calculated for individual translated segments\u2014generally sentences\u2014by comparing them with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality. Punctuation or grammatical correctness is not taken into account when scoring.  \nFew human translations will attain a perfect BLEU score, since a perfect score would indicate that the candidate is identical to one of the reference translations. For this reason, it is not necessary to attain a perfect score. Given that there are more opportunities to match with the addition of multiple reference translations, we encourage having one or more reference translations that will be useful for maximizing the BLEU score.  \n$$P = {m \\over w_t}$$\nm: Number of candidate words in reference.\n*wt: Total number of words in candidate.  \nTypically, the above computation considers individual words or unigrams of candidate that occur in target. However, for more accurate evaluations of a match, one could compute bi-grams or even trigrams and average the score obtained from various n-grams to compute the overall BLEU score.  \nROUGE  \nAs opposed to the BLEU score, the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) evaluation metric measures the recall. It\u2019s typically used for evaluating the quality of generated text and in machine translation tasks. However, since it measures recall, it's used in summarization tasks. It\u2019s more important to evaluate the number of words the model can recall in these types of tasks.  \nThe most popular evaluation metrics from the ROUGE class are ROUGE-N and ROUGE-L:  \nRouge-N: measures the number of matching 'n-grams' between a reference (a) and test (b) strings.\n$$Precision= {\\text{number of n-grams found in both a and b} \\over \\text{number of n-grams in b}}$$\n$$Recall= {\\text{number of n-grams found in both a and b} \\over \\text{number of n-grams in a}}$$\nRouge-L: measures the longest common subsequence (LCS) between a reference (a) and test (b) string.\n$$Precision= {LCS(a,b) \\over \\text{number of uni-grams in b}}$$\n$$Recall= {LCS(a,b) \\over \\text{number of uni-grams in a}}$$\nFor both Rouge-N and Rouge-L:\n$$F1={2 \\times\\text{precision} \\over recall}$$  \n{% if extra.ring == 'internal' %}\nExample code to estimate the difference between two strings using a Rouge1 score: ffmodel Rouge Score Evaluator\n{% endif %}  \nSemantic similarity  \nThe semantic similarity between two sentences refers to how closely related their meanings are. To do that, each string is first represented as a feature vector that captures its semantics/meanings. One commonly used approach is generating embeddings of the strings (for example, using an LLM) and then using cosine similarity to measure the similarity between the two embedding vectors. More specifically, given an embedding vector (A) representing a target string, and an embedding vector (B) representing a reference one, the cosine similarity is computed as follows:  \n$$ \\text{cosine similarity} = {A \\cdot B \\over ||A|| ||B||}$$  \nAs shown above, this metric measures the cosine of the angle between two non-zero vectors and ranges from -1 to 1. 1 means the two vectors are identical and -1 means they are dissimilar.  \nFunctional Correctness  \nFunctional correctness evaluates the accuracy of NL-to-code generation tasks when the LLMs is tasked with generating code for a specific task in natural language. In this context, functional correctness evaluation is used to assess whether the generated code produces the desired output for a given input.  \nFor example, To use functional correctness evaluation, we can define a set of test cases that cover different inputs and their expected outputs. For instance, we can define the following test cases:  \ntext\nInput: 0\nExpected Output: 1\nInput: 1\nExpected Output: 1\nInput: 2\nExpected Output: 2\nInput: 5\nExpected Output: 120\nInput: 10\nExpected Output: 3628800  \nWe can then use the LLMs-generated code to calculate the factorial of each input and compare the generated output to the expected output. If the generated output matches the expected output for each input, we consider the test case to have passed and conclude that the LLMs is functionally correct for that task.  \nThe limitation of functional correctness evaluation is that sometimes it is cost prohibitive to set up an execution environment for implementing generated code. Additionally, functional correctness evaluation does not take into account the following important factors of the generated code:  \nReadability  \nMaintainability  \nEfficiency  \nMoreover, it is difficult to define a comprehensive set of test cases that cover all possible inputs and edge cases for a given task. This difficulty can limit the effectiveness of functional correctness evaluation.  \nRule-based Metrics  \nFor domain-specific applications and experiments, it might be useful to implement rule-based metrics. For instance, assuming we ask the model to generate multiple completions for a given task. We might be interested in selecting output that maximizes the probability of certain keywords being present in the prompt. Additionally, there are situations in which the entire prompt might not be useful \u2013 only key entities might be of use. Creating a model that performs entity extraction on generated output can be used to evaluate the quality of the predicted output as well. Given many possibilities, it is good practice to think of custom, rule-based metrics that are tailored to domain-specific tasks. Here we provide examples of some widely used rule-based evaluation metrics for both NL2Code and NL2NL use cases:  \nSyntax correctness: This metric measures whether the generated code conforms to the syntax rules of the programming language being used. This metric can be evaluated using a set of rules that check for common syntax errors. Some examples of common syntax errors are missing semicolons, incorrect variable names, or incorrect function calls.  \nFormat check: Another metric that can be used to evaluate NL2Code models is the format of the generated code. This metric measures whether the generated code follows a consistent and readable format. It can be evaluated using a set of rules that check for common formatting issues, such as indentation, line breaks, and whitespace.  \nLanguage check: A language check metric evaluates whether the generated text or code is written understandably and consistent with the user's input. This check can be evaluated using a set of rules that check for common language issues, such as incorrect word choice or grammar.  \nKeyword presence: This metric measures whether the generated text includes the keywords or key phrases that were used in the natural language input. It can be evaluated using a set of rules. These rules check for the presence of specific keywords or key phrases that are relevant to the task being performed.  \nHuman feedback  \nHuman-based feedback is always going to be a good (if not the gold standard) option for evaluating the quality of LLM-generate output and is particularly relevant if there is no ground truth dataset. Depending on the application, reviewers with domain expertise may be required. This process can be as simple as having a human verify output and assign it a pass/fail mark. However, for a more refined human evaluation process the following steps can be taken:  \nIdentifying a group of domain experts or people familiar with the problem space.  \nGenerating a shareable asset that can be viewed by multiple reviewers. For example, a shared excel spreadsheet can be saved for later use in experiments.  \nHaving each individual review each sample and assigning a score for each completion. Alternatively, reviewers could split up work and review different sets of data points to get through the review process faster.  \nAdditionally, having notes assigned to each reviewed data point gives insight as to how each specific reviewer interpreted each completion for each data point. Having notes is useful as it serves as a reference point for users. These users may have not reviewed the dataset before, but want to understand the thought process involved with each review. Notes could also serve as a convention for standardizing the evaluation process. Learnings derived from each reviewed note could potentially be applied to other samples within the dataset.  \nDepending on human resources available, getting human feedback might be a tedious task. Scenarios that might involve this sort of evaluation need to be carefully assessed.  \nOne can also take human feedback to another level and incorporate it into the LLM-based system using reinforcement learning from human feedback (RLHF). RLHF is a technique that uses methods from reinforcement learning to directly optimize a language model with human feedback. The basic idea is to train a reward model that predicts how much a human would like a given response, and then use that reward model to guide the fine-tuning of the language model.  \nRLHF: Illustrating Reinforcement Learning from Human Feedback  \nHuman evaluation: Guiding Pre training in Reinforcement Learning with Large Language Models  \nReinforcement Learning - Microsoft Research  \nLLM-based Evaluation  \nAnother developing method of evaluating the performance of LLMs is asking the model to score itself! This method is gaining popularity (see recent results from Bing) and requires some of these inputs, depending on the approach:  \nquestion  \nanswer  \ncontext  \nground truth  \nWe can take output produced by the model and prompt the model to determine the quality of the completions generated. This method of evaluation is likely to become more popular with the emergence GPT-4 given the model's ability to accurately score the quality of predicted output. The following steps are typically required to use this evaluation method:  \nGenerating output predictions from a given test set.  \nPrompt the model to focus on assessing the quality of output-given reference text and sufficient context (for example, criteria for evaluation).  \nFeed the prompt into the model and analyze results.  \nThe model should be able to provide a score given sufficient prompting and context. While GPT-4 has yielded fairly good results with this method of evaluation, a human in the loop is still required to verify the output generated by the model. The model may not perform as well in domain-specific tasks or situations that involve applying specific methods to evaluate output. Therefore, the behavior of the model should be studied closely depending on the nature of the dataset. Keep in mind that performing LLM-based evaluation requires its own prompt engineering. Below is a sample prompt template used in an NL2Python application.  \n```text\nYou are an AI-based evaluator. Given an input (starts with --INPUT) that consists or a user prompt (denoted by STATEMENT)\nand the two completions (labelled EXPECTED and GENERATED), please do the following:\n1- Parse user prompt (STATEMENT) and EXPECTED output to understand task and expected outcome.\n2- Check GENERATED code for syntax errors and key variables/functions.\n3- Compare GENERATED code to EXPECTED output for similarities/differences, including the use of appropriate Python functions and syntax.\n4- Perform a static analysis of the GENERATED code to check for potential functional issues, such as incorrect data types, uninitialized variables,\nand improper use of functions.\n5- Evaluate the GENERATED code based on other criteria such as readability, efficiency, and adherence to best programming practices.\n6- Use the results of steps 2-5 to assign a score to the GENERATED code between 1 to 5, with a higher score indicating better quality.\nThe score can be based on a weighted combination of the different criteria.\n7- Come up an explanation for the score assigned to the GENERATED code. This should also mention if the code is valid or not\nWhen the above is done, please generate an ANSWER that includes outputs:\n--ANSWER\nEXPLANATION:\nSCORE:\nBelow are two example:\n\nExample 1\n\n--INPUT\nSTATEMENT = create a cube\nEXPECTED = makeCube()\nGENERATED = makeCube(n='cube1')\n--ANSWER\nSCORE: 4\nEXPLANATION: Both completions are valid for creating a cubes . However, the GENERATED one differs by including the cube name (n=cube1), which is not necessary.\n\nExample 2\n\n--INPUT\nSTATEMENT = make cube1 red\nEXPECTED = changeColor(color=(1, 0, 0), objects=[\"cube1\"])\nGENERATED = makeItRed(n='cube1')\n--ANSWER\nSCORE: 0\nEXPLANATION: There is no function in the API called makeItRed. Therefore, this is a made-up function.\nNow please process the example blow\n--INPUT\nSTATEMENT = {prompt}\nEXPECTED = {expected_output}\nGENERATED = {completion}\n--ANSWER\n```  \nMetrics for RAG pattern  \nThe Retrieval-Augmented Generation (RAG) pattern is a popular method for improving the performance of LLMs. The pattern involves retrieving relevant information from a knowledge base and then using a generation model to generate the final output. Both the retrieval and generation models can be LLMs. The following metrics from the RAGAS implementation (RAGAS is an Evaluation framework for your Retrieval Augmented Generation pipelines - see below) require the retrieved context per query, and can be used to evaluate the performance of the retrieval model and the generation model:  \nGeneration-related metrics:  \nFaithfulness: Measures the factual consistency of the generated answer against the given context. If any claims are made in the answer that cannot be deduced from context, then these will be penalized . This is done using a two-step paradigm that includes creation of statements from the generated answer followed by verifying each of these statements against the context (inferencing). It is calculated from answer and retrieved context. The answer is scaled to (0,1) range where 1 is the best.  \nAnswer Relevancy: Refers to the degree to which a response directly addresses and is appropriate for a given question or context. This does not take the factuality of the answer into consideration but rather penalizes the presence of redundant information or incomplete answers given a question. It is calculated from question and answer.  \nRetrieval related metrics:  \nContext Relevancy: Measures how relevant retrieved contexts are to the question. Ideally, the context should only contain information necessary to answer the question. The presence of redundant information in the context is penalized. Conveys quality of the retrieval pipeline. It is calculated from question and retrieved context.  \nContext Recall: Measures the recall of the retrieved context using the annotated answer as ground truth. An annotated answer is taken as proxy for ground truth context. It is calculated from ground truth and retrieved context.  \nOutput of an LLM evaluator  \nThe output of an LLM evaluator is usually a score (0-1) and optionally an explanation which is something we don't get with traditional metrics.  \nRubric-based Ranking of LLM-generated content  \n{% if extra.ring == 'internal' %}  \n\u2139\ufe0f To view a tutorial on how to use rubric-based ranking, few-shot prompting, and chain-of-thought prompting click here.  \n{% endif %}  \nRubric-based ranking is a method of evaluating the quality of content generated by LLMs. It involves comparing LLM generated content to a set of predefined criteria, called rubrics, that are relevant to the task being performed by the LLM. The content is then given a rank based on how well it matches the rubrics. As this can be used to establish \"ground truth\", the rubrics can be created by subject matter experts.  \nA consistent rubric is useful for comparing the performance of different LLMs on the same task. It can also be useful for comparing the performance of the same LLM on different tasks.  \nThe rubric should meet a certain minimum criteria to be effective while finding a balance between being too simple and too complex:  \nBe concise  \nEach score on the relevancy scale should be well defined  \nThe scale should be able to disambiguate highly relevant chunks  \nExample of a rubric:  \njson\nRelevancy: the quality or state of being closely connected or appropriate:\n- 0: the text chunk is completely irrelevant, does not mention anything about the query or is completely contrary to the query.\n- 1: the text chunk provides some relevance to the query but significant context is missing.\n- 2: the text chunk is relevant and provides sufficient context to discern that the original document provides information that will address the full concern of the query.\n- 3: the text chunk is relevant and provides enough context that it is clear no other text chunks will be necessary to address the query.  \nA few ways to improve the performance of rubric-based ranking are:  \nFew-shot prompting:  \nThe process of providing examples for each score type in the rubric. The examples are used to train a classifier that can be used to evaluate the quality of LLM-generated content. The classifier can then be used to evaluate the quality of LLM-generated content by assigning a score to each chunk of content.  \nChain-of-thought prompting (CoT):  \nThe process of walking through the logic to arrive at a conclusion during the prompt. CoT has empirically shown to improve performance on a range of arithmetic, commonsense, and symbolic reasoning tasks.  Here we experiment with CoT wrt. the task of chunk relevancy scoring given a query.  \nAutomatic Test Generation  \nWe can also use LLMs for Automatic Test Generation, where an LLM generates a diverse range of test cases, including different input types, contexts, and difficulty levels:  \nGenerated test cases: The LLM being evaluated is tasked with solving the generated test cases.  \nPredefined metrics: An LLM-based evaluation system then measures the model\u2019s performance using predefined metrics, such as relevance and fluency.  \nComparison and ranking: The results are compared to a baseline or other LLMs, offering insights into the relative strengths and weaknesses of the models.  \nImplementations  \nPrompt Flow Custom Evaluation: A custom chat and evaluation flow for JSON summarization based on a specified schema.  \nAzure Machine Learning prompt flow: Nine built-in evaluation methods available, including classification metrics.  \nFFModel: A framework for LLM systems that includes NLP metrics for LLM evaluation.  \nOpenAI Evals: Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks (github.com).  \nRAGAS: Metrics specific for RAG  \nRecent findings  \nGPT-4 outperforms GPT-3.5 in zero-shot learning throughout almost all evaluated tasks and both models exhibit limited performance in Inductive, Mathematical, and Multi-hop Reasoning Tasks (https://arxiv.org/pdf/2305.12477v1.pdf)",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\working-with-llms\\metrics\\eval-metrics.md"
    },
    {
        "chunkId": "chunk370_0",
        "chunkContent": "rings:\n- internal  \nMonitoring generative AI applications  \nAs the adoption of generative AI applications continues to grow, so does the\nnecessity for robust monitoring. These applications, powered by intricate\ndata models and algorithms, aren't exempt from the challenges faced by any\nother software system. Yet, their unique nature makes their monitoring\nrequirements distinctive. generative AI apps interact with a vast array of\ndata, generate varied outputs, and often operate under tight performance\nconstraints. The quality, performance, and efficiency of these applications\ndirectly impact user experience and operational costs. Therefore, a structured\napproach to monitoring and telemetry isn't only beneficial but critical.  \nMonitoring offers a real-time lens into an application's health, performance,\nand functionality. For generative AI, this means observing the model's\naccuracy, understanding user interactions, optimizing costs, and more.\nTelemetry provides the raw data necessary for such monitoring, encompassing\neverything from logs and traces to specific metrics.  \nThis guide walks you through the essentials of monitoring generative AI\napplications. It offers a roadmap for capturing, analyzing, and acting on\ntelemetry data to help your AI services run efficiently. We focus on key\noperational telemetry across the entire generative AI application.  \nWhy monitor generative AI applications  \nGenerative AI applications are reshaping how industries operate, making them\ninvaluable assets. Without the right monitoring, even the most\nsophisticated generative AI application can stumble. Here's why it's paramount\nto keep a close watch on these systems:  \nEnsuring model accuracy and reliability: Models evolve, and with\nevolution can come drifts in accuracy. Continuous monitoring ensures the\noutputs remain aligned with expectations and standards. Furthermore, as these\nmodels learn and adapt, monitoring helps in verifying the consistency and\nreliability of their predictions.  \nDetecting anomalies and performance issues: Generative AI can\noccasionally produce unexpected results or behave erratically due to\nunforeseen data scenarios or underlying system issues. Monitoring can\nidentify such anomalies, enabling quick mitigation.  \nUnderstanding user interactions and feedback: Monitoring user\ninteractions gives insights into how well the application meets user needs.\nBy observing user queries, feedback, and behavior patterns, one can make\niterative improvements to enhance the user experience.  \nValidating costs and optimizing operations: Running AI models,\nespecially at scale, can be resource-intensive. Monitoring provides visibility\ninto resource consumption and operation costs, aiding in optimization and\nensuring the most efficient use of available resources.  \nBasic Concepts in Telemetry  \nTelemetry is the process of collecting and transmitting data from remote\nsources to receiving stations for analysis. In the realm of generative AI\napplications, telemetry involves capturing key operational data to monitor\nand improve the system's performance and user experience. Here are some\nfoundational concepts:  \nLogs: Records of events that occur within an\napplication. For generative AI, logs can capture information such as user\ninput, model responses, and any errors or exceptions that arise.  \nTraces: Traces offer a detailed path of a request as it moves through\nvarious components of a system. Tracing can be invaluable in understanding\nthe flow of data from embeddings to chat completions, pinpointing bottlenecks,\nand troubleshooting issues.  \nMetrics: These are quantitative measures that give insights into the\nperformance, health, and costs of a system. In AI, metrics can\nencompass everything from request rate and error percentages to specific model\nevaluation measures.  \nTelemetry is the backbone of a well-monitored AI system, offering the\ninsights necessary for continuous improvement. For a deeper dive into these\nconcepts and more, check the Engineering Fundamentals for\nlogging,\ntracing,\nand metrics.  \nLogging  \nIn generative AI applications, logging plays a pivotal role in shedding light\non interactions, system behavior, and overall health.  \nHere are some recommended logs for OpenAI services:  \nRequests: Logging request metrics, such as response times,\nstop reasons, and specific model parameters to understand both the demand and\nperformance of the system.  \nInput prompts: Capturing user inputs helps developers grasp how users\nare engaging with the system, paving the way for potential model refinements.  \nModel-generated responses: Logging model outputs facilitates auditing\nand quality checks, ensuring that the model behaves as intended.  \nPrompts and responses could be larger than whats appropriate for a\nlog message. If this is the case, save the prompts and responses\nin a suitable database or storage service and provide a reference\nID for later retrieval and analysis.  \nDeveloper teams should collect all errors or anomalies for\ndiagnostic purposes. To control log volume, throttle informational\nlogs should by using a sampling rate or controlled by setting log levels.  \nBe sure to anonymize and secure sensitive data to uphold user privacy and\ntrust.  \nTracing  \nIn generative AI applications, tracing offers a granular, step-by-step view\nof a request's journey through the system. Each of these individual steps or\noperations is a \"span.\" A collection of spans forms a trace that\nrepresents the complete path and lifecycle of a request.  \nHere are the primary spans you might typically see in AI workflows:  \nAPI call span: Call span represents the inception and duration of an API\nrequest. It provides insights into entry points, initial user intentions,\nand the overarching time taken for the entire request to process.  \nService Processing Span: This covers the time and operations when the\nrequest navigates through services. It's especially useful to\nhighlight potential bottlenecks or areas in the system needing optimization.  \nModel Inference Span: This critical span captures the actual time taken\nby the AI model to process the input and make a prediction or generate a\nresponse. It helps gauge the model's efficiency and performance. These spans\ncan also be updated to capture evaluation metrics, whether user driven or\nAI driven.  \nData Fetching Span: Before model processing, there might be a need to\nfetch supplementary data from databases or other storage using embeddings or\nother methods of search. This span traces the duration and operation of that\ndata retrieval and can capture accuracy metrics.  \nRemember to embed privacy and data protection principles when implementing\ntracing, to keep user data confidential and stay regulatory compliant.  \nMetrics  \nMetrics serve as quantifiable measures that shed light on various performance,\nhealth, and usage aspects of the system.  \nHere are some key metrics for generative AI applications:  \nRequest Rates (Requests Per Second): This metric provides\ninsights into the load and demand on the system, enabling scalability planning\nand indicating popular usage times.  \nError Rates: Keeping tabs on the percentage of requests that result in\nerrors is essential. A spike in error rates can indicate problems with the\nmodel, the infrastructure, or both.  \nLatency Metrics: These measure the time taken to process a request.\nTypically, teams segment them into percentiles like P50 (median), P95, and P99\nto show the range of experiences users might experience. Monitoring these\nensures users receive timely responses.  \nModel-specific Metrics: Depending on the application, metrics such as\nBLEU score for translation quality or perplexity for language models might be\nessential. These offer a gauge of the model's predictive performance.  \nCost metrics: Capturing costs, such as the number of tokens consumed, is\nespecially relevant when deploying models in cloud environments. Metrics like\ncost per transaction or API call offer insights into operational expenses. This\nincludes monitoring the number of tokens in prompts and completions, as these\ncan affect costs.  \nReviewing and acting upon these metrics facilitates proactive system\ntuning, ensures user satisfaction, and helps in maintaining cost-efficiency.  \nTags  \nTags, in the telemetry world, offer context to the data collected,\nenriching the metrics, logs, or traces with metadata that can give deeper\ninsights or better filtering capabilities.  \nHere are some valuable tags for generative AI applications:  \nCorrelation IDs: These unique identifiers enable correlation of every\npiece of telemetry from a single user interaction, even across different\ncomponents or services. These IDs are useful at both the request and session\nlevel and are invaluable for troubleshooting and understanding user journeys.  \nModel Tags: These give context on which specific AI model version or\nconfiguration was used for a given interaction. Including tags such as:  \nMax Tokens: Max number of tokens specified for a response.  \nFrequency Penalty: Penalties associated with frequent output tokens.  \nPresence Penalty: Penalties related to the presence of specific output tokens.  \nTemperature: Determines the randomness of the model's response.  \nModel: Identifier for the model version or variant.  \nPrompt Template: The structure or pattern followed by the user's prompt.  \nFinish Reason: Indicates why the model finished the response.  \nLanguage: Language or locale information for the request.  \nOperational Tags: These can include details about the application,\ninfrastructure or environment, such as App Id, Server Id,\nServer Location, or Deployment Version, assisting in pinpointing\nissues or understanding performance variances between different deployments\nor regions of the application or the generative AI services utilized.  \nUser Interaction Tags: While ensuring privacy, tags that give insights\ninto user behavior or type of interaction can be beneficial. For example,\nInteraction Type could be 'query', 'command', or 'feedback'.  \nAlways be sure that tagging respects user privacy regulations and\nbest practices.  \nEmbedding telemetry  \nTelemetry for embeddings in an operational context is vital for ensuring that\nAI systems are providing accurate, relevant, and efficient responses to user\nqueries in real-time. Capturing specific metrics related to embeddings can offer\nactionable insights into the system's behavior during live user interactions.  \nHere are key metrics tailored to operational telemetry for embeddings:  \nDistance and Similarity Measures:  \nThis metric provides insights into how close related user queries are to\nthe results fetched by the system.  \nMonitoring these measures in real-time can help identify if the system is\nreturning highly relevant, diverse, or irrelevant content to\nusers. For instance, consistently close embedding distances for varied user\nqueries might indicate a lack of diversity in results.  \nFrequency of Specific Embedding Uses:  \nBy keeping tabs on which embeddings are accessed most frequently during live\ninteractions, operators can discern current user preferences and system trends.  \nFrequent access to certain embeddings might indicate high relevance and\npopularity of specific content. On the flip side, rarely accessed\nembeddings might hint at content that isn't resonating with users or\npotential issues with the recommendation or search algorithm.  \nIncorporating telemetry for these embedding metrics in an operational setting\nfacilitates swift adjustments, ensuring users consistently receive relevant and\naccurate content. Regular reviews of this telemetry also assist in fine-tuning\nAI systems to better align with evolving user needs and preferences.  \nSpecial considerations for ChatCompletions  \nChatCompletions, as a fundamental part of conversational AI, present unique\nchallenges and opportunities in monitoring. These completions, being dynamic\nand tailored to individual user inputs, can vary widely in quality and relevance.\nOperational monitoring, therefore, requires specific considerations to make sure\nthe system's effectiveness during live interactions.  \nHere are some areas of emphasis:  \nUser Satisfaction Metrics:  \nSession Lengths: Monitoring the duration of user sessions can offer\ninsights into engagement levels. Extended interactions may indicate user\nsatisfaction, while abrupt session ends might hint at issues or frustrations.  \nRepeat Interactions: Tracking how often users return for multiple\nsessions can serve as a direct indicator of the perceived value and\nreliability of the chat system.  \nAbandoned vs. Completed Interactions:  \nKeeping tabs on interactions where users drop off before receiving or after\ngetting a response can help identify potential pitfalls or shortcomings in\nthe AI's response quality or relevancy.  \nAnalyzing reasons for abandonment (whether due to long response times,\nunsatisfactory answers, or system errors) can provide actionable insights\nfor improvements.  \nContext Switching Frequencies and Metrics:  \nContext is vital in conversations. Monitoring how often the AI system\nswitches contexts within a session can offer clues about its ability to\nmaintain topic consistency.  \nHigh context-switching might point to issues in the AI's understanding of\nuser intent or its ability to support a coherent conversational flow.  \nMonitoring infrastructure and tools  \nInstrumenting applications with telemetry data is critical for understanding\nand optimizing system performance. A combination of OpenTelemetry and Azure\nMonitor provides a comprehensive framework for capturing, processing, and\nvisualizing this telemetry. Here's a breakdown of the components and their\nfunctionalities:  \nOpenTelemetry  \nClient SDKs: OpenTelemetry offers client SDKs tailored for many\nprogramming languages and environments, such as\nC#,\nJava, and\nPython. These SDKs\nmake it easy for developers to seamlessly integrate telemetry collection into\ntheir applications.  \nCollector: Serving as an intermediary, the\nOpenTelemetry collector\norchestrates the telemetry data flow. It consolidates, processes, possibly\nredacts sensitive data, and then channels this telemetry to designated\nstorage solutions.  \nAzure Monitor  \nMetrics: Beyond merely storing metrics,\nAzure Monitor\nenriches them with visualization tools and alerting capabilities, ensuring\nteams are always cognizant of system health and performance.  \nTraces: Logs and traces ingested by Azure Monitor undergo a detailed\nanalysis, making it simpler to query and dissect the journey of\nrequests/responses within the system.\nMore on Azure Monitor Traces.  \nOpen Source Tools  \nPrometheus: Renowned for its monitoring capabilities,\nPrometheus is an\nopen-source system that provides insights by scrutinizing events and\nmetrics. Its versatility allows integration with a range of platforms,\nincluding Azure.  \nGrafana: An open-source platform for monitoring and observability,\nGrafana\nmeshes flawlessly with both OpenTelemetry and Azure Monitor, offering\ndevelopers advanced visualization tools they can tailor to specific\nproject needs.  \nElasticSearch: A search and analytics engine,\nElasticSearch is often chosen by\nteams who want a scalable search solution combined with log and event data analytics.  \nData analysis and insights  \nEffective monitoring is just the first step. Extracting insights from\nthe deluge of data is what drives meaningful improvements. Here's a brief\noverview of how to harness this data:  \nAnalyze Across Telemetry Types: Dive into logs, traces, and metrics to\ndiscern patterns and irregularities. This analysis paves the way for holistic\nsystem insights and decision-making.  \nAutomated Alerting: Set up automated alerts that notify the team\nof anomalies or potential issues, ensuring rapid response and mitigation.  \nCorrelate Metrics: Correlating disparate metrics can unveil\ndeeper insights, spotlighting areas for enhancement that the team might\nhave otherwise overlooked.  \nTelemetry-driven Feedback Loop: By understanding how models interact with\nlive data and user queries, data scientists and developers can enhance accuracy\nand user experience.  \nConclusion  \nMonitoring generative AI applications isn't just about system health. It's a\ngateway to refinement, understanding, and evolution. By embedding telemetry and\nleveraging modern tools, teams can illuminate the intricate workings of their AI\nsystems. This insight, when acted upon, results in applications that aren't\nonly robust and efficient but also aligned with user needs. Embrace these\npractices to be sure your AI applications are always at the forefront of\ndelivering exceptional value.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\working-with-llms\\monitoring\\monitoring.md"
    },
    {
        "chunkId": "chunk371_0",
        "chunkContent": "rings:\n- public  \nPrompt Engineering Recommendation  \nPrompt engineering can significantly improve the quality of the LLM output. However, it can also be challenging, as it requires understanding the model's capabilities and limitations, as well as the domain and task at hand. Below are some recommendations for prompt engineering when using large language models.  \nProvide clear instructions \u2013 make sure the prompt is specific on what you want the model to generate, such as the expected format, length, and tone of the completion. For example, if you want the model to generate a summary of a news article, you can specify the number of sentences, the main points to cover, and the style of writing.  \nConsider delineating key components of the prompt or desired completion format with keywords, symbols, or tags - this can help the LLM more clearly identify meaningful information and disambiguate detailed instructions. You can use a combination of strategies in a single prompt, e.g. indicating the start and end of the completion with phrases like \"Summary:\" and \"<|end|>\". You can also instruct the LLM to look for context provided in a certain format, which can help with more complex prompt templating - e.g. \"Summarize the text delimited with triple quotes. Use fewer than 25 words. '''\\<text to summarize>''' \".  \nOffer context \u2013 provide some context about the application, the domain, and the user's intention. This context can help the model to generate more relevant and coherent outputs. For example, if you want the model to generate code, you can include some import statements to focus the model on the relevant libraries and modules. You can also provide some comments or descriptions to explain the purpose and logic of the code. For conversational agents, you can provide some background information about the user, the topic, and the goal of the conversation.  \nStart with zero-shot learning (ZSL) and then few-shot learning (FSL) \u2013 ZSL can be useful when you have no or limited data for your task, or when you want to test the model's creativity and diversity. FSL can be useful when you have some data for your task, or when you want to guide the model's behavior and quality. You can try with different numbers of examples as needed, and select those examples dynamically based on the input or the feedback. For example, if you want the model to generate a product review, you can start with ZSL and see what the model produces. Provide a few examples of positive or negative reviews to steer the model's sentiment and style.  \nTry rearranging your prompt \u2013 the order of the elements in your prompt can affect the model's attention and influence. Few-shot examples or other information at the bottom of the prompt will bias the completion results more than earlier ones at the top. You can experiment with different arrangements of your prompt to see how the model responds and what works best for your task. For example, if you want the model to generate a headline for a news article, you can try putting the article text at the top or the bottom of the prompt. See how the model captures the main idea and the tone of the article.  \nVary history length \u2013 in multi-turn user applications, such as chatbots, the length of the history included in the prompt can provide enough context to the model to generate natural and consistent responses. However, you also need to monitor the model's behavior and clear or limit the history when drift is noticed. Drift is when the model does one of the following actions:  \nDeviates from the topic or the goal of the conversation  \nRepeats itself  \nContradicts itself  \nYou can do the following to identify and prevent drift:  \nVary the history length based on the complexity and the coherence of the conversation  \nTopic modeling  \nSentiment analysis  \nRepetition detection  \nOptimize few-shot selection \u2013 the quality and relevance of the few-shot examples you provide to the model can have a significant effect on the model's performance and generalization. You can train or fine-tune semantic similarity models on your data to select more relevant few-shot samples. Semantic similarity models can measure the similarity between two texts based on their meaning and content, rather than their surface form or syntax. You can use semantic similarity models to rank and filter your data, and select the most similar examples to your input or your task. For example, if you want the model to generate a recipe, you can use a semantic similarity model to select a few examples of recipes that have similar ingredients, cuisines, or steps to your input.  \nInstruct the model how to reason \u2013 you can improve the LLM\u2019s ability to do some reasoning with techniques such as chain-of-thought and self-ask. For instance, chain-of-thoughts enables LLMs to decompose multi-step problems into intermediate steps. It enables the LLMs to solve complex reasoning problems that are not solvable with standard prompting methods.  \nHelp the LLM more easily handle ambiguous context or queries by instructing it to rephrase questions in a way that makes it easier for it to answer clearly - the Rephrase and Respond paper provide starting prompts and helpful guidance on how this technique allows the LLM to reframe questions posed by humans in a way that is easier for LLMs to reason about, as well as provide suggestions to users on ways to reformulate queries to be more effective. This technique can be composed with other reasoning methods like chain-of-thought to improve reasoning performance and provide a more conversational and user-friendly chat experience.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\working-with-llms\\prompt-engineering\\prompt-engineer-recommend.md"
    },
    {
        "chunkId": "chunk372_0",
        "chunkContent": "rings:\n- public  \nPrompt Engineering  \nWhat is Prompt Engineering?  \nLarge language models (LLMs) have the ability to learn new tasks on the fly, without requiring any explicit training or parameter updates. This mode of using LLMs is called in-context learning. It relies on providing the model with a suitable input prompt that contains instructions and/or examples of the desired task. The input prompt serves as a form of conditioning that guides the model's output, but the model does not change its weights. In-context learning can be applied in different settings such as zero-shot, one shot, or few-shot learning. It depends on the amount of information that needs to be included in the input prompt.  \nThe process of designing and tuning the natural language prompts for specific tasks, with the goal of improving the performance of LLMs is called Prompt Engineering.  \nPrompt engineering can significantly improve the performance of LLMs on specific tasks. It is done by providing instructions and contextual information that help guide the model's output. By carefully designing prompts, researchers can steer the LLM attention towards the most relevant information for a given task, leading to more accurate and reliable outputs. Additionally, prompt engineering can help mitigate the problem of \"catastrophic forgetting,\" where an LLM may forget previously learned information when fine-tuned on a new task. By using carefully designed prompts, the model can retain relevant knowledge while still adapting to new tasks.  \nPrompt Components (Examples)  \nWhile it is considered a new field, there is already a rich literature (e.g., articles, blogs, papers, repos, etc.) on prompt engineering techniques. Different types of contents can be included in the prompt. A comprehensive review of these methods and contents is out of the scope of this document. However, in this section we discuss a few widely methods/contents. The figure below also shows an illustrative example.  \nInstructions and other static context  \nStatic context description refers to providing fixed information to the LLM. This information can include content and format instructions, database schema, or any other contextual information that is relevant to the task.  Herein we provide three widely used static context examples for prompt engineering:  \nSystem message to inform LLMs conversational or functional style:\nA system message can be used to inform the LLM about the context. The context maybe the type of conversation it is engaging in or the function it is supposed to perform. It helps the LLM generate more appropriate responses. For example, if the LLM is expected to perform a customer service function, the prompt could include a system message such as  \ntext\nYou are a friendly AI agent who can provide assistance to the customer regarding their recent order.  \nSimilarly, if the LLM is engaging in a casual conversation, the prompt could include a message such as  \ntext\nLet's have a friendly chat about your favorite hobbies.  \nDefine expected completion formatting by formatting input text:\nBy formatting input text in a specific way, we can guide the LLMs towards generating more relevant responses. For example, if the LLM is trained to generate recipe instructions, the input text could be formatted as \"First, [action], then [action], and finally [action].\" This formatting guides the LLM towards generating instructions in a sequential manner. Similarly, if the LLM is trained to generate summaries of news articles, the input text could be formatted as \"The article talks about [topic]. It discusses [point 1], [point 2], and [point 3].\" This formatting guides the LLM towards generating a summary that covers the main points of the article.\nOther examples in static context formatting can be used in code generation applications. Here we can see the importance of using the correct comment character for the language that you want to generate.\nPrompt:\n/* Create a javascript dictionary with 5 cities and their location */\nCompletion:\njavascript\nvar cities = {\n'Los Angeles': [34.0522, -118.2437],\n'New York': [40.7128, -74.0059],\n'San Francisco': [37.7749, -122.4194],\n'Chicago': [41.8781, -87.6298],\n'Houston': [29.7604, -95.3698]\n};  \nIn this case, we get a properly formatted JavaScript dictionary with the requested information.\nNow, if you change \"JavaScript\" to python and leave the comment characters the same you get:\nPrompt: /* Create a python dictionary with 5 cities and their location */\nCompletion:  \nC+\nPyObject *pDict = PyDict_New();\nPyDict_SetItemString(pDict, \"San Francisco\", Py_BuildValue(\"(f,f)\", 37.77493, -122.419416));\nPyDict_SetItemString(pDict, \"New York\", Py_BuildValue(\"(f,f)\", 40.7127, -74.0059));\nPyDict_SetItemString(pDict, \"London\", Py_BuildValue(\"(f,f)\", 51.507351, -0.127758));\nPyDict_SetItemString(pDict, \"Paris\", Py_BuildValue(\"(f,f)\", 48.856614, 2.352222));\nPyDict_SetItemString(pDict, \"Tokyo\", Py_BuildValue(\"(f\nf)\", 35.689487, 139.691706));  \nHere we get C++ code making a python dictionary, because of the comment character being /* instead of #.  \nSupporting information to enrich the prompt context:\nAdding supporting information to the prompt can provide more context for the LLM, which can improve the accuracy of its responses. For example, if the prompt asks the LLM to generate a response to a specific question, providing additional information can help the LLM generate a more relevant response. A good example would be:  \ntext\nPlease explain transformer language model to a 15-year-old student.  \nSimilarly, the LLM generates a more accurate and persuasive description when provided additional information if asked to generate a product description. A good example would be:  \n{% raw %}  \n```text\nWrite a witty product description in a conversational style so young adult shoppers understand\nwhat this product does and how it benefits them.\nUse the following product details to summarize your description:\n\nTitle: {{shopify.title}}\nType: {{shopify.type}}\nVendor: {{shopify.vendor}}\nTags: {{shopify.tags}}\n```  \n{% endraw %}  \nTask-specific knowledge enrichment (data augmented generation)  \nPrompt engineering by task-specific knowledge enrichment involves retrieving relevant knowledge from a large corpus of text and incorporating it into the prompt to improve the performance of language models. This enrichment is also known as Data Augmented Generation. One way to achieve this enrichment is through a knowledge retrieval strategy, or Retrieval-Augmented Generation (RAG). This strategy involves chunking and indexing bulk knowledge context, followed by embedding similarity context selection. Here's how this approach works:  \nImporting knowledge data from data sources as document store.\nMultiple data sources could be used to build knowledge document store. For example, you could import the following data:  \nCar reviews from websites such as Edmunds and Consumer Reports  \nTechnical manuals in pdf format from car manufacturers  \nNews articles from sources such as Reuters and CNN  \nEach of these data sources would be represented as a separate document within the document store.\n- Chunking the text into smaller, possibly overlapping and more manageable segments.\nThe chunk can be implemented by a static size of token (ex, 1000). The chunk size is defined by considering token size limit of LLMs' model inference. It can also be carried out by embedded titles, topics, or natural paragraph styles in the knowledge data.\n- Embedding generation / indexing for efficient retrieval.\nThe final step is to generate embeddings and index the segmented text for efficient retrieval. This generation involves representing each segment as a vector or a set of features that capture its key semantic attributes. For example, you could generate embeddings using a pretrained language model (ex., BERT, text_embedding-ada-002) and save it as a vector store. Next, an index is created that maps each embedding to the corresponding. This index allows you to quickly retrieve all the documents that contain relevant information based on their similarity to the query embedding.\n- Retrieval of relevant context (e.g., chunks).\nOnce the knowledge base has been chunked and indexed, the next step is to select the most relevant pieces of information to incorporate into the prompt. One approach to do that is by semantic search. Specifically, embeddings of the indexed knowledge segments are compared with the embeddings of the input prompt to identify the most similar pieces of information.\n- Carefully consider and experiment with ways of phrasing the prompt to reduce hallucinations by instructing the LLM to ground its answers in the provided context.\nOne option is grounding via the \"according to\" technique - e.g. \"According to, \\<insert context here>, answer the following user question.\"\nYou can also explicitly instruct the LLM to say \"I don't know\" if the provided context doesn't contain the information required to answer the question.  \nThis method of prompt engineering by task-specific knowledge enrichment can be highly effective in improving the accuracy and relevance of LLM responses. By incorporating relevant knowledge into the prompt, the LLM can generate more informed and accurate responses. These responses can enhance the user's experience and increase the effectiveness of the system. The following figure provides one example of task-specific knowledge enrichment architecture design.  \nAdditional information and considerations regarding the RAG pattern, as well as a sample architecture leveraging Azure AI Search, can be found in the Azure AI documentation.  \nFew-shot examples  \nFew-shot examples involve providing a few input and output examples (input-output pairs) to the LLM to guide its completions in both content and format. The following example is a simple few-shot classification:  \ntext\napple: fruit\norange: fruit\nzucchini: vegetable  \nNow, if we want to know if a tomato is a fruit or vegetable, we include this few-shot example prior to input:  \ntext\napple: fruit\norange: fruit\nzucchini: vegetable\ntomato:  \nTo which the LLM (e.g, GPT-3) responds with \"fruit\".  \nThe previous example is a case of static few shot example. No matter what object we are trying to classify, the same examples are used. However, there are cases where we may want to pick different few shot examples dynamically based on the input prompt. To do that, a library/bank of few-shot examples is created. Each example is represented in a feature space (e.g., embeddings using a pre-trained model). Then, when a new prompt is presented, the few-shot examples that are most similar in that feature space are selected to guide the language model. This method is useful when the following statements are true:  \nThe few-shot bank data is large and diverse  \nThe examples share common underlying pattern  \nFor example, if the few-shot bank data consists of various examples of restaurant reviews, embeddings can capture similarities in the language used to describe the quality of food, service, and atmosphere. The most similar examples can be used to optimize the language model inference.  \n{% if ring.extra == 'internal' %}\nThe FFModel few_shot_embedding_component implements this technique.\n{% endif %}  \nThere are different techniques to improve/optimize the dynamic few-shot selection further. One approach is to filter or categorize the examples in the few-shot for faster retrieval of more relevant examples. To do that, the few-shot bank examples are labeled using intentions or tasks. A custom model can be trained to classify those examples. (e.g., sports, entertainment, politics, etc.). When a new prompt is presented, the classifier is used to predict the task or intention of the prompt. Then, the few-shot examples that are most relevant to the predicted task are selected to instruct the language model inference. The following figure provides an example of the architecture design for dynamic few-shot example retrieval. This retrieval uses the embedding similarity or intention prediction classifier method.  \nHistory  \nPrompt engineering using session history involves tracking the history of a conversation between the user and the language model. This method can help the language model generate more accurate responses by taking into account the context of the conversation. Here's an example of how LLMs track the history of conversation to help generate accurate responses:  \ntext\nUser: The capital of India?\nLLM: The capital of India is New Delhi.\nUser: What is the population of this city?\nLLM: As of 2021, the estimated population of New Delhi is around 31.8 million people.  \nIn this scenario, the LLM is able to use the history of the conversation to understand \"this city\" refers to \"New Delhi\".  \nAnother example takes the following multi-turn NL2Code interaction where the user's requests follow the # comment character and the model's code follows.  \n```python\n\nAdd a cube named \"myCube\"\n\ncube(name=\"myCube\")\n\nMove it up three units\n\nmove(0, 5, 0, \"myCube\")\n```  \nFor the second request (\"Move, it up three units\") the model is only able to get the correct completion because the previous interaction is included.  \nOne helpful trick for deciding if you need session history is to put yourself in the place of the model, and ask yourself \"Do I have all of the information I need to do what the user wants?\"  \n{% if ring.extra == 'internal' %}\nThe FFModel session_selector component includes previous examples from the same session.\n{% endif %}  \nChallenges and limitations of prompt engineering  \nWhile prompt engineering can be a useful approach to improve the accuracy and effectiveness of LLMs inference results. It is not without its challenges and limitations. Herein we summarize three major challenges in prompt engineering.  \nToken size limit for prompt input: Most LLMs have a limit on the number of tokens that can be used as input to generate a completion. This limit can be as low as a few dozen tokens. This limit can restrict the amount of context that can be used to generate accurate completions.  \nData for prompt engineering are not always available: For example, prompts may require domain-specific knowledge or language that is not commonly used in everyday communication. In such cases, it may be challenging to find suitable data to use for prompt engineering. Additionally, the quality of the data used for prompt engineering affects the quality of the prompts.  \nIncrease the complexity of the solution and make it more difficult to compare and evaluate different models. As the number of prompts increases, it becomes more difficult to keep track of the various experiments and to isolate the effect of the prompts on the final output. This tracking difficulty can lead to confusion and make it more challenging to draw meaningful conclusions from the experiments.  \nIntroduce additional latency and costs. Generating high-quality prompts requires time and resources, and this latency can slow down the overall process of model development and deployment. Additionally, more complex prompts can significant increase the prompt token size in each LLMs calling, which can increase the cost of running experiments.  \nSmall changes to the prompts can result in very different outputs. It can make it difficult to predict how the model will behave in different situations and can lead to unexpected results. It becomes problematic in applications where accuracy and consistency are critical, such as in automated customer service or medical diagnosis.  \nResponse quality, particularly in more domain-specific scenarios, can be difficult to evaluate via programmatic metrics alone. Meaningful evaluation can also require incorporating actual user feedback, and experimenting with multiple prompts can introduce considerations like preference testing or A/B testing to best tune the prompts used for a more user-friendly experience.  \nResources  \nOpenAI prompt engineering guide  \nAzure AI documentation on prompt engineering",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\working-with-llms\\prompt-engineering\\prompt-engineering.md"
    },
    {
        "chunkId": "chunk373_0",
        "chunkContent": "author: shanepeckham\ntitle: Security guidance for Large Language Models\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: It's not a good idea to use LLMs in high-risk and/or autonomous scenarios. LLMs may be biased, fabricate/hallucinate information, have reasoning errors, and struggle at certain tasks. They are susceptible to prompt injection, jailbreak attacks, and data poisoning attacks. Sensitive or confidential data may be leaked. When connected to other systems, they may take unintended actions.\nrings:\n- public  \nSecurity guidance for Large Language Models  \nIt's not a good idea to use Large Language Models (LLMs) in high-risk and autonomous scenarios.  \nLLMs may be biased, fabricate/hallucinate information, have reasoning errors, and struggle at certain tasks. They are susceptible to prompt injection, jailbreak attacks, and data poisoning attacks. Sensitive or confidential data may be leaked. When connected to other systems, they may take unintended actions.  \nBe mindful that LLMs is nascent technology. There are no proven, ironclad defenses for preventing manipulation of your LLM. For every clever defense, there seems to be a clever attack or workaround.  \nTherefore, it\u2019s best to use LLMs in low-stakes applications combined with human oversight.  \nLLM-specific threats  \nA broad list of threats has been published by OWASP:\nOWASP Top 10 List for Large Language Models version 0.1.  \nRecommended mitigations  \nLog and monitor LLM interactions (inputs/outputs) to detect and analyze potential prompt injections, data leakage, and other malicious or undesired behaviors.  \nImplement strict input validation and sanitization for user-provided prompts:  \nClearly delineate user input to minimize risk of prompt injection. For example, instead of using a prompt like Summarize the following text: {user input}, you should go out of your way to clarify that the provided user input isn't part of the prompt itself: `Summarize the text below wrapped in triple backticks: {user input}  \nSanitize user input. It may contain the delimiter sequence you use to delineate user input, etc.  \nRestrict the LLM\u2019s access to sensitive resources, limit its capabilities to the minimum required, and isolate it from critical systems and resources.  \nRed team the LLM by crafting input to cause undesired behavior.  \nClearly label LLM-generated content as being generated by AI and encourage human review.  \nIf using OpenAI, use their free Moderation API to evaluate user inputs before sending them to OpenAI's Completion or Chat. It also allows your service to filter out requests that would otherwise hit OpenAI endpoints with content that goes against OpenAI usage policies.  \nYou can also use the Moderation API to evaluate model responses before returning them to the user.  \nOther approaches to explore  \nUse prompt chaining to first ask whether the user's input attempts to do anything malicious like a prompt injection attack.  \nConsider if the OpenAI Evals framework can be used to evaluate the security of your LLM.  \nLook up your AI model in the AI Risk Database. Enter the model name or URL in the search bar of the homepage.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\working-with-llms\\security\\security-recommend.md"
    },
    {
        "chunkId": "chunk374_0",
        "chunkContent": "Introduction to LLM-based Applications  \nLarge Language Models (LLMs) are AI models that can understand human language and use that understanding to take certain actions. They are trained on huge data sets with many parameters, which makes them very robust and broad in terms of how they can be used. One of the most common use cases of LLMs is to generate content based on one or more prompts from a user. This can include text generation, machine translation, question answering, summarization, and even chatbots that can hold conversations with humans.  \nCommon Scenarios  \nThere are many different customer problems that can be solved using Large Language Models (LLMs) with no fine-tuning. GPT-3, GTP-3.5 and GPT-4.0 are prompt-based models, and they require just a text prompt input to generate its responses. At the same time, the quality of the responses heavily depends on prompts themselves. Therefore, to use prompt-based models effectively it's important to setup a strong Large Language Models Development and Operations (LLMOps) process that will include recommendations and tools helping to set up an experimentation and development environments for prompt engineering.  \nLet\u2019s look at some scenarios where Large Language Models are a useful component, even without fine-tuning. It will help us to understand what we need to develop and, finally, operationalize.  \nScenario 1: The basic service applies LLMs to input data to produce a summary, extract entities, or format data according to a pattern. For example, a quality assurance engineering assistant could help by taking natural language descriptions of an issue and reformatting to match a specific format. In this case, we need to develop a backend to implement a single step flow to invoke an LLM, pass input data to it and return the LLM\u2019s response back. The request to the LLM can be created using the few-shot approach, when it contains a system message, the input, and a few examples to tune LLM\u2019s response.  \nScenario 2: A chatbot that provides abilities to extract some data using existing APIs. For example, it can be an application to order a pizza. It\u2019s a conversational application, and a pizzeria\u2019s API can provide data about available pizza types, toppings, and sales. The LLM can be used to collect all needed information from users in a human friendly way. In this case, the development process is concentrated around a backend as well as a user interface (UI). The backend must implement a conversational flow that might include history, API connections management and own logic to invoke different APIs. In this scenario we are using APIs as is, and we are assuming that they are black boxes with known inputs and outputs.  \nScenario 3: A chatbot that finds and summarizes information spread in several data sources, both structured and unstructured. For example, it can be a chatbot for service agents who use it to find answers for users\u2019 questions in a big set of documents. It\u2019s not enough to return references to documents that contain the needed information, but it\u2019s important to extract the exact answer in a human readable format.  \nIn this scenario the development process should be focused around:  \nUI that is the chatbot itself.  \nA backend that implements a complex flow gluing LLMs with other services, such as a search service. The backend might support memory, chain several LLM requests, manage connections to external services, load documents. The documents themselves need to be stored in a searchable form, e,g. a vector database and to be stored they may need to be transformed (converted from image to text, or from audio or video to text), chunked, and embedded.  \nServices that must be tuned and configured to retrieve results. The tuning process might include components such as various pipelines to do data ingestion, indexes, serverless components to extend functionality of the services.  \nThese three scenarios might not cover all possible usage of LLMs, but they cover the most common projects, and we can use them to understand what kinds of components we need to build.  \nPrimary components for an LLM based application  \nBased on the above scenarios, the following three different components need to be developed:  \nClient Applications: It can be a web application, a plug-in for an existing conversational tool and any other application that satisfies the customer. This artifact is out of scope of this document since there is no LLM related components and all standard DevOps practices should be applied. It is a critical part of the solution and will be addressed in other documents.  \nBackend System: This is a primary component of all presented scenarios. The goal of this component is to implement the flow logic, interact with LLMs and other services, manage configurations and connections. In fact, when we refer to an LLM backend, we have in mind an inferencing service, if we use Machine Learning terminology. It is a piece of software that will be published into production, and we need to build operationalization for this component.  \nManaged Services: Azure Cognitive Search is a good example of this component. In fact, it can be any other service that supports search queries (typically vector-based) and its own data ingestion pipelines. This includes Azure Cosmos DB, relational databases such as Postgres, or cache services such as Redis Search, and so on. The most common set of projects here are related to RAG (Retrieval Augmented Generation), RAG is a technique that enables a Large Language Model (LLM) to utilize your own data to generate responses.  \nPlease refer to this document for more details on Monitoring & Observability, Security & Privacy, Data Platform and LLMOps to build, deploy and maintain an LLM based application.  \nFor detailed guidelines on LLMOps, please refer here and for LLMOps for RAG pattern, please find more information here.  \nNext in this document, we would like to elaborate on another important aspect of LLM-related backends: programming languages and LLM libraries.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\working-with-llms\\use-cases\\common-scenarios.md"
    },
    {
        "chunkId": "chunk375_0",
        "chunkContent": "author: shanepeckham\ntitle: Understanding Large Language Model (LLM) strengths and weaknesses\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Large language models, such as GPT-3, are powerful tools that can generate natural language across various domains and tasks. However, they are not perfect and have limitations and risks that need to be considered before deciding to use them for real-world use cases. Below, we provide some recommendations for the use cases of large language models.  \nrings:\n- public  \nUnderstanding strengths and weaknesses of Large Language Models  \nLarge Language Models (LLMs), such as GPT-3, are powerful tools that can generate natural language across various domains and tasks. However, they are not perfect and have limitations and risks that need to be considered before deciding to use them for real-world use cases. This articles provides some recommendations for the use cases of large language models.  \nThese models can be best used for generative applications  \nLLMs are trained on massive amounts of text. The objective is to learn the statistical patterns of language, and predict the most likely word given the previous words. Therefore, they are most suited for the following scenarios that require generating coherent and fluent text:  \nWriting stories  \nWriting essays  \nWriting captions  \nWriting headlines  \nGenerating natural language from structured data  \nWriting code from natural language specifications  \nSummarizing long documents  \nHowever, they may not perform well on tasks that require more logical reasoning, factual knowledge, or domain-specific expertise. For the latter, sufficient relevant information needs to be augmented to the prompt to ground the model.  \nBad answers, factual errors, and other problematic output will happen  \nLarge language models are not infallible, and they may produce output that is incorrect, misleading, biased, offensive, or harmful. This failure can happen for one of the following reasons:  \nData quality issues  \nModel limitations  \nAdversarial inputs  \nUnintended consequences  \nTherefore, the use case should be designed in a way that minimizes the effect and frequency of such failures. It should also provide mechanisms for detecting, correcting, and reporting them. For example, the use case could include quality checks, feedback loops, human oversight, or ethical guidelines.  \nSmaller models might work better than LLMs  \nLLMs are general-purpose models that can handle a wide range of tasks. They may not be optimal for specific tasks that require more specialized knowledge or skills. In many cases, a smaller, purpose-built NLP model may outperform GPT-3 for a narrow, non-generation task.  \nFor example, take a task that involves classifying text into predefined categories, such as sentiment analysis, spam detection, or topic modeling. That task may benefit from a model that is trained and fine-tuned on a relevant dataset and objective, rather than a generic model that tries to fit all possible scenarios. A purpose-built NLP model may also be more efficient, interpretable, and explainable than a large language model.  \nUse caution when sharing LLM output  \nLLMs are not recommended for use cases where outputs are directly consumed by end users. LLMs can generate plausible and convincing text, but they cannot guarantee its accuracy, reliability, or suitability for a given purpose. Therefore, we do not recommend use cases where the model outputs are directly presented to an end user, especially in high-risk or high-stakes contexts.  \nParticular care should be taken when the end user lacks the knowledge or expertise necessary to verify the validity of an LLM response. Consider these examples:  \nMedical advice  \nLegal guidance  \nFinancial information  \nEducational content  \nIn these cases, a human expert should be involved in the process. They should either review, edit, or approve the model outputs, or provide more context, clarification, or disclaimer.  \nConclusion  \nIn conclusion, LLMs are powerful and versatile tools that can enable many novel and useful applications. They also have limitations and risks that need to be carefully considered and addressed. We hope that these recommendations can help developers and users of large language models to make informed and responsible decisions about their use cases.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\working-with-llms\\use-cases\\use-case-recommend.md"
    },
    {
        "chunkId": "chunk376_0",
        "chunkContent": "Generative AI Use Cases  \nSCENARIO DESCRIPTION USE CASES STANDARDIZED DOCUMENT WRITING ASSISTANT Utilize LLMs to write a \"first draft\" of a standardized document that conforms to strict requirements in form and substance. Most data will come from internal sources, and often enterprises will have numerous examples of previously written documents or document sections. The final output usually requires perfect accuracy i.e. a human must carefully vet any AI-generated content for accuracy and regulatory compliance. \u2022 ASSETS : Report Generation to Reduce troubleshooting time and resulting production line stoppage : P&G \u2022 ASSETS : Use historic proposals to assist creating responses to new requests from prospects : Telstra \u2022 ASSETS : Help sales executives to processing all available information and creating of a bill material : Siemens CREATIVE CONTENT CREATION OR IDEATION (BRAND-ALIGNED AUGMENTED IDEATION) Utilize GenAI to inform or draft one-off or freeform documents, including both text and imagery. In certain cases, GenAI outputs will be expected to conform to user or brand-defined style and/or formatting guidelines. These scenarios expect to support an active, human \"driver\" and therefore have less concerns about accuracy and usually have no compliance requirements. \u2022 ASSETS : Generate virtual operation room to monitor global assets centrally : Shell INFORMATION SUMMARIZATION AGENT Use GenAI to machine read documents, text and imagery, and write a more concise summary.This can include: - Bulk summarization (generate summaries for these 1000 documents), real-time summarization (summarize the key points in this document), or contextual summarization (write different summaries for different audiences or contexts). - Image and video summarization to provide information about the content for further analysis, search or user knowledge (e.g., movie overview) This can be a standalone scenario but could also commonly be one capability for an agent/assistant scenario. \u2022 ASSETS : Summary of the customer claim to accelerate customer care agent responses : Lufthansa \u2022 PEOPLE : Streamline the pre-authorization process for medications and medical procedures : CVS NATURAL LANGUAGE TO API Utilize natural language to invoke APIs that provide the response data to the client. If the covered APIs are not responsive to the natural language input, the AI should reply that the given capability is not supported rather than invoking the \"best\" answer. This scenario is not a developer scenario. This scenario is likely to reside within a multi-capability agent. \u2022 ASSETS : GraphQL, various API : Cognite ENTERPRISE DATA SEARCH Natural language initiated search of company data. Includes Q&A \u2022 ASSETS : Chat based Q&A across operating procedures and work instructions : ROCKWELL CURATED EXPERIENCE ASSISTANT Real-time assistant that curates a user  experience through an experience through guidance, recommendations, or executing next logical steps. Includes facilitated experiences like education or tour guide \u2022 ASSETS : Sales Academy Metaverse Training Solution with AI companion : CCH \u2022 ASSETS : AI assisted programming of a vehicle navigation system : GM \u2022 PEOPLE : Hyper-personalized shopping experiences across their group companies : Flipkart \u2022 PEOPLE : Assist customers in buying products (Tires) : Canadian Tire VIRTUAL EXPERT ASSISTANT Virtual assistant for specialized tasks that require domain expertise. Tasks may include analysis driven action, process defined action Use cases may require AI augmented capabilities  (e.g., anomaly detection, data analytics) \u2022 ASSETS : Trouble ticket logging and tracking, enable a customer agent to request action : Telstra \u2022 ASSETS : Monitor supply chain, manufacturing to identify bottlenecks and automate decisions : CCH \u2022 ASSETS : Process and validate videos from surveillance cameras to detect suspicious activity : Sony NATURAL LANGUAGE TO CODE AI assisted code generation. May include domain specific code or protocols This is a developer scenario. \u2022 ASSETS : Generate PLC code that uses Ladder Logic : ROCKWELL \u2022 ASSETS : Natural language to SQL : Honeywell \u2022 ASSETS : Natural Language to Python : Cognite \u2022 ASSETS : AI assisted prompt generation : CVS, Novartis, HSBC AI-AUGMENTED DATA CURATION AI assisted data classification and labeling. Includes generating synthetic data from sample set or profile \u2022 ASSETS : data contextualization for various energy related analysis : P&G \u2022 PEOPLE : Generate synthetic data based on customer criteria : LSEG \u2022 PEOPLE : AI assisted corpus curation of mission critical corpus (Documents Ingest + Automatic tagging) : TDBank, Manulife, Novartis ,US DOD, Walmart, Flipkart, CVS, HSBC AI LIFECYCLE MANAGEMENT ML tools and processes such Custom Model and Foundation Model development, experimentation and MLOps May include natural language inputs and report out. \u2022 ASSETS : Creating a platform where SE can deploy various developed model in production : Schneider \u2022 ASSETS : Compute Vision Toolkit (CVT) framework to deploy solution across factories & region : Michelin \u2022 PEOPLE : Centralized hub for AI performance and quality management : Flipkart \u2022 PEOPLE : Custom Domain Models AI DevOps : Novartis, Eli Lily, US Army, HCA Sanofi ANALYST ASSIST An AI agent that can perform entry-level tasks, including invoking tools, fetching data, summarizing data, or calling APIs, in support of a research/analyst workflow 1. Provide a chat interface for a filtering down financial securities 2. Process news articles to extract relevant data for a specific persona and portfolio 3. Provide a natural language interface to invoke modeling tools (i.e. regressions, pricing models, etc.) MULTI-CAPABILITY ASSISTANT/AGENT An AI agent that sits atop multiple AI systems and can route requests to or orchestrate multiple interactions with the appropriate underlying system. As a router, the agent would understand user intent and hand-off responsibility to the best system for the job, e.g. send a policy question to \"HR Policy RAG Agent\". As an orchestrator, the agent would request responses from one or more services, utilize those responses to inform next actions (whether to invoke more services), and then finally synthesize a response back to the client.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\generative-ai\\working-with-llms\\use-cases\\use-cases.md"
    },
    {
        "chunkId": "chunk377_0",
        "chunkContent": "author: shanepeckham\ntitle: MLOps 101\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Machine Learning DevOps (MLOps) is an organizational approach that relies on a combination of people, process, and technology. This approach delivers Machine Learning solutions in a robust, scalable, reliable, and automated way. This guide provides a balanced view across the three areas of people, process, and technology.\nrings:\n- public\ntags:\n- Automotive\n- Manufacturing  \nMLOps 101  \nDelivering Machine Learning  \nMachine Learning DevOps (MLOps) is an organizational approach that relies on a combination of people, processes, and technology. This approach delivers Machine Learning solutions in a robust, scalable, reliable, and automated way. This guide provides a balanced view across this combined three areas.  \nHow Machine Learning DevOps is different from DevOps  \nFor an intro to Azure's tools for Machine Learning and MLOps, check out the Azure ML Intro document.  \n{% if extra.ring == 'internal' %}  \nCustomer Readiness: MLOps  \nIs the customer organization ready for MLOps? Refer to the Customer Readiness: MLOps section for more info.  \n{% endif %}  \nExploration Precedes Development and Operations  \nData science projects are different from application development or data engineering projects. Data science projects may or may not make it to production. After an initial analysis, it might become clear that the business outcome can't be achieved with the available datasets. Because of this reason, an exploration phase is usually the first step in a data science project. The goal in this phase is to define and refine the problem and run exploratory data analysis. During exploratory data analysis, statistics and visualizations are used to confirm or falsify the problem hypotheses. There must be a common understanding that the project might not extend beyond this phase. It's important to make this phase as seamless as possible to have a quick turnaround.  \nData scientists work most efficiently with tools of their choice. Unless there are strict security requirements, it is better not to enforce too restrictive processes at this stage. For some guidance on introducing security, refer to the Data Privacy section  \nReal data is needed for data exploration work.  \nThe experimentation and development stage usually begins when there is enough confidence that the data science project is feasible and can provide real business value. This stage is when development practices become increasingly important. It's a good practice to capture metrics for all of the experiments that are done at this stage. It's also important to incorporate source control, which makes it possible to compare models and toggle between different versions of the code if needed. Development activities include the following tasks:  \nThe refactoring, testing, and automation of exploration code into repeatable experimentation pipelines  \nThe creation of model serving applications and pipelines.  \nRefactoring code into more modular components and libraries helps increase reusability and testability, and it allows for performance optimization. Finally, what is deployed into staging and production environments is the model serving application or batch inference pipelines.  \nInfrastructure reliability and performance must be monitored, similar to what's done for a regular application with traditional DevOps. The quality of the data, the data profile, and the performance of the model must be continuously monitored against its business objectives to mitigate factors such as data drift.  \nMachine Learning models require retraining over time to stay relevant in a changing environment. Refer to the Data Drift section for more information.  \nData Science Lifecycle Requires an Adaptive Way of Working  \nIf you apply a typical DevOps way of working to a data science project, you might not find success. Success might not be achieved because of the uncertain nature of data quality, and the correlation (or lack thereof) between independent and dependent variables. Exploration and experimentation are recurring activities and need to be practiced throughout a machine\nlearning project. The teams at Microsoft follow a project lifecycle and working process that was developed to reflect data science-specific activities. The Team Data Science Process and The Data Science Lifecycle Process are examples of reference implementations.  \nData Quality Requirements and Data Availability Constrain the Work Environment  \nFor a Machine Learning team to effectively develop Machine Learning-infused applications, it is essential to have access to production data or data that is representative of the production data.\nFor a Machine Learning team to effectively develop Machine Learning-infused applications, it is essential to have access to production data or data that is representative of the production data.  \nMachine Learning requires a greater operational effort  \nUnlike traditional software, a Machine Learning solution is constantly at risk of degradation because of its dependency on data quality. To maintain a high-quality solution once in production, continuous monitoring and re-evaluation of the data & model quality is critical. It's expected that a production model requires timely retraining, redeployment, and tuning. These tasks come on top of day-to-day security, infrastructure monitoring, or compliance requirements and require special expertise.  \nMachine Learning teams require specialists and domain experts  \nWhile data science projects share roles with regular IT projects, the success of a Machine Learning team highly depends on Machine Learning technology specialists and domain subject matter experts. Where the technology specialist has the right background to do end-to-end Machine Learning experimentation, the domain expert can support the specialist to analyze and synthesize the data, or qualify the data for use.  \nThe following common technical roles that are unique to data science projects:  \nDomain Expert  \nData Engineer  \nData Scientist  \nAI Engineer  \nModel Validator  \nMachine Learning Engineer  \nTo learn more about roles and tasks within a typical data science team, also refer to the Team Data Science Process.  \nSeven principles for Machine Learning DevOps  \nWhen you plan to adopt MLOps for your next Machine Learning project, consider applying the following core principles as the foundation to any project.  \nVersion control code, data, and experimentation outputs  \nUnlike traditional software, data has a direct influence on the quality of Machine Learning models. Along with versioning your experimentation code base, version your datasets to ensure you can reproduce experiments or inference results. Versioning experimentation outputs like models can save effort and the computational cost of recreating them.  \nUse multiple environments  \nTo segregate development and testing from production work, replicate your infrastructure in at least two environments. Access control for users might differ in each environment.  \nManage infrastructure and configurations-as-code  \nWhen you create and update infrastructure components in your work environments, use infrastructure as code to prevent inconsistencies between environments. Manage Machine Learning experiment job specifications as code. Managing specifications as code makes it possible to easily rerun and reuse a version of your experiment across environments.  \nTrack and manage Machine Learning experiments  \nTrack the performance KPIs and other artifacts of your Machine Learning experiments. When you keep a history of job performance, it allows for a quantitative analysis of experimentation success, and enables greater team collaboration and agility.  \nTest code, validate data integrity, model quality  \nTest your experimentation code base for the following items:\n- Correctness of data preparation functions\n- Correctness of feature extraction functions\n- Data integrity\n- Model performance  \nMachine Learning continuous integration and delivery  \nUse continuous integration to automate test execution in your team. To ensure that only a high-quality model might land in production, include the following processes:\n- Model training as part of continuous training pipeline.\n- A/B testing as part of your release.  \nMonitor services, models, and data  \nWhen you serve Machine Learning models in an operationalized environment, it's critical to monitor these services for their infrastructure uptime, compliance, and model quality. Set up monitoring for identifying data and model drift to understand whether retraining is required. Alternatively, set up triggers for automatic retraining.  \nMLOps at organizational scale: AI factories  \nA data science team might decide they can manage a handful of Machine Learning use cases internally. The adoption of Machine Learning DevOps (MLOps) helps set up project teams for better quality, reliability, and maintainability of solutions through the following items:  \nbalanced teams  \nsupported processes  \ntechnology automation  \nThis adoption allows the team to scale and focus on the development of new use cases.  \nAs the number of use cases grows in an organization, the management burden of supporting these use cases grows linearly, or even more. The challenge becomes how to use organizational scale to do the following:  \nAccelerate time-to-market  \nQuicken assessment of use case feasibility  \nEnable repeatability  \nDetermine how to best utilize available resources and skill sets across the full range of projects.  \nAn AI factory is the development of a repeatable business process and a collection of standardized artifacts that optimized the following aspects:  \nTeam set-up  \nRecommended practices  \nMLOps strategy  \nArchitectural patterns  \nReusable templates tailored to business requirements  \nThis process and the artifacts accelerate the development and deployment of a large set of Machine Learning use cases.  \nStandardize on repeatable architectural patterns  \nRepeatability is a key part of developing a factory process. Data science teams can do the following by developing a few repeatable architectural patterns that cover most of the Machine Learning use cases for their organization:  \nAccelerate project development  \nImprove consistency across projects  \nOnce these patterns are in place, most projects can use these patterns and reap the following benefits:  \nAccelerated design phase  \nAccelerated approvals from IT and security teams when they reuse tools across projects  \nAccelerated development due to reusable infrastructure as code templates and project templates (which are covered in more detail in the next section).  \nThe architectural patterns can include but aren't limited to the following topics:  \nPreferred services for each stage of the project  \nData connectivity and governance  \nA Machine Learning DevOps (MLOps) strategy tailored to the requirements of the industry, business, or data classification  \nExperiment management process Champion or Challenger models  \nFacilitate cross-team collaboration and sharing  \nShared code repositories and utilities can accelerate the development of Machine Learning solutions. These repositories can be developed in a modular way during project development so they are generic enough to be used by other projects. They can be made available in a central repository that all data science teams can access.  \nShare and reuse of intellectual property  \nAt the beginning of a project, the following intellectual property should be reviewed to maximize code reuse:  \nInternal code, such as packages and modules, which have been designed for reuse within the organization.  \nDatasets, which have been created in other Machine Learning projects or that are available in the Azure ecosystem.  \nExisting data science projects with similar architecture and business problems.  \nGitHub or open-source repos that can accelerate the project.  \nProject retrospectives should include an action item to review if elements of the project can be shared and generalized for broader reuse, so that the list of assets listed above organically grows with time.  \nTo help sharing and discovery, many companies have introduced shared repositories for the organization of code snippets and Machine Learning artifacts. The following artifacts in Azure Machine Learning can be defined-as-code that allows you to share efficiently across projects and workspaces:  \nDatasets  \nModels  \nEnvironments  \nPipelines  \nProject templates  \nMany companies have standardized on a project template to kick start a new project to do the following things:  \nAccelerate the migration of existing solutions  \nMaximize code reuse when starting a new project  \nCentral data management  \nThe process to get access to data for exploration or production usage can be time-consuming. Many companies centralize their data management to bring data producers and data consumers together and to help with easier data access for Machine Learning experimentation.  \nShared utilities  \nEnterprise-wide centralized dashboards can be implemented to consolidate the following logging and monitoring information:  \nError logging  \nService availability  \nService Telemetry  \nModel performance monitoring.  \nCreate a specialist Machine Learning engineering team  \nMany companies have implemented the role of the Machine Learning engineer. The Machine Learning engineer specializes in the following aspects:  \nThe creation and operation of robust Machine Learning pipelines  \nDrift monitoring and retraining workflows  \nMonitoring dashboards  \nThey drive the overall responsibility for industrializing the Machine Learning solution from development to production. They work closely with data engineering, architects, and security and operations to ensure all the necessary controls are in place.  \nWhile data science requires deep domain expertise, Machine Learning engineering as a discipline is more technical in focus. This difference makes the Machine Learning engineer more flexible to work across various projects and business departments. Large data science teams can benefit from a specialized Machine Learning engineering team. This specialized team can drive repeatability and reuse of automation workflows across various use cases and business departments.  \nEnablement and documentation  \nIt's important to provide clear guidance on the AI factory process to new and existing teams and users. This guidance will ensure consistency and reduce the amount of effort required of the Machine Learning engineering team. Consider designing content specifically for the various roles in your organization.  \nEveryone has a unique learning style, so a mixture of the following types of documents can help accelerate the adoption of the AI factory framework.  \nCentral hub with links to all artifacts.  \nTraining and enablement plan designed for each role  \nHigh-level summary presentation of the approach along with a companion video  \nDetailed documentation  \nHow-to videos  \nReadiness assessments  \nEthics  \nEthics play an instrumental role in the design of an AI solution. If ethical principles aren't implemented, trained models can exhibit the same bias present in the data they were trained on. This issue can result in the project being discontinued and more importantly, it can risk the organization's reputation.  \nTo ensure that the key ethical principles that the company stands for are implemented across projects, a list of these principles must be provided. Along with the principles, ways of validating them from a technical perspective during the testing phase must also be provided. Use the Machine Learning features in Azure Machine Learning to learn the following things:  \nWhat responsible Machine Learning is  \nWays you can put it into practice  \nRefer to the Responsible AI section.  \nFor more detailed information, see the Machine Learning DevOps guide  \nFor an intro to Azure's tools for Machine Learning and MLOps, check out the Azure ML Intro document.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\index.md"
    },
    {
        "chunkId": "chunk378_0",
        "chunkContent": "Before The Engagement  \nBefore starting a customer engagement, review MLOps Anti Patterns, and the Customer Readiness Guide for additional perspectives.  \nThese documents help further explain why MLOps exists, as well as steps and questions to follow with your customer.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\before-engagement-start\\index.md"
    },
    {
        "chunkId": "chunk379_0",
        "chunkContent": "ML Anti-Patterns: The Why of MLOps  \nBelow are some anti-patterns for a Machine Learning project that are worth understanding, and how the MLOps process and guidance in this Playbook can mitigate these.  \nNot implementing a Hypothesis driven development approach  \nOutcome: Assumptions regarding business outcomes and solution features are not tested and evidence based. The wrong features are built, the solution and its success are as a result poorly understood.  \nMitigation: Implement a Hypothesis driven development approach  \nNot tracking ML Experiments  \nOutcome: Past experiments cannot be repeated, learnt from or understood. Effort and development time needs to be duplicated.  \nMitigation: Refer to the Experiment tracking section  \nMachine Learning model development is not treated with the same rigour as the rest of the software development process  \nOutcome: The underlying code cannot be easily understood, scaled or automated within a pipeline. The solution is unstable and complex and expensive to maintain and change.  \nMitigation: Refer to the ML Pipelines section amongst others is this Playbook for guidance around ML model lifecycle management.  \nMachine Learning model development is not conducted in tandem with the software development team  \nMachine Learning projects are said to be 5% ML code and 95% \"glue code\", that is code that integrates the various components of model development into repeatable and automated pipelines.  \nOutcome: The various roles within the team do not understand how the different components are integrated, resulting in loss of development velocity due to prolonged troubleshooting to address issues and a slower release cycle.  \nMitigation: MLOps is a cross discipline and collaborative function that has the same requirements as DevOps. Refer to the Roles and Frameworks section for more info.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\before-engagement-start\\mlops-anti-patterns.md"
    },
    {
        "chunkId": "chunk380_0",
        "chunkContent": "Customer Readiness: MLOps  \nIs the customer organization ready for MLOps?  \nLike DevOps, MLOps incorporates aspects across people, process and technology.  \nRefer to the Machine Learning operations maturity model to help determine the customer's operational maturity level with regards to MLOps.  \nEnvisioning and Problem Formulation  \nBefore beginning a data science investigation, we need to define a problem statement which the data science team can explore; this problem statement can have a significant influence on whether the project is likely to be successful.  \nRefer to the Envisioning and Problem Formulation guidance from the ISE Code with engineering Playbook  \nThe Feasibility Study  \nThe main goal of Machine Learning (ML) feasibility studies is to assess whether it is feasible to solve the problem satisfactorily using ML and with the available data and is thus key to any MLOPs project.  \nRefer to the ML Feasibility Studies from the ISE Code with engineering Playbook  \nThe Architecture Design Sessions (ADS)  \nEarly in an engagement Microsoft works with customers to understand their unique goals and objectives and establish a definition of done. Microsoft dives deep into existing customer infrastructure and architecture to understand potential constraints. Additionally, we seek to understand and uncover specific non-functional requirements that influence the solution.  \nDuring this time the team uncovers many unknowns, leveraging all new-found information, in order to help generate an impactful design that meets customer goals. After ADS it can be helpful to conduct Engineering Feasibility Spikes to further de-risk technologies being considered for the engagement.  \nData Curation  \nIn addition to the readiness related to people, process and technology, ability to understand their own data and data curation process is another important part of customer readiness. Refer to the following section for more information about readiness on Data Curation",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\before-engagement-start\\mlops-customer-readiness.md"
    },
    {
        "chunkId": "chunk381_0",
        "chunkContent": "author: shanepeckham\ntitle: Azure ML vs Azure Databricks\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: When comparing Databricks and Azure ML, it's important to keep in mind that they serve different purposes. While Databricks is ideal for analyzing large datasets using Spark, Azure ML is better suited for developing and managing end-to-end machine learning workflows.\nrings:\n- public  \nAzure ML vs Azure Databricks  \nWhen comparing Databricks and Azure ML, it's important to keep in mind that they serve different purposes. While Databricks is ideal for analyzing large datasets using Spark, Azure ML is better suited for developing and managing end-to-end machine learning workflows.  \nThere are advantages and disadvantages to using Databricks for ML workloads. The following information does not enumerate a full list and is subject to being extended or modified in the future.  \nCommon features between Databricks and Azure ML  \nWe added this section to define/correct some perceptions regarding Azure ML vs Databricks.  \nMLflow is the primary logging library for both platforms  \nAzure ML CLI v2 and SDK v2 use MLflow as a primary logging instrument. This makes it easy to reuse existing code from Databricks.  \nTabular big data  \nAzure ML CLI and SDK v2 have native integration with Spark that allows the execution of workloads on big data using Spark code.  \nBoth platforms have a UI for experimentation  \nWhen using Databricks, data scientists can continue to use interactive clusters and notebooks in the Databricks UI. In Azure ML, there are compute instances and the Azure ML Pipeline Designer. It's worth noting that all the features mentioned work well from initial experimentation only. Code should be migrated to build an enterprise ready end-to-end flow.  \nBoth platforms support autoscaling  \nAzure ML provides built-in auto-scaling options for most of the compute options. Databricks clusters spin up and scale for processing massive amounts of data when needed and spin down when not in use. Also, Databricks has (Databricks) Pools that use a managed cache of virtual machines to quickly scale clusters up and down.  \nBoth platforms have model registry  \nAzure ML provides a central model registry for the entire organization with full lineage for models. This lineage spans from data and Python dependencies, to the training run, all the way to deployments. Databricks has MLflow model registry for consistent, secure model deployment and management. Models are made available in a consistent, open format for deployment from the MLflow Model Registry to Kubernetes services, cloud or OSS inference services, or edge computing solutions.  \nBoth platforms have automated machine learning enabled  \nAzure ML Provides more sophisticated ML model creation compared to Databricks. With Databricks, NLP, CV models are not supported currently. Also, direct deployment of models to AKS or containers is not available at the moment. More details here  \nAdvantages of Azure ML over Databricks  \nHere are details about Azure ML advantages vs Databricks.  \nComplex MLOps processes  \nMLOps is simpler with Azure ML. In Databricks we need to implement MLflow projects and deal with wheel files to start an experiment. The process is distinctly different from the usual experience in the Databricks UI on an Interactive cluster. It means that existing knowledge might not be enough to implement an enterprise training flow.  \nInability to utilize several clusters for complex pipelines  \nAn MLflow Project allows the definition of several steps in the flow, but does not easily support using different clusters on a per step basis. As a workaround, several MLflow projects may be executed in series, but makes the MLOps pipeline much more complex.  \nAzure ML has a better way to pre-process unstructured data  \nThanks to the parallel job you can control how to split files into batches and configure parallelization using just a few lines of code. This capability is powerful to use for unstructured data.  \nAzure ML can deploy models  \nSee Endpoints for inference in production to learn how you can utilize compute clusters to deploy models into production rather than just train them.  \nProductivity for all skill levels  \nAzure ML provides a better user interface to make it easier for a novice user to get started. It has productivity for all skill levels including code-first (notebooks/IDEs), low-code (AutoML), and no-code (Designer), whereas Databricks caters mostly to professionals.  \nAzure ML has plenty of out-of-the-box features  \nAzure ML provides full lifecycle tools, including Data Labeling, Data Drift detection, production monitoring (via Azure Monitor and Application Insights), managed endpoints, and more (for example, Integrated Responsible ML), as well as Hybrid ML capabilities (for compute utilization) with Azure Arc integration.  \nAdvantages of Databricks over Azure ML  \nReasons to use Databricks rather than Azure ML.  \nVarious data sources are supported  \nDatabricks allows you to get data from many different data sources. Databricks is deeply integrated with Delta Lake including built-in data versioning, time travel, and the option to use the Databricks Feature Store built on top of Delta Lake. Azure ML uses Azure Blob as the main data source and another service is often required to implement a data ingestion pipeline prior to starting training. Refer to this table to understand the data sources directly supported by AzureML and Databricks.  \nData Sources AzureML Databricks Azure Data Lake Storage Gen1 Yes Yes Azure Data Lake Storage Gen2 Yes Yes Azure Blob Storage Yes Yes Azure Cosmos DB Yes Yes Amazon S3 Yes Yes MongoDB No Yes Tableau No Yes Power BI No Yes  \nIntegration with Apache Spark  \nDatabricks is built on top of Apache Spark, a powerful distributed computing framework enabling fast, efficient processing of large datasets. Apache Spark provides a unified analytics engine for processing large-scale data processing, including machine learning algorithms. Databricks allows users to build and train machine learning models at scale using Spark, which is well suited for structured data.  \nDecision factors  \nAzure ML is cost-effective (lower TCO) with many out-of-the box ML features. It has more choice of deployment options in the Azure ecosystem and provides productivity for all skill levels.  \nAzure Databricks is chosen because of its data preparation capabilities and its batch stream processing. There is also a feature store provided. Databricks has ML as one component in a bigger data lake suite that includes streaming, data warehousing, and ETL.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\mlops-in-databricks\\databricks-vs-azureml.md"
    },
    {
        "chunkId": "chunk382_0",
        "chunkContent": "Experimentation in Databricks  \nTo build an end to end ML flow in Databricks we recommend to use MLFlow projects, but this approach is very complicated to use in the experimentation phase where data scientists need to have access to a good tool for testing various ideas and running many experiments. In the case of Databricks, data scientists tend to use Databricks UI, notebooks and interactive clusters for their experiments. Let's discuss some best practices in MLOps for Databricks that you can apply in the experimentation phase.  \nSync Notebooks on Every Commit into the Development Branch  \nIt's beneficial to separate experimentation notebooks and \"production ready\" code into different folders like notebooks and projects:  \nnotebooks folder: to host all notebooks with experiments that are not going to be pushed to production \"as is\", but code from the notebooks might be used in MLFlow projects.  \nprojects folder: contains all MLFlow projects (i.e. training and scoring) that we are planning to operationalize and deploy to production as completed artifacts.  \nlibs folder: source code that can be shared between notebooks and MLFlow projects in a form of wheel files.  \nObviously, all the latest notebooks should be available to data scientists who are working in the UI. Additionally, all wheel files from the libs folder should be deployed to available interactive cluster environments. To achieve this we would recommend one of two options.  \nOption 1 (old): Sync Notebooks only using DevOps Flow  \nThe first option is to build a special DevOps flow that is synchronizing all the notebooks and wheel files right after any merge into the development branch. Right after the sync, data scientists have access to the latest version of the notebooks and they can clone them and start working as usual. The process is described by the following diagram:  \nTherefore, implementing the flow from the diagram above, we are making sure that we always have the latest notebooks as well as wheels from the development branch in Databricks (UI and interactive clusters).  \nAn example of the sync process for Azure DevOps you can find here. This Build uses the Databricks CLI, configures the connection to Databricks, and imports the notebooks folder from the repository to /Shared/notebooks into Databricks. One more Azure DevOps Build shows how to create and deploy wheel files.  \nWhen all notebooks in the development branch are in sync, data scientist can clone any of the notebooks into a personal folder:  \nOnce the notebooks are in the personal folder, it's possible to link them to a branch. To do this, you can use the Revision History tab and click on Git: Not linked link:  \nThe window above allows you to create a new branch.  \nOnce you finish editing, you can sync the notebook with your branch (commit changes) and create a pull request:  \nOption 2 (recommended): Sync Notebooks using Databricks Repos  \nLatest version of the Azure Databricks interface has a new functionality called Databricks Repos that enables Git integration with Databricks. Thanks to Databricks Repos any data scientist can clone the whole repository into their own sandbox and have access to all the standard git features like create a branch, send a PR, pull and push repositories.  \nTherefore, using Databricks Repos you don't need to setup any DevOps flow. Moreover, data scientists are able to use code from the libs folder directly, and you don't need to care about wheel files at this point.  \nUse Syncing Capabilities for VS Code and Databricks  \nVS Code provides an option to install extensions. One of the extensions is the VS Code Extension for Databricks that allows us to work with notebooks and run them on a remote Databricks cluster from VS Code directly.  \nThanks to Databricks Repos it's possible to combine VS Code and Databricks UI working at the same branch. We can clone a repository and create and publish a branch in VS Code as usual. Once we need to work at the branch in the Databricks UI, we can clone the repo into Databricks and pick our own branch to start modifying notebooks in the Databricks UI:  \nWe just need to remember to push all the changes if we want to switch back and forth from Databricks to VS Code.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\mlops-in-databricks\\experimentation-in-databricks.md"
    },
    {
        "chunkId": "chunk383_0",
        "chunkContent": "author: shanepeckham\ntitle: MLOps Implementation in Databricks using MLflow Projects\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: MLOps Implementation in Databricks using MLflow Projects\nrings:\n- public  \nUsing MLflow Projects for MLOps in Databricks  \nIn the Model Release document, we showed an example of the MLOps process. The process includes all stages from creating a feature branch to releasing into the production environment:  \nThis process looks the same in Databricks as in Azure ML, Kubeflow or any other ML framework/compute. The only difference for Databricks is the requirement to deploy wheel files alongside the execution of the MLflow projects. MLflow tools allow the execution of projects from the command-line interface (CLI). It does this by uploading all the required code to a DBFS automatically, but all related python wheels must be uploaded to the DBFS in advance. When implementing MLOps for Databricks it's important to build a set of scripts that will wrap around MLflow tools, adding wheel deployment capabilities. The script(s) should implement the following set of actions in sequence:  \nPick a unique experiment name to execute and a folder name in the DBFS to deploy wheels.  \nCreate the folder in the DBFS and upload wheels.  \nModify the MLflow project configuration file using new references to the wheel files in the DBFS.  \nUse MLflow tools to start the experiment with a generated experiment name.  \nNext, let's discuss these actions in detail.  \nPicking Unique Experiment and Folder Names  \nWhen running experiments from different branches we need to make sure that each branch has its own unique experiment name. That way, we can create the experiment name dynamically using the branch name as a part of it. It makes it possible to have several runs on the same branch under the same experiment name. Then we are able to compare results between them.  \nIf we execute an MLflow project from a DevOps system, there must be a variable that allows us access to the branch name. For example, in Azure Pipelines $(Build.SourceBranchName) can be utilized.  \nWhen executing an MLflow project from a local computer, it's possible to use a command like this:  \nbash\ngit_branch=$(git rev-parse --abbrev-ref HEAD)  \nDeploying Wheel Files into a DBFS  \nThe Databricks command-line interface (CLI) provides an easy-to-use interface with the Azure Databricks platform and it can be used to deploy the wheel files into desired locations in DBFS. Here is an example of a command used from Azure Pipelines:  \nbash\ndbfs cp -r --overwrite $(Build.ArtifactStagingDirectory)/wheel dbfs:/wheels/$(Build.SourceBranchName)/$(wheel_folder_name)/  \nConfiguration and Execution for MLflow Projects  \nDatabricks has been integrated with MLflow and allows us to implement all MLOps related tasks with less effort. MLflow refers to \"projects\" rather than \"pipelines\", where a \"project\" is a folder that contains all pipeline related attributes:  \nMLProject file: defines all entry points to execute.  \n{your_name}.yaml file: defines all dependencies that should be installed on the compute cluster.  \n{your_name}.json file: a cluster definition.  \nscripts (Python/Bash): implementation of the steps.  \nConfiguring MLflow projects to use generated and uploaded wheel files requires two files: the dependencies file and the cluster definition file.  \nThe dependencies file contains all the libraries to install and references to all the custom wheel files that should be installed on the main node of a cluster. For example, the file can look like this:  \nyaml\nname: training\nchannels:\n- defaults\n- anaconda\n- conda-forge\ndependencies:\n- python=3.8\n- scikit-learn\n- pip\n- gcc\n- prophet\n- pmdarima\n- pip:\n- mlflow\n- /dbfs/wheels/builds/WHEELPATH\n- pyarrow  \nThe /dbfs/wheels/builds/WHEELPATH is just a placeholder for the wheel that we should copy into the DBFS in advance. We can modify this placeholder on the fly.  \nThe wheel file from the YAML file will be installed to the main node only. In some cases, it's not enough and you need to install the wheel to the workers. It's possible to do this using the cluster definition file. Here is an example for your reference:  \njson\n{\n\"new_cluster\": {\n\"spark_version\": \"10.3.x-scala2.12\",\n\"num_workers\": 4,\n\"node_type_id\": \"Standard_DS3_v2\"\n},\n\"libraries\": [\n{\n\"pypi\":{\n\"package\": \"convertdate\"\n}\n},\n{\n\"pypi\": {\n\"package\": \"/dbfs/wheels/builds/WHEELPATH\"\n}\n}\n]\n}  \nAn MLflow SDK allows us to execute an MLflow project on a local computer or in a git repository using just a couple lines of code:  \n```py\n\noptional if experiment_id is provided\n\nmlflow.set_experiment(experiment_folder)\nmlflow.run(\n\"{project_folder}\",\nbackend=\"databricks\",\nbackend_config=\"cluster.json\",\n# optional if set_experiment() is called\nexperiment_id =\"{experiment_id}\",\nparameters=...)\n```  \nIn the first line, we are setting an experiment name that should include a real path in Databricks (like /User/me/my_exp_1). In the second line, put all project things together and execute the project on a new automated cluster. Alternatively, the experiment_id can be passed into mlflow.run() instead of setting the experiment by name. There are several benefits that come from the code above:  \nExperiments can be generated dynamically based on your branch name as we discussed above.  \nA new automated cluster on each run allows us to make sure that we always have up-to-date dependencies.  \nBecause the code above can be executed using a local project folder, it's easy to start the project from a local computer or from DevOps host.  \nNext step  \nTry the Databricks MLOps Template: An MLOps example based on Azure Databricks.  \n[!div class=\"nextstepaction\"]\nView GitHub repo",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\mlops-in-databricks\\index.md"
    },
    {
        "chunkId": "chunk384_0",
        "chunkContent": "author: shanepeckham\ntitle: MLOps implementation in Databricks using workflows and repos\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: This diagram demonstrates an end-to-end MLOps process that is common in enterprise. This process supports three different environments that can be implemented using a single Databricks workspace or separated across three different workspaces. Common tasks include moving code from one branch into another (PR, merging), invoking the model approval process in the production environment, and running integration and unit tests. Most of the tasks can be done thanks to Azure DevOps Builds or GitHub Workflows. Models can be stored and tracked in MLflow model repository that has been integrated with Databricks.\nrings:\n- public  \nUsing workflows and repos for MLOps in Databricks  \nThe following diagram demonstrates an end-to-end MLOps process that is common in enterprise:  \nMLOps Architecture  \nThis process supports three different environments that can be implemented using a single Databricks workspace or separated across three different workspaces.  \nCommon tasks include moving code from one branch into another (PR, merging), invoking the model approval process in the production environment, and running integration and unit tests. Most of the tasks can be done thanks to Azure DevOps Builds or GitHub Workflows. Models can be stored and tracked in a MLflow model repository that has been integrated with Databricks.  \nHowever, the process requires some additional features to be feasible:  \nThe ability to run code that might be separated in several files and custom libraries.  \nJob clusters support.  \nAn option to implement ML flow as a set of independent steps.  \nIn the MLOps Template based on MLflow Projects, we have all the features from MLflow framework and some custom rules and naming conventions.  \nWorkflows  \nThanks to the Workflow management in Databricks it's possible to create a job that contains one or more tasks. The job can be invoked by using API, by schedule or through UI. Moreover, each task in the job can have its own job cluster configured. It's possible to use API to create jobs and use a custom naming convention to give the choice to differentiate jobs in a multi-user environment. Try a basic workflow using this tutorial.  \nDatabricks repos  \nDatabricks Repos allows us to have a copy of a repository in Databricks, and run workflows against it. Databricks supports branches and a rich API to interact with Databricks Repos. Therefore, it's possible to implement your own branching strategy in Databricks. You can also implement automation for integration testing or model training on toy or full datasets. This document contains more details about how Databricks Repos can be used in the development process.  \nNext steps  \nThe MLOps for Databricks Solution Accelerator implements this architecture using the latest Databricks features discussed in this content.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\mlops-in-databricks\\mlops-in-databricks-dbx.md"
    },
    {
        "chunkId": "chunk385_0",
        "chunkContent": "author: shanepeckham\ntitle: MLOps Implementation in Databricks using MLflow Projects\nms.author: shanepec\nms.service: ai playbook\ndescription: MLOps Implementation in Databricks using MLflow Projects\nrings:\n- public  \nMLOps Implementation in Databricks using MLflow Projects  \nIntroduction  \nIn the Model Release document, we showed an example of the MLOps process. The process included stages from creating a feature branch to a release into the production environment:  \nThis process looks the same in Databricks as in Azure ML, Kubeflow or any other ML framework/compute. The only difference for Databricks is the requirement to deploy wheel files alongside with execution of the MLflow projects. MLflow tools allow the execution of projects from the command-line interface (CLI). It does this by uploading all the required code to DBFS automatically, but all related python wheels should be uploaded to DBFS in advance. When implementing MLOps for Databricks it's important to build a set of scripts that will wrap around MLflow tools, adding wheel deployment capabilities. The script(s) should implement the following set of actions in sequence:  \nPick a unique experiment name to execute and a folder name in DBFS to deploy wheels.  \nCreate the folder in DBFS and upload wheels.  \nModify the MLflow project configuration file using new references to the wheel files in DBFS.  \nUse MLflow tools to start the experiment with generated experiment name.  \nLet's discuss these actions in details.  \nPicking a Unique Experiment and Folder Names  \nRunning experiments from different branches we need to make sure that each branch has its own unique experiment name. That way, we can create the experiment name dynamically using the branch name as a part of it. It will help us to have several runs on the same branch under the same experiment name. So, we will be able to compare results between them.  \nIf we execute an MLflow project from a DevOps system, there should be a variable that allows us to get access to the branch name. For example, in Azure Pipelines $(Build.SourceBranchName) can be utilized.  \nWhen executing an MLflow project from a local computer, it's possible to use a command like this:  \nbash\ngit_branch=$(git rev-parse --abbrev-ref HEAD)  \nDeploying Wheel Files into DBFS  \nThe Databricks command-line interface (CLI) provides an easy-to-use interface to the Azure Databricks platform and it can be used to deploy the wheel files into desired location in DBFS. Here is an example of the command from Azure Pipelines:  \nbash\ndbfs cp -r --overwrite $(Build.ArtifactStagingDirectory)/wheel dbfs:/wheels/$(Build.SourceBranchName)/$(wheel_folder_name)/  \nMLflow Projects Configuration and Execution  \nDatabricks has been integrated with MLflow that allows us to implement all MLOps related tasks with less effort. MLflow refers to \"projects\" rather than \"pipelines\", where a \"project\" is a folder that contains all pipeline related attributes:  \nMLProject file: defines all entry points to execute  \n{your_name}.yaml file: defines all dependencies that should be installed on the compute cluster  \n{your_name}.json file: a cluster definition  \nscripts (Python/Bash): implementation of the steps  \nConfiguring MLflow projects to use generated and uploaded wheel files requires two files: the dependencies file and the cluster definition file.  \nThe dependencies file contains all the libraries to install and references to all the custom wheel files that should be installed on the main node of a cluster. For example, the file can look like this:  \nyaml\nname: training\nchannels:\n- defaults\n- anaconda\n- conda-forge\ndependencies:\n- python=3.8\n- scikit-learn\n- pip\n- gcc\n- prophet\n- pmdarima\n- pip:\n- mlflow\n- /dbfs/wheels/builds/WHEELPATH\n- pyarrow  \nThe /dbfs/wheels/builds/WHEELPATH is just a placeholder for the wheel that we should copy into DBFS in advance. We can modify this placeholder on fly.  \nThe wheel file from the YAML file will be installed to the main node only. In some cases, it's not enough and you need to install the wheel to the workers. It's possible to do using the cluster definition file. Here is an example for your reference:  \njson\n{\n\"new_cluster\": {\n\"spark_version\": \"10.3.x-scala2.12\",\n\"num_workers\": 4,\n\"node_type_id\": \"Standard_DS3_v2\"\n},\n\"libraries\": [\n{\n\"pypi\":{\n\"package\": \"convertdate\"\n}\n},\n{\n\"pypi\": {\n\"package\": \"/dbfs/wheels/builds/WHEELPATH\"\n}\n}\n]\n}  \nMLflow SDK allows us to execute an MLflow project on a local computer or in a git repository using just a couple lines of code:  \n```py\n\noptional if experiment_id is provided\n\nmlflow.set_experiment(experiment_folder)\nmlflow.run(\n\"{project_folder}\",\nbackend=\"databricks\",\nbackend_config=\"cluster.json\",\n# optional if set_experiment() is called\nexperiment_id =\"{experiment_id}\",\nparameters=...)\n```  \nIn the first line, we are setting an experiment name that should include a real path in Databricks (like /User/me/my_exp_1). In the second line, put all project things altogether and execute the project on a new automated cluster. Alternatively, the experiment_id can be passed into mlflow.run() instead of setting the experiment by name. There are several benefits that we can get from the code above:  \nExperiments can be generated dynamically based on your branch name as we discussed above.  \nNew automated cluster on each run allows us to make sure that we always have up-to-date dependencies.  \nBecause the code above can be executed using a local project folder, it's easy to start the project from a local computer or from DevOps host.  \nNext Step  \nTry the Databricks MLOps Template: An MLOps example based on Azure Databricks.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\mlops-in-databricks\\mlops-in-databricks.md"
    },
    {
        "chunkId": "chunk386_0",
        "chunkContent": "CPU, Memory or Disk bounding issues in Azure ML  \nWhile running training workflows in Azure ML it's important to understand possible resource bounding issues. In general, a training pipeline can be CPU, memory, or I/O bound that can lead to the pipeline failure with non-clear exception message about the problem. The goal of this document is to explain ways how to recognize bounding issues as well as provide some recommendations about how to fix them.  \nWays to monitor resources  \nIt's important to know how to monitor compute resources during the training process prior to digging into possible issues with resources. Azure ML supports several ways to monitor resources, but the simplest one is the Monitoring tab. The tab is available in Azure ML Studio, and it includes various utilization graphs including memory and CPU.  \nOne more useful way to monitor resources is looking at log files. Azure ML tracks all resource performance related metrics in Logs/perf folder in CSV format.  \nWe are using the Monitoring tab in this document to illustrate examples.  \nMonitoring memory usage  \nMemory bound training pipelines tend to fail unexpectedly. In fact, it's the most common resource issue and one of few that leads to failed execution. Moreover, your pipeline may work for hours and days prior to unexpectedly fail. The following image demonstrates such a pipeline:  \nThe image above shows a pipeline that reached maximum memory utilization in the beginning, but failed in only two hours. That's why it's very important to start monitoring resource usage in the beginning. The following image shows an error message that is affiliated with a memory related fail:  \nIn general, to solve a memory issue you need to use another compute instance with more memory, but it's not always helpful. Let's look at a use case for parallel job workload. Using all the cores to run a parallel job is a common approach, but it's often leads to memory issues. Picking a better node may not help in this case, and a general recommendation is to reduce number of threads per compute node to see memory utilization not more than 90-95%. Obviously, this approach will decrease CPU utilization. The image below demonstrates a memory usage graph from a real workload, but CPU usage for this workload was is about 50% only.  \nManaging Disk Space on a Compute Node  \nDevelopers tend to ignore disk space monitoring for compute nodes, but it's one more problem that can lead to a training pipeline execution fail. It's happening, if the pipeline is working with big data and each particular node has to pre-process TBs of data.  \nWhen you mount a blob storage to a compute node and start reading some files on the mounting disk, Azure ML caches the files locally to improve performance. In general, you should not care about the cache, because Azure ML automatically prunes it once it reaches the maximum limit. The following image demonstrates cache pruning based on the disk usage diagram:  \nThe problem is that the default pruning limit is very high, and if your workload uses disk space to store some intermediate data itself, it's easy to get out of disk space exception. The simplest way to avoid the exception is to reduce Azure ML cache and tune pruning parameters. Azure ML team has published an example with a list of environment variables to tune for pruning management. For example, DATASET_MOUNT_FILE_CACHE_PRUNE_THRESHOLD and DATASET_MOUNT_FILE_CACHE_PRUNE_TARGET are very useful to initiate pruning.  \nCPU bound workloads  \nIt's a common situation when training workloads use to GPU rather than CPU for complex computational tasks, but in some cases it's not possible due to lack of GPU support by external libraries. So, we are still facing some training workloads that uses CPU extensively for computational tasks. If you reach 100% CPU utilization, we would recommend to look at Fsv2-series that has a CPU with Intel AVX-512 technology. We had a chance to see triple performance increase for some workloads like OCR compare to any Dv or Ev node.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\working-with-azure-ml\\azure-ml-bounding-issues.md"
    },
    {
        "chunkId": "chunk387_0",
        "chunkContent": "author: shanepeckham\ntitle: Model Deployment into Production\u202fStrategies\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Our primary deployment artifact in this study is a model (a binary file or a set of files in general) and related inference (scoring) code.\nrings:\n- public  \nModel deployment into Production\u202fStrategies  \nOur primary deployment artifact in this study is a model (a binary file or a set of files in general) and related inference (scoring) code. We make the following assumptions:  \nA model has been trained in Azure Machine Learning service and is available in the Model Repository in the development environment.  \nSeveral models may need to be used in the inference pipeline. Therefore, no limits are placed on execution time for the inference workload.  \nThe Production environment will likely be different from the development environment. Also, some services may not be available in the development environment.  \nTechnologies-related matrix  \nAll our inferencing workloads can be divided into three separate groups:  \nReal-time: Scoring implementation is on a per\u2013record basis, and response time is not more than 60 seconds.  \nNear real-time scoring: Scoring implementation is on a per-record basis, but there is no way to satisfy the request fast enough due to the complexity and/or size of the model. Even the complexity of the workflow itself may be higher (complex data preprocessing algorithms, several models in the flow, etc.).  \nBatch scoring: Scoring is done for a set of records simultaneously and happens on a schedule. Response time varies from minutes to hours or even days.  \nDifferent solutions are often used for training and hosting AI models. For example, models trained on the Azure ML may not be run on Azure ML for inferencing.  \nThe following table summarizes options that can work for an Azure ML-managed deployment approach and for a custom deployment approach. The table below summarizes this comparison:  \nScoring type Managed by Azure ML Custom deployment Real-time scoring Azure Kubernetes Service (AKS)/Arc Kubernetes, Azure ML Online Endpoints Azure Functions, Azure Apps, Azure Container Instances (ACI), Unmanaged Kubernetes/Azure Container Apps/IoT Edge Near real-time scoring N/A Azure Durable Functions, AKS with Keda/a Queue service/Triton/Azure Function runtime Batch scoring Azure ML Pipelines, Batch Scoring Endpoint Batch Scoring using Databricks More options like KubeFlow or even Durable Azure Functions are possible, but they are not common for batch scoring with more limitations and much complexity  \nDeployment options  \nDifferent options are available that have their own advantages.  \nReal-time scoring  \nBelow are brief discussions of some different deployment options.  \nManaged Azure Kubernetes Service (AKS)/Arc Kubernetes  \nIn the case when a customer prefers to manage infrastructure themselves and connect components from different subscriptions, AKS is the best option. Azure ML Workspace can be linked to AKS and deployment can happen automatically using standard or custom images. This choice provides the highest flexibility in managing a cluster, which enables many enterprise grade cluster services to be utilized.  \nIf the Kubernetes is outside the Azure cloud, it can still be connected to Azure using Azure Arc (let's use the Arc Kubernetes name for such a compute target). With an Arc Kubernetes cluster, it's possible to build, train, and deploy models into any infrastructure on-premises and across multi-cloud using Kubernetes.  \nAdvantages:  \nSelf-managed infrastructure (advantage and disadvantage).  \nProven technology that supports auto-scaling, monitoring, VNET, and GPU nodes.  \nAKS and Azure ML can be in different subscriptions.  \nExternal Kubernetes cluster can be used thanks to Azure Arc.  \nDisadvantages:  \nUser responsibility for the infrastructure.  \nComplexity.  \nFor more information:  \nEndpoints for inference in production  \nDeploy a model to an Azure Kubernetes Service cluster  \nManaged Online Endpoints  \nThe recommended approach by the Azure ML team.  \nAdvantages:  \nAzure ML managed infrastructure based on Azure ML Compute.  \nLogging and monitoring are supported (out-of-the-box).  \nNative blue/green deployment.  \nDisadvantages:  \nNo ability to publish several endpoints to the same compute cluster.  \nVNET support still is in Public Preview.  \nFor more information:  \nEndpoints for inference in production  \nDeploy and score a machine learning model by using an online endpoint  \nAzure Functions  \nAzure Functions is a serverless compute service in Azure. The Azure Functions service is effective if it is possible to implement an inferencing script as a single thread process, particularly where a GPU is not required. In most cases, the model is likely to be small and a data preprocessing flow is not needed or doesn\u2019t require many resources. Typically event-driven, short-lived and low compute workloads are recommended. However, Azure Functions may not be the optimal choice for high compute workloads.  \nAdvantages:  \nSupports a staging deployment Platform as a Service approach if deployed to Azure.  \nCan be built in many different programming languages like Python and C#.  \nSupports deployment in containers, if needed, and can be deployed locally.  \nDisadvantages:  \nDoesn\u2019t support GPU.  \nLatency in ML solutions can be a problem.  \nAzure ML doesn\u2019t support this way to deploy.  \nFor more information:  \nQuickstart: Create a function in Azure with Python using Visual Studio Code  \nAzure Container Instances (ACI)  \nAzure Container Instance (ACI) is an effective way to deploy images and containers in an environment that is less complex than a full orchestration cluster/system such as Azure Kubernetes Service (AKS). Containers can be deployed via images stored in a public or private registry. Runtimes and environments are customizable per image and both Windows and Linux are supported. Deployments of approved images increase version control, testing, and consistency across containers.  \nNOTE: Using ACI in production isn't recommended since there are limitations to ACI availability and scaling. Refer to the AKS solutions for a containerized deployment approach.  \nAdvantages:  \nSupports deployment in containers and can be deployed locally.  \nImages can be stored in an enterprise Azure Container Registry (ACR) with versioning.  \nACI, ACR and Azure ML can be in different subscriptions.  \nDisadvantages:  \nCannot control orchestration of a cluster.  \nSince ACI is a manged service, certain network restrictions apply, including dynamic IPs.  \nLimitations for scalability and high availability.  \nFor more information:  \nWhat is Azure Container Instances?  \nFrequently asked questions about Azure Container Instances  \nUnmanaged Kubernetes/Azure container Apps/IoT Edge  \nAll the compute targets in this option have one common property: they are not supported by Azure ML. Therefore, if there is a requirement to support one of these targets, it's your responsibility to build a custom image and deploy it to the compute. This option makes sense if all other options are not available.  \nAdvantages:  \nSupports non-standard scenarios.  \nDisadvantages:  \nUser responsibility for the infrastructure.  \nUser responsibility for building the right image and deployment.  \nComplexity  \nFor more information:  \nAzure Container Apps documentation  \nNear real-time scoring  \nThis discusses the benefits for monitoring model performance and operation in near realtime.  \nAzure Durable Functions  \nAzure Durable Functions are effective if it is possible to implement an inferencing script as a single thread process, particularly when a GPU is not required. In most cases the model is likely to be small, a data preprocessing flow is not needed and doesn\u2019t require many resources. It is worth noting that Azure Function framework is open source. So, it can be deployed anywhere.  \nAdvantages:  \nSupports staging deployment Platform as a Service approach if deployed to Azure.  \nCan be built in many different programming languages like Python and C#.  \nSupports deployment in containers if needed and can be deployed locally.  \nDisadvantages:  \nDoesn\u2019t support GPU.  \nFor more information:  \nWhat are Durable Functions?  \nAzure Functions with KEDA  \nThis is an extension of the previous option. Here we can combine AKS, Azure Functions, Service Bus Queue and Triton Server (optional) to deploy a scalable, near-real-time service to the cloud. We can use Azure Functions (even not durable) on AKS to do inferencing and scale scoring nodes based on the number of incoming requests thanks to KEDA. Triton (https://developer.nvidia.com/nvidia-triton-inference-server) or DeepSpeed (https://www.deepspeed.ai/) are used to accelerate computations on GPUs. Service Bus Queue is utilized to get requests and provide responses.  \nAdvantages:  \nCan be used as a custom reference architecture for most complex scenarios.  \nLarge models are supported.  \nDisadvantages:  \nComplex infrastructure that requires manual deployment and management.  \nDeep Knowledge in some technologies like Kubernetes is required.  \nBatch scoring  \nUsed for periodic scoring.  \nAzure ML Pipelines  \nAzure ML supports the ParallelRunStep class that is useful for parallel data preprocessing and for batch inferencing. In general, a batch inferencing pipeline is no different from a training pipeline, but it uses existing model(s) rather than training a new model. Let\u2019s look at some advantages/disadvantages of batch inferencing:  \nAdvantages:  \nAzure ML Compute(s) can be utilized with CPU or GPU nodes when needed.  \nAbility to use different node types for different steps.  \nTraining code for data preprocessing can be reused.  \nMLOps (Machine Learning development and Operation process) is the same as for training pipeline (except moving code into production).  \nPotentially, it can be executed on-premises (via Azure Arc).  \nDisadvantages:  \nAzure ML is required to be in the Production environment (including related services).  \nData should be located in a supported data storage that might require more services like Azure Data Factory to orchestrate the pipeline\\n-  \nFor more information:  \nTutorial: Create production machine learning pipelines  \nBatch scoring endpoints  \nBatch scoring endpoints are based on the ParallelRunStep and Azure ML Compute, and they bring some simplicity to the Azure ML Pipeline approach. The main idea is to allow for the ability to deploy a batch scoring pipeline using just one line of code (CLI), model, and scoring script. Thus, all the code that creates a pipeline is not needed. At the same time, it assumes that inferencing pipeline is a single-step flow. So, it\u2019s unusable for complex pipelines.  \nAdvantages:  \nAllows developers to focus their efforts on scoring script only.  \nAzure ML Compute(s) can be utilized with CPU/GPU nodes when needed.  \nMLOps can be implemented as a series of bash scripts thanks to Azure ML CLI 2.0.  \nDisadvantages:  \nSingle step pipelines only (a single step based on ParallelRunStep).  \nAzure Blob should be used as for now.  \nAzure Machine Learning Workspace is required in the Production environment (including related services).  \nFor more information:  \nDeploy models for scoring in batch endpoints  \nBatch Scoring using Databricks  \nDatabricks can be used for Batch inferencing if Azure Machine Learning is not allowed in the Production environment. This decision must be made carefully because it introduces complexity to the scoring pipeline and to the MLOps process. The typical MLOps pattern in this scenario uses MLFlow projects, but access to the code repository is required to execute the scoring flow.  \nAdvantages:  \nAbility to use CPU or GPU automated clusters by request.  \nIntegration with MLFlow and some integration with Azure.  \nDisadvantages:  \nDatabricks MLOps is different and it adds further complexity to the project.  \nDeep Spark knowledge is required to implement batch scoring on a cluster rather than on a single node.  \nNo clear/simple ability to use different clusters (node types) for different steps.  \nHard to reuse existing code from Azure ML to implement pipelines.  \nFor more information:  \nBatch scoring of Spark models on Azure Databricks  \nProduction vs. development subscriptions  \nIt is a common situation that development and training take place in one subscription, but deployment of the inferencing service into production is taking place in another subscription. NOTE: Batch Online Endpoint and Online Endpoints cannot be deployed outside the subscription where Azure ML is located. Thus, if you want to use Online Endpoints, you need to deploy a separate instance of Azure ML Workspace, copy your model into this workspace during the deployment and execute the deployment from there. A separate Azure ML Workspace is required for Azure ML Pipelines as well.  \nDeployment summary  \nBatch Inferencing Near Real-time Real-time Azure ML Pipelines Recommended approach for any workload NA NA Batch online endpoints Single step inferencing workloads NA NA Databricks Custom Workloads integrated with MLFlow and other Spark features NA NA Managed AKS NA NA Effective way when customer wants to manage and control infrastructure. AKS can be in a different subscription Azure Container Instances NA NA Efficient way to deploy approved, tested, and versioned images/environments regardless of trained and deployed subscriptions Online endpoints NA NA Recommended way for real-time scoring. Infrastructure managed by Azure ML. Simple deployment process with scaling abilities based on Azure ML Compute Azure Functions NA NA Simple workloads and small models, but Azure ML is not required Azure Durable Functions NA CPU-based workloads, small models NA Unmanaged AKS with KEDA NA Custom workloads with any complexity, but deep tech knowledge is required Can be used for custom images when Azure ML is not available  \nFor more information  \nA more generalized document outlining different Azure compute options available can be found here:  \nAzure compute decision tree doc.  \nHow to deploy managed online endpoints with Azure CLI  \nHow to deploy with Triton using Azure CLI",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\working-with-azure-ml\\azure-ml-deployment.md"
    },
    {
        "chunkId": "chunk388_0",
        "chunkContent": "Invoking Azure ML Pipeline From Azure DevOps (Private Preview)  \nNote:\nThis feature is in private preview. To start using it, please, visit this link for details.  \nOne of the challenges integrating Azure ML pipelines into AzDo CI/CD pipelines is that the Azure ML pipeline can take long time to execute and the Azure DevOps agent running CI/CD would timeout. The solution presented here uses agent-less Azure DevOps jobs and Azure ML ability to notify Azure DevOps upon the pipeline completion (aka Azure ML Job Wait).  \nThe Azure ML Wait Job is a special REST service the accepts a job id and Azure DevOps call back info. The callback will be invoked when a pipeline finishes either successfully or due to a failure. This callback can be used for CI/CD flows.  \nThere are two options of using the Azure ML Job Wait feature:  \nUtilize Azure DevOps extension for Azure Machine Learning that is supporting by Azure ML team.  \nInvoke the Azure ML Job Wait REST API directly using the InvokeRESTAPI task in Azure DevOps.  \nOption 1. Using the extension  \nWhile the feature is in preview, you need to fill this form to get access for yourself or for your customers. Once your request has been approved, you will be able to install the extension:  \nThe following page contains very detailed information about how to use the extension. Additionally, we have added the extension into MLOps Template for Azure ML SDK v2 as well as MLOps Template for Azure CLI v2. Both templates demonstrate how to use the extension during the Continues Integration phase. This phase is a very common place to run a model long-running training on the full dataset.  \nOption 2. Using the InvokeRESTAPI task  \nAzure DevOps Builds have the Invoke Rest Api task which allows for the sending of an HTTP request and waiting for the response.  \nTo set it up the InvokeRESTAPI task a service connection is required. The connection doesn't contain any secure information, and it's just an object that contains an URI of a desired service. To setup it from Azure ML you can follow the steps below:  \nSign in to your organization (https://dev.azure.com/{yourorganization}}) and select your project.  \nSelect Project settings > Service connections.  \nSelect + New service connection, select Generic as the type of service connection that you need, and then select Next.  \nFill in the Server URL it should be in the following form: <https://{region}.experiments.azureml.net/webhook/v1.0/> where region is the region of your Azure ML workspace.  \nThe request to Azure ML will be authorized by providing the necessary information in the body so user name and password are not need to be filled in here.  \nSpecify the name of the service connection.  \nSelect Save.  \nOnce the service connection is ready, it's possible to configure the InvokeRESTAPI task using callback information in the body of the request.  \nAzure DevOps template yaml utilizing these capabilities is located here.  \nAbbreviated code snippet is shown below:  \nyaml\n- job: AzureML_Wait_Pipeline\ndisplayName: Waiting for Azure ML Pipeline\npool: server\nsteps:\n- task: InvokeRESTAPI@1\ndisplayName: Waiting for Job Results\ninputs:\nconnectionType: connectedServiceName\nserviceConnection: $(AML_REST_CONNECTION)\nmethod: PUT\nbody: \"{ \\\"EventInfo\\\": {\n\\\"Id\\\": \\\"$(RUN_NAME)\\\",\n\\\"EventType\\\":\\\"RunTerminated\\\" },\n\\\"CallbackService\\\": \\\"AzureDevOps\\\",\n\\\"ServiceDetails\\\": {\n\\\"JobId\\\": \\\"$(system.jobId)\\\",\n\\\"HubName\\\": \\\"$(system.HostType)\\\",\n\\\"TaskInstanceId\\\": \\\"$(system.TaskInstanceId)\\\",\n\\\"PlanId\\\": \\\"$(system.planId)\\\",\n\\\"ProjectId\\\": \\\"$(system.teamProjectId)\\\",\n\\\"PlanUri\\\": \\\"$(system.CollectionUri)\\\",\n\\\"AuthToken\\\": \\\"$(system.AccessToken)\\\",\n\\\"TaskInstanceName\\\": \\\"InvokeRESTAPI\\\",\n\\\"TimelineId\\\": \\\"$(system.TimelineId)\\\" } }\"\nheaders: \"{\\n\\\"Content-Type\\\":\\\"application/json\\\",\n\\n\\\"Authorization\\\":\\\"Bearer $(AAD_TOKEN)\\\" \\n}\"\nurlSuffix: \"subscriptions/$(SUBSCRIPTION_ID)/ \\\nresourceGroups/$(RESOURCE_GROUP)/ \\\nproviders/Microsoft.MachineLearningServices/ \\\nworkspaces/$(WORKSPACE_NAME)/webhooks/CreateWebhook\"\nwaitForCompletion: true  \npool: server parameter specifies agent-less job.\nSee the documentation\nfor the details.  \nEventInfo body element can be filled in using available by default system.* variables.  \nOther required parameters:  \nAML_REST_CONNECTION - Azure DevOps service connection name.  \nRUN_NAME - the run id of an experiment that has been started.  \nAAD_TOKEN - a token authorizing the request to the management.azure.com resource. Can be obtained using az account get-access-token.  \nSUBSCRIPTION_ID, RESOURCE_GROUP, WORKSPACE_NAME -\nfully qualify the Azure ML workspace.  \nIf you used a similar feature in Azure ML v1, pay attention that the current implementation of the Azure ML Job Wait feature are not starting a new job but getting the run id of an existing job (including pipeline one). If the job has been completed already, you still can use the feature to get its status.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\working-with-azure-ml\\azure-ml-from-azdo.md"
    },
    {
        "chunkId": "chunk389_0",
        "chunkContent": "Using non-Python programming languages in Azure ML  \nNon-Python programming languages are still very important and popular among developers and data scientists in financial and IoT industries. That's why it is important to understand if we can use anything rather than Python in Azure ML workloads.  \nUsing Azure ML v1, software engineers and data scientists got the perception that Python is the only programming language of choice. It is true due to some limitations in Azure ML v1:  \nAzure ML v1 doesn't support fully capable REST API: there is a way to invoke published pipelines, but it doesn't allow you to do any operations with entities in Azure ML.  \nPythonScriptStep has been built for Python only: the step has been presented as a core building block for Machine Learning pipelines. Later, CommandStep has been added, it has some limitations as well.  \nAzure ML SDK for Python is a requirement for training scripts: everything starting from basic logging functionality and up to a model registration requires access to Azure ML SDK that is available for Python only (Run.get_context()).  \nIn Azure ML v2 all the limitations have been removed:  \nREST API allows us to execute any operations in Azure ML starting from resource deployment and up to running a training job: thanks to that we can use any programming language to interact with Azure ML entities.  \nCommand job is the core element for building pipelines: Azure ML v2 doesn't have PythonScriptStep anymore, and there are many examples that show how to use the command job for non-Python code execution.  \nMLFlow is a way to build Azure ML agnostic code: MLFlow is a primary way to log metrics and artifacts in Azure ML v2. MLFlow supports REST API as well, and it can be utilized in any programming language.  \nThe only limitation that we found in Azure ML v2 is parallel job that should be implemented in Python only. This limitation can be neglected due to an option to call non-Python code from Python. In other words, you just need a skeleton in Python, but logic can be implemented in any other language as you can see in this example.  \nAzure ML v2 examples contains various examples in non-Python programming languages. Looking at the folder, you can find training jobs in Java, Julia and R. As a part of the Microsoft Solution Playbook we are maintaining a repo that demonstrate how to use Azure ML from C/C++.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\working-with-azure-ml\\azure-ml-non-python.md"
    },
    {
        "chunkId": "chunk390_0",
        "chunkContent": "author: shanepeckham\ntitle: Databricks Step Configuration\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Azure ML SDK v1 includes the DatabricksStep class that allows us to execute Databricks workloads as a part of a bigger Azure Machine Learning pipeline. In this document, you can find some tips about how to solve common challenges configuring DatabricksStep.\nrings:\n- public  \nStep configuration for Databricks  \nAzure ML SDK v1 includes the DatabricksStep class that allows us to execute Databricks workloads as a part of a bigger Azure Machine Learning pipeline. Here are some tips about how to solve common challenges configuring DatabricksStep.  \nIdentifying the Databricks script directory  \nCreating a Databricks step object requires both a directory name and the name of the file to be executed. The issue is that uploaded files and folders are placed in locations with randomized locations. As a result, Python import-s doesn't work.  \nThe workaround is to get AZUREML_SCRIPT_DIRECTORY_NAME from arguments in the script (the argument is provided automatically, but you need to parse it) and append it to sys.path before doing imports:  \n```py\n\nargs is an ArgumentParser object with all the arguments to the script\n\nparser.add_argument(\"--AZURE_SCRIPT_DIRECTORY_NAME\") is required when you are parsing the arguments\n\nroot = os.path.join(\"/dbfs\", args.AZURE_SCRIPT_DIRECTORY_NAME)\n\nsys.path.append(root)\n```  \nUsing environment variables  \nEnvironment variables need to be initialized prior to using the Run class.  \nRun.get_context() cannot be used due to lack of required environment variables. These variables should be initialized explicitly in code. For example, you can use the following code:  \n```py\ndef get_current_run():\n\"\"\"\nRetrieves current Azure ML run for a Databricks step.\n\"\"\"\nparser = argparse.ArgumentParser()\n\n```",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\working-with-azure-ml\\databricks-step-configuration.md"
    },
    {
        "chunkId": "chunk391_0",
        "chunkContent": "Data Drift Operationalization in Azure ML  \nIntroduction  \nIn this document we are planning to explain how to operationalize and automate custom data drift related workloads in Azure Machine Learning. In general, data drift components should be deployed in sync with inferencing infrastructure, because the inferencing service is responsible for collecting new data. The data drift components can be injected differently based on the inferencing scenario, for example:  \nreal-time inferencing for supervised Machine Learning  \nbatch inferencing for supervised Machine Learning  \ninferencing for unsupervised Machine Learning scenarios (like clustering)  \nWe are not evaluating near real-time inferencing scenarios for supervised ML because they are similar to real-time from a data drift operationalization perspective.  \nWe will discuss the following questions in this document:  \nWhat a data drift workload is from an Azure ML technology perspective.  \nHow to deploy changes into data drift workloads.  \nHow to invoke data drift in an automated way or by request.  \nHow to collect the outcome from data drift workload and implement notifications.  \nWhat the difference is for data drift operationalization between supervised vs unsupervised ML scenarios.  \nMain Deployment Artifact  \nBecause this document is more about data drift operations rather than about algorithms themselves, we can define a data drift implementation script as the main deployment artifact. While we won't go too deep in details about how it\u2019s implemented, we need to make some assumptions about it:  \nThe script itself should have access to a training dataset that has been used to train the current model. In the case of unsupervised ML, it can be a baseline dataset that we are holding from a previous algorithm execution or that we used initially to test the selected Machine Learning approach.  \nThe script should have access to new data with the ability to group them by timeframes in order to execute the data drift algorithm on a new (most recent batch from previous execution) batch of data. In the case of batch inferencing or for unsupervised algorithms, the inferencing service can update the entire dataset at once, and filtering might be not required.  \nThe script requires some computational power that might not be limited to CPU or to a single machine (node/instance) only.  \nThe script is not supposed to return results in real-time, but it\u2019s designed to be running on schedule or by request to process data.  \nHaving the assumptions above in mind as well as the fact that we are using Azure ML as the primary platform for training, we can switch our focus from a general script to an Azure Machine Learning pipeline and state Azure ML Pipeline as our primary deployment artifact (aka DataDrift Pipeline) for this document. In this case MLOps process will contain blocks to create, register, and schedule the pipeline execution as well as the data drift algorithm implementation itself.  \nPay attention to the fact that we assume that an Azure Machine Learning Workspace has been deployed to production to support the Data Drift Pipeline, even if the inferencing process doesn\u2019t require the workspace. Of course, we can host the Data Drift Pipeline in the development environment and find a way to execute it on real production data, but it's more of a manual process and we are not evaluating this case here.  \nAdditional Deployment Artifacts  \nWe have two more deployment artifacts that are related to the data drift:  \nModel (for supervised ML only): a Data Drift Pipeline cannot exist in a vacuum; it requires at least a training dataset and a dataset with new incoming data as two parameters. Both parameters are related to a model that we are using for scoring in production. That's why we have to follow the deployment process for the model to make sure that we will be able to locate the right versions of the datasets. We will use baseline as a term for the training dataset and target as a term for new incoming data that we are planning to analyze for drift.  \nInferencing service: it can be anything that serves our model, starting from Azure ML online endpoint and up to a custom container that might be deployed to AKS. We don't care much about how the service has been implemented, but it should be capable to collect incoming data in a desired way and store them in a selected storage.  \nTherefore, we have two additional deployment artifacts that are not directly related to the Data Drift Pipeline, but that are still very important for Data Drift operations.  \nData Drift Pipeline Development Process  \nPrior to discussing how to deploy the Data Drift Pipeline, we need to spend some time on its development. The Data Drift Pipeline can be treated as a general Azure ML Pipeline. As such, all standard procedures should be executed to guarantee code quality. In Azure ML it means that the pipeline has own folder in the repository that includes pipeline definition (YAML file as an example), environment metadata, and pipeline steps implementation that can share code with the training pipeline.  \nAs usual, we assume that a developer is modifying the pipeline in its own working branch, and there should be a way to test it even prior to create a Pull Request. So, we would recommend to create two small datasets (we assume that we will use uri_folder or mltable data assets in Azure ML) that will emulate the real life scenario. The first dataset is our toy training set that we can use not just to test the Data Drift Pipeline, but our training pipeline too (strongly recommended). The second set is a toy set for scoring \u2013 it can be a special set that allows developers to emulate a drift and see if the algorithm works fine. Both dataset names should be available in advance, and they should be modified as needed.  \nOnce all changes are ready, the developer can create a PR and initiate all the standard checks: linting, unit testing and integration testing. Integration testing should be available in order to understand that the pipeline can be published and executed with no errors, and both toy datasets from above can be used to execute the integration test. Integration testing means that we are executing the pipeline using the toy set to check if the pipeline works as intended.  \nOnce this stage is completed, we assume that we are ready to deploy production code.  \nDeployment process for supervised ML scenarios  \nSince we defined all our deployment artifacts, we are ready to describe the deployment process based on the artifacts\u2019 states. Let\u2019s divide the content of the section into the following blocks/steps:  \nHow to prepare model metadata.  \nPublishing DataDrift pipeline as a part of the model deployment pipeline.  \nPublishing DataDrift pipeline in its own deployment pipeline.  \nHow to prepare a model  \nUsually, we are training a model using a development branch. Not every model is going to production, and it should pass some additional tests and manual reviews prior publishing it. At the same time, during training we know our input datasets, and it\u2019s wise to store this information for future use in case the model is promoted to production. So, we would recommend using training DevOps process to prepare all input information for data drift, and associate it with the model using an ability to store references in the model\u2019s metadata. Usually, it\u2019s something that we are doing in the training pipeline right after the training (or validation, if we have this step). We can store the asset details using tags of the model that can be done for custom or mlflow models. If the model has been promoted to the production environment, we should be ready to copy training assets to there as well that might be challenging. So, we would recommend to share data storage between two environments to avoid duplicating of data, if it's possible.  \nPublishing Data Drift Pipeline as a part of the model deployment pipeline  \nThe Data Drift Pipeline can be published as a part of the model deployment process.  \nOnce the model has been published, we can extract metadata and make a decision about our baseline and target datasets, for example: should we copy data or create new empty folders or use existing data assets with no changes. Right after that we can publish the pipeline itself using the target and baseline references in the pipeline creation code. Finally, the new pipeline should be scheduled to execute, and its old schedule should be deleted. It's nice to have the first run of the pipeline based on the previous schedule details to make sure that we are not delaying the drift detection every time when we update the model, but it makes sense if the target/baseline sets have the same schema.  \nThe following diagram illustrates the process of the deployment:  \nPublishing Data Drift Pipeline in its own deployment pipeline  \nIn some cases we might want to update the Data Drift Pipeline only with no inferencing service updates. In this case we need to host one more Build DevOps pipeline that can be executed manually. This Build process has to look at the existing model in production, read all Data Drift-related information, publish the pipeline and schedule it, removing previous scheduling at the same time. We assume that we are not changing the schema of the incoming data at this point since we are not updating inferencing infrastructure. So, this pipeline will duplicate the Data Drift-related stage from the previous Build.  \nDeployment process for unsupervised ML scenarios  \nWe will not discuss unsupervised ML workloads at depth as the Data Drift Pipeline operation is very simple for these scenarios. As we don't have a model the inferencing infrastructure is solely concerned with preparing the datasets correctly. For example, if we have clustering, a task of the inferencing service will be to host a dataset for clustering in a way to make it available to Data Drift Pipeline. The baseline dataset can be fixed or we can update (or merge) it to new one if no drift has been detected. Therefore, Data Drift Pipeline can be treated as an independent artifact and we can deploy it on its own. Triggering of the pipeline can be done based on a schedule (rare), or on new data added to the blob storage (common) or orchestrated right after unsupervised ML workloads (the most recommended, but an orchestrator is required). Of course, the Data Drift Pipeline can be a part of the Unsupervised ML Pipeline.  \nScheduling Data Drift  \nThere are two main ways to coordinate executing our Data Drift Pipeline: in batch and at runtime. In the first case, it can be executed right after the batch scoring pipeline has completed. And in the second case, it can be triggered either by manual request or on some automated schedule.  \nBatch vs Realtime Inferencing  \nFor real time services we have to either schedule our Data Drift Pipeline or execute it manually, but Batch Inferencing brings a couple more options:  \nData drift step(s) can be injected into the inferencing pipeline directly - it makes sense if we have pretty big batches of data. In this case MLOps process is going to be simpler - we have just one inferencing pipeline, and we don't need to care about model metadata and location for the target dataset.  \nThe Data Drift Pipeline can be executed right after the Batch Inferencing Pipeline - in this case the deployment process will be the same as for real time scenario, but there should be a pipeline orchestrator somewhere and it's wise to execute both pipelines at the same time and use different clusters for them to improve the speed of the calculations.  \nNotifications for Drift Detection  \nLet\u2019s say, for example, the scheduled pipeline has detected data drift pass some threshold. It would be great for two things to happen: the detailed results are preserved in persistent storage, and we are notified so that we may act on it.  \nDepending on the findings, we may want to address this data drift by doing something like re-training our currently deployed model on new data. This may involve investigation into what might be causing the data drift and/or additional actions prior to re-training (e.g., copying data from on-premises to the cloud), so it\u2019s a good idea to be notified.  \nOne way to go about achieving automated notifications is by using Application Insights. This way, the pipeline can (for example) log data drift metrics manually and then App Insights can send an email notification (or a variety of other actions) if some of the metrics are out of range.  \nRollback Strategy  \nDepending on the state of the model, we may need to redeploy the previous model to production instead of the latest. In this proposed pattern, we have rollback abilities by design. We just need to execute the deployment pipeline to approve the model manually and pick the previous artifact. This will bring the previous model back to production along with its training dataset.  \nIt\u2019s important to note that in this case we need to store all the training sets, and track their versions properly. Usually, it doesn't affect storage space if the training set did not change much during the training flow.  \nAdditional Data Drift Resources  \nNile Wilson's drift blogpost: Building A Clinical Data Drift Monitoring System With Azure DevOps, Azure Databricks, And MLflow  \nniwilso/data-drift-monitor: Example code for integrating a custom data drift monitor into an MLOps solution using Azure Databricks and MLFlow.\n{% if extra.ring == 'internal' %}  \nData Drift in Images: An implementation of drift monitoring with code\n{% endif %}",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\working-with-azure-ml\\datadrift_operationalization.md"
    },
    {
        "chunkId": "chunk392_0",
        "chunkContent": "author: shanepeckham\ntitle: Experimentation Tools and Setup in Azure ML\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Recommended core suite of tools for any MLOps engagement\nrings:\n- public  \nExperimentation tools and setup in Azure ML  \nThe basics  \nBased on customer engagements, the recommended core suite of tools for any MLOps engagement includes:  \nVisual Studio Code.  \nAzure Machine Workspace (minimum: 1 workspace for Dev, 1 for Production).  \nAzure Machine Learning Compute Instances for Experimentation (1 per dev is common).  \nA Centralized Git Repo (for example, Azure DevOps or GitHub).  \nAn agreed upon folder structure.\nSee Sample Structure below.  \nWhy use VS Code?  \nVisual Studio Code, often called VS Code, is an open-source development environment, which has a large library of extensions to customize it to each developer's needs. VS Code is able to run Jupyter Notebooks which data scientists are familiar with. It also provides tools such as Intellisense and Intellicode to help them migrate their notebooks into reusable scripts.  \nBy combining the power of VS Code along with Azure ML's Compute Instances, data scientists have access to a powerful suite of tools. The Compute Instance can be pre-configured ahead of time with the necessary packages and kernels that a data scientist needs. It allows them to focus on their work instead of environment setup.  \nSample folder structure  \nA clear folder structure helps manage a complex project such as an MLOps engagement. A common successful pattern is:  \ntext\n\u251c\u2500\u2500 .devcontainer       <- Container definition for local development\n\u2502   \u251c\u2500\u2500 devcontainer.json\n\u2502   \u2514\u2500\u2500 Dockerfile\n\u2502\n\u251c\u2500\u2500 .pipelines          <- CI/CD pipeline definitions\n\u2502   \u251c\u2500\u2500 ci-build.yaml\n\u2502   \u251c\u2500\u2500 pr-build.yaml\n\u2502   \u2514\u2500\u2500 deploy.yaml\n\u2502\n\u251c\u2500\u2500 data                <- Datasets, if of reasonable size\n\u2502   \u251c\u2500\u2500 external        <- Data from third party sources\n\u2502   \u251c\u2500\u2500 raw             <- Original, immutable data dump\n\u2502   \u251c\u2500\u2500 interim         <- Intermediate data that has been transformed\n\u2502   \u2514\u2500\u2500 processed       <- Final, canonical data sets for modeling\n\u2502\n\u251c\u2500\u2500 docs                <- Collection of markdown files or Sphinx project\n\u2502\n\u251c\u2500\u2500 notebooks           <- Jupyter Notebooks for experimentation (linting optional)\n\u2502   \u2514\u2500\u2500 usrID_userStoryNumber_description.ipynb\n\u2502\n\u251c\u2500\u2500 mlops               <- ML pipeline definition code (such as CLI v2 .YAML files)\n\u2502   \u2514\u2500\u2500 training\n\u2502       \u251c\u2500\u2500 pipeline.yml\n\u2502       \u2514\u2500\u2500 train-env.yml\n\u2502\n\u251c\u2500\u2500 src                 <- Model creation code, for example, data prep, training, and scoring\n\u2502   \u2514\u2500\u2500 train\n|       \u2514\u2500\u2500 train.py\n\u2502   \u2514\u2500\u2500 score\n|       \u2514\u2500\u2500 score.py  \nThe goal with a folder structure such as this is to provide logical working spaces for the various pieces of an MLOps engagement. During experimentation, much of the work is done in the notebooks folder, where data scientists may check in Jupyter Notebooks.  \nFor another example of an MLOps folder structure, review the MLOps Template for Azure ML CLI V2.  \nNotebooks are traditionally difficult to version control since each re-running of a notebook causes the underlying code of the notebook to change. Tools such as nb-clean and nbQA can be integrated into PR gates or, better yet, pre-commit hooks to ease integration of notebooks with version control. While this integration enables basic linting and code formatting within notebooks, these tools may be new to the data scientist's workflows. Their use should be evaluated against disruption on a project-by-project basis.  \nSetting up for experimentation  \nOnce the repo has been created and the core requisites are met, the team is ready to connect to a Compute Instance and begin experimentation.  \nFrom VS Code, developers and data scientists can connect to their compute instance by configuring a remote compute instance, or configuring a remote Jupyter server.  \nOnce connected to the Compute Instance, users can clone the Git repo directly to the Workspace file system.  \nAnd with that connection, users have a managed, scalable compute target to perform experimentation on. It is cloud-based and connected directly to the Azure Machine Learning Workspace.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\working-with-azure-ml\\experimentation-in-azure-ml.md"
    },
    {
        "chunkId": "chunk393_0",
        "chunkContent": "author: shanepeckham\ntitle: Sync-Async Tasks Pattern in MLOps Pipeline\nms.topic: conceptual\nms.author: shanepec\nms.service: azure\ndescription: Machine learning operations (MLOps) process needs to combine practices in DevOps and machine learning\nrings:\n- public\ntags:\n- Government  \nSync-async tasks pattern in MLOps Pipeline  \nMachine learning operations (MLOps) process needs to combine practices in DevOps and machine learning. In a software project, testing and validating pipelines usually takes a few hours or less to run. Most software projects can complete their unit tests in a few hours. Pipeline tasks that run synchronously or in parallel are enough in most cases.\nHowever, in a machine learning project, the training and validation steps can take a long time to run, from a few hours to a few days. It's not practical to wait for the training to finish before moving on to the next step. So, during the MLOps flow design, we need to take different approaches and find a way to combine synchronous and asynchronous steps in order to run the end-to-end training process efficiently.  \nWe will introduce different practices to implement Sync-Async tasks patterns in the MLOps pipeline using the Azure pipeline.  \nUse synchronous task and toy dataset for ML build validation and unit testing  \nDuring the build validation phase, we want to validate the code quality quickly and ensure we implement the data processing code and ML algorithm correctly. The algorithm code should be able to train a model successfully using the provided dataset.\nTo speed up the process, we can use a small toy dataset to reduce the resource and time required for training the model. We can also reduce the training epoch and parameter range to reduce the training time further.\nThis approach uses synchronous pipeline tasks for preparing data and running the training. Because ML model training time is limited, the task can wait for the training to finish and then move on to the next step.  \nThe following code snippet submits an ML training using Azure ML CLI v2 and waits for the result in an ADO task.  \n```yaml\nparameters:\n- name: amlJobExecutionScript\ntype: string\n- name: amlJobSetCommand\ntype: string\n\nsteps:\n- task: AzureCLI@2\ndisplayName: Run Azure ML Pipeline and Wait for Results\ninputs:\nazureSubscription: $(AZURE_RM_SVC_CONNECTION)\nscriptType: bash\nworkingDirectory: $(System.DefaultWorkingDirectory)\nscriptLocation: inlineScript\ninlineScript: |\nexport AZUREML_CURRENT_CLOUD=\"AzureCloud\" #Choose a different value according to your cloud environement: AzureCloud, AzureChinaCloud, AzureUSGovernment, AzureGermanCloud\nrun_id=$(az ml job create -f ${{ parameters.amlJobExecutionScript }} \\\n${{ parameters.amlJobSetCommand }})\necho \"RunID is $run_id\"\nif [[ -z \"$run_id\" ]]\nthen\necho \"Job creation failed\"\nexit 3\nfi\naz ml job show -n $run_id --web\nstatus=$(az ml job show -n $run_id --query status -o tsv)\nif [[ -z \"$status\" ]]\nthen\necho \"Status query failed\"\nexit 4\nfi\nrunning=(\"NotStarted\" \"Queued\" \"Starting\" \"Preparing\" \"Running\" \"Finalizing\")\nwhile [[ ${running[*]} =~ $status ]]\ndo\nsleep 15\nstatus=$(az ml job show -n $run_id --query status -o tsv)\necho $status\ndone\nif [[ \"$status\" != \"Completed\" ]]\nthen\necho \"Training Job failed\"\nexit 3\nfi\n```  \nOther parameters:  \nAZURE_RM_SVC_CONNECTION: Azure DevOps service connection name.  \namlJobExecutionScript: Local path to the YAML file containing the Azure ML job specification.  \namlJobSetCommand: Additional Azure ML job parameters. For example, --name training-object-detection to specify the job name.  \nUse asynchronous tasks  \nUse asynchronous tasks for ML model training steps in both test and production pipelines.  \nMLOps pipeline usually includes multiple steps, such as data preprocessing, model training, model evaluation, model registration, and model deployment. Sometimes, we need to run ML training in both the integration test and production environments. For example, a defect detection system might want to retrain an ML model using the existing algorithm with a newly updated dataset from a production line. To automate the process, we want to ensure the whole MLOps pipeline can pass the integration test and then run correctly in the production environment. However, the model training step could take a long time to finish. We need to use asynchronous tasks to run the model training step and prevent the long waiting time in the main pipeline.  \nIn Azure DevOps, the Microsoft-hosted agent has a job time-out limitation. You can have a job running for the maximum of 360 minutes (6 hours). The pipeline will fail if the model training step is longer than the time limitation. There are a few ways to implement an asynchronous pipeline task in Azure DevOps to prevent this problem.  \nUse Azure Pipeline REST API tasks  \nWe recommend using Azure Pipeline REST API tasks to invoke published Azure ML pipelines. A post-back event is sent when the task is complete.  \nIn this approach, you publish your Azure ML pipeline and get a REST endpoint for the pipeline. Then you can use Azure Pipeline REST API task to invoke published Azure ML pipelines and wait for the post-back events. To wait for the post-back event, we need to set the waitForCompletion attribute of the REST API task to true.  \n{% if extra.ring == 'internal' %}\nThe Invoking Azure ML Pipeline From Azure DevOps document in this playbook has more detail implementation introduction.\n{% endif %}  \nUse REST calls to invoke other Azure Pipelines  \nYou can use an Azure ML component to invoke other Azure Pipelines using a REST call.  \nAn Azure ML component is a self-contained piece of code that accomplishes a task in a machine learning pipeline. It is the building block of an Azure ML pipeline.  \nIn this implementation, we use Azure ML CLI/SDK v2 to submit the Azure ML pipeline job. And in the final step of a pipeline job, use an Azure ML component to invoke the REST API of another Azure Pipeline to trigger the next steps.  \nFollowing are code snippets of an abbreviate reference implementation of the trigger Azure pipeline Azure ML component.  \nTrigger Azure Pipeline python code: ado-pipeline-trigger.py  \n```python\nimport requests\nfrom requests.structures import CaseInsensitiveDict\nimport os\nimport argparse\nfrom azure.keyvault.secrets import SecretClient\nfrom azure.identity import DefaultAzureCredential\n\ndef parse_args():\n\"\"\"Parse input args\"\"\"\n# setup arg parser\nparser = argparse.ArgumentParser()\n\ndef get_run_id(modelpath):\n\"\"\"Read run_id from MLmodel\"\"\"\nmlmodel_path = os.path.join(modelpath, \"MLmodel\")\nrun_id = \"\"\nwith open(mlmodel_path, \"r\") as modelfile:\nwhile(True):\nline = modelfile.readline()\nif not line:\nbreak\nif \"run_id\" in line:\nrun_id = line.split(':')[1].strip()\nbreak\nreturn run_id\n\ndef get_secret_value(kv_name, secret_name):\n\"\"\"Get the secret value from keyvault\"\"\"\nkv_uri = f\"https://{kv_name}.vault.azure.com\"\ncredential = DefaultAzureCredential()\nclient = SecretClient(vault_url=kv_uri, credential=credential)\nprint(f\"Retrieving ADO personal access token {secret_name} from {kv_name}.\")\n\ndef trigger_pipeline(args):\n\"\"\"Trigger Azure Pipeline\"\"\"\nrun_id = get_run_id(args.modelpath)\nsecret_value = get_secret_value(args.kvname, args.secretname)\nheaders = CaseInsensitiveDict()\nbasic_auth_credentials = ('', secret_value)\nheaders[\"Content-Type\"] = \"application/json\"\n\nrun script\n\nif name == \"main\":\n# parse args\nargs = parse_args()\n# trigger model registration pipeline\ntrigger_pipeline(args)\n```  \nTrigger Azure Pipeline Azure ML component: component_pipeline_trigger.yaml  \n```yaml\n$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\nname: trigger_pipeline\ndisplay_name: trigger Azure pipeline\nversion: 1\ntype: command\ninputs:\nmodelpath:\ntype: mlflow_model\nmodelname:\ntype: string\nkvname:\ntype: string\nsecretname:\ntype: string\norg:\ntype: string\nproject:\ntype: string\nbranch:\ntype: string\napiversion:\ntype: string\npipelineid:\ntype: integer\npipelineversion:\ntype: integer\n\ncode: ../../../src/pipeline_trigger/\nenvironment: azureml:sklearn-jsonline-keyvault-env@latest\ncommand: >-\npython ado-pipeline-trigger.py\n--modelpath ${{inputs.modelpath}}\n--modelname ${{inputs.modelname}}\n--kvname ${{inputs.kvname}}\n--secretname ${{inputs.secretname}}\n--org ${{inputs.org}}\n--project ${{inputs.project}}\n--branch ${{inputs.branch}}\n--apiversion ${{inputs.apiversion}}\n--pipelineid ${{inputs.pipelineid}}\n--pipelineversion ${{inputs.pipelineversion}}\n```  \nSubscribe to Azure ML Event Grid events  \nYou can subscribe to Azure ML Event Grid events and use a supported event handler to trigger another Azure Pipeline.  \nAzure Machine Learning manages the entire lifecycle of machine learning process. During the lifecycle, Azure ML will publish several status events in Event Grid, such as a completion of training runs event or a registration and deployment of models event. We can use a supported event handler to subscribe these events and react to them.  \nHere are the supported Azure ML events:  \nEvent type Subject format Microsoft.MachineLearningServices.RunCompleted experiments/{ExperimentId}/runs/{RunId} Microsoft.MachineLearningServices.ModelRegistered models/{modelName}:{modelVersion} Microsoft.MachineLearningServices.ModelDeployed endpoints/{serviceId} Microsoft.MachineLearningServices.DatasetDriftDetected datadrift/{data.DataDriftId}/run/{data.RunId} Microsoft.MachineLearningServices.RunStatusChanged experiments/{ExperimentId}/runs/{RunId}  \nFor example, to continue the MLOps pipeline when the ML training is finished, we will subscribe RunCompleted event and trigger another Azure Pipeline when the event is published. To trigger another Azure Pipeline, we can use Azure Automation runbooks,  Logic Apps, or Azure Functions and implement the trigger next step code in one of them.  \nRun Azure ML pipelines using Azure Pipeline agents  \nIn this approach, we use Azure Pipeline agents to run the Azure ML pipeline. Because there is no job time-out for self-hosted agents, it can trigger an Azure ML training task and wait for the training to finish, then move on to the next step.  \nThis approach uses synchronized tasks. However, we need to install the agent and maintain the environment that runs the agent.  \nFor more information  \nMLOps for Python with Azure Machine Learning - Azure Architecture Center | Microsoft Learn  \nMLOps: Machine learning model management - Azure Machine Learning | Microsoft Learn  \nSystem design patterns for machine learning",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\mlops\\working-with-azure-ml\\sync-async-mlops-patterns.md"
    },
    {
        "chunkId": "chunk394_0",
        "chunkContent": "Evaluating Azure Cognitive Search Results  \nWhether the IR system is meeting the information needs of the user is a critical element of the end-to-end design, development and maintenance of the system. IR system evaluation is an active area of research with a variety of techniques and metrics; one such dimension is offline vs. online approaches.  \nThe offline evaluation often happens during the initial system development phase. It requires a test collection, which consists of (i) a set of queries (or questions, and we will use them interchangeably in this document), (ii) for each query, a set of judged documents and (iii) the document collection for retrieval. Publicly available datasets such as MSMARCO are best suited for developing general-purpose IR systems. When designing and developing a domain-specific IR system, we need to consider constructing a test collection based on the real use case.  \nWhen we have a fully functioning system, online evaluation is often employed, particularly in industry and industrial research. Online evaluation observes the real users\u2019 interaction in-situ when they engage with the system. Different from the offline evaluation, online evaluation relies on implicit measurement such as user clicks, dwell time and purchase decision.  \nDocument Collection  \nThe document set is the basic dataset where the relevant information is expected. Where possible, we should use the live data: the same data that will be deployed in the live system. However, when such a condition cannot be satisfied, we need to have a set of representative documents that can simulate the real distribution of the data. The prepared document collection needs to be ingested and indexed using the search service, and in our case, the Azure Cognitive Search.  \nQuery Set  \nThere are two ways to gather a set of testing queries, using existing query logs or gathering through user studies (interview-based or survey-based). When facing a cold-start problem and there are no query logs available, we need to gather a query set.  \nIn order to be able to measure the performance of our developed system, it is better to have at least 50 questions. Same as the document collection, the set of selected questions are expected to reflect the real users\u2019 information need, which are the ones end users will issue to the system. More importantly, we also need to make sure different query types are covered. In our case, there are roughly two common types of queries:  \n(i) Navigational queries, of which the answers can be found in only one document of a specific type, for example, What\u2019s Microsoft\u2019s earning this year? and  \n(ii) Informational queries, which are open-ended questions that start with \"What\", \"Why\" and \"How\" and can be answered using different types of documents, covering multiple asset classes. For example, \"What\u2019s the impact of trade war on economy?\"  \nOne important point needs to be also considered when using an interview-based method is to also gather context information from interviewees. When possible, ask the interviewees to describe the scenario of issuing such a question. Taking the question \"What\u2019s the impact of the trade war in economy?\" as an example, we may want to further understand which \"economy\" the end user is talking about or any specific aspects the user is looking for.  \nIn this project, we observe that most questions are context-dependent, and when such context information is missing, it will be difficult in both system development and judgment experiments. For example, when a user is asking about \"strength of the USD\", the user assumes a currency compared to the USD, which is often the local currency. Similarly, users often omit specific market names, having assumptions of the market based on their customers and their own profile. This missing information results in ambiguous queries, and in some cases, even domain-experts find it difficult to understand the intent.  \nJudgment Set  \nFor each query, we need to gather relevance judgment for documents in the collection. However, it is expensive for us to gather judgments for all documents, so we adopt the pooling strategy that is often used in standard test collection construction. Note that, the assumption of using the pooling strategy is that the number of relevant documents is much less than the number of irrelevant ones. Therefore, we can assume the unjudged documents are not relevant in our evaluation process.  \nEvaluation Metrics  \nWe focus on the offline metrics in the current phase and will consider online evaluation when we have a living system. Two metrics are our primary focus: Reciprocal Rank (RR) and Normalized Discounted Cumulative Gain at different cutoffs (NDCG@k).  \nThe image below is an example of a ranked list. \"System\" is the ranked lists, with documents represented using \"D#\", the \"Rank\" is the rank position of each document, and the \"Relevance\" is the relevance value of the current document, where \"0\" is not relevant, \"1\" is marginally relevant, \"2\" is relevant and \"?\" is unjudged. The \"Judged Set\" is an example of labeled documents, where numbers in the parentheses indicate the relevance values of the document.  \nReciprocal Rank (RR) measures the position where the first relevant document appears and uses the reciprocal of that rank as the evaluation score. This metric requires binary relevance values and is often used in the retrieval-based question answering tasks. For the example results in the figure below, if we only consider the relevant results, then the system will have a RR score of \u00bd=0.5. D4 was ranked second and it was the first returned document that was relevant to the query.  \nNormalized Discounted Cumulative Gain at different cutoffs (NDCG@k) is a commonly used evaluation metric in retrieval systems. Its value is computed based on the current ranked list and an \"ideal\" ranked list. In the figure below, we will first obtain an \"ideal\" ranked list based on the judged set by sorting the documents according to the relevance value in descending order, and then we can compute the discounted cumulative gain on this ideal ranked list (IDCG). Then the final NDCG value is obtained using DCG/IDCG, where DCG is the score carried out on the real system ranked list. Compared to RR, this metric uses graded relevance and takes the recall into consideration. Besides the wide usage of the metric, another reason for us to choose the metric is the multi-stage retrieval design: since the document retrieval is the first stage in the system, we may want to measure both recall and precision.  \nCase Study: Azure Cognitive Search Evaluation for video search  \nThis example illustrates how to track and evaluate a search solution that returns results for video clips. The user will query for a particular subject via a querystring, and then depending on which video they click on in the results, we can glean the efficacy of the underlying search index.  \nDefinitions  \nTotal Relevant Items: The expected number of relevant items return for a query.  \nTotal Retrieved Items: The search results shown on a single page i.e. 10,20,30\u2026  \nRelevant items retrieved: Total number of videos marked as relevant  \nRank Position (K): The rank of the selected search result in the Total Retrieved Items result set  \nWhat do we need to capture?  \nUpon querying for videos (or any item, document, image etc), we need to log the Rank (K) of the selected item in the result set.  \nTotal results on the page in a user search session. A session needs to be defined, but this can be 1 day or 1 hour for example. It is recommended to understand from the users what a typical session may look like as part of their workflow.  \nThe querystring that the user entered for search.  \nUnranked Search Results Evaluation  \nIn this metric the ranking of items is not taken into account, the limitation of this approach is that we are not determining whether the top results are most relevant, only that relevant results are returned.  \nRelevant items are marked/explored by the user. We want to maximize precision and recall scores, as this will indicate the results returned by the index are optimum. Note, it is almost impossible to evaluate precision and recall since modern search engines return millions of documents and receiving a reliable evaluation is impractical  \nPrecision (P): Total Relevant Items Retrieved / Total Retrieved Items  \nRecall (R) =  Total Retrieved Items / Total Relevant Items  \nF1 measure =  2PR / P + R  \nRanked Search Results Evaluation  \nIn this metric we take into account the rank of the item selected within the result set. To optimize our underlying search index we want the most relevant items to be the first items displayed to the user.  \nPrecision@K = The number of relevant items at K / K  \nRecall@K = The number of relevant items at K / Total Relevant Items  \nReciprocal Rank = 1 / rank of first relevant item  \nAverage Precision = Sum(P@K) / the number of relevant items  \nMetrics  \nCumulative Gain (CG): Cumulative Gain is the sum of the graded relevance values of all results in a search result list. This predecessor of DCG does not include the rank (position) of a result in the result list into the consideration of the usefulness of a result set.  \nDiscounted Cumulative Gain: The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized, as the graded relevance value is reduced logarithmically proportional to the position of the result.  \nNormalized Discounted Cumulative Gain (nDCG): The nDCG values for all queries can be averaged to obtain a measure of the average performance of a search engine's ranking algorithm.  \nExample implementation: Query performance dashboard for monitoring Cognitive Search using Application Insights  \nThis Application Insights sample demonstrates an approach for deep monitoring of query usage and performance of an Azure Cognitive Search index. It includes a JSON template that creates a workbook and dashboard in Application Insights and a Jupyter Notebook that populates the dashboard with simulated data.  \n\u2139\ufe0f Refer to Query performance dashboard for monitoring Cognitive Search using Application Insights for the sample code.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\search\\azure-cognitive-search-evaluation.md"
    },
    {
        "chunkId": "chunk395_0",
        "chunkContent": "Best practices and optimising results with Azure Cognitive Search  \nOverview  \nInformation Retrieval (IR) systems (or \"search systems\") are systems that help users find their information needs in a collection. Information needs are something in the users' mind and are often represented in the form of queries. Understanding users\u2019 information needs and finding the right information for them requires an intelligent system that is beyond term matching.  \nAzure Cognitive Search is a search-as-a-service cloud solution that gives developers APIs and tools for adding a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications. Your code or a tool invokes data ingestion (indexing) to create and load an index. Optionally, you can add cognitive skills to apply AI processes during indexing. Doing so can add new information and structures useful for search and other scenarios.  \nOn the other side of your service, your application code issues query requests and handles responses. The search experience is defined in your client using functionality from Azure Cognitive Search, with query execution over a persisted index that you create, own, and store in your service.  \nFunctionality is exposed through a simple REST API or .NET SDK that masks the inherent complexity of information retrieval. In addition to APIs, the Azure portal provides administration and content management support, with tools for prototyping and querying your indexes. Because the service runs in the cloud, infrastructure and availability are managed by Microsoft.  \nPotential Applications  \nThere are various application scenarios where we can leverage Azure search besides building an enterprise search system to retrieve internal documents. Here are several potential application scenarios:  \nQuestion-answering system: When we are developing a question answering system, we may consider a multi-stage system where the first stage is built on top of Azure search. For the second stage, we can implement complex natural language processing techniques (e.g. summarization) to generate answers.  \nSystematic review or legal search: These types of retrieval problems are domain-specific, and they are recall-based, which aims to find all relevant documents.  \nPub/Sub systems: This is a type of search which we refer to as \"document routing\", where users\u2019 queries are often fixed, and documents come in a streaming fashion. In these systems, we may index user queries using Azure search, and route documents to the relevant queries.  \nGeospatial Search: This is a type of search where you can search any items based on their geolocation. You can visualize results with a map. To help this type of search, you have a parameter, \"search area coordinates\", which provides a polygon to search your items on the map.  \nMulti-media Search system: This is a type of search that allows for searching across multi-modal data types, i.e. other than text documents. We can ingest many document types with several data enhancing techniques such as image recognition, image detection, OCR text extraction from images, video transcribing and more.\nExamples of media search:  \nImproving Video Search using AI - A full solution that automates the analysis of videos with Azure Video Indexer, creates and improves the search index with Video Indexer insights and optional metadata. Provides a web UI application to search and see results and logs the behavior and feedback of users (in order for the admin or data science team to improve the search index). Lastly it also provides a dashboard which displays statistics on platform usage and search performances.  \nAI Enrichment Pipeline tutorial - Complete sample for processing text, image and video files through a full enrichment pipeline with event grid, service bus, functions, logic apps, cognitive services and video indexer  \nExtracting and indexing data from forms. Azure Cognitive Search can index content extracted from forms so that they may be searched.\nExamples of searching data from forms:  \nMicrosoft Fabric - Create a custom search engine and question-answering system  \nAzure Cognitive Search from Forms  \nAzure AI Search (Azure Cognitive Search) Search Types  \nThere are three main types of searches in Azure AI Search for information retrieval:  \nKeyword Search (Full-text search): matches plain text between the query and the data stored in the index in Azure AI Search. This search type is recommended when the case requires exact words matching like proper names, numbers, license plates, domain names, phrase etc. ...\nLimitation: It does not use any semantic meaning so it might struggle with synonyms and similar texts with different words.  \nVector Search: the query and document are converted to embeddings (numerical vectors) instead of working with the plain text directly. This mathematical representation can capture the contextual meaning of the text. The search will be done in the vector space using Nearest neighbors search algorithms like HNSW & Exhausted KNN.\nLimitation: it might miss some important keywords and return related content but not the best.  \nExhaustive KNN is a brute-force search that searches the entire vector space for documents that are most similar to the query. It does this by calculating the distances between all pairs of data points and finding the exact k nearest neighbors for a query point.\nWhen to use:\nThis algorithm may be used for cases where high recall is more important than search performance. Because it's computationally intensive, exhaustive KNN may be used for small to medium datasets but not for large datasets.  \nThe HNSW algorithm, as an approximate nearest neighbor (ANN) search algorithm in high-dimensional spaces, organizes data into a hierarchical graph in order to quickly find approximate nearest neighbors. It does this using navigating data points through the graph while maintaining a trade-off between search accuracy and computational cost.\nWhen to use:\nHNSW is recommended for most scenarios due to its efficiency in larger data sets. HNSW is faster than exhaustive KNN. HNSW has several configuration parameters that can be tuned to achieve the desired throughput, latency, and recall for your search application. For more information, see HNSW parameters.  \nHybrid Search (keyword + Vector): it is a combination between both previous methods. It executes both in parallel and Reciprocal Rank Fusion (RRF) algorithm is used to merge their results. This search type is recommended when the case requires both exact words matching, and the contextual meaning is essential.  \nAzure AI Search introduces a second step called Semantic Ranking that can be enabled after the hybrid search (initial ranking step) and is developed in collaboration with Bing. It is a set of advanced algorithms to reorder the retrieved text from hybrid search (L1). It can rank top 50 results from L1 and it showed a huge improvement in the performance. It is called semantic_hybrid in LangChain. Here is another link that helps in understanding how to boost precision in detail using semantic ranking. Utilizing internal semantic ranking in search can also eliminate the need for an external ranker in certain scenarios. Conducting straightforward experiments to quantify this effect can inform the decision-making process before proceeding with the implementation.  \nAs per multiple experiments run by our product team (published here) and our crews, it is recommended to use semantic hybrid search for most of the scenarios. However, the accuracy of chosen search method can be variant case by case.  \nIndexing and Index Enrichment Strategies  \nIndexer Scheduling Strategies  \nEnterprise projects may require updating the index all the time. For example, you may work on important document updates that are time-critical and require immediate (near real-time) reflection into index, or in some cases, a regular update. Depending on your business scenario, there are 2 ways to ingest documents into Azure Cognitive Search, namely:  \nPull Strategy  \nData ingestion is managed by the indexer. You have your data in your storage account and the Azure Cognitive Search indexer will ingest and re-index at your selected frequency. The index will not reflect the real-time state of your data with this approach.  \nPush Strategy  \nData ingestion managed by you. You have your data in your storage account or your application interface that you push into the Azure Cognitive Search index programmatically by using the REST API or SDK. Using this approach the index can reflect your data in real-time.  \nIn enterprise scenarios, we can use both strategies together to ingest documents into Azure Cognitive Search. We can use pull strategy for the initial run to setup our environment after the first CI/CD pipeline deployment and then a push strategy whenever there is an update on the data.  \nOne Master Index vs Multiple Index  \nIndex design is a core element of the development of an IR system. When there are multiple types of documents that differ in structure, a key decision needs to be made on whether to use a single index or multiple indices. Here are some considerations to take into account when deciding on the correct strategy:  \nConsideration One Master Index Multiple Indices Index definition size Long, unified fields Shorter, Multiple separated files Maintenance cost of index definition Easy, change only once Hard, only for a quick change you may need to modify every single file Query Send to one index Need to issue same queries over multiple indices Intent Detection Good to have but not necessary Must have or having lists aggregation Deployment Easy, only once Hard, you need to create all indexes/indices Results Aggregation No need Required for multi-type intent  \nSeparated multiple indexes/indices require multiple queries to different indexes/indices at the same time or manage query intent in Query API level to detect which index will be selected to search.  \nWhile using an individual index for each document type can help preserve their structural information as much as possible, the maintenance cost can be higher compared to having only one master index. For example, we need make changes to all index definition configuration files if one common field requires an update.  Let\u2019s assume we have a dataset which has 20 different document types, and we need to store all details per document type. It can be difficult to manage index field definitions per document type when your index definitions share a common base in 20 different document types. When you change any common index definition (searchable, filterable, facetable, retrievable, sortable, etc.) you need to reflect these changes in 20 different index definitions, which introduces maintenance complexity.  \nTune HNSW Vector Search Parameters  \nHierarchical Navigable Small World (HNSW) is a type of approximate nearest neighbor (AAN) search algorithm. ANN is a general term for any algorithm that can find similar vectors in a large and high-dimensional space without searching through all the data points.  \nHNSW offers a high-performance, memory-efficient solution for approximate nearest neighbor search in high-dimensional spaces. HNSW creates a multi-layer graph structure that organizes the data points and speed up the search process. Azure AI Search uses HNSW for its ANN algorithm.  \nHNSW algorithm in Azure AI Search allows users to adjust the trade-off between accuracy and optimal performance by tunning key parameters below:  \nm: Sets out the number of edges created for every new node during construction. Increasing this parameter may improve recall and reduce retrieval times for datasets with high intrinsic dimensionality at the expense of consuming more memory and be slower to insert. This is because the graph becomes more interconnected. Default is 4. The range is 4 to 10. Lower values should return less noise in the results.  \nef_construction: Sets the size of the list containing the nearest neighbors, which is used during index time. Increasing this parameter may improve index quality, at the expend of increased indexing time. At a certain point, increasing this parameter leads to diminishing returns. Default is 400. The range is 100 to 1,000.  \nef_search: Sets the size of the list containing the nearest neighbors, which is used during search time. Increasing this parameter may improve search results, at the expense of slower search. At a certain point, increase this parameters lets to diminishing returns. Default is 500. The range is 100 to 1,000.  \nThe recommended values for these parameters depend on the dataset size and the desired recall.  We suggest the following steps to find the optimal values:  \nStart out with m=4 and ef_construction=100.  \nRun benchmarks, iterating over ef_construction until you get a recall higher than the desired recall.  \nRe-index by setting ef_construction to the value discovered in step 2.  \nincrease m and iterate over.  \nIndex Enrichment  \nWhen we want to have a retrieval system beyond keyword-based search and have a better semantic understanding of the documents, we can perform text enrichment. The enrichment can be key phrases extraction, NER (named entity recognition) etc.  \nThe following repository showcases a collection of small and discrete data enrichment functions, using a variety of infrastructures, specifically built for Azure Cognitive Search, but that can be used in any data enrichment pipeline. These PowerSkills contain standard API interfaces so that they can be consistently consumed.  \nRefer to the Azure Search PowerSkills for assets, guidance and examples.  \nQuerying  \nThe Azure search query syntax is a full specification of a multi-step query re-writing operation. Parameters on the request provide matching criteria for finding documents in an index, including or excluding certain fields, boosting certain fields, execution instructions passed to the engine, directives for shaping the response and providing relevance ranking.  \nThis is a sample query with parameters to Azure Cognitive Search to retrieve the most relevant results.  \n```json\n{\n\n}\n```  \nAzure Cognitive Search provides a REST API to fill with your constructed query to retrieve results. See the Search Results documentation for more detail.  \nWhen users send their natural language queries to a retrieval system, first we need to convert it to Azure search syntax (see above code snippet), consider search parameters such as searchable fields, search query type etc.  \nOnce you prepare the query following the search syntax, the query evaluation process will transform it into a query tree and compute the retrieval score in the index.  \nIn order to have effective results, we need a Query Re-Writing API where we can preprocess the natural language input. Query Re-Writing is the process that adds to, rephrases or refines some of the keywords in the query.  \nA query is viewed as user\u2019s expression of information need in the form of a set of keywords. In combination with document enrichment to improve the semantic understanding of the system, the re-writing process can further improve the search effectiveness.  \nBelow is a list of commonly adopted query re-writing components:  \nSpell Corrections & Auto-Capitalization  \nLanguage Identification  \nDetect Synonyms  \nDetect Intent  \nDetect Entities  \nQuery Reformulation  \nDocument Retrieval  \nDocument Ranking  \nAzure Cognitive Search has the following built-in search capabilities:  \nCore Search Features Free-form text search Full-text search is a primary use case for most search-based apps. Queries can be formulated using a supported syntax. Simple query syntax provides logical operators, phrase search operators, suffix operators, precedence operators. Lucene query syntax includes all operations in simple syntax, with extensions for fuzzy search, proximity search, term boosting, and regular expressions. Relevance Simple scoring is a key benefit of Azure Cognitive Search. Scoring profiles are used to model relevance as a function of values in the documents themselves. For example, you might want newer products or discounted products to appear higher in the search results. You can also build scoring profiles using tags for personalized scoring based on customer search preferences you\u2019ve tracked and stored separately. Geo-search Azure Cognitive Search processes, filters, and displays geographic locations. It enables users to explore data based on the proximity of a search result to a physical location. Filters and facets Faceted navigation is enabled through a single query parameter. Azure Cognitive Search returns a faceted navigation structure you can use as the code behind a categories list, for self-directed filtering (for example, to filter catalog items by price-range or brand). Filters can be used to incorporate faceted navigation into your application\u2019s UI, enhance query formulation, and filter based on user- or developer-specified criteria. Create filters using the OData syntax. User experience features Autocomplete can be enabled for type-ahead queries in a search bar. Search suggestions also works off of partial text inputs in a search bar, but the results are actual documents in your index rather than query terms. Synonyms associates equivalent terms that implicitly expand the scope of a query, without the user having to provide the alternate terms. Hit highlighting applies text formatting to a matching keyword in search results. You can choose which fields return highlighted snippets. Sorting is offered for multiple fields via the index schema and then toggled at query-time with a single search parameter. Paging and throttling your search results is straightforward with the finely tuned control that Azure Cognitive Search offers over your search results.  \nAdjusting Query result rankings with Scoring Profiles (Model Tuning)  \nScoring refers to the computation of a search score for every item returned in search results. The score reflects the probability of the current item being relevant to users\u2019 information needs. A high score suggests that the item is more likely to be relevant. Therefore, the returned ranked list sorts items based on the retrieval scores, in descending order. Azure Cognitive Search computes the retrieval score based on the TF-IDF model, but you can customize the model through a scoring profile. Scoring profiles give you greater control over the ranking of items in search results. For example, you might want to boost items based on their revenue potential, promote newer items, or perhaps boost items that have been in inventory too long.  \nA scoring profile is part of the index definition, composed of weighted fields, functions, and parameters.  \nScoring profiles have two ways of boosting: providing weight on fields and providing a boosting function. We can combine these 2 methods to boost in a scoring profile.  \nPlease refer to Add scoring profiles to a search index  \nSemantic Search improvements with Metadata  \nTo enhance the quality of retrieval and LLM generation results in Azure AI Search, explore the integration of index metadata into the Semantic Configuration. Incorporating metadata, such as document type, source, author, category, summary, status, or publication date, into user queries can significantly improve the accuracy and relevance of search results.  \nThorough Metadata Analysis  \nConduct a meticulous analysis to identify metadata columns that can have potential impact on the search results. Understand the nuances of each metadata field and its impact on search performance.  \nIncorporate Index Metadata in the Semantic Configuration  \nBegin by including metadata information in the search index and semantic settings.  \nExample: If analyzing a document repository, metadata analysis may reveal key attributes such as document type, source, author, category, summary, status or publication date, influencing search relevance.  \nStringified Metadata in Search Index:  \nConsider including metadata information in a stringified manner (if more than one field is identified as useful) as a searchable field in the search index. Also explore pre-filtering the search space based on specific document metadata information.  \nExample: Converting metadata fields like \"tags\" or \"categories\" into strings enables efficient search queries, aiding users in finding documents with specific attributes.  \nConfigure Semantic Search:  \nRefer to this article for information on semantic configuration and also to understand semantic ranking process. Consider including content and metadata fields like article summary in the prioritized_content parameter. Aso look at including metadata fields that can help narrow down the search in prioritized_keywords like categories etc.  \nExample: Prioritizing keywords like \"important\" or \"relevant\" and content fields like \"title\" or \"summary\" or \"content\" can significantly improve the ranking of relevant documents.  \nSample of the semantic configuration:  \n```  json\n{\n\"semantic_configuration_name\": \"semantic_config\",\n\"semantic_settings\":{\n\"configuration\": [\n{\n\"name\": \"semantic_config\",\n\"prioritized_fields\": {\n\"title_field\": {\n\"field_name\": \"Title\"\n},\n\"prioritized_content_fields\": [\n{\n\"field_name\": \"content\"\n},\n{\n\"field_name\": \"ArticleSummary\"\n}\n],\n\"prioritized_keywords_fields\": [\n{\n\"field_name\": \"Category1\"\n},\n{\n\"field_name\": \"Category2\"\n},\n{\n\"field_name\": \"Category3\"\n}\n],\n}\n}\n]\n}\n}\n\nFilterable Metadata Columns:\n\nDesignate specific metadata columns, such as categories, as filterable for improved search functionality. However, proceed with caution, as incomplete document categorization may pose the risk of filtering out potentially relevant search results.\n\nExample: Designating \"categories\" as filterable allows users to refine search results based on predefined categories, improving precision in results.\n\nUser Query Augmentation\n\nExplore enhancing search accuracy by letting users choose relevant metadata, such as categories, when making queries. Offer options like \"Technology,\" \"Science,\" or \"Education\" for users to select. Incorporating chosen categories as metadata columns enables efficient filtering and prioritization of search results based on user preferences. Additionally, combining document metadata with the user's query refines the search, providing more contextually relevant and accurate results.\n\nExample: A user querying \"recent articles in category X\" benefits from query augmentation, ensuring search results are tailored to the specified category and recency.\n\nImplementing this practice, with a keen focus on thorough metadata analysis, will contribute to more accurate source identification and enhance the overall performance of Azure AI Semantic Search. It's important to note that plenty of experimentation should be conducted to figure out what works best for a specific context. This iterative process will allow for fine-tuning and optimizing the approach.\n\nBoosting with Selected Fields (Field Weights)\n\nYou can select any searchable fields in the Azure Cognitive Search index to boost. In addition, you specify the multiplier for the relative\nmagnitude of the boost. For example, matches in a title of a document might be 5 times more important than in the body of the document.\nThis method is very good to use if there is a well-understood relationship in your fields, such as matching in Title, Category or Body fields.\n\nOne interesting approach is to extract key phrases in a document and boost matches in the key phrases field over the main body field. Assigning a weight on any field can boost its search score.\n\nIf you have one master index and if weight varies by document type, you need to be careful when assigning weights. It's a best practice\nto use a test dataset to compare small changes to weights in order to verify that the outcomes are still correct. Boosting some fields may affect the rankings of other document types in unexpected ways.\n\nNOTE: You can evaluate different methods (scoring profiles) using a test collection and offline evaluation metric. You can send the list of your queries using a different scoring profile each time to validate that your scoring profiles do indeed affect the ranking as intended.\n\nPlease refer to Define a scoring profile in Azure search for more info.\n\nScoring Functions\n\nIn some scenarios, you would like to boost on some other factors than word matches and weights on fields. Scoring functions provide boosting abilities such as freshness of the document, geographic distance between your reference item and other items on your index, or the range of values for a numeric field.\n\nThere are four built-in scoring functions available in Azure search services: Magnitude, Freshness, Distance and Tag. These are described in this document:\n\nAdd scoring profiles to a search index for more info.\n\nExpanding Search with Synonyms\n\nSynonyms in search engines associate equivalent terms that implicitly expand the scope of a query, without the user having to provide the term. For example, \u2018Microsoft\u2019 and Microsoft\u2019s NASDAQ Stock Code \u2018MSFT\u2019 stands for same entity; \u2018Google\u2019, \u2018GOOG\u2019 and \u2018Alphabet Inc.\u2019 represent for the same entity too. There are two techniques to define synonyms in Azure Cognitive Search:\n\nExpansion\n\nBy specifying a comma separated list of synonyms, Azure Cognitive Search will automatically expand the search query:\n\ntext\n\"USA, United States, United States of America\"\n\nWith the rule above a search query for \"USA\" will be expanded to \"USA\" OR \"United States\" OR \"United States of America\".\n\nExplicit Mapping\n\nBy specifying an explicit mapping convention, Azure Cognitive Search automatically maps all specified terms to a standard term:\n\ntext\n\"Washington, Wash., WA => WA\"\n\nA search query for either \"Washington\", or \"Wash.\", or \"WA\" will be replaced with a search query for \"WA\". This mapping is uni-directional.\n\nRefer to Synonyms in Azure Cognitive Search for more info.\n\nCustom Analyzers\n\nThis refers to a user-defined configuration of a combination of existing elements, consisting of one tokenizer (required) and optional filters (char or token).\n\nWe can use custom analyzers for various use cases to improve search quality, and this analyzer can be used during both index and retrieval time. For example, language analyzers can do the following\n\nNon-essential words (stopwords) and punctuation are removed.\n\nPhrases and hyphenated words are broken down into component parts.\n\nUpper-case words are lower-cased.\n\nWords are reduced to root forms so that a match can be found regardless of tense.\n\nRefer to Add custom analyzers to string fields in an Azure Cognitive Search index for more information.\n\nEvaluating Search\n\nWhether the IR system is meeting the information needs of the user is a critical element of the end-to-end design, development and maintenance of the system. IR system evaluation is an active area of research with a variety of techniques and metrics; one such dimension is offline vs. online approaches.\n\nThe offline evaluation often happens during the initial system development phase. It requires a test collection, which consists of (i) a set of queries (or questions, and we will use them interchangeably in this document), (ii) for each query, a set of judged documents and (iii) the document collection for retrieval. While there are publicly available datasets such as MSMARCO, they are best suited for developing a general purpose IR system. When designing and developing a domain-specific IR system, we need to consider constructing a test collection based on the real use case.\n\nWhen we have a fully functioning system, online evaluation is often employed, particularly in industry and industrial research. Online evaluation observes the real users\u2019 interaction in-situ when they engage with the system. Different from the offline evaluation, online evaluation relies on implicit measurement such as user clicks, dwell time and purchase decision.\n\nRefer to the Search Evaluation Components for a deployable feedback and evaluation solution for Azure Cognitive Search.\n\nDocument Collection\n\nThe document set is the basic dataset where the relevant information is expected. Where possible, we should use the live data: the same data that will be deployed in the live system. However, when such a condition cannot be satisfied, we need to have a set of representative documents that can simulate the real distribution of the data. The prepared document collection needs to be ingested and indexed using the search service, and in our case, the Azure Cognitive Search.\n\nQuery Set\n\nThere are two ways to gather a set of testing queries, using existing query logs or gathering through user studies (interview-based or survey-based). When facing a cold-start problem and there are no query logs available, we need to gather a query set.\n\nIn order to be able to measure the performance of our developed system, it is better to have at least 50 questions. Same as the document collection, the set of selected questions are expected to reflect the real users\u2019 information need, which are the ones end users will issue to the system. More importantly, we also need to make sure different query types are covered. In our case, there are roughly two common types of queries:\n\n(i) Navigational queries, of which the answers can be found in only one document of a specific type, for example, What\u2019s Microsoft\u2019s earning this year? and\n\n(ii) Informational queries, which are open-ended questions that start with \"What\", \"Why\" and \"How\" and can be answered using different types of documents, covering multiple asset classes. For example, What\u2019s the impact of trade war on economy?\n\nOne important point needs to be also considered when using an interview-based method is to also gather context information from interviewees. When possible, ask the interviewees to describe the scenario of issuing such a question. Taking the question \"What\u2019s the impact of the trade war in economy?\" as an example, we may want to further understand which \"economy\" the end user is talking about or any specific aspects the user is looking for.\n\nIn this project, we observe that most questions are context-dependent, and when such context information is missing, it will be difficult in both system development and judgment experiments. For example, when a user is asking about \"strength of the USD\", the user assumes a currency compared to the USD, which is often the local currency. Similarly, users often omit specific market names, having assumptions of the market based on their customers and their own profile. This missing information results in ambiguous queries, and in some cases, even domain-experts find it difficult to understand the intent.\n\nJudgment Set\n\nFor each query, we need to gather relevance judgment for documents in the collection. However, it is expensive for us to gather judgments\nfor all documents, so we adopt the pooling strategy that is often used in standard test collection construction. Note that, the assumption of using the pooling strategy is that the number of relevant documents is much less than the number of irrelevant ones. Therefore, we can assume the unjudged documents are not relevant in our evaluation process.\n\nEvaluation Metrics\n\nWe focus on the offline metrics in the current phase and will consider online evaluation when we have a living system. Two metrics are our primary focus: Reciprocal Rank (RR) and Normalized Discounted Cumulative Gain at different cutoffs (NDCG@k).\n\nThe image below is an example of a ranked list. \"System\" is the ranked lists, with documents represented using \"D#\", the \"Rank\" is the rank position of each document, and the \"Relevance\" is the relevance value of the current document, where \"0\" is not relevant, \"1\" is marginally relevant, \"2\" is relevant and \"?\" is unjudged. The \"Judged Set\" is an example of labeled documents, where numbers in the parentheses indicate the relevance values of the document.\n\nReciprocal Rank (RR) measures the position where the first relevant document appears and uses the reciprocal of that rank as the evaluation score. This metric requires binary relevance values and is often used in the retrieval-based question answering tasks. For the example results in the figure below, if we only consider the relevant results, then the system will have a RR score of \u00bd=0.5. D4 was ranked second and\nit was the first returned document that was relevant to the query.\n\nNormalized Discounted Cumulative Gain at different cutoffs (NDCG@k) is a commonly used evaluation metric in retrieval systems. Its value is computed based on the current ranked list and an \"ideal\" ranked list. In the figure below, we will first obtain an \"ideal\" ranked list based on the judged set by sorting the documents according to the relevance value in descending order, and then we can compute the discounted cumulative gain on this ideal ranked list (IDCG). Then the final NDCG value is obtained using DCG/IDCG, where DCG is the score carried out on the real system ranked list. Compared to RR, this metric uses graded relevance and takes the recall into consideration. Besides the wide usage of the metric, another reason for us to choose the metric is the multi-stage retrieval design: since the document retrieval is the first stage in the system, we may want to measure both recall and precision.\n\nSome other useful references\n\nUsing Semantic Search in Synapse for Data Discovery\n\nAzure Search Knowledge mining\n\nAzure Search custom skills",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\search\\index.md"
    },
    {
        "chunkId": "chunk396_0",
        "chunkContent": "Using Non-Ubuntu Images in Azure ML  \nOverview  \nThe purpose of this document is to show how RedHat based images can run Azure Machine Learning training and inference workloads. This process can be used for other Linux flavors, with some modifications. The process is divided into the following tasks which will be explored below:  \nExplore Azure ML base images  \nBuild Suitable Azure ML-Ready RedHat image  \nTest Azure ML RedHat image for model training  \nTest Azure ML RedHat image for model inference  \nExplore Azure ML Base Images  \nTo build an image that will be able to run Azure ML workloads, we must understand what is in an Azure ML base image. Based upon the AzureML-Containers repo (https://github.com/Azure/AzureML-Containers), an Azure ML base CPU image (e.g., OpenMPI CPU - Ubuntu 22.04) has the following:  \nInstallation of Common Dependencies  \nInstallation of Inference System Requirements: libcurl3, liblttng-ust0, libunwind8, libxml++2.6-2v5, nginx-light, psmisc, rsyslog, runit, unzip, wget  \nInference python scripts and binaries: azureml-functions, azureml-logger, azureml-server, azureml-util, iot-server (compiled binaries)  \nInstallation of Conda Environment  \nInstallation of Open-MPI  \nInstallation of MSODCSQL17  \nBuild Suitable Azure ML-Ready RedHat Image  \nAn Azure ML-ready image can be created based upon the main components shown in the Azure ML Base Images section. A sample requirements.txt file is shown below:  \ntext\nazureml-mlflow==1.42.0\ninference-schema[numpy-support]==1.3.0\nipykernel~=6.0\nmatplotlib\nmlflow==2.3.0\nnumpy==1.24.2\npandas>=1.1,<1.2\npsutil>=5.8,<5.9\nscikit-learn==1.2.2\nscipy==1.10.1\ntqdm>=4.59,<4.60\nxlrd==2.0.1  \nA sample Dockerfile is shown below:  \n```Docker\nARG BASE_REGISTRY=registry.access.redhat.com\nARG BASE_IMAGE=ubi8/ubi\nARG BASE_TAG=8.7\n\nPulls based on arguments\n\nFROM ${BASE_REGISTRY}/${BASE_IMAGE}:${BASE_TAG} AS runtime-image\n\nSets root user\n\nUSER 0\n\nContainer initial package and python installation\n\nRUN useradd project -m -d /home/project \\\n&& mkdir -p /app \\\n&& chown -R project /app \\\n&& dnf update -y \\\n&& dnf install -y \\\nbinutils \\\ngit \\\niproute \\\nlibgomp \\\nopenssh \\\nopenssh-server \\\nprocps-ng \\\npython38 \\\npython3-pip \\\nunzip \\\nwget \\\n&& alternatives --set python /usr/bin/python3.8 \\\n&& dnf clean all \\\n&& rm -rf /var/cache/dnf\n\nRUN dnf install -y gcc \\\n&& dnf install -y gcc-c++ \\\n&& dnf install -y make\n\nInstall Conda\n\nENV PATH=\"/root/miniconda3/bin:$PATH\"\nRUN wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh \\\n&& sh Miniconda3-latest-Linux-x86_64.sh -b \\\n&& conda --version\n\nOpen-MPI installation\n\nARG OPENMPI_VERSION=4.1.0\nRUN mkdir /tmp/openmpi && \\\ncd /tmp/openmpi && \\\nwget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.0.tar.gz && \\\ntar zxf openmpi-4.1.0.tar.gz && \\\ncd openmpi-4.1.0 && \\\n./configure --enable-orterun-prefix-by-default && \\\nmake -j $(nproc) all && \\\nmake install && \\\nldconfig && \\\nrm -rf /tmp/openmpi\n\nInstall Msodbcsql17\n\nRUN dnf update -y \\\n&& dnf install -y curl \\\n&& curl https://packages.microsoft.com/config/rhel/8/prod.repo > /etc/yum.repos.d/mssql-release.repo \\\n&& curl -sSL https://packages.microsoft.com/keys/microsoft.asc > ./microsoft.asc \\\n&& dnf remove unixODBC-utf16 unixODBC-utf16-devel \\\n&& ACCEPT_EULA=Y dnf install -y msodbcsql17\n\nInstall fastai and mlflow\n\nRUN conda install -c fastchan fastai \\\n&& pip install mlflow azureml-mlflow\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt && rm requirements.txt\n\nOn Fedora based images, aliases are not always assigned depending on who builds the base image, so include full path.\n\nIf not running root, use links in /usr/bin/\n\nCMD [\"/bin/bash\"]\n```  \nYour conda dependencies and requirements.txt may be different depending up the training model type.  \nTest Azure ML RedHat Image for Model Training  \nA training job for a sample model was able to be successfully run using a RedHat image. For more details on how to train a model using a custom Docker image, refer to Train a model using a custom Docker image  \nTest Azure ML RedHat Image for Model Inference  \nA batch inference endpoint was able to be successfully deployed using a RedHat based image. For more details on how to use a batch endpoint for inference, refer to Use batch endpoints for batch scoring.",
        "source": "..\\data\\docs\\code-with-mlops\\technology-guidance\\working-with-azure-ml\\azure-ml-non-ubutu-images.md"
    }
]