{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. Search Experiment\n",
    "\n",
    "Search is at the core of the RAG pattern. Precise, efficient, and consistent search is critical when implementing a solution based on RAG.\n",
    "\n",
    "There are four types of search [options](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-data?tabs=ai-search#search-options): keyword search, semantic search, vector search, hybrid search.\n",
    "\n",
    "### [The Role of Search](https://github.com/microsoft/rag-openai/blob/main/topics/RAG_EnablingSearch.md#the-role-of-search)\n",
    "\n",
    "The main purpose of the search tool is to bring the first cut of relevant documents for further analysis by the large language model - it is there to filter the noise and reduce the result set for the model to summarize.\n",
    "\n",
    "Search is at the heart of a RAG solution - it is the mechanism that ensures that the context that is sent to the prompt contains relevant information for it to answer the question.\n",
    "\n",
    "<!-- ### [Evaluating the Retrieval Component](https://github.com/microsoft/rag-openai/blob/main/topics/RAG_EnablingEvaluation.md#evaluating-the-retrieval-component)\n",
    "\n",
    "Regarding the Retrieval component, the dataset is composed of question and citation instead of question and answer.\n",
    "\n",
    "- question: the user question\n",
    "- citation: the piece(s) of text that contains the relevant content to answer the user question\n",
    "- answer: the final answer in a human readable/friendly format\n",
    "  Evaluating the Retrieval component means to evaluate if for a given query (user question) the search engine is returning the relevant citation(s). -->\n",
    "\n",
    "üìù**Hypothesis**\n",
    "\n",
    "The hypothesis for this experiment is an exploratory one: \"Can introducing a new type of search improve the system's performance?\"\n",
    "\n",
    "üéØ **Measure of Success**\n",
    "\n",
    "Retrieval information is a well-known problem and the classic metrics are: Precision, Recall, F1 Score, Mean Average Precision (MAP), Mean Normalized Discounted Cumulative Gain (Mean NDCG) and Mean Reciprocal Rank (MRR). More details can be found at Evaluating Information Retrieval Models: A Comprehensive Guide to Performance Metrics.\n",
    "\n",
    "[Evaluation Metrics](https://github.com/microsoft/rag-openai/blob/main/topics/RAG_EnablingEvaluation.md#evaluation-metrics)\n",
    "Linnk: https://medium.com/@prateekgaurav/evaluating-information-retrieval-models-a-comprehensive-guide-to-performance-metrics-78aadacb73b4#:~:text=Evaluating%20Information%20Retrieval%20Models%3A%20A%20Comprehensive%20Guide%20to,...%206%206.%20Mean%20Reciprocal%20Rank%20%28MRR%29%20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%run -i ./pre-requisites.ipynb\n",
    "%run -i ./helpers/search.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import (\n",
    "    VectorizedQuery,\n",
    "    QueryType,\n",
    "    QueryCaptionType,\n",
    "    QueryAnswerType,\n",
    ")\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "search_index_name = \"index_chunks_2\"\n",
    "search_client = SearchClient(\n",
    "    endpoint=service_endpoint, index_name=search_index_name, credential=credential\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perform a keyword search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunkId: chunk63_0\n",
      "source: ..\\data\\docs\\code-with-engineering\\code-reviews\\pull-request-template.md\n",
      "Score: 15.121317\n",
      "chunkContent: Work Item ID\n",
      "\n",
      "For more information about how to contribute to this repo, visit this page\n",
      "\n",
      "Description\n",
      "\n",
      "Should include a concise description of the changes (bug or feature), it's impact, along with a summary of the solution\n",
      "\n",
      "Steps to Reproduce Bug and Validate Solution\n",
      "\n",
      "Only applicable if the work is to address a bug. Please remove this section if the work is for a feature or story\n",
      "Provide details on the environment the bug is found, and detailed steps to recreate the bug.\n",
      "This should be detailed enough for a team member to confirm that the bug no longer occurs\n",
      "\n",
      "PR Checklist\n",
      "\n",
      "Use the check-list below to ensure your branch is ready for PR.  If the item is not applicable, leave it blank.\n",
      "\n",
      "[ ] I have updated the documentation accordingly.\n",
      "\n",
      "[ ] I have added tests to cover my changes.\n",
      "\n",
      "[ ] All new and existing tests passed.\n",
      "\n",
      "[ ] My code follows the code style of this project.\n",
      "\n",
      "[ ] I ran the lint checks which produced no new errors nor warnings for my changes.\n",
      "\n",
      "[ ] I have checked to ensure there aren't other open Pull Requests for the same update/change.\n",
      "\n",
      "Does this introduce a breaking change?\n",
      "\n",
      "[ ] Yes\n",
      "\n",
      "[ ] No\n",
      "\n",
      "If this introduces a breaking change, please describe the impact and migration path for existing applications below.\n",
      "\n",
      "Testing\n",
      "\n",
      "\n",
      "chunkId: chunk7_4\n",
      "source: ..\\data\\docs\\code-with-dataops\\capabilities\\analytical-systems\\common-capabilities\\data-pipeline-operability\\index.md\n",
      "Score: 11.86177\n",
      "chunkContent: How do I empower project teams to build Data Products?\n",
      "\n",
      "Provide a Data Exploratory Environment\n",
      "\n",
      "Aside from the usual data zones, individual workspaces for data users and engineers should be setup using a section within the larger logical data lake. Furthermore, appropriate access to tools and environments for experimentation should be provided to them.\n",
      "\n",
      "More information:\n",
      "\n",
      "MLOps: Experimentation\n",
      "\n",
      "Facilitate data discovery and governance through a data catalog\n",
      "\n",
      "In order for users to start building data products, they need a way to discover and explore existing datasets available to them along with relevant business metadata, lineage and governance controls. Having a data catalog is imperative to have your data products easily discoverable and properly governed.\n",
      "\n",
      "More information:\n",
      "\n",
      "DataOps: Data Catalog\n",
      "\n",
      "Operationalizing the Data Platform\n",
      "\n",
      "How do I efficiently operate my data platform?\n",
      "\n",
      "Automate deployments and Testing\n",
      "\n",
      "To automate deployments and testing, all artifacts required to build the analytical system should be added to a source control system such as git. These artifacts include infrastructure-as-code, database objects (schema definitions, functions, stored procedures, etc.), application data, pipeline definitions, data validation and transformation logic. There should be a safe and repeatable process to deploy changes with automated tests through dev, test and production stages. This process is mainly to ensure correctness and to identify bugs early.\n",
      "\n",
      "More information:\n",
      "\n",
      "DataOps: Code-First Development\n",
      "\n",
      "\n",
      "chunkId: chunk162_3\n",
      "source: ..\\data\\docs\\code-with-engineering\\developer-experience\\going-further.md\n",
      "Score: 11.143963\n",
      "chunkContent: To use the docker-compose.yml file instead of Dockerfile, we need to adjust devcontainer.json with:\n",
      "\n",
      "{% raw %}\n",
      "\n",
      "json\n",
      "{\n",
      "    \"name\": \"My Application\",\n",
      "    \"dockerComposeFile\": [\"docker-compose.yml\"],\n",
      "    \"service\": \"my-workspace\"\n",
      "    ...\n",
      "}\n",
      "\n",
      "{% endraw %}\n",
      "\n",
      "This approach can be applied for many other tools by preparing what would be required. The idea is to simplify developers' lives and new developers joining the project.\n",
      "\n",
      "Custom tools\n",
      "\n",
      "While working on a project, any developer might end up writing a script to automate a task. This script can be in bash, python or whatever scripting language they are comfortable with.\n",
      "\n",
      "Let's say you want to ensure that all markdown files written are validated against specific rules you have set up. As we have seen above, you can include the tool markdownlint in your Dev Container . Having the tool installed does not mean developer will know how to use it!\n",
      "\n",
      "Consider the following solution structure:\n",
      "\n",
      "{% raw %}\n",
      "\n",
      "\n",
      "<iterator object azure.core.paging.ItemPaged at 0x2bc1a86ce10>\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I test my solution\"\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    select=[\"chunkId\", \"chunkContent\", \"source\"],\n",
    "    top=1,\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"chunkId: {result['chunkId']}\")\n",
    "    print(f\"source: {result['source']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(f\"chunkContent: {result['chunkContent']}\")\n",
    "    print(\"\\n\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform a vector search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunkId: chunk178_4\n",
      "source: ..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\team-agreements\\team-manifesto.md\n",
      "Score: 0.85869\n",
      "chunkContent: Tools\n",
      "\n",
      "Generally team sessions are enough for building a manifesto and having a consensus around it, and if there is a need for improving it in a structured way, there are many blogs and tools online, any retrospective tool can be used.\n",
      "\n",
      "Resources\n",
      "\n",
      "Technical Agility*\n"
     ]
    }
   ],
   "source": [
    "query = \"tools for software development\"\n",
    "\n",
    "query_embeddings = get_query_embedding(query)\n",
    "vector_query = VectorizedQuery(\n",
    "    vector=query_embeddings[0], k_nearest_neighbors=1, fields=\"chunkContentVector\"\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=None,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"chunkId\", \"chunkContent\", \"source\"],\n",
    "    top=1,\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"chunkId: {result['chunkId']}\")\n",
    "    print(f\"source: {result['source']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(f\"chunkContent: {result['chunkContent']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Perform a hybrid search\n",
    "\n",
    "Hybrid Retrieval brings out the best of Keyword and Vector Search\n",
    "\n",
    "Keyword and vector retrieval tackle search from different perspectives, which yield complementary capabilities. Vector retrieval semantically matches queries to passages with similar meanings. This is powerful because embeddings are less sensitive to misspellings, synonyms, and phrasing differences and can even work in cross lingual scenarios. Keyword search is useful because it prioritizes matching specific, important words that might be diluted in an embedding.\n",
    "\n",
    "User search can take many forms. Hybrid retrieval consistently brings out the best from both retrieval methods across query types. With the most effective L1, the L2 ranking step can significantly improve the quality of results in the top positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunkId: chunk80_12\n",
      "source: ..\\data\\docs\\code-with-dataops\\industry-solutions\\dataops-for-automotive\\dataops-avops-platform.md\n",
      "Score: 0.030751174315810204\n",
      "chunkContent: Similarly, there are rules set up in the ADLS blob storage for moving the data from \"Hot\" to \"Cold\" tier as data travels to \"DERIVED\" from \"RAW\" Zone.\n",
      "\n",
      "Solution also provides flexibility in terms of configurations for data lifecycle to meet the customer needs.\n",
      "\n",
      "Cosmos DB stores the structured data of this solution, that is the foundational units of this solution viz. Measurement and Datastream json files.\n",
      "\n",
      "The lineage information for Datastream is finally stored in the Cosmos DB. An API (Metadata API) is exposed over Cosmos DB to query the structured data and that also helps in tracking the data lineage.\n",
      "\n",
      "Cosmos DB serves multiple purposes for the AVOps platform. Not only does it store the structured metadata of Rosbag files, but it also allows multiple clients to work with the system through a storage layer manager (Metadata API).\n",
      "\n",
      "Key requirement with lineage tracking was to be able to reach to specific measurement/Rosbag file from processed data (or a Datastream). Decision of keeping lineage in Cosmos DB with Mongo graph query made it easy and simple to kick-start traceability support. Although, Purview could be the first choice for tracking the data lineage, it could have made the solution little more expensive, bulkier and involved some headache of writing custom APIs on top of cosmos for lineage (to integrate with it).\n",
      "\n",
      "Compute\n",
      "chunkId: chunk9_7\n",
      "source: ..\\data\\docs\\code-with-dataops\\capabilities\\analytical-systems\\common-capabilities\\storage\\index.md\n",
      "Score: 0.029462365433573723\n",
      "chunkContent: Data sets possess distinct lifecycle, with early stages characterized by frequent access to certain data. However, as the data ages, the need for access decreases dramatically. Some data remains unused in the cloud, with infrequent access once stored, while others may expire days or months after creation. Some data sets are actively read and modified throughout their lifecycle. Data can be transitioned to different access tiers or expired using Azure Storage Lifecycle Management's rule-based policies.\n",
      "\n",
      "Azure storage offers different access tiers namely hot, cool and archive. By default when data is created, it's stored under hot tier. The data then can be moved or deleted based on Storage Lifecycle Management rules, including an option to move it to cool or archive tiers.\n",
      "\n",
      "An important consideration when using storage tiers is the rehydration process from the archive tier. Reading data from archive tier requires data rehydration first. The rehydration process may be time-consuming and expensive. For more information, see overview of blob rehydration from the archive tier.\n",
      "\n",
      "A sample implementation can be found in MDW repo: Storage (Single). These Terraform based IaC templates demonstrate Azure storage accounts/containers creation and data lifecycle rule configuration at scale.\n",
      "\n",
      "See Lifecycle management overview for more information.\n",
      "\n",
      "Storage on Microsoft Fabric\n",
      "chunkId: chunk154_1\n",
      "source: ..\\data\\docs\\code-with-engineering\\design\\sustainability\\sustainable-action-disclaimers.md\n",
      "Score: 0.025402450934052467\n",
      "chunkContent: Running an edge device negates many of the benefits of hyperscale compute facilities, so considering the local energy grid mix and the typical timing of the workloads is important to determine if this is beneficial overall.  The larger volume of data that needs to be transmitted, the more this solution becomes appealing. For example, sending large amounts of audio and video content for processing.\n",
      "\n",
      "ACTION: Consider physically shipping data to the provider\n",
      "\n",
      "Shipping physical items has its own carbon impact, depending on the mode of transportation, which needs to be understood before making this decision.  The larger the volume of data that needs to be transmitted the more this options may be beneficial.\n",
      "\n",
      "ACTION: Consider the energy efficiency of languages\n",
      "\n",
      "When selecting a programming language, the most energy efficient programming language may not always be the best choice for development speed, maintenance, integration with dependent systems, and other project factors. But when deciding between languages that all meet the project needs, energy efficiency can be a helpful consideration.\n",
      "\n",
      "ACTION: Use caching policies\n",
      "\n",
      "A cache provides temporary storage of resources that have been requested by an application. Caching can improve application performance by reducing the time required to get a requested resource. Caching can also improve sustainability by decreasing the amount of network traffic.\n"
     ]
    }
   ],
   "source": [
    "query = \"scalable storage solution\"\n",
    "query_embeddings = query_embeddings = get_query_embedding(query)\n",
    "vector_query = VectorizedQuery(\n",
    "    vector=query_embeddings[0], k_nearest_neighbors=3, fields=\"chunkContentVector\"\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"chunkId\", \"chunkContent\", \"source\"],\n",
    "    top=1,\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"chunkId: {result['chunkId']}\")\n",
    "    print(f\"source: {result['source']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(f\"chunkContent: {result['chunkContent']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Perform a semantic hybrid search - Required Semantic Ranker enabled\n",
    "\n",
    "For this, we would first need to update our index and add semantic configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SemanticConfiguration,\n",
    "    SemanticSearch,\n",
    "    SemanticField,\n",
    "    SemanticPrioritizedFields,\n",
    ")\n",
    "\n",
    "\n",
    "def create_index(search_index_name):\n",
    "\n",
    "    client = SearchIndexClient(service_endpoint, credential)\n",
    "    index = client.get_index(search_index_name)\n",
    "\n",
    "    # 2. Define the semantic Settings\n",
    "    # Note: It requires semantic ranker enabled on your search service\n",
    "    # https://learn.microsoft.com/en-us/azure/search/semantic-search-overview\n",
    "    # https://learn.microsoft.com/en-us/azure/search/semantic-how-to-query-request?tabs=portal%2Cportal-query\n",
    "    # https://learn.microsoft.com/en-us/azure/search/semantic-how-to-query-request?tabs=sdk%2Cportal-query\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"my-semantic-config\",\n",
    "        prioritized_fields=SemanticPrioritizedFields(\n",
    "            # title_field=SemanticField(field_name=\"title\"),\n",
    "            # keywords_fields=[SemanticField(field_name=\"category\")],\n",
    "            content_fields=[SemanticField(field_name=\"chunkContent\")],\n",
    "        ),\n",
    "    )\n",
    "    semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "    index.semantic_search = semantic_search\n",
    "\n",
    "    result = client.create_or_update_index(index)\n",
    "    print(f\"{result.name} created or updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_chunks_2 created or updated\n"
     ]
    }
   ],
   "source": [
    "create_index(search_index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunkId: chunk80_13\n",
      "source: ..\\data\\docs\\code-with-dataops\\industry-solutions\\dataops-for-automotive\\dataops-avops-platform.md\n",
      "Score: 0.020846012979745865\n",
      "chunkContent: Azure Batch is the heart of compute in this solution. Azure Batch natively supports massively parallel processing, which fine-tunes the performance and turn-around time of data pipelines.\n",
      "\n",
      "For a sample Rosbag extraction process, multiple topics (lidar, radar, camera1, camera2 etc.) need to be extracted. It is designed to extract all the topics in with multiple jobs running in parallel.\n",
      "\n",
      "Azure Batch supports Azure spot VM instances, which reduces the cost of the Batch workloads. Setting up an auto-scale formula helps further in cost optimization.\n",
      "\n",
      "Solution provides flexibility in terms of hosting the compute for the API/services exposed.\n",
      "\n",
      "Azure Function can help optimize cost of burst kind of traffic scenarios, when services are idle for most of the time.\n",
      "\n",
      "AKS (Azure Kubernetes Service) can be also in consideration when there is a requirement of multiple microservice's communication.\n",
      "\n",
      "If it‚Äôs about the ease of usability, App service/ App service with containers can be used.\n",
      "\n",
      "References\n",
      "\n",
      "Microsoft's AVOps reference architecture\n",
      "\n",
      "AVOps Solution Kit: AVOps solution kit is a Dataops platform which helps customers to kick-start quickly on high scale data ingestion requirements.\n",
      "Reranker Score: 2.985140800476074\n",
      "Caption: <em>Azure Batch</em> is the heart of compute in this solution.<em> Azure Batch</em> natively supports massively parallel processing, which fine-tunes the performance and turn-around time of data pipelines. For a sample Rosbag extraction process, multiple topics (lidar, radar, camera1, camera2 etc.) need to be extracted.\n",
      "\n",
      "chunkId: chunk152_12\n",
      "source: ..\\data\\docs\\code-with-devsecops\\Enterprise-Solutions\\governance\\secure-software-supply-chain-for-containerized-workloads.md\n",
      "Score: 0.021601775661110878\n",
      "chunkContent: This implementation delivers a notation-based secure software supply chain in Azure built on Azure Kubernetes Service (AKS) and Azure Container Registry (ACR).\n",
      "\n",
      "Either Azure Pipelines or GitHub Actions can be used in the build phase to create the software release and security artifacts. Microsoft's sbom-tool is used to generate an SBOM from the source code, dependencies, and the container image packages. A vulnerability report is generated using Aquasec's Trivy tool (only supports Linux workloads). Notation is used to sign the container image and security artifacts with a signing cert stored in Azure Key Vault. The ORAS tool is used to bundle the signatures, security artifacts, and container image (software release), and then to publish the release bundle to Azure Container Registry.\n",
      "\n",
      "Policy enforcement is enabled in the deploy phase by using Open Policy Agent's Gatekeeper and the Ratify verification tool. Ratify acts as an external data provider for Gatekeeper and facilitates exposing the appropriate information from the release bundle to the policy engine. Ratify verifies component signatures using a root CA certificate stored in its configuration.\n",
      "\n",
      "The following policies are enforced:\n",
      "\n",
      "All images must be signed to ensure workload integrity\n",
      "\n",
      "All images must have an attached and signed SBOM and vulnerability scan result\n",
      "\n",
      "All images may only be retrieved from allowed list of container registries\n",
      "Reranker Score: 1.5704656839370728\n",
      "Caption: This implementation delivers a notation-based secure software supply chain in Azure built on<em> Azure Kubernetes Service</em> (AKS) and<em> Azure Container Registry</em> (ACR). Either Azure Pipelines or GitHub Actions can be used in the build phase to create the software release and security artifacts.\n",
      "\n",
      "chunkId: chunk150_3\n",
      "source: ..\\data\\docs\\code-with-devsecops\\Enterprise-Solutions\\Cloud-Native-Application-Bundle-(CNAB)-for-Azure-Trusted-Research-Environment-(TRE)\\index.md\n",
      "Score: 0.012048192322254181\n",
      "chunkContent: Duffle - Originally while the CNAB specification was developed, Duffle (duffle.sh) was used as a reference implementation to assist other people when developing their own implementations. Now that the CNAB v1 spec has been stable for years, it is no longer needed and has been archived.\n",
      "\n",
      "Porter is now the only open-source implementation of CNAB and is constantly being improved and maintained.\n",
      "\n",
      "Porter is Microsoft recommended tool for customers to use. It is a part of the CNCF (cloud native computing foundation) and is maintained by multiple companies in addition to individual community members.\n",
      "\n",
      "Case Studies\n",
      "\n",
      "EY teams uses Porter to combine different Azure components infrastructure into a single application bundle. This reduces the complexity of dependency management and allows EY teams to reduce the operational overhead of managing its hundreds of applications.\n",
      "\n",
      "Azure Trusted Research Environment (Azure TRE) - An OSS solution developed and maintained by CSE, which uses Porter to bundle deployment of Azure Resources and third-party solutions on Azure. We will see an example later on from this project.\n",
      "\n",
      "Building a bundle\n",
      "\n",
      "A Porter bundle is defined by a Porter manifest file named porter.yaml. The manifest defines metadata about the bundle, such as its name or what parameters it accepts. In addition, it also defines actions that the bundle can execute, like install, upgrade and uninstall along with custom actions.\n",
      "Reranker Score: 1.2104779481887817\n",
      "Caption: Azure Trusted Research Environment (Azure TRE) - An OSS solution developed and maintained by CSE, which uses Porter to bundle deployment of Azure Resources and third-party solutions on Azure. We will see an example later on from this project. Building a bundle  A Porter bundle is defined by a Porter manifest file named porter.yaml.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is azure sarch?\"\n",
    "\n",
    "query_embeddings = get_query_embedding(query)\n",
    "vector_query = VectorizedQuery(\n",
    "    vector=query_embeddings[0], k_nearest_neighbors=3, fields=\"chunkContentVector\"\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"chunkId\", \"chunkContent\", \"source\"],\n",
    "    query_type=QueryType.SEMANTIC,\n",
    "    semantic_configuration_name=\"my-semantic-config\",\n",
    "    query_caption=QueryCaptionType.EXTRACTIVE,\n",
    "    query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=1,\n",
    ")\n",
    "\n",
    "semantic_answers = results.get_answers()\n",
    "for answer in semantic_answers:\n",
    "    if answer.highlights:\n",
    "        print(f\"Semantic Answer: {answer.highlights}\")\n",
    "    else:\n",
    "        print(f\"Semantic Answer: {answer.text}\")\n",
    "    print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"chunkId: {result['chunkId']}\")\n",
    "    print(f\"source: {result['source']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(f\"chunkContent: {result['chunkContent']}\")\n",
    "    # print(f\"Title: {result['title']}\")\n",
    "    print(f\"Reranker Score: {result['@search.reranker_score']}\")\n",
    "\n",
    "    captions = result[\"@search.captions\"]\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\\n\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Conclusions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
