
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>3.1. Embeddings Experiment &#8212; AI Workshop</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '3.1.experiment_embedding';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="3.2. Chunking Experiment" href="3.2.experiment_chunking.html" />
    <link rel="prev" title="Experiments" href="3.0.experiments.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="0.intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="AI Workshop - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="AI Workshop - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="0.intro.html">
                    Welcome
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">1. Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1.setup.html">Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="1.rag-intro.html">RAG - Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="1.context.html">Context</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">2. Implementation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2.rag-implementation.html">RAG - Baseline implementation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">3. Experimentation</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="3.0.experiments.html">Experiments</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">3.1. Embeddings Experiment</a></li>
<li class="toctree-l1"><a class="reference internal" href="3.2.experiment_chunking.html">3.2. Chunking Experiment</a></li>
<li class="toctree-l1"><a class="reference internal" href="3.3.end_to_end_evaluation.html">3.3. End-to-end evaluation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">4. Production</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="4.post-production.html">Post-production</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="5.1.generation-qa.html">Generation of synthetic data</a></li>



</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F3.1.experiment_embedding.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/3.1.experiment_embedding.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>3.1. Embeddings Experiment</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-overview">Experiment Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-text-embedding-ada-002-from-openai">1. Use <code class="docutils literal notranslate"><span class="pre">text-embedding-ada-002</span></code> from OpenAI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-intfloat-e5-small-v2-from-hugging-face">2. Use <code class="docutils literal notranslate"><span class="pre">intfloat/e5-small-v2</span></code> from Hugging Face</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis">Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">üìà Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-two-indexes">üë©‚Äçüíª Create two indexes</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#create-a-new-index-and-upload-the-embeddings-created-with-intfloat-e5-small-v2-model">üë©‚Äçüíª Create a new index and upload the embeddings created with intfloat/e5-small-v2 model.</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-dataset">üìä Evaluation Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-metrics">üéØ Evaluation metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-the-system-using-text-embedding-ada-002-model">üë©‚Äçüíª 1. Evaluate the system using <em>text-embedding-ada-002</em> model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-the-system-using-infloat-e5-small-v2-model">üë©‚Äçüíª2. Evaluate the system using <em>infloat/e5-small-v2</em> model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusions">üí° Conclusions</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="embeddings-experiment">
<h1>3.1. Embeddings Experiment<a class="headerlink" href="#embeddings-experiment" title="Link to this heading">#</a></h1>
<p>According to <a class="reference external" href="https://mitsloan.mit.edu/ideas-made-to-matter/tapping-power-unstructured-data">multiple estimates</a>, 80% of data generated by businesses today is unstructured data such as text, images, or audio. This data has enormous potential for machine learning applications, but there is <em>some</em> work to be done before it can be used directly. <a class="reference external" href="https://medium.com/analytics-vidhya/introduction-to-word-embeddings-c2ba135dce2f">Embeddings</a> are the backbone of our system. Our goal is to understand how different embeddings have an impact on the returned results for a given query.</p>
<p>Which Embeddings Model to use?! Glad you asked! There are several options available:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://openai.com/blog/new-embedding-models-and-api-updates?ref=haihai.ai">OpenAI models</a>, such as: <a class="reference external" href="https://platform.openai.com/docs/guides/embeddings/what-are-embeddings">text-embedding-ada-002</a>, text-embedding-3-small, text-embedding-3-large</p></li>
<li><p>Open source models, which you can find at <a class="reference external" href="https://huggingface.co/models">HuggingFace</a>. The <a class="reference external" href="https://huggingface.co/spaces/mteb/leaderboard">MTEB Leaderboard</a> ranks the performance of embeddings models on a few axis, though not all models can be run locally.</p></li>
</ol>
<!-- relevancy of -->
<section id="experiment-overview">
<h2>Experiment Overview<a class="headerlink" href="#experiment-overview" title="Link to this heading">#</a></h2>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Topic</strong></p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>üìù <strong>Hypothesis</strong></p></td>
<td><p>Exploratory hypothesis: ‚ÄúCan introducing a new word embedding method improve the system‚Äôs performance?‚Äù</p></td>
</tr>
<tr class="row-odd"><td><p>‚öñÔ∏è <strong>Comparison</strong></p></td>
<td><p>We will compare <strong>text-embedding-ada-002</strong> (from OpenAI) and <strong>infloat/e5-small-v2</strong> (open-source)</p></td>
</tr>
<tr class="row-even"><td><p>üéØ <strong>Evaluation Metrics</strong></p></td>
<td><p>We will look at Accuracy and Cosine Similarity to compare the performance.</p></td>
</tr>
<tr class="row-odd"><td><p>üìä <strong>Data</strong></p></td>
<td><p>The data that we will use consists of <a class="reference internal" href="#../data/docs/code-with-engineering/"><span class="xref myst">code-with-engineering</span></a> and <a class="reference internal" href="#../data/docs/code-with-mlops/"><span class="xref myst">code-with-mlops</span></a> sections from Solution Ops repository which were previously pre-chunked in chunks of 180 tokens with 30% overlap <a class="reference download internal" download="" href="_downloads/a386584edd55612607aa4bbb3b7276f1/fixed-size-chunks-engineering-mlops-180-30.json"><span class="xref download myst">fixed-size-chunks-engineering-mlops-180-30.json</span></a>.</p></td>
</tr>
<tr class="row-even"><td><p>üìä <strong>Evaluation Dataset</strong></p></td>
<td><p><a class="reference download internal" download="" href="_downloads/bcdfa9bc085268b4c728951df4f5374c/qa_pairs_solutionops.json"><span class="xref download myst">300 question-answer</span></a> pairs generated from <a class="reference internal" href="#../data/docs/code-with-engineering/"><span class="xref myst">code-with-engineering</span></a> and <a class="reference internal" href="#../data/docs/code-with-mlops/"><span class="xref myst">code-with-mlops</span></a> sections from Solution Ops repository. See <a class="reference internal" href="5.1.generation-qa.html"><span class="std std-doc">Generation QA Notebook</span></a> for insights on how they were generated.</p></td>
</tr>
</tbody>
</table>
<!-- üìù **Hypothesis**

Exploratory hypothesis: "Can introducing a new word embedding method improve the system's performance?"

üéØ **Evaluation Metrics**

For this experiment we will look at Accuracy and Cosine Similarity to compare the performance. -->
<!-- As we highlighted in the `Chapter 3. Experiments`, our system has two components: the retrieval and the generative one. Take a moment to think what would be the part that would be impacted if we change the embedding model? <details markdown="1">

<summary> Hint:</summary>

Embeddings are used for transforming the input query from plain text into a vector, as well as for vectorizing the documents we have in our index. Therefore, it contributes to how well the system can retrieve relevant documents based on the input query and the documents. As mentioned in `Chapter 3. Experiments`, the evaluation metrics for this case will be accuracy, cosine similarity and Discounted cumulative gain.

</details> -->
<!-- üìä **Data**

In this experiment, the data that we would like to embed consists of the first 200 documents from the Solution Ops Playbook, which were previously chunked in size of 300. The dataset can be found at [chunks-solution-ops-200-300-0.json](./output/chunks-solution-ops-200-300-0.json). -->
<!-- ## üëÄ Get to know the data

Before we try out different embedding models, let's first try to understand the data. In what follows, you will see the data being clustered and keywords extracted from each cluster. To accomplish this, we performed Dimensionality Reduction, using [t-SNE](https://towardsdatascience.com/what-why-and-how-of-t-sne-1f78d13e224d). If you want to see the code we've been using to accomplish this, go to [t-SNE.ipynb](./helpers/t-SNE.ipynb). -->
<!-- # %run -i ./helpers/t-SNE.ipynb -->
<!-- As we have seen from the cluster from above, the data `can` be clustered, and the clusters seem to be different from one another. One is centered on data (sql, databricks) vs backlog related (stories, sprint, team) vs engineering fundamentals (security, testing, code). However, if we think about these clusters on a broader sense, they are part of one big cluster, which is IT. -->
</section>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Link to this heading">#</a></h2>
<p>Import necessary libraries</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">run</span> -i ./pre-requisites.ipynb

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluation dataset: </span><span class="si">{</span><span class="n">path_to_evaluation_dataset</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pre-chunked documents: </span><span class="si">{</span><span class="n">pregenerated_fixed_size_chunks</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Evaluation dataset: ./output/qa/evaluation/qa_pairs_solutionops.json
Pre-chunked documents: ./output/pre-generated/chunking/fixed-size-chunks-engineering-mlops-180-30.json
</pre></div>
</div>
</div>
</div>
</section>
<section id="use-text-embedding-ada-002-from-openai">
<h2>1. Use <code class="docutils literal notranslate"><span class="pre">text-embedding-ada-002</span></code> from OpenAI<a class="headerlink" href="#use-text-embedding-ada-002-from-openai" title="Link to this heading">#</a></h2>
<p>This model has a maximum token limit of <a class="reference external" href="https://platform.openai.com/docs/guides/embeddings/embedding-models">8191</a>. Usage is priced per input token, it is available either as Pay-As-You-Go or as Provisioned Throughput Units (PTUs) model. More price related info can be found <a class="reference external" href="https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/">here</a>.</p>
<p>Let‚Äôs create a function which is responsible to embed an input query using `text-embedding-ada-002. We will use the REST API, <a class="reference external" href="https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796#embeddings">here</a> you can see the documentation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span>



<span class="k">def</span> <span class="nf">oai_query_embedding</span><span class="p">(</span>
    <span class="n">query</span><span class="p">,</span>
    <span class="n">endpoint</span><span class="o">=</span><span class="n">azure_aoai_endpoint</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">azure_openai_key</span><span class="p">,</span>
    <span class="n">api_version</span><span class="o">=</span><span class="s2">&quot;2023-07-01-preview&quot;</span><span class="p">,</span>
    <span class="n">embedding_model_deployment</span><span class="o">=</span><span class="n">azure_openai_embedding_deployment</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Query the OpenAI Embedding model to get the embeddings for the given query.</span>
<span class="sd">    Args:</span>
<span class="sd">    query (str): The query for which to get the embeddings.</span>
<span class="sd">    endpoint (str): The endpoint for the OpenAI service.</span>
<span class="sd">    api_key (str): The API key for the OpenAI service.</span>
<span class="sd">    api_version (str): The API version for the OpenAI service.</span>
<span class="sd">    embedding_model_deployment (str): The deployment for the OpenAI embedding model.</span>

<span class="sd">    Returns:</span>

<span class="sd">    list: The embeddings for the given query.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">request_url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">endpoint</span><span class="si">}</span><span class="s2">/openai/deployments/</span><span class="si">{</span><span class="n">embedding_model_deployment</span><span class="si">}</span><span class="s2">/embeddings?api-version=</span><span class="si">{</span><span class="n">api_version</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span><span class="p">,</span> <span class="s2">&quot;api-key&quot;</span><span class="p">:</span> <span class="n">api_key</span><span class="p">}</span>
    <span class="n">request_payload</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">}</span>
    <span class="n">embedding_response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
        <span class="n">request_url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">request_payload</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">embedding_response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
        <span class="n">data_values</span> <span class="o">=</span> <span class="n">embedding_response</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s2">&quot;data&quot;</span><span class="p">]</span>
        <span class="n">embeddings_vectors</span> <span class="o">=</span> <span class="p">[</span><span class="n">data_value</span><span class="p">[</span><span class="s2">&quot;embedding&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">data_value</span> <span class="ow">in</span> <span class="n">data_values</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">embeddings_vectors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;failed to get embedding: </span><span class="si">{</span><span class="n">embedding_response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>üë©‚Äçüíª Try it out. Feel free to pass another query:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;Hello&quot;</span>

<span class="n">query_vectors</span> <span class="o">=</span> <span class="n">oai_query_embedding</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The query is: </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The embedded vector is: </span><span class="si">{</span><span class="n">query_vectors</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The length of the embedding is: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">query_vectors</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">Exception</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">line</span> <span class="mi">3</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;Hello&quot;</span>
<span class="ne">----&gt; </span><span class="mi">3</span> <span class="n">query_vectors</span> <span class="o">=</span> <span class="n">oai_query_embedding</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The query is: </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The embedded vector is: </span><span class="si">{</span><span class="n">query_vectors</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nn">Cell In[2], line 38,</span> in <span class="ni">oai_query_embedding</span><span class="nt">(query, endpoint, api_key, api_version, embedding_model_deployment)</span>
<span class="g g-Whitespace">     </span><span class="mi">36</span>     <span class="k">return</span> <span class="n">embeddings_vectors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="g g-Whitespace">     </span><span class="mi">37</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">38</span>     <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;failed to get embedding: </span><span class="si">{</span><span class="n">embedding_response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="ne">Exception</span>: failed to get embedding: {&#39;error&#39;: {&#39;code&#39;: &#39;401&#39;, &#39;message&#39;: &#39;Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.&#39;}}
</pre></div>
</div>
</div>
</div>
</section>
<section id="use-intfloat-e5-small-v2-from-hugging-face">
<h2>2. Use <code class="docutils literal notranslate"><span class="pre">intfloat/e5-small-v2</span></code> from Hugging Face<a class="headerlink" href="#use-intfloat-e5-small-v2-from-hugging-face" title="Link to this heading">#</a></h2>
<p>This model is open source and has a size 0.13 GB. The model is limited to working with English texts and can handle texts with a maximum of 512 tokens. Being open sourced, it means there is no price associated with it, you can download it locally, you can fine-tune it etc.
<a class="reference external" href="https://huggingface.co/intfloat/e5-small-v2#e5-small-v2">The embedding size is 384</a>.</p>
<p><strong>üë©‚Äçüíª Embed an input query using <code class="docutils literal notranslate"><span class="pre">e5-small-v2</span> <span class="pre">model</span></code></strong></p>
<p>Look at the <a class="reference external" href="https://huggingface.co/intfloat/e5-small-v2">&lt;/&gt; Use in sentence-transformers</a> section from Hugging Face.</p>
<details markdown="1">
<summary> üîç Solution. Expand this only if you got stuck: </summary>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="nb">input</span> <span class="o">=</span> <span class="s2">&quot;Hello&quot;</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;intfloat/e5-small-v2&quot;</span><span class="p">)</span>
<span class="n">embedded_input</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">normalize_embeddings</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The query is: </span><span class="si">{</span><span class="nb">input</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The embedded vector is: </span><span class="si">{</span><span class="n">embedded_input</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The length of the embedding is: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">embedded_input</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</details>
</section>
<section id="analysis">
<h2>Analysis<a class="headerlink" href="#analysis" title="Link to this heading">#</a></h2>
<p>So far we have been looking at two different embedding models and we‚Äôve listed some of their characteristics. Let‚Äôs try now to evaluate how well each model performs in our context. For this, we should first create embeddings from our data.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the interest of time, we have pre-generated the embeddings for you:</p>
<ul class="simple">
<li><p>The pre-generated embeddings using <code class="docutils literal notranslate"><span class="pre">intfloat/e5-small-v2</span></code> can be found at <a class="reference download internal" download="" href="_downloads/83a3dcff04741dbdffe38a0f7e227f3a/fixed-size-chunks-180-30-engineering-mlops-e5-small-v2.json"><span class="xref download myst">fixed-size-chunks-180-30-engineering-mlops-e5-small-v2.json</span></a></p></li>
<li><p>The pre-generated embeddings using <code class="docutils literal notranslate"><span class="pre">text-embedding-ada-002</span></code> can be found at <a class="reference download internal" download="" href="_downloads/d295485e187eb076b477aeb4e4dd33d7/fixed-size-chunks-180-30-engineering-mlops-ada.json"><span class="xref download myst">fixed-size-chunks-180-30-engineering-mlops-ada.json</span></a></p></li>
</ul>
</div>
<p>Let‚Äôs load the path to each file. Note the name of variables:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">run</span> -i ./pre-requisites.ipynb

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pre-generated embeddings using intfloat/e5-small-vs: </span><span class="si">{</span><span class="n">pregenerated_fixed_size_chunks_embeddings_os</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pre-generated embeddings using text-embedding-ada-002: </span><span class="si">{</span><span class="n">pregenerated_fixed_size_chunks_embeddings_ada</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Pre-generated embeddings using intfloat/e5-small-vs: ./output/pre-generated/embeddings/fixed-size-chunks-180-30-engineering-mlops-e5-small-v2.json
Pre-generated embeddings using text-embedding-ada-002: ./output/pre-generated/embeddings/fixed-size-chunks-180-30-engineering-mlops-ada.json
</pre></div>
</div>
</div>
</div>
</section>
<section id="evaluation">
<h2>üìà Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading">#</a></h2>
<p>In this workshop, to separate our experiments, we will take the <strong>Full Reindex</strong> strategy and we will create a new index per embedding model.
Therefore, for each embedding model we will:</p>
<ol class="arabic simple">
<li><p>Create a new index. Note: make sure to give a relevant name.</p></li>
<li><p>Populate the index with the embeddings that you have generated at the previous steps.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can reuse available functions from <a class="reference internal" href="helpers/search.html"><span class="std std-doc">./helpers/search.ipynb</span></a>, such as: <em>create_index</em> and <em>upload_data</em>.</p>
</div>
<section id="create-two-indexes">
<h3>üë©‚Äçüíª Create two indexes<a class="headerlink" href="#create-two-indexes" title="Link to this heading">#</a></h3>
<p>By running the next cell, all the functions from search.ipynb will become available:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%capture</span> --no-display
<span class="o">%</span><span class="n">run</span> <span class="o">-</span><span class="n">i</span> <span class="o">./</span><span class="n">helpers</span><span class="o">/</span><span class="n">search</span><span class="o">.</span><span class="n">ipynb</span>
</pre></div>
</div>
</div>
</div>
<p>Sample code for creating a new index and uploading the data which was previously embedded using AOI model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Create a new index</span>
<span class="c1"># TODO: Replace the prefix with a relevant name given your embedding model</span>
<span class="n">new_index_name</span> <span class="o">=</span> <span class="s2">&quot;fixed-size-chunks-180-30-engineering-mlops-ada&quot;</span>
<span class="n">vector_size</span> <span class="o">=</span> <span class="mi">1536</span>  <span class="c1"># TODO: Replace with the vector size of your embedding model</span>
<span class="n">create_index</span><span class="p">(</span><span class="n">new_index_name</span><span class="p">)</span>

<span class="c1"># Uncomment the following when running the cell:</span>
<span class="c1"># # 2. Upload the embeddings to the new index</span>
<span class="c1"># # TODO: Replace the embeddings_file_path to point to the right file path</span>
<span class="c1"># embeddings_file_path = pregenerated_fixed_size_chunks_embeddings_ada</span>
<span class="c1"># upload_data(file_path=embeddings_file_path,</span>
<span class="c1">#             search_index_name=new_index_name)</span>
</pre></div>
</div>
</div>
</div>
<section id="create-a-new-index-and-upload-the-embeddings-created-with-intfloat-e5-small-v2-model">
<h4>üë©‚Äçüíª Create a new index and upload the embeddings created with intfloat/e5-small-v2 model.<a class="headerlink" href="#create-a-new-index-and-upload-the-embeddings-created-with-intfloat-e5-small-v2-model" title="Link to this heading">#</a></h4>
<details markdown="1">
<summary> üîç Solution. Expand this only if you got stuck: </summary>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># 1. Create a new index</span>
<span class="n">new_index_name</span> <span class="o">=</span> <span class="s2">&quot;fixed-size-chunks-180-30-engineering-mlops-e5-small-v2&quot;</span>
<span class="n">vector_size</span> <span class="o">=</span> <span class="mi">384</span>  <span class="c1"># TODO: Replace with the vector size of your embedding model</span>
<span class="n">create_index</span><span class="p">(</span><span class="n">new_index_name</span><span class="p">,</span> <span class="n">vector_size</span><span class="p">)</span>

<span class="c1"># 2. Upload the embeddings to the new index</span>
<span class="n">embeddings_file_path</span> <span class="o">=</span> <span class="n">pregenerated_fixed_size_chunks_emebddings_os</span>
<span class="n">upload_data</span><span class="p">(</span><span class="n">file_path</span><span class="o">=</span><span class="n">embeddings_file_path</span><span class="p">,</span> <span class="n">search_index_name</span><span class="o">=</span><span class="n">new_index_name</span><span class="p">)</span>
</pre></div>
</div>
</details>
</section>
</section>
<section id="evaluation-dataset">
<h3>üìä Evaluation Dataset<a class="headerlink" href="#evaluation-dataset" title="Link to this heading">#</a></h3>
<p>Note: The evaluation dataset can be found at <a class="reference download internal" download="" href="_downloads/bcdfa9bc085268b4c728951df4f5374c/qa_pairs_solutionops.json"><span class="xref download myst">qa_pairs_solutionops.json</span></a>. The format is:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="nt">&quot;user_prompt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="err">The</span><span class="w"> </span><span class="err">ques</span><span class="kc">t</span><span class="err">io</span><span class="kc">n</span>
<span class="nt">&quot;output_prompt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="err">The</span><span class="w"> </span><span class="err">a</span><span class="kc">ns</span><span class="err">wer</span>
<span class="nt">&quot;context&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="err">The</span><span class="w"> </span><span class="err">releva</span><span class="kc">nt</span><span class="w"> </span><span class="err">piece</span><span class="w"> </span><span class="err">o</span><span class="kc">f</span><span class="w"> </span><span class="err">i</span><span class="kc">nf</span><span class="err">orma</span><span class="kc">t</span><span class="err">io</span><span class="kc">n</span><span class="w"> </span><span class="kc">fr</span><span class="err">om</span><span class="w"> </span><span class="err">a</span><span class="w"> </span><span class="err">docume</span><span class="kc">nt</span>
<span class="nt">&quot;chunk_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="err">The</span><span class="w"> </span><span class="err">ID</span><span class="w"> </span><span class="err">o</span><span class="kc">f</span><span class="w"> </span><span class="kc">t</span><span class="err">he</span><span class="w"> </span><span class="err">chu</span><span class="kc">n</span><span class="err">k</span>
<span class="nt">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="err">The</span><span class="w"> </span><span class="err">pa</span><span class="kc">t</span><span class="err">h</span><span class="w"> </span><span class="kc">t</span><span class="err">o</span><span class="w"> </span><span class="kc">t</span><span class="err">he</span><span class="w"> </span><span class="err">docume</span><span class="kc">nt</span><span class="p">,</span><span class="w"> </span><span class="err">i.e.</span><span class="w"> </span><span class="s2">&quot;..\\data\\docs\\code-with-dataops\\index.md&quot;</span>
</pre></div>
</div>
</section>
<section id="evaluation-metrics">
<h3>üéØ Evaluation metrics<a class="headerlink" href="#evaluation-metrics" title="Link to this heading">#</a></h3>
<p>Let us try to evaluate our baseline model. We will have two metrics:</p>
<ul>
<li><p>Cosine similarity:</p>
<p>Using cosine similarity we will calculate how similar in meaning is the first text that was retrieved from the search index compare to the text that was used to formulate the question (and hence, to answer to it). Note: our search index returns the top 3 nearest neighbors, but we will look at the first retrieved one. We will then calculate the mean and median cosine across our evaluation dataset.</p>
</li>
<li><p>Accuracy:</p>
<p>By accuracy we mean how many times the search returned the document (the file path to the document) which we expected in our evaluation dataset. We will return the percentage of successfully retrieved documents across our evaluation dataset.</p>
<!-- `Retrieval_evaluation` function is going through the evaluation dataset and, for each `user_prompt`, it embeds it using the `embedding_function` passed as parameter and then it does a vector search in the Index with name `search_index_name`. If the retrieved documents includes the `source` from the evaluation dataset, then it is considered a success. Note: This can also be adapted to ensure that the `first` retrieved document is the expected one. -->
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">norm</span>


<span class="k">def</span> <span class="nf">calculate_cosine_similarity</span><span class="p">(</span><span class="n">expected_document_vector</span><span class="p">,</span> <span class="n">retrieved_document_vector</span><span class="p">):</span>
    <span class="n">cosine_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">expected_document_vector</span><span class="p">,</span> <span class="n">retrieved_document_vector</span><span class="p">)</span> <span class="o">/</span> \
        <span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">expected_document_vector</span><span class="p">)</span><span class="o">*</span><span class="n">norm</span><span class="p">(</span><span class="n">retrieved_document_vector</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">cosine_sim</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">ntpath</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">norm</span>


<span class="k">def</span> <span class="nf">calculate_metrics</span><span class="p">(</span><span class="n">evaluation_data_path</span><span class="p">,</span> <span class="n">embedding_function</span><span class="p">,</span> <span class="n">search_index_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Evaluate the retrieval performance of the search index using the evaluation data set.</span>
<span class="sd">    Args:</span>
<span class="sd">    evaluation_data_path (str): The path to the evaluation data set.</span>
<span class="sd">    embedding_function (function): The function to use for embedding the question.</span>
<span class="sd">    search_index_name (str): The name of the search index to use for retrieval.</span>

<span class="sd">    Returns:</span>
<span class="sd">    list: The cosine similarities between the expected documents and the top retrieved documents.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">evaluation_data_path</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The path to the evaluation data set </span><span class="si">{</span><span class="n">evaluation_data_path</span><span class="si">}</span><span class="s2"> does not exist. Please check the path and try again.&quot;</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="n">nr_correctly_retrieved_documents</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">nr_qa</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">cosine_similarities</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">evaluation_data_path</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">evaluation_data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">evaluation_data</span><span class="p">:</span>
            <span class="n">user_prompt</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;user_prompt&quot;</span><span class="p">]</span>
            <span class="n">expected_document</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;source&quot;</span><span class="p">]</span>
            <span class="n">expected_document_vector</span> <span class="o">=</span> <span class="n">embedding_function</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">])</span>

            <span class="c1"># 1. Search in the index</span>
            <span class="n">search_response</span> <span class="o">=</span> <span class="n">search_documents</span><span class="p">(</span>
                <span class="n">search_index_name</span><span class="o">=</span><span class="n">search_index_name</span><span class="p">,</span>
                <span class="nb">input</span><span class="o">=</span><span class="n">user_prompt</span><span class="p">,</span>
                <span class="n">embedding_function</span><span class="o">=</span><span class="n">embedding_function</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">retrieved_documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">ntpath</span><span class="o">.</span><span class="n">normpath</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s2">&quot;source&quot;</span><span class="p">])</span>
                                   <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">search_response</span><span class="p">]</span>
            <span class="n">top_retrieved_document</span> <span class="o">=</span> <span class="n">search_response</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;chunkContentVector&quot;</span><span class="p">]</span>

            <span class="c1"># 2. Calculate cosine similarity between the expected document and the top retrieved document</span>
            <span class="n">cosine_similarity</span> <span class="o">=</span> <span class="n">calculate_cosine_similarity</span><span class="p">(</span>
                <span class="n">expected_document_vector</span><span class="p">,</span> <span class="n">top_retrieved_document</span><span class="p">)</span>
            <span class="n">cosine_similarities</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cosine_similarity</span><span class="p">)</span>

            <span class="c1"># 3. If the expected document is part of the retrieved documents,</span>
            <span class="c1"># we will consider it correctly retrieved</span>
            <span class="k">if</span> <span class="n">ntpath</span><span class="o">.</span><span class="n">normpath</span><span class="p">(</span><span class="n">expected_document</span><span class="p">)</span> <span class="ow">in</span> <span class="n">retrieved_documents</span><span class="p">:</span>
                <span class="n">nr_correctly_retrieved_documents</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="n">nr_qa</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">nr_correctly_retrieved_documents</span> <span class="o">/</span> <span class="n">nr_qa</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">}</span><span class="s2">% of the documents were correctly retrieved from Index </span><span class="si">{</span><span class="n">index_name</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">cosine_similarities</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">run</span> -i ./pre-requisites.ipynb
</pre></div>
</div>
</div>
</div>
</section>
<section id="evaluate-the-system-using-text-embedding-ada-002-model">
<h3>üë©‚Äçüíª 1. Evaluate the system using <em>text-embedding-ada-002</em> model<a class="headerlink" href="#evaluate-the-system-using-text-embedding-ada-002-model" title="Link to this heading">#</a></h3>
<details markdown="1">
<summary> üîç Sample code. Feel free to expand it. It may take up to 4 minutes to run: </summary>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># TODO: Replace the prefix with a relevant name given your embedding model</span>
<span class="kn">from</span> <span class="nn">statistics</span> <span class="kn">import</span> <span class="n">mean</span><span class="p">,</span> <span class="n">median</span>

<span class="n">index_name</span> <span class="o">=</span> <span class="s2">&quot;fixed-size-chunks-180-30-engineering-mlops-ada&quot;</span>

<span class="n">cosine_similarities</span> <span class="o">=</span> <span class="n">calculate_metrics</span><span class="p">(</span>
    <span class="n">evaluation_data_path</span><span class="o">=</span><span class="n">path_to_evaulation_dataset</span><span class="p">,</span>
    <span class="n">embedding_function</span><span class="o">=</span><span class="n">oai_query_embedding</span><span class="p">,</span>
    <span class="n">search_index_name</span><span class="o">=</span><span class="n">index_name</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">avg_score</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">cosine_similarities</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Avg cosine similarity score:</span><span class="si">{</span><span class="n">avg_score</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">median_score</span> <span class="o">=</span> <span class="n">median</span><span class="p">(</span><span class="n">cosine_similarities</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Median cosine similarity score: </span><span class="si">{</span><span class="n">median_score</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</details>
</section>
<section id="evaluate-the-system-using-infloat-e5-small-v2-model">
<h3>üë©‚Äçüíª2. Evaluate the system using <em>infloat/e5-small-v2</em> model<a class="headerlink" href="#evaluate-the-system-using-infloat-e5-small-v2-model" title="Link to this heading">#</a></h3>
<p>Using the <code class="docutils literal notranslate"><span class="pre">calculate_metrics</span></code> function, calculate the metrics using the infloat/e5-small-v2 open source model.</p>
<details markdown="1">
<summary> üîç Sample code. It may take up to 4 minutes to run:</summary>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># TODO: Replace the prefix with a relevant name given your embedding model</span>
<span class="n">index_name</span> <span class="o">=</span> <span class="s2">&quot;fixed-size-chunks-180-30-engineering-mlops-e5-small-v2&quot;</span>
<span class="n">cosine_similarities</span> <span class="o">=</span> <span class="n">calculate_metrics</span><span class="p">(</span>
    <span class="n">evaluation_data_path</span><span class="o">=</span><span class="n">path_to_evaluation_dataset</span><span class="p">,</span>
    <span class="n">embedding_function</span><span class="o">=</span><span class="n">embed_chunk</span><span class="p">,</span>
    <span class="n">search_index_name</span><span class="o">=</span><span class="n">index_name</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">avg_score</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">cosine_similarities</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Avg score:</span><span class="si">{</span><span class="n">avg_score</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">median_score</span> <span class="o">=</span> <span class="n">median</span><span class="p">(</span><span class="n">cosine_similarities</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Median score: </span><span class="si">{</span><span class="n">median_score</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</details>
</section>
</section>
<section id="conclusions">
<h2>üí° Conclusions<a class="headerlink" href="#conclusions" title="Link to this heading">#</a></h2>
<p>What conclusions can you reach? Are you surprised by the results? In what cases would you find useful to use the open source model? Discuss these questions and any other ideas you may have with your colleagues.</p>
<details markdown="1">
<summary> Possible conclusions. Expand this only after you reached your own conclusions: </summary>
<p><img alt="results.png" src="_images/results-embedding.png" /></p>
<p>Open source models can be useful when you need more control over the model, such as running it offline, fine-tuning it, or customizing it for your specific needs. As it was proven in our experiment, the trade-off is excellent. However, open source models may require more engineering effort, have lower performance on some tasks, and have less safety and content filtering features than closed source models.</p>
</details>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="3.0.experiments.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Experiments</p>
      </div>
    </a>
    <a class="right-next"
       href="3.2.experiment_chunking.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">3.2. Chunking Experiment</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-overview">Experiment Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-text-embedding-ada-002-from-openai">1. Use <code class="docutils literal notranslate"><span class="pre">text-embedding-ada-002</span></code> from OpenAI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-intfloat-e5-small-v2-from-hugging-face">2. Use <code class="docutils literal notranslate"><span class="pre">intfloat/e5-small-v2</span></code> from Hugging Face</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis">Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">üìà Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-two-indexes">üë©‚Äçüíª Create two indexes</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#create-a-new-index-and-upload-the-embeddings-created-with-intfloat-e5-small-v2-model">üë©‚Äçüíª Create a new index and upload the embeddings created with intfloat/e5-small-v2 model.</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-dataset">üìä Evaluation Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-metrics">üéØ Evaluation metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-the-system-using-text-embedding-ada-002-model">üë©‚Äçüíª 1. Evaluate the system using <em>text-embedding-ada-002</em> model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-the-system-using-infloat-e5-small-v2-model">üë©‚Äçüíª2. Evaluate the system using <em>infloat/e5-small-v2</em> model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusions">üí° Conclusions</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Raouf Aliouat & Adina Stoll
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>