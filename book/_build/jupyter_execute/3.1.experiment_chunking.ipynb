{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Chunking - Ingestion\n",
    "\n",
    "The search solution is comprised of both **ingestion** and **retrieval**. One does not exist without the other.\n",
    "\n",
    "While the other experiments are focused on data retrieval, ingestion plays equal importance in the effectiveness of the search solution.\n",
    "\n",
    "Certain aspects of data ingestion need to be experimented as part of the experimentation phase:\n",
    "\n",
    "```{note}\n",
    "[Learnings from other engagements](https://github.com/microsoft/rag-openai/blob/main/topics/RAG_EnablingSearch.md#ingestion-and-retrieval)\n",
    "Other pre and post-processing techniques include: Optical Character Recognition, data conversation, use of Azure Form Recognizer to extract information from the documents, chunking, summarization, post-processing to make data more \"human like\", video captioning, speech to text, tagging, etc. are all methods that need to be considered and experimented with as part of the ingestion pipeline experimentation.\n",
    "\n",
    "On another hand, choosing the final mechanism to ingest data is a decision that can wait until experimentation is completed: for instance, if the data repository will be Azure Cognitive Search, the dev team has options such as: using native indexers, building durable functions, using Azure Machine Learning pipelines, or even using Azure Data Factory. All of them are viable options.\n",
    "\n",
    "These decisions, however, can, and should, wait until the tool itself is chosen, and the pre and post-processing operations including tagging and other document enrichment are well defined\n",
    "```\n",
    "\n",
    "https://github.com/microsoft/rag-openai/blob/main/topics/RAG_EnablingSearch.md#learnings-from-engagements-1\n",
    "\n",
    "When processing data, splitting the source documents into chunks requires care and expertise to ensure the resulting chunks are small enough to be effective during fact retrieval but not too small so that enough context is provided during summarization.\n",
    "\n",
    "```{note}\n",
    "Our goal here is not to identify which chunking strategy is the “best” in general but rather to demonstrate how various choices of chunking may have a non-trivial impact on the ultimate outcome from the retrieval-augmented-generation solution.\n",
    "```\n",
    "\n",
    "[Learnings fromm other engagements](https://github.com/microsoft/rag-openai/blob/main/topics/RAG_EnablingSearch.md#learnings-from-engagements-1)\n",
    "\n",
    "<!-- https://vectara.com/blog/grounded-generation-done-right-chunking/#:~:text=In%20the%20context%20of%20Grounded%20Generation%2C%20chunking%20is,find%20natural%20segments%20like%20complete%20sentences%20or%20paragraphs. -->\n",
    "\n",
    "## Why Chunking Size Matters\n",
    "\n",
    "As mentioned [here](https://learn.microsoft.com/en-us/azure/search/semantic-search-overview), the models used to generate embedding vectors have maximum limits on the text fragments provided as input. For example, the maximum length of input text for the Azure OpenAI embedding models is **8,191** tokens. Given that each token is around 4 characters of text for common OpenAI models, this maximum limit is equivalent to around 6000 words of text. If you're using these models to generate embeddings, it's critical that the input text stays under the limit. Partitioning your content into chunks ensures that your data can be processed by the Large Language Models (LLM) used for indexing and queries.\n",
    "\n",
    "**Relevance and Granularity**: A small chunk size, like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the similarity _top_k_ setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the _Faithfulness and Relevancy_ metrics. These measure the absence of ‘hallucinations’ and the ‘relevancy’ of responses based on the query and the retrieved contexts respectively.\n",
    "\n",
    "**Response Generation Time**: As the chunk_size increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.\n",
    "\n",
    "In essence, determining the optimal chunk_size is about striking a balance: capturing all essential information without sacrificing speed. It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use case and dataset.\n",
    "\n",
    "https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5\n",
    "\n",
    "Example code: https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/data-chunking/textsplit-data-chunking-example.ipynb\n",
    "\n",
    "Read [Common Chunking Technique](https://learn.microsoft.com/en-us/azure/search/semantic-search-overview), [Content overlap considerations](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents#content-overlap-considerations), [Simple example of how to create chunks with sentences](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-chunk-documents#content-overlap-considerations)\n",
    "\n",
    "CODE: https://github.com/microsoft/rag-openai/blob/438999a5470bef7946fa1c8714ed1090e1ed40c3/samples/searchEvaluation/customskills/utils/chunker/text_chunker.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%pip install langchain-community==0.0.18\n",
    "# %pip install langchain-core==0.1.20\n",
    "%pip install unstructured==0.12.3\n",
    "%pip install unstructured-client==0.17.0\n",
    "%pip install langchain==0.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import glob\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "import os\n",
    "# Code also https://github.com/microsoft/rag-openai/blob/438999a5470bef7946fa1c8714ed1090e1ed40c3/samples/searchEvaluation/customskills/utils/chunker/text_chunker.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_folder(path, totalNumberOfDocuments=200) -> list[str]:\n",
    "    print(\"Loading documents...\")\n",
    "    markdown_documents = []\n",
    "    i = 0\n",
    "    for file in tqdm.tqdm(glob.glob(path, recursive=True)):\n",
    "        loader = UnstructuredFileLoader(file)\n",
    "        document = loader.load()\n",
    "        markdown_documents.append(document)\n",
    "        if i == totalNumberOfDocuments:\n",
    "            return markdown_documents\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\adnegrau\\AppData\\Local\\Temp\\ipykernel_60352\\3463060496.py:8: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  documents = load_documents_from_folder(\"..\\data\\docs\\**\\*.md\", totalNumberOfDocuments)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def create_chunks_and_save_to_file(path_to_output, totalNumberOfDocuments=200, chunk_size=300, chunk_overlap=30) -> list:\n",
    "    try:\n",
    "        if(os.path.exists(path_to_output)):\n",
    "            print(f\"Chunks already created at: {path_to_output} \")\n",
    "            return\n",
    "        \n",
    "        documents = load_documents_from_folder(\"..\\data\\docs\\**\\*.md\", totalNumberOfDocuments)\n",
    "\n",
    "        print(\"Creating chunks...\")\n",
    "        markdown_splitter = MarkdownTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        lengths = {}\n",
    "        all_chunks = []\n",
    "        chunk_id = 0\n",
    "        for document in tqdm.tqdm(documents):\n",
    "            current_chunks_text_list = markdown_splitter.split_text(\n",
    "                document[0].page_content\n",
    "            )  # output = [\"content chunk1\", \"content chunk2\", ...]\n",
    "\n",
    "            for i, chunk in enumerate(\n",
    "                current_chunks_text_list\n",
    "            ):  # (0, \"content chunk1\"), (1, \"content chunk2\"), ...\n",
    "                current_chunk_dict = {\n",
    "                    \"chunkId\": f\"chunk{chunk_id}_{i}\",\n",
    "                    \"chunkContent\": chunk,\n",
    "                    \"source\": document[0].metadata[\"source\"],\n",
    "                }\n",
    "                all_chunks.append(current_chunk_dict)\n",
    "\n",
    "            chunk_id += 1\n",
    "\n",
    "            n_chunks = len(current_chunks_text_list)\n",
    "            # lengths = {[Number of chunks]: [number of documents with that number of chunks]}\n",
    "            if n_chunks not in lengths:\n",
    "                lengths[n_chunks] = 1\n",
    "            else:\n",
    "                lengths[n_chunks] += 1\n",
    "\n",
    "        with open(path_to_output, \"w\") as f:\n",
    "            json.dump(all_chunks, f)\n",
    "        print(f\"Chunks created: \", lengths)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating chunks: {e}\")\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the chunks\n",
    "\n",
    "Note:\n",
    "\n",
    "- we are only chunking the first `totalNumberOfDocuments` from `..\\data\\docs\\**\\*.md`\n",
    "- `chunk_size` is the number of tokens a chunk should have\n",
    "- `chunk_overlap` is the percentage of overlap between two chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks already created at: ./output/chunks-solution-ops-200.json \n"
     ]
    }
   ],
   "source": [
    "totalNumberOfDocuments = 200\n",
    "path_to_chunks_output = f\"./output/chunks-solution-ops-{totalNumberOfDocuments}.json\"\n",
    "chunks = create_chunks_and_save_to_file(path_to_chunks_output, totalNumberOfDocuments, chunk_size=300, chunk_overlap=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, to separate our experiments, we will take the _Full Reindex_ strategy by creating a new index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:18: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\adnegrau\\AppData\\Local\\Temp\\ipykernel_60352\\2823348645.py:18: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  documents = load_documents_from_folder(\"..\\data\\docs\\**\\*.md\", totalNumberOfDocuments)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 'solution-ops-chunking-300-30' created or updated\n",
      "Embeddings were already created for chunked data at: ./output/chunks-solution-ops-200.json \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 991 documents to Index: solution-ops-chunking-300-30\n"
     ]
    }
   ],
   "source": [
    "%run -i ./helpers/search.ipynb\n",
    "\n",
    "# 1. Create the new index\n",
    "new_index_name = \"solution-ops-chunking-300-30\"\n",
    "create_index(new_index_name)\n",
    "\n",
    "# 2. Generate embeddings for the new chunks\n",
    "generated_embeddings_path = f\"./output/chunks-solution-ops-embedded-{totalNumberOfDocuments}.json\"\n",
    "generate_embeddings_for_chunks_and_save_to_file(path_to_chunks_file=path_to_chunks_output, path_to_output=generated_embeddings_path)\n",
    "\n",
    "# 3. Upload the embeddings to the new index\n",
    "upload_data(file_path=generated_embeddings_path, search_index_name=new_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic chunking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in skillset: SplitSkill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload files to a storage account so we can create an Indexer\n",
    "https://github.com/microsoft/rag-openai/blob/438999a5470bef7946fa1c8714ed1090e1ed40c3/samples/searchEvaluation/upload_files.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}