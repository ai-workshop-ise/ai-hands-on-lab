{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG - Implementation\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this part, we will build the building blocks of a RAG solution.\n",
    "\n",
    "1. Creation of a Search Index\n",
    "2. Upload of data\n",
    "3. Perform search\n",
    "4. Creation of a prompt\n",
    "5. Wire everything together\n",
    "\n",
    "<!-- To create the index we need the following objects:\n",
    "\n",
    "- Data Source - a `link` to some data storage\n",
    "- Azure Index - defines the data structure over which to search\n",
    "  - Create an empty index based on an index schema\n",
    "  - Fill in the data using the Search Indexer (below\\_)\n",
    "- Azure Search Indexer - which acts as a crawler that retrieves data from external sources, can also trigger skillsets (Optical Character Recognition) -->\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of this section is to familiarize yourself with RAG in a hands-on way, so that later on we can experiment with different aspects.\n",
    "\n",
    "This will also represent a baseline for our RAG application.\n",
    "\n",
    "## Setup\n",
    "\n",
    "<!-- First, we install the necessary dependencies.\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/azure/chat_with_your_own_data.ipynb -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "%run -i ./pre-requisites.ipynb\n",
    "%run -i ./helpers/search.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries and environment variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    VectorSearchProfile,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearch,\n",
    "    HnswParameters\n",
    ")\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "import os.path\n",
    "\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-07-01-preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a Search Index\n",
    "\n",
    "<!-- https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples/sample_index_crud_operations.py\n",
    "\n",
    "https://github.com/microsoft/rag-experiment-accelerator/blob/development/rag_experiment_accelerator/init_Index/create_index.py\n",
    "\n",
    "Used for overall Fields and Semantic Settings inspiration - https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/azure-search-vector-python-huggingface-model-sample.ipynb\n",
    "\n",
    "Used for SearchField inspiration - https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/search/azure-search-documents/samples/sample_vector_search.py -->\n",
    "\n",
    "For those familiar with relational databases, you can imagine that:\n",
    "\n",
    "- A (search) index ~= A table\n",
    "  - it describes the [schema of your data](https://learn.microsoft.com/en-us/azure/search/search-what-is-an-index#schema-of-a-search-index)\n",
    "  - it consists of [`field definitions`](https://learn.microsoft.com/en-us/azure/search/search-what-is-an-index#field-definitions) described by [`field attributes`](https://learn.microsoft.com/en-us/azure/search/search-what-is-an-index#field-attributes) (searchable, filterable, sortable etc)\n",
    "- A (search) document ~= A row in your table\n",
    "\n",
    "In our case, we would like to represent the following:\n",
    "\n",
    "| Field              | Type            | Description                                                             | Searchable |\n",
    "| ------------------ | --------------- | ----------------------------------------------------------------------- | ---------- |\n",
    "| ChunkId            | SimpleField     | The id of the chunk, in the form of `source_document_name+chunk_number` |            |\n",
    "| Source             | SimpleField     | The path to the source document                                         |\n",
    "| ChunkContent       | SearchableField | The content of the chunk                                                |\n",
    "| ChunkContentVector | SearchField     | The vectorized content of the chunk                                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell bellow to define a function which creates an index with the above described schema:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(search_index_name, service_endpoint, key):\n",
    "    client = SearchIndexClient(service_endpoint, AzureKeyCredential(key))\n",
    "\n",
    "    # 1. Define the fields\n",
    "    fields = [\n",
    "        SimpleField(\n",
    "            name=\"chunkId\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            sortable=True,\n",
    "            filterable=True,\n",
    "            key=True,\n",
    "        ),\n",
    "        SimpleField(\n",
    "            name=\"source\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            sortable=True,\n",
    "            filterable=True,\n",
    "        ),\n",
    "        SearchableField(name=\"chunkContent\", type=SearchFieldDataType.String),\n",
    "        SearchField(\n",
    "            name=\"chunkContentVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=1536,  # the dimension of the embedded vector\n",
    "            vector_search_profile_name=\"my-vector-config\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # 2. Configure the vector search configuration\n",
    "    vector_search = VectorSearch(\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"my-vector-config\",\n",
    "                algorithm_configuration_name=\"my-algorithms-config\"\n",
    "            )\n",
    "        ],\n",
    "        algorithms=[\n",
    "            # Contains configuration options specific to the hnsw approximate nearest neighbors  algorithm used during indexing and querying\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"my-algorithms-config\",\n",
    "                kind=\"hnsw\",\n",
    "                # https://learn.microsoft.com/en-us/python/api/azure-search-documents/azure.search.documents.indexes.models.hnswparameters?view=azure-python-preview#variables\n",
    "                parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    # The size of the dynamic list containing the nearest neighbors, which is used during index time.\n",
    "                    # Increasing this parameter may improve index quality, at the expense of increased indexing time.\n",
    "                    ef_construction=400,\n",
    "                    # The size of the dynamic list containing the nearest neighbors, which is used during search time.\n",
    "                    # Increasing this parameter may improve search results, at the expense of slower search.\n",
    "                    ef_search=500,\n",
    "                    # The similarity metric to use for vector comparisons.\n",
    "                    # Known values are: \"cosine\", \"euclidean\", and \"dotProduct\"\n",
    "                    metric=\"cosine\",\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    index = SearchIndex(\n",
    "        name=search_index_name,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search,\n",
    "    )\n",
    "\n",
    "    result = client.create_or_update_index(index)\n",
    "    print(f\"Index: {result.name} created or updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to create the index. If the index already exists, it will be updated. Make sure to update the `seach_index_name` variable to a unique name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: first_index created or updated\n"
     ]
    }
   ],
   "source": [
    "search_index_name = \"first_index\"\n",
    "create_index(search_index_name, service_endpoint, search_index_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload the Data to the Index\n",
    "\n",
    "### 2.1 Chunking\n",
    "\n",
    "Data ingestion requires a special attention as it can impact the outcome of the RAG solution. What chunking strategy to use, what AI Enrichment to perform are just few of the considerations. Further discussion and experimentation will be done in `Chapter 3. Experimentation - Chunking`.\n",
    "\n",
    "In this baseline setup, we have previously chunked the data based on a fixed size (180 tokens) and overlap of 30%.\n",
    "\n",
    "The chunks can be found [here](./output/pre-generated/chunking/fixed-size-chunks-engineering-mlops-180-30.json). You can take a look at the content of the file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Embedding - TODO update the path\n",
    "\n",
    "Embedding the chunks in vectors can also be done in various ways. Further discussion and experimentation will be done in `Chapter 3. Experimentation - Embeeding`.\n",
    "\n",
    "In this baseline setup, we will take a vanilla approach, where:\n",
    "\n",
    "- We used the embedding model from OpenAI, [`text-embedding-ada-002`](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) since this is one obvious choice to start with\n",
    "\n",
    "The outcome of this \"vanilla\" chunking strategy can be found [here](./output/pre-generated/embeddings/fixed-size-chunks-180-30-batch-engineering-mlops-ada.json). You can take a look at the content of the file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the path to the embedded chunks: TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 180\n",
    "chunk_overlap = 30\n",
    "path_to_embedded_chunks = f\"./output/pre-generated/embeddings/fixed-size-chunks-{\n",
    "    chunk_size}-{chunk_overlap}-batch-engineering-mlops-ada.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Upload the data to the Index\n",
    "\n",
    "<!-- https://github.com/microsoft/rag-experiment-accelerator/blob/development/rag_experiment_accelerator/ingest_data/acs_ingest.py -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(file_path, search_index_name):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            documents = json.load(file)\n",
    "\n",
    "        search_client = SearchClient(\n",
    "            endpoint=service_endpoint,\n",
    "            index_name=search_index_name,\n",
    "            credential=credential,\n",
    "        )\n",
    "        search_client.upload_documents(documents)\n",
    "        print(\n",
    "            f\"Uploaded {len(documents)} documents to Index: {search_index_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading documents: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 3236 documents to Index: first_index\n"
     ]
    }
   ],
   "source": [
    "upload_data(path_to_embedded_chunks, search_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Perform a vector search\n",
    "\n",
    "<!-- https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-search-outperforming-vector-search-with-hybrid/ba-p/3929167 -->\n",
    "\n",
    "<!-- There are two layers of execution: retrieval and ranking.\n",
    "\n",
    "- Retrieval - also called L1, has the goal to quickly find all the documents from the index that satisfy the search criteria (possibly across millions or billions of documents). These are scored to pick the top few (typically in order of 50) to return to the user or to feed the next layer. Azure AI Search supports three different models:\n",
    "\n",
    "  - Keyword: Uses traditional full-text search methods – content is broken into terms through language-specific text analysis, inverted indexes are created for fast retrieval, and the BM25 probabilistic model is used for scoring.\n",
    "\n",
    "  - Vector: Documents are converted from text to vector representations using an embedding model. Retrieval is performed by generating a query embedding and finding the documents whose vectors are closest to the query’s. We used Azure Open AI text-embedding-ada-002 (Ada-002) embeddings and cosine similarity for all our tests in this post.\n",
    "  - Hybrid: Performs both keyword and vector retrieval and applies a fusion step to select the best results from each technique. Azure AI Search currently uses Reciprocal Rank Fusion (RRF) to produce a single result set.\n",
    "\n",
    "- Ranking – also called L2, takes a subset of the top L1 results and computes higher quality relevance scores to reorder the result set. The L2 can improve the L1's ranking because it applies more computational power to each result. The L2 ranker can only reorder what the L1 already found – if the L1 missed an ideal document, the L2 can't fix that. L2 ranking is critical for RAG applications to make sure the best results are in the top positions.\n",
    "  - Semantic ranking is performed by Azure AI Search's L2 ranker which utilizes multi-lingual, deep learning models adapted from Microsoft Bing. The Semantic ranker can rank the top 50 results from the L1.\n",
    "\n",
    "https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-search-outperforming-vector-search-with-hybrid/ba-p/3929167 -->\n",
    "\n",
    "There are [various types of search](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-data?tabs=ai-search#search-options) that one can perform such as: keyword search, semantic search, vector search, hybrid search. Since we generated embeddings for our chunks and we would like to leverage the power of vector search, in this baseline solution we will perform a simple vector search. Further discussion and experimentation will be done in `Chapter 3. Experimentation - Search`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a vector similarity search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query_embeddings):\n",
    "    search_client = SearchClient(\n",
    "        service_endpoint, search_index_name,\n",
    "        credential=credential\n",
    "    )\n",
    "\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=query_embeddings, k_nearest_neighbors=3, fields=\"chunkContentVector\"\n",
    "    )\n",
    "\n",
    "    results = search_client.search(\n",
    "        search_text=None,\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"chunkContent\", \"chunkId\", \"source\"],\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    for document in results:\n",
    "        item = {}\n",
    "        item[\"chunkContent\"] = document[\"chunkContent\"]\n",
    "        item[\"source\"] = document[\"source\"]\n",
    "        item[\"chunkId\"] = document[\"chunkId\"]\n",
    "        documents.append(item)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the search_documents function to find the most similar documents to a given query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunkContent': 'Steps\\n\\nDesign Phase: Both developers design the interface together. This includes:\\n\\nMethod signatures and names\\nWriting documentation or docstrings for what the methods are intended to do.\\nArchitecture decisions that would influence testing (Factory patterns, etc.)\\n\\nImplementation Phase: The developers separate and parallelize work, while continuing to communicate.\\n\\nDeveloper A will design the implementation of the methods, adhering to the previously decided design.\\nDeveloper B will concurrently write tests for the same method signatures, without knowing details of the implementation.\\n\\nIntegration & Testing Phase: Both developers commit their code and run the tests.',\n",
       "  'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\virtual-collaboration.md',\n",
       "  'chunkId': 'chunk16_3'},\n",
       " {'chunkContent': 'In order to minimize the risk and set the expectations on the right way for all parties, an identification phase is important to understand each other.\\nSome potential steps in this phase may be as following (not limited):\\n\\nWorking agreement\\n\\nIdentification of styles/preferences in communication, sharing, learning, decision making of each team member\\n\\nTalking about necessity of pair programming\\n\\nDecisions on backlog management & refinement meetings, weekly design sessions, social time sessions...etc.\\n\\nSync/Async communication methods, work hours/flexible times\\n\\nDecisions and identifications of charts that will be helpful to provide transparent and true information to everyone\\n\\nIdentification of \"Software Craftspersonship\" areas which means the tools and methods will be widely used during the engagement and taking the required actions on team upskilling side if necessary.',\n",
       "  'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\teaming-up.md',\n",
       "  'chunkId': 'chunk15_1'},\n",
       " {'chunkContent': 'Integration & Testing Phase: Both developers commit their code and run the tests.\\n\\nUtopian Scenario: All tests run and pass correctly.\\nRealistic Scenario: The tests have either broken or failed due to flaws in testing. This leads to further clarification of the design and a discussion of why the tests failed.\\n\\nThe developers will repeat the three phases until the code is functional and tested.\\n\\nWhen to follow the RTT strategy\\n\\nRTT works well under specific circumstances. If collaboration needs to happen virtually, and all communication is virtual, RTT reduces the need for constant communication while maintaining the benefits of a joint design session. This considers the human element: Virtual communication is more exhausting than in person communication.',\n",
       "  'source': '..\\\\data\\\\docs\\\\code-with-engineering\\\\agile-development\\\\advanced-topics\\\\collaboration\\\\virtual-collaboration.md',\n",
       "  'chunkId': 'chunk16_4'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What does the develop phase include\"\n",
    "embedded_query = oai_query_embedding(query)\n",
    "search_documents(embedded_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(query, documents):\n",
    "    system_prompt = f\"\"\"\n",
    "\n",
    "    Instructions:\n",
    "\n",
    "    \"You are an AI assistant that helps users answer questions given a specific context.\n",
    "    You will be given a context (Retrieved Documents) and asked a question (User Question) based on that context.\n",
    "    Your answer should be as precise as possible and should only come from the context.\n",
    "    Please add citation after each sentence when possible in a form \"(Source: source+chunkId),\n",
    "    where both 'source' and 'chunkId' are taken from the Retrieved Documents.\"\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    ## Retrieve Documents:\n",
    "    {documents}\n",
    "\n",
    "    ## User Question\n",
    "    {query}\n",
    "    \"\"\"\n",
    "\n",
    "    final_message = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt + \"\\nEND OF CONTEXT\"},\n",
    "    ]\n",
    "\n",
    "    return final_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to call the Chat Completion endpoint\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/migration?tabs=python-new%2Cdalle-fix#chat-completions\n",
    "https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796#completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "def call_llm(messages: list[dict]):\n",
    "    client = AzureOpenAI(\n",
    "        api_key=azure_openai_key,\n",
    "        api_version=\"2023-07-01-preview\",\n",
    "        azure_endpoint=azure_aoai_endpoint\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=azure_openai_chat_deployment, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Finally, put all the pieces together\n",
    "\n",
    "Note: Usually in a RAG solution there is an intent extraction step.\n",
    "However, since we are having a QA system and not a chat, in our workshop we are assuming that the intent is the actual query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_rag_solution(query):\n",
    "    try:\n",
    "        # 1. Embed the query using the same embedding model as your data in the Index\n",
    "        query_embeddings = oai_query_embedding(query)\n",
    "\n",
    "        # Intent recognition - skipped in our workhsop\n",
    "        \n",
    "        # 1. Search for relevant documents\n",
    "        search_response = search_documents(query_embeddings)\n",
    "\n",
    "        # 2. Create prompt with the query, retrieved documents and conversation (kept to \"\")\n",
    "        prompt_from_chunk_context = create_prompt(query, search_response)\n",
    "\n",
    "        # 3. Call the Azure OpenAI GPT model\n",
    "        response = call_llm(prompt_from_chunk_context)\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User question: What does the develop phase include?\n",
      "Response: The development phase in agile software development refers to a specific stage where developers design the interface together. This includes: method signatures and names, writing documentation or docstrings for what the methods are intended to do, architecture decisions that would influence testing (Factory patterns, etc.). Both developers separate and parallelize work, while continuing to communicate. Developer A will design the implementation of the methods, adhering to the previously decided design, and developer B will concurrently write tests for the same method signatures, without knowing the details of the implementation. (Source: ..\\data\\docs\\code-with-engineering\\agile-development\\advanced-topics\\collaboration\\virtual-collaboration.md, chunk16_3)\n"
     ]
    }
   ],
   "source": [
    "query = \"What does the develop phase include?\"\n",
    "print(f\"User question: {query}\")\n",
    "\n",
    "response = custom_rag_solution(query)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! **This answer** seems to make sense.\n",
    "\n",
    "Now... what?\n",
    "\n",
    "- Is this _good enough_?\n",
    "- What does _good enough_ even mean?\n",
    "- How can I prove that this works _as expected_?\n",
    "- What does _works as expected_ even mean?!\n",
    "\n",
    "Let's go to `Chapter 3. Experimentation`, to try to tackle these questions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
