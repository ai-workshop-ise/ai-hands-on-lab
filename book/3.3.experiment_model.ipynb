{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end evaluation\n",
    "\n",
    "<!-- ## Experiment Overview\n",
    "\n",
    "| **Topic**                 | Description                                                                                                                                                                                                                                                                                                         |\n",
    "| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| ðŸ“ **Hypothesis**         | Exploratory hypothesis: \"Can introducing a new language model improve the system's performance?\"                                                                                                                                                                                                                    |\n",
    "| âš–ï¸ **Comparison**         | We will compare **GPT3-3.5** (from OpenAI) to **Mistral**(open-source)                                                                                                                                                                                                                                              |\n",
    "| ðŸŽ¯ **Evaluation Metrics** | We will look at human-centric metrics ([Groundedness, Relevance, Coherence, Similarity, Fluency](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/concept-model-monitoring-generative-ai-evaluation-metrics?view=azureml-api-2)) using another LLM as judge approach to compare the performance |\n",
    "| ðŸ“Š **Evaluation Dataset** | 300 question-answer pairs generated from [code-with-engineering](../data/docs/code-with-engineering/) and [code-with-mlops](../data/docs/code-with-mlops/) sections from Solution Ops repository.                                                                                                                   | -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this section we will evaluate end-to-end our RAG solution using human-centric metrics ([Groundedness, Relevance, Coherence, Similarity, Fluency](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/concept-model-monitoring-generative-ai-evaluation-metrics?view=azureml-api-2)) as well as [ROUGE](https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499), which is a traditional evaluation metric.\n",
    "\n",
    "We will use MLflow open-source tool, more precisely, the [MLflow LLM Evaluate](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#mlflow-llm-evaluate) API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ./pre-requisites.ipynb\n",
    "%run -i ./helpers/search.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chat'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "azure_openai_key = os.environ[\"azure_openai_key\"]\n",
    "azure_aoai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "aoi_deployment_name = os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"]\n",
    "\n",
    "os.environ.setdefault(\"OPENAI_API_KEY\", azure_openai_key)\n",
    "os.environ.setdefault(\"OPENAI_API_BASE\", azure_aoai_endpoint)\n",
    "os.environ.setdefault(\"OPENAI_API_VERSION\", \"2023-05-15\")\n",
    "os.environ.setdefault(\"OPENAI_API_TYPE\", \"azure\")\n",
    "os.environ.setdefault(\"OPENAI_DEPLOYMENT_NAME\", aoi_deployment_name)\n",
    "\n",
    "import mlflow\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from azureml.core import Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "def call_llm(messages: list[dict]):\n",
    "    client = AzureOpenAI(\n",
    "        api_key=azure_openai_key,\n",
    "        api_version=\"2023-07-01-preview\",\n",
    "        azure_endpoint=azure_aoai_endpoint\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=azure_openai_chat_deployment, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, search_index_name, embedding_function):\n",
    "    query_embeddings = embedding_function(query)\n",
    "\n",
    "    # 1. Search for relevant documents\n",
    "    search_response = search_documents(\n",
    "        query_embeddings, search_index_name, embedding_function)\n",
    "    # 2. Create prompt with the query, retrieved documents\n",
    "    prompt_from_chunk_context = create_prompt(query, search_response)\n",
    "\n",
    "    # 3. Call the Azure OpenAI GPT model\n",
    "    response = call_llm(prompt_from_chunk_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to generate answers for each question in evaluation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "%run -i ./helpers/search.ipynb\n",
    "\n",
    "def generate_answers_for_qa(evaluation_data_path, search_index_name, embedding_function, path_to_output):\n",
    "    try:\n",
    "        with open(evaluation_data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            evaluation_data = json.load(file)\n",
    "            generated_qa = []\n",
    "            for data in evaluation_data:\n",
    "                question = data[\"user_prompt\"]\n",
    "\n",
    "                # 1. Search in the index\n",
    "                search_response = search_documents(\n",
    "                    search_index_name=search_index_name,\n",
    "                    input=question,\n",
    "                    embedding_function=embedding_function,\n",
    "                )\n",
    "                retrieved_sources = [os.path.normpath(response[\"source\"])\n",
    "                                   for response in search_response]\n",
    "                retrieved_contexts = [response[\"chunkContext\"]\n",
    "                                   for response in search_response]\n",
    "                retrieved_chunk_ids = [response[\"chunkId\"]\n",
    "                                   for response in search_response]\n",
    "                # 2. Create prompt with the query and retrieved documents\n",
    "                prompt = create_prompt(question, search_response)\n",
    "\n",
    "                # 3. Call GPT-3 model to generate an answer\n",
    "                # given the question and the retrieved documents\n",
    "                response = call_llm(prompt)\n",
    "\n",
    "                current_qa = {\n",
    "                    \"user_prompt\": question,\n",
    "                    \"output_prompt\": data[\"output_prompt\"],\n",
    "                    \"context\": data[\"context\"],\n",
    "                    \"chunk_id\": data[\"chunk_id\"],\n",
    "                    \"source\": os.path.normpath(data[\"source\"]),\n",
    "                    \"root_chunk_id\": data[\"root_chunk_id\"],\n",
    "\n",
    "                    \"generated_output\": response,\n",
    "                    \"retrieved_context\": retrieved_contexts,\n",
    "                    \"retrieved_source\": retrieved_sources,\n",
    "                    \"retrieved_chunk_id\": retrieved_chunk_ids\n",
    "                }\n",
    "\n",
    "                generated_qa.append(current_qa)\n",
    "\n",
    "            with open(path_to_output, \"w\") as f:\n",
    "                json.dump(generated_qa, f)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate answers using ADA + fixed size chunking - 10 min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m embedding_function \u001b[38;5;241m=\u001b[39m oai_query_embedding\n\u001b[0;32m      7\u001b[0m path_to_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output/qa/results/fixed-size-chunks-180-30-engineering-mlops-ada-2.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mgenerate_answers_for_qa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluation_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_index_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_to_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[75], line 8\u001b[0m, in \u001b[0;36mgenerate_answers_for_qa\u001b[1;34m(evaluation_data_path, search_index_name, embedding_function, path_to_output)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(evaluation_data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 8\u001b[0m         evaluation_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m      9\u001b[0m         generated_qa \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m evaluation_data:\n",
      "Cell \u001b[1;32mIn[75], line 8\u001b[0m, in \u001b[0;36mgenerate_answers_for_qa\u001b[1;34m(evaluation_data_path, search_index_name, embedding_function, path_to_output)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(evaluation_data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 8\u001b[0m         evaluation_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m      9\u001b[0m         generated_qa \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m evaluation_data:\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Projects\\Workshop2024\\.venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\Workshop2024\\.venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run -i ./helpers/search.ipynb\n",
    "\n",
    "evaluation_data_path = \"./output/qa/evaluation/qa_pairs_solutionops.json\"\n",
    "\n",
    "search_index_name = \"fixed-size-chunks-180-30-batch-engineering-mlops-ada\"\n",
    "embedding_function = oai_query_embedding\n",
    "path_to_output = \"./output/qa/results/fixed-size-chunks-180-30-engineering-mlops-ada-2.json\"\n",
    "\n",
    "generate_answers_for_qa(evaluation_data_path, search_index_name, embedding_function, path_to_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate answers using semantic chunking + open source embedding model\n",
    "\n",
    "Note: we need to create such an index! It is not part of the previous experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 'semantic-chunking-eval' created or updated\n",
      "Uploaded 1216 documents to Index: semantic-chunking-eval\n"
     ]
    }
   ],
   "source": [
    "%run -i ./helpers/search.ipynb\n",
    "\n",
    "# 1. Create the new index\n",
    "index_name = \"semantic-chunking-eval\"\n",
    "embedding_path = \"./output/pre-generated/embeddings/semantic-chunking-engineering-mlops-e5-small-v2.json\"\n",
    "vector_size = 384  # TODO: Replace with the vector size of your embedding model\n",
    "create_index(index_name, vector_size)\n",
    "\n",
    "# 2. Generate embeddings for the new chunks\n",
    "# path_to_chunks_file = path_to_chunks_output\n",
    "# generated_embeddings_path = f\"./output/generated/{fixed_chunks_output_prefix}-embedded-{totalNumberOfDocuments}-{chunk_size}-{chunk_overlap}.json\"\n",
    "# generate_embeddings_for_chunks_and_save_to_file(path_to_chunks_file=path_to_chunks_output, path_to_output=generated_embeddings_path)\n",
    "\n",
    "# 3. Upload the embeddings to the new index\n",
    "upload_data(file_path=embedding_path, search_index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "%run -i ./helpers/search.ipynb\n",
    "\n",
    "evaluation_data_path = \"./output/qa/evaluation/qa_pairs_solutionops.json\"\n",
    "search_index_name = \"semantic-chunking-eval\"\n",
    "embedding_function = intfloat_e5_small_v2_query_embedding\n",
    "path_to_output = \"./output/qa/results/semantic-chunking-intfloat.json\"\n",
    "\n",
    "generate_answers_for_qa(evaluation_data_path, search_index_name, embedding_function, path_to_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a job (experiment) in Azure Machine Learning Studio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'195504501528839704'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "subscription_id = os.environ[\"subscription_id\"]\n",
    "resource_group_name = os.environ[\"resource_group_name\"]\n",
    "workspace_name = os.environ[\"workspace_name\"]\n",
    "\n",
    "# experiment_name = \"semantic-chunking-intfloat-e5-small-v2\"\n",
    "# mlflow.create_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "# !az login\n",
    "ws = Workspace.get(name=workspace_name,\n",
    "                   subscription_id=subscription_id,\n",
    "                   resource_group=resource_group_name)\n",
    "\n",
    "mlflow_tracking_uri = ws.get_mlflow_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human-Centric Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate these metrics, we will take the [LLM as Judge approach](https://arxiv.org/pdf/2311.09476.pdf) and will use GPT-4 as the judge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [LLM as Judge](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#metrics-with-llm-as-the-judge)\n",
    "Set the LLM model that you would like to use as judge.\n",
    "Note: This steps assumes that you have previously deployed this model in your Azure OpenAI resource group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = \"openai:/gpt-4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness metric\n",
    "\n",
    "This metric evaluates how faithful the model's generated output is to the context provided. High scores mean that the outputs contain information that is in line with the context, while low scores mean that outputs may disagree with the context (input is ignored). It uses [Likert scale](https://en.wikipedia.org/wiki/Likert_scale).\n",
    "\n",
    "This metric is similar to [Groundedness](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/concept-model-monitoring-generative-ai-evaluation-metrics?view=azureml-api-2#groundedness) from Azure Machine Learning or Prompt Flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import faithfulness, EvaluationExample\n",
    "\n",
    "\n",
    "# Create a good and bad example for faithfulness in the context of this problem\n",
    "faithfulness_examples = [\n",
    "    EvaluationExample(\n",
    "        input=\"How do I disable MLflow autologging?\",\n",
    "        output=\"mlflow.autolog(disable=True) will disable autologging for all functions. In Databricks, autologging is enabled by default. \",\n",
    "        score=2,\n",
    "        justification=\"The output provides a working solution, using the mlflow.autolog() function that is provided in the context.\",\n",
    "        grading_context={\n",
    "            \"context\": \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) â†’ None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\"\n",
    "        },\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"How do I disable MLflow autologging?\",\n",
    "        output=\"mlflow.autolog(disable=True) will disable autologging for all functions.\",\n",
    "        score=5,\n",
    "        justification=\"The output provides a solution that is using the mlflow.autolog() function that is provided in the context.\",\n",
    "        grading_context={\n",
    "            \"context\": \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) â†’ None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\"\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "faithfulness_metric = faithfulness(\n",
    "    model=\"openai:/gpt-4\", examples=faithfulness_examples)\n",
    "\n",
    "# print(faithfulness_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance metric\n",
    "\n",
    "This metric evaluates how relevant the model's generated output is with respect to both the input and the provided context. High scores mean that the model has understood the context and correct extracted relevant information from the context, while low score mean that output has completely ignored the question and the context and could be hallucinating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import relevance, EvaluationExample\n",
    "\n",
    "\n",
    "relevance_metric = relevance(model=\"openai:/gpt-35-turbo\")\n",
    "# print(relevance_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity metric\n",
    "\n",
    "This metric evaluates how similar the model's generated output is compared to the information in the ground_truth. High scores mean that your model outputs contain similar information as the ground_truth, while low scores mean that outputs may disagree with the ground_truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import answer_similarity\n",
    "similarity_metric = answer_similarity(model=\"openai:/gpt-35-turbo\")\n",
    "# print(similarity_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fluency metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no pre-defined fluency metric in MLflow. However, we can create [custom LLM metrics](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#creating-custom-llm-evaluation-metrics).\n",
    "\n",
    "We will define fluency in the exact same way as it is defined in [Azure SDK for Python](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/ai/azure-ai-generative/azure/ai/generative/evaluate/pf_templates/built_in_metrics/qa/gpt_fluency_prompt.jinja2).\n",
    "\n",
    "Therefore, fluency will measure the quality of individual sentences in the answer, and whether they are well-written and grammatically correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluency_example_1 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What did you have for breakfast today?\",\n",
    "    output=\"Breakfast today, me eating cereal and orange juice very good.\",\n",
    "    justification=\"The answer completely lacks fluency\",\n",
    "    score=1,\n",
    ")\n",
    "fluency_example_2 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"How do you feel when you travel alone?\",\n",
    "    output=\"Alone travel, nervous, but excited also. I feel adventure and like its time.\",\n",
    "    justification=\"The answer mostly lacks fluency\",\n",
    "    score=2,\n",
    ")\n",
    "fluency_example_3 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"When was the last time you went on a family vacation?\",\n",
    "    output=\"Alone travel, nervous, but excited also. I feel adventure and like its time.\",\n",
    "    justification=\"The answer is partially fluent\",\n",
    "    score=3,\n",
    ")\n",
    "fluency_example_4 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What is your favorite thing about your job?\",\n",
    "    output=\"My favorite aspect of my job is the chance to interact with diverse people. I am constantly learning from their experiences and stories.\",\n",
    "    justification=\"The answer is mostly fluent\",\n",
    "    score=4,\n",
    ")\n",
    "fluency_example_5 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"Can you describe your morning routine?\",\n",
    "    output=(\n",
    "        \"Every morning, I wake up at 6 am, drink a glass of water, and do some light stretching.\"\n",
    "        \"After that, I take a shower and get dressed for work. Then, I have a healthy breakfast, \"\n",
    "        \"usually consisting of oatmeal and fruits, before leaving the house around 7:30 am.\",\n",
    "    ),\n",
    "    justification=\"The answer is completely fluent\",\n",
    "    score=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluency = mlflow.metrics.genai.make_genai_metric(\n",
    "    name=\"fluency\",\n",
    "    definition=(\n",
    "        \"Fluency measures the quality of individual sentences in the answer, and whether they are well-written and grammatically correct. \"\n",
    "        \"Consider the quality of individual sentences when evaluating fluency. \"\n",
    "        \"Given the question and answer, score the fluency of the answer between one to five stars using the following rating scale:\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"One star: the answer completely lacks fluency\"\n",
    "        \"Two stars: the answer mostly lacks fluency\"\n",
    "        \"Three stars: the answer is partially fluent\"\n",
    "        \"Four stars: the answer is mostly fluent\"\n",
    "        \"Five stars: the answer has perfect fluency\"\n",
    "    ),\n",
    "    examples=[fluency_example_1, fluency_example_2,\n",
    "              fluency_example_3, fluency_example_4, fluency_example_5],\n",
    "    model=\"openai:/gpt-35-turbo\",\n",
    "    # parameters={\"temperature\": 0.0},\n",
    "    # aggregations=[\"mean\", \"variance\"],\n",
    "    # greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coherence\n",
    "\n",
    "There is no pre-defined coherence metric in MLflow. However, we can create [custom LLM metrics](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#creating-custom-llm-evaluation-metrics).\n",
    "\n",
    "We will define fluency in the exact same way as it is defined in [Azure SDK for Python](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/ai/azure-ai-generative/azure/ai/generative/evaluate/pf_templates/built_in_metrics/qa/gpt_coherence_prompt.jinja2).\n",
    "\n",
    "Therefore, coherence of an answer will be measured by how well all the sentences fit together and sound naturally as a whole.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_example_1 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What is your favorite indoor activity and why do you enjoy it?\",\n",
    "    output=\"I like pizza. The sun is shining.\",\n",
    "    justification=\"The answer completely lacks fluency\",\n",
    "    score=1,\n",
    ")\n",
    "coherence_example_2 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"Can you describe your favorite movie without giving away any spoilers?\",\n",
    "    output=\"It is a science fiction movie. There are dinosaurs. The actors eat cake. People must stop the villain.\",\n",
    "    justification=\"The answer mostly lacks fluency\",\n",
    "    score=2,\n",
    ")\n",
    "coherence_example_3 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What are some benefits of regular exercise?\",\n",
    "    output=\"Regular exercise improves your mood. A good workout also helps you sleep better. Trees are green.\",\n",
    "    justification=\"The answer is partially fluent\",\n",
    "    score=3,\n",
    ")\n",
    "coherence_example_4 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"How do you cope with stress in your daily life?\",\n",
    "    output=\"I usually go for a walk to clear my head. Listening to music helps me relax as well. Stress is a part of life, but we can manage it through some activities.\",\n",
    "    justification=\"The answer is mostly fluent\",\n",
    "    score=4,\n",
    ")\n",
    "coherence_example_5 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What can you tell me about climate change and its effects on the environment?\",\n",
    "    output=(\n",
    "        \"Climate change has far-reaching effects on the environment. \"\n",
    "        \"Rising temperatures result in the melting of polar ice caps, contributing to sea-level rise. \"\n",
    "        \"Additionally, more frequent and severe weather events, such as hurricanes and heatwaves, can cause disruption to ecosystems and human societies alike.\"\n",
    "    ),\n",
    "    justification=\"The answer is completely fluent\",\n",
    "    score=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence = mlflow.metrics.genai.make_genai_metric(\n",
    "    name=\"coherence\",\n",
    "    definition=(\n",
    "        \"Coherence of an answer is measured by how well all the sentences fit together and sound naturally as a whole. \"\n",
    "        \"Consider the overall quality of the answer when evaluating coherence. \"\n",
    "        \"Given the question and answer, score the coherence of answer between one to five stars using the following rating scale:\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"One star: the answer completely lacks coherence\"\n",
    "        \"Two stars: the answer mostly lacks coherence\"\n",
    "        \"Three stars: the answer is partially coherent\"\n",
    "        \"Four stars: the answer is mostly coherent\"\n",
    "        \"Five stars: the answer has perfect coherency\"\n",
    "    ),\n",
    "    examples=[coherence_example_1, coherence_example_2,\n",
    "              coherence_example_3, coherence_example_4, coherence_example_5],\n",
    "    model=\"openai:/gpt-35-turbo\",\n",
    "    # parameters={\"temperature\": 0.0},\n",
    "    # aggregations=[\"mean\", \"variance\"],\n",
    "    # greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# experiment_name = \"test-experiment\"\n",
    "# mlflow.create_experiment(experiment_name, artifact_location=\"s3://your-bucket\")\n",
    "# TODO: \"ground_truth\" https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_similarity\n",
    "\n",
    "\n",
    "def evaluate(input_path, experiment_name, run_name):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        qa_evluation_data = json.load(file)\n",
    "        df = pd.DataFrame.from_records(qa_evluation_data)\n",
    "        df[\"user_prompt\"] = df[\"user_prompt\"].astype(str)\n",
    "        df[\"output_prompt\"] = df[\"output_prompt\"].astype(str)\n",
    "        df[\"retrieved_context\"] = df[\"retrieved_context\"].astype(str)\n",
    "        df[\"generated_output\"] = df[\"generated_output\"].astype(str)\n",
    "\n",
    "        mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        with mlflow.start_run(run_name=run_name) as run:\n",
    "            results = mlflow.evaluate(data=df,\n",
    "                                      # predictions=\"output_prompt\",\n",
    "                                      predictions=\"generated_output\",\n",
    "                                      model_type=\"question-answering\",\n",
    "                                      extra_metrics=[\n",
    "                                          faithfulness_metric,\n",
    "                                          relevance_metric,\n",
    "                                          coherence,\n",
    "                                          fluency,\n",
    "                                          #   similarity_metric,\n",
    "                                        #   mlflow.metrics.latency()\n",
    "                                          ],\n",
    "                                      evaluator_config={\n",
    "                                          \"col_mapping\": {\n",
    "                                              \"inputs\": \"user_prompt\",  # Define the column name for the input\n",
    "                                              # \"context\": \"context\",\n",
    "                                              \"context\": \"retrieved_context\",\n",
    "                                              #   \"targets\": \"output_prompt\"\n",
    "                                          }\n",
    "                                      })\n",
    "\n",
    "            # mlflow.log_metric('toxicity', results.metrics['toxicity/v1/p90'])\n",
    "            mlflow.log_metric('faithfulness_mean',\n",
    "                              results.metrics['faithfulness/v1/mean'])\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the fixed-size chunking strategy with ada - took > 12 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "experiment_name = \"semantic-chunking-intfloat-e5-small-v2\"\n",
    "mlflow.create_experiment(experiment_name)\n",
    "\n",
    "input_path = \"./output/qa/results/semantic-chunking-intfloat.json\"\n",
    "run_name = \"4metrics\"\n",
    "results_semantic_chunking = evaluate(input_path, experiment_name, run_name)\n",
    "\n",
    "print(results_semantic_chunking.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latency/mean': 0.0,\n",
       " 'latency/variance': 0.0,\n",
       " 'latency/p90': 0.0,\n",
       " 'toxicity/v1/mean': 0.0005107220515492372,\n",
       " 'toxicity/v1/variance': 2.3427437314922485e-06,\n",
       " 'toxicity/v1/p90': 0.0006927762471605093,\n",
       " 'toxicity/v1/ratio': 0.0,\n",
       " 'faithfulness/v1/mean': 3.8060200668896322,\n",
       " 'faithfulness/v1/variance': 1.4406326551157145,\n",
       " 'faithfulness/v1/p90': 5.0,\n",
       " 'relevance/v1/mean': 3.47,\n",
       " 'relevance/v1/variance': 1.3824333333333334,\n",
       " 'relevance/v1/p90': 5.0,\n",
       " 'coherence/v1/mean': 3.530201342281879,\n",
       " 'coherence/v1/variance': 0.7323093554344399,\n",
       " 'coherence/v1/p90': 4.0,\n",
       " 'fluency/v1/mean': 3.508417508417508,\n",
       " 'fluency/v1/variance': 0.6876395832624789,\n",
       " 'fluency/v1/p90': 4.0}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_semantic_chunking.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_generated_results = {'latency/mean': 0.0,\n",
    "                         'latency/variance': 0.0,\n",
    "                         'latency/p90': 0.0,\n",
    "                         'toxicity/v1/mean': 0.0005107220515492372,\n",
    "                         'toxicity/v1/variance': 2.3427437314922485e-06,\n",
    "                         'toxicity/v1/p90': 0.0006927762471605093,\n",
    "                         'toxicity/v1/ratio': 0.0,\n",
    "                         'faithfulness/v1/mean': 3.8060200668896322,\n",
    "                         'faithfulness/v1/variance': 1.4406326551157145,\n",
    "                         'faithfulness/v1/p90': 5.0,\n",
    "                         'relevance/v1/mean': 3.47,\n",
    "                         'relevance/v1/variance': 1.3824333333333334,\n",
    "                         'relevance/v1/p90': 5.0,\n",
    "                         'coherence/v1/mean': 3.530201342281879,\n",
    "                         'coherence/v1/variance': 0.7323093554344399,\n",
    "                         'coherence/v1/p90': 4.0,\n",
    "                         'fluency/v1/mean': 3.508417508417508,\n",
    "                         'fluency/v1/variance': 0.6876395832624789,\n",
    "                         'fluency/v1/p90': 4.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the semantic chunking strategy with eval embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "experiment_name = \"fixed-size-180-30-ada\"\n",
    "mlflow.create_experiment(experiment_name)\n",
    "\n",
    "input_path = \"./output/qa/results/fixed-size-chunks-180-30-engineering-mlops-ada.json\"\n",
    "run_name = \"4metrics\"\n",
    "results_fixed_size_ada = evaluate(input_path, experiment_name, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latency/mean': 0.0,\n",
       " 'latency/variance': 0.0,\n",
       " 'latency/p90': 0.0,\n",
       " 'toxicity/v1/mean': 0.0002763730055691364,\n",
       " 'toxicity/v1/variance': 1.1839384498113026e-07,\n",
       " 'toxicity/v1/p90': 0.0005062455020379276,\n",
       " 'toxicity/v1/ratio': 0.0,\n",
       " 'faithfulness/v1/mean': 2.6755852842809364,\n",
       " 'faithfulness/v1/variance': 2.132213286204852,\n",
       " 'faithfulness/v1/p90': 5.0,\n",
       " 'relevance/v1/mean': 2.276666666666667,\n",
       " 'relevance/v1/variance': 1.7534555555555555,\n",
       " 'relevance/v1/p90': 4.0,\n",
       " 'coherence/v1/mean': 2.59,\n",
       " 'coherence/v1/variance': 1.0485666666666669,\n",
       " 'coherence/v1/p90': 4.0,\n",
       " 'fluency/v1/mean': 2.622895622895623,\n",
       " 'fluency/v1/variance': 0.9487013796778109,\n",
       " 'fluency/v1/p90': 4.0}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_fixed_size_ada.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_generated_results = {'latency/mean': 0.0,\n",
    "                         'latency/variance': 0.0,\n",
    "                         'latency/p90': 0.0,\n",
    "                         'toxicity/v1/mean': 0.0002763730055691364,\n",
    "                         'toxicity/v1/variance': 1.1839384498113026e-07,\n",
    "                         'toxicity/v1/p90': 0.0005062455020379276,\n",
    "                         'toxicity/v1/ratio': 0.0,\n",
    "                         'faithfulness/v1/mean': 2.6755852842809364,\n",
    "                         'faithfulness/v1/variance': 2.132213286204852,\n",
    "                         'faithfulness/v1/p90': 5.0,\n",
    "                         'relevance/v1/mean': 2.276666666666667,\n",
    "                         'relevance/v1/variance': 1.7534555555555555,\n",
    "                         'relevance/v1/p90': 4.0,\n",
    "                         'coherence/v1/mean': 2.59,\n",
    "                         'coherence/v1/variance': 1.0485666666666669,\n",
    "                         'coherence/v1/p90': 4.0,\n",
    "                         'fluency/v1/mean': 2.622895622895623,\n",
    "                         'fluency/v1/variance': 0.9487013796778109,\n",
    "                         'fluency/v1/p90': 4.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Relevance  Faithfulness  Coherence   Fluency\n",
      "semantic_chunking_eval    3.470000      3.806020   3.530201  3.508418\n",
      "fixed_size_chunking_ada   2.276667      2.675585   2.590000  2.622896\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "keys = ['relevance/v1/mean', 'faithfulness/v1/mean',\n",
    "        'coherence/v1/mean', 'fluency/v1/mean']\n",
    "semantic_chunking_eval_values = [\n",
    "    results_semantic_chunking.metrics[key] for key in keys]\n",
    "fixed_size_chunking_ada_values = [\n",
    "    results_fixed_size_ada.metrics[key] for key in keys]\n",
    "table = [semantic_chunking_eval_values, fixed_size_chunking_ada_values]\n",
    "df = pd.DataFrame(table, columns=['Relevance', 'Faithfulness', 'Coherence', 'Fluency'], index=[\n",
    "                  'semantic_chunking_eval', 'fixed_size_chunking_ada'])\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
